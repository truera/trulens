{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ““ Context Relevance Benchmarking: ranking is all you need.\n",
    "\n",
    "The numerical scoring scheme adopted by _TruLens_ feedback functions is intuitive\n",
    "for generating aggregated results from eval runs that are easy to interpret and\n",
    "visualize across different applications of interest. However, it begs the\n",
    "question how trustworthy these scores actually are, given they are at their core\n",
    "next-token-prediction-style generation from meticulously designed prompts.\n",
    "Consequently, these feedback functions face typical large language model (LLM)\n",
    "challenges in rigorous production environments, including prompt sensitivity and\n",
    "non-determinism, especially when incorporating Mixture-of-Experts and\n",
    "model-as-a-service solutions like those from _OpenAI_. \n",
    "\n",
    "Another frequent inquiry from the community concerns the intrinsic semantic\n",
    "significance, or lack thereof, of feedback scoresâ€”for example, how one would\n",
    "interpret and instrument with a score of 0.9 when assessing context relevance in\n",
    "a RAG application or whether a harmfulness score of 0.7 from GPT-3.5 equates to\n",
    "the same from `Llama-2-7b`.\n",
    " \n",
    "For simpler meta-evaluation tasks, when human numerical scores are available in\n",
    "the benchmark datasets, such as `SummEval`, it's a lot more straightforward to\n",
    "evaluate feedback functions as long as we can define reasonable correlation\n",
    "between the task of the feedback function and the ones available in the\n",
    "benchmarks. Check out our preliminary work on evaluating our own groundedness\n",
    "feedback functions:\n",
    "https://www.trulens.org/trulens/groundedness_smoke_tests/#groundedness-evaluations\n",
    "and our previous blog, where the groundedness metric in the context of RAG can\n",
    "be viewed as equivalent to the consistency metric defined in the SummEval\n",
    "benchmark. In those cases, calculating MAE between our feedback scores and the\n",
    "golden set's human scores can readily provide insights on how well the\n",
    "groundedness LLM-based feedback functions are aligned with human preferences. \n",
    "\n",
    "Yet, acquiring high-quality, numerically scored datasets is challenging and\n",
    "costly, a sentiment echoed across institutions and companies working on RLFH\n",
    "dataset annotation.\n",
    "\n",
    "Observing that many [information retrieval (IR)\n",
    "benchmarks](https://huggingface.co/datasets/ms_marco/viewer/v2.1) use binary\n",
    "labels,  we propose to frame the problem of evaluating LLM-based feedback\n",
    "functions (meta-evaluation) as evaluating a recommender system. In essence, we\n",
    "argue the relative importance or ranking based on the score assignments is all\n",
    "you need to achieve meta-evaluation against human golden sets. The intuition is\n",
    "that it is a sufficient proxy to trustworthiness if feedback functions\n",
    "demonstrate discriminative capabilities that reliably and consistently assign\n",
    "items, be it context chunks or generated responses, with weights and ordering\n",
    "closely mirroring human preferences. \n",
    "\n",
    " In this following section, we illustrate how we conduct meta-evaluation\n",
    " experiments on one of Trulens most widely used feedback functions: `context\n",
    " relevance` and share how well they are aligned with human preferences in\n",
    " practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q scikit-learn litellm trulens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import groundedness feedback function\n",
    "from benchmark_frameworks.eval_as_recommendation import compute_ece\n",
    "from benchmark_frameworks.eval_as_recommendation import compute_ndcg\n",
    "from benchmark_frameworks.eval_as_recommendation import precision_at_k\n",
    "from benchmark_frameworks.eval_as_recommendation import recall_at_k\n",
    "from benchmark_frameworks.eval_as_recommendation import score_passages\n",
    "from test_cases import generate_ms_marco_context_relevance_benchmark\n",
    "from trulens.core import TruSession\n",
    "\n",
    "TruSession().reset_database()\n",
    "\n",
    "benchmark_data = []\n",
    "for i in range(1, 6):\n",
    "    dataset_path = f\"./datasets/ms_marco/ms_marco_train_v2.1_{i}.json\"\n",
    "    benchmark_data.extend(\n",
    "        list(generate_ms_marco_context_relevance_benchmark(dataset_path))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(benchmark_data)\n",
    "df = df.iloc[:500]\n",
    "print(len(df.groupby(\"query_id\").count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"query_id\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define feedback functions for contexnt relevance to be evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.providers.litellm import LiteLLM\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "# GPT 3.5\n",
    "gpt3_turbo = OpenAI(model_engine=\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "def wrapped_relevance_turbo(input, output, temperature=0.0):\n",
    "    return gpt3_turbo.context_relevance(input, output, temperature)\n",
    "\n",
    "\n",
    "gpt4 = OpenAI(model_engine=\"gpt-4-1106-preview\")\n",
    "\n",
    "\n",
    "def wrapped_relevance_gpt4(input, output, temperature=0.0):\n",
    "    return gpt4.context_relevance(input, output, temperature)\n",
    "\n",
    "\n",
    "# # GPT 4 turbo latest\n",
    "gpt4_latest = OpenAI(model_engine=\"gpt-4-0125-preview\")\n",
    "\n",
    "\n",
    "def wrapped_relevance_gpt4_latest(input, output, temperature=0.0):\n",
    "    return gpt4_latest.context_relevance(input, output, temperature)\n",
    "\n",
    "\n",
    "# Anthropic\n",
    "claude_2 = LiteLLM(model_engine=\"claude-2\")\n",
    "\n",
    "\n",
    "def wrapped_relevance_claude2(input, output, temperature=0.0):\n",
    "    return claude_2.context_relevance(input, output, temperature)\n",
    "\n",
    "\n",
    "claude_2_1 = LiteLLM(model_engine=\"claude-2.1\")\n",
    "\n",
    "\n",
    "def wrapped_relevance_claude21(input, output, temperature=0.0):\n",
    "    return claude_2_1.context_relevance(input, output, temperature)\n",
    "\n",
    "\n",
    "# Define a list of your feedback functions\n",
    "feedback_functions = {\n",
    "    \"GPT-3.5-Turbo\": wrapped_relevance_turbo,\n",
    "    \"GPT-4-Turbo\": wrapped_relevance_gpt4,\n",
    "    \"GPT-4-Turbo-latest\": wrapped_relevance_gpt4_latest,\n",
    "    \"Claude-2\": wrapped_relevance_claude2,\n",
    "    \"Claude-2.1\": wrapped_relevance_claude21,\n",
    "}\n",
    "\n",
    "backoffs_by_functions = {\n",
    "    \"GPT-3.5-Turbo\": 0.5,\n",
    "    \"GPT-4-Turbo\": 0.5,\n",
    "    \"GPT-4-Turbo-latest\": 0.5,\n",
    "    \"Claude-2\": 1,\n",
    "    \"Claude-2.1\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the benchmark\n",
    "results = []\n",
    "\n",
    "K = 5  # for precision@K and recall@K\n",
    "\n",
    "# sampling of size n is performed for estimating log probs (conditional probs)\n",
    "# generated by the LLMs\n",
    "sample_size = 1\n",
    "for name, func in feedback_functions.items():\n",
    "    try:\n",
    "        scores, groundtruths = score_passages(\n",
    "            df,\n",
    "            name,\n",
    "            func,\n",
    "            backoffs_by_functions[name]\n",
    "            if name in backoffs_by_functions\n",
    "            else 0.5,\n",
    "            n=1,\n",
    "        )\n",
    "\n",
    "        df_score_groundtruth_pairs = pd.DataFrame({\n",
    "            \"scores\": scores,\n",
    "            \"groundtruth (human-preferences of relevancy)\": groundtruths,\n",
    "        })\n",
    "        df_score_groundtruth_pairs.to_csv(\n",
    "            f\"./results/{name}_score_groundtruth_pairs.csv\"\n",
    "        )\n",
    "        ndcg_value = compute_ndcg(scores, groundtruths)\n",
    "        ece_value = compute_ece(scores, groundtruths)\n",
    "        precision_k = np.mean([\n",
    "            precision_at_k(sc, tr, 1) for sc, tr in zip(scores, groundtruths)\n",
    "        ])\n",
    "        recall_k = np.mean([\n",
    "            recall_at_k(sc, tr, K) for sc, tr in zip(scores, groundtruths)\n",
    "        ])\n",
    "        results.append((name, ndcg_value, ece_value, recall_k, precision_k))\n",
    "        print(f\"Finished running feedback function name {name}\")\n",
    "\n",
    "        print(\"Saving results...\")\n",
    "        tmp_results_df = pd.DataFrame(\n",
    "            results,\n",
    "            columns=[\"Model\", \"nDCG\", \"ECE\", f\"Recall@{K}\", \"Precision@1\"],\n",
    "        )\n",
    "        print(tmp_results_df)\n",
    "        tmp_results_df.to_csv(\"./results/tmp_context_relevance_benchmark.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"Failed to run benchmark for feedback function name {name} due to {e}\"\n",
    "        )\n",
    "\n",
    "# Convert results to DataFrame for display\n",
    "results_df = pd.DataFrame(\n",
    "    results, columns=[\"Model\", \"nDCG\", \"ECE\", f\"Recall@{K}\", \"Precision@1\"]\n",
    ")\n",
    "results_df.to_csv((\"./results/all_context_relevance_benchmark.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make sure results_df is defined and contains the necessary columns\n",
    "# Also, ensure that K is defined\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Graph for nDCG, Recall@K, and Precision@K\n",
    "plt.subplot(2, 1, 1)  # First subplot\n",
    "ax1 = results_df.plot(\n",
    "    x=\"Model\",\n",
    "    y=[\"nDCG\", f\"Recall@{K}\", \"Precision@1\"],\n",
    "    kind=\"bar\",\n",
    "    ax=plt.gca(),\n",
    ")\n",
    "plt.title(\"Feedback Function Performance (Higher is Better)\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "# Graph for ECE\n",
    "plt.subplot(2, 1, 2)  # Second subplot\n",
    "ax2 = results_df.plot(\n",
    "    x=\"Model\", y=[\"ECE\"], kind=\"bar\", ax=plt.gca(), color=\"orange\"\n",
    ")\n",
    "plt.title(\"Feedback Function Calibration (Lower is Better)\")\n",
    "plt.ylabel(\"ECE\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
