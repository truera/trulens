{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Evaluation - evaluating your LLM-as-judge with TruLens\n",
    "\n",
    "Meta evaluation is the process of evaluating evaluation methods themselves. Here we are measuring and benchmarking the performance of LLM-based evaluators (aka LLM-as-judge), where the main focus of performance is human alignment. In other words, how closely aligned the generated scores are with human evaluation processes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "In TruLens, we implement this as a special case of GroundTruth evaluation, since we canonically regard human preferences as the groundtruth in most LLM tasks. \n",
    "\n",
    "For experiment tracking, we provide a suite of automatic metric computation via Aggregator, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.benchmark.benchmark_frameworks.tru_benchmark_experiment import (\n",
    "    BenchmarkParams,\n",
    ")\n",
    "from trulens.benchmark.benchmark_frameworks.tru_benchmark_experiment import (\n",
    "    TruBenchmarkExperiment,\n",
    ")\n",
    "from trulens.benchmark.benchmark_frameworks.tru_benchmark_experiment import (\n",
    "    create_benchmark_experiment_app,\n",
    ")\n",
    "from trulens.core import TruSession\n",
    "from trulens.feedback import GroundTruthAggregator\n",
    "\n",
    "tru = TruSession()\n",
    "tru.reset_database()\n",
    "\n",
    "golden_set = [\n",
    "    {\n",
    "        \"query\": \"who are the Apple's competitors?\",\n",
    "        \"response\": \"Apple competitors include Samsung, Google, and Microsoft.\",\n",
    "        \"expected_score\": 1.0,  # groundtruth score annotated by human\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"what is the capital of France?\",\n",
    "        \"response\": \"Paris is the capital of France.\",\n",
    "        \"expected_score\": 1.0,\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"what is the capital of Spain?\",\n",
    "        \"response\": \"I love going to Spain.\",\n",
    "        \"expected_score\": 0,\n",
    "    },\n",
    "]\n",
    "\n",
    "prompts = [entry[\"query\"] for entry in golden_set]\n",
    "responses = [entry[\"response\"] for entry in golden_set]\n",
    "true_labels = [entry[\"expected_score\"] for entry in golden_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import snowflake.connector\n",
    "from trulens.providers.cortex import Cortex\n",
    "\n",
    "snowflake_connection_parameters = {\n",
    "    \"account\": os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "    \"user\": os.environ[\"SNOWFLAKE_USER\"],\n",
    "    \"password\": os.environ[\"SNOWFLAKE_USER_PASSWORD\"],\n",
    "}\n",
    "snowflake_connection = snowflake.connector.connect(\n",
    "    **snowflake_connection_parameters\n",
    ")\n",
    "provider = Cortex(\n",
    "    snowflake_connection,\n",
    "    model_engine=\"mistral-large\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "# output is feedback_score\n",
    "def context_relevance_ff(input, output, benchmark_params) -> float:\n",
    "    return provider.context_relevance(\n",
    "        question=input,\n",
    "        context=output,\n",
    "        temperature=benchmark_params[\"temperature\"],\n",
    "    )\n",
    "\n",
    "\n",
    "# output is (feedback_score, confidence_score)\n",
    "def context_relevance_ff_with_confidence(\n",
    "    input, output, benchmark_params\n",
    ") -> Tuple[float, float]:\n",
    "    return provider.context_relevance_verb_confidence(\n",
    "        question=input,\n",
    "        context=output,\n",
    "        temperature=benchmark_params[\"temperature\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect all prompt and expected responses from the golden set and pass to GroundTruthAggregator as ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_agg_func = GroundTruthAggregator(true_labels=true_labels).mae\n",
    "\n",
    "\n",
    "benchmark_experiment = TruBenchmarkExperiment(\n",
    "    feedback_fn=context_relevance_ff,\n",
    "    agg_funcs=[mae_agg_func],\n",
    "    benchmark_params=BenchmarkParams(temperature=0.5),\n",
    ")\n",
    "\n",
    "tru_benchmark_arctic = create_benchmark_experiment_app(\n",
    "    app_name=\"MAE\", app_version=\"1\", benchmark_experiment=benchmark_experiment\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_benchmark_arctic as recording:\n",
    "    feedback_res = tru_benchmark_arctic.app(golden_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check: compare the generated feedback scores with the passed in ground truth labels [1, 1, 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_res  # generate feedback scores from our context relevance feedback function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ece_agg_func = GroundTruthAggregator(true_labels=true_labels).ece\n",
    "\n",
    "benchmark_experiment = TruBenchmarkExperiment(\n",
    "    feedback_fn=context_relevance_ff_with_confidence,\n",
    "    agg_funcs=[ece_agg_func],\n",
    "    benchmark_params=BenchmarkParams(temperature=0.5),\n",
    ")\n",
    "\n",
    "tru_benchmark_arctic_calibration = create_benchmark_experiment_app(\n",
    "    app_name=\"Expected Calibration Error (ECE)\",\n",
    "    app_version=\"1\",\n",
    "    benchmark_experiment=benchmark_experiment,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_benchmark_arctic_calibration as recording:\n",
    "    feedback_results = tru_benchmark_arctic_calibration.app(golden_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_results  # a tuple of (generate_feedback_scores, confidence_scores)  from our context relevance feedback function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_leaderboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users can also define custom aggregator functions and register them easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of custom aggregation function\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def custom_aggr_function(\n",
    "    scores: List[float], aggregator: GroundTruthAggregator\n",
    ") -> float:\n",
    "    # Example: Calculate the average of top k scores\n",
    "    if aggregator.k is None:\n",
    "        raise ValueError(\"k must be set for custom aggregation.\")\n",
    "    top_k_scores = sorted(scores, reverse=True)[: aggregator.k]\n",
    "    return sum(top_k_scores) / len(top_k_scores) if top_k_scores else 0\n",
    "\n",
    "\n",
    "gt_aggregator = GroundTruthAggregator(true_labels=true_labels, k=3)\n",
    "\n",
    "# Register a custom aggregation function\n",
    "gt_aggregator.register_custom_agg_func(\"mean_top_k\", custom_aggr_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the below `my_custom_aggr_fnc` can be passed into agg_funcs parameters of `tru.BenchmarkExperiment` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_custom_aggr_fnc = gt_aggregator.mean_top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_custom_aggr_fnc([\n",
    "    5,\n",
    "    5,\n",
    "    1,\n",
    "    2,\n",
    "])  # top 3 scores are [5, 5, 2], so the average is 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
