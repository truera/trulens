{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets preprocessing:\n",
    "\n",
    "Datasets that need some preprocessing before they can be used in `TruBenchmarkExperiment` class:\n",
    "1. Snowflake IT (internal): both rephrased and regular?, this should be used for all 3 in the triad\n",
    "2. SummEval (CNN and DailyMail summarizations with annotation) for groundedness\n",
    "3. QAGS (CNN and DailyMail with Turkers' annotation) for groundedness\n",
    "4. QAGS (XSUM with Turkers' annotation) for groundedness\n",
    "5. MSMARCO V2 for context relevance\n",
    "6. HotPot QA for answer relevance \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Pin random seed\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "# SummEval\n",
    "def generate_summeval_groundedness_golden_set(file_path):\n",
    "    def calculate_expected_score(normalized_metrics_lst, weights_lst):\n",
    "        assert len(normalized_metrics_lst) == len(weights_lst)\n",
    "        return round(\n",
    "            sum(\n",
    "                normalized_metrics_lst[i] * weights_lst[i]\n",
    "                for i in range(len(normalized_metrics_lst))\n",
    "            )\n",
    "            / sum(weights_lst),\n",
    "            2,\n",
    "        )\n",
    "\n",
    "    with open(file_path) as f:\n",
    "        for line in f:\n",
    "            # Each line is a separate JSON object\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "\n",
    "                # Ensure the expected keys exist in the JSON\n",
    "                try:\n",
    "                    row = data\n",
    "                    assert (\n",
    "                        len(row[\"machine_summaries\"]) == len(row[\"consistency\"])\n",
    "                    ), \"Mismatch in lengths of machine_summaries and consistency\"\n",
    "\n",
    "                    # Iterate over the summaries and create the desired dictionary structure\n",
    "                    for i in range(len(row[\"machine_summaries\"])):\n",
    "                        yield {\n",
    "                            \"query\": row.get(\n",
    "                                \"text\", \"\"\n",
    "                            ),  # Default to empty string if key not found\n",
    "                            \"expected_response\": row[\"machine_summaries\"][i],\n",
    "                            \"expected_score\": calculate_expected_score(\n",
    "                                [\n",
    "                                    (row[\"consistency\"][i] - 1)\n",
    "                                    / 4,  # Normalize from [1, 5] to [0, 1]\n",
    "                                ],\n",
    "                                [1.0],\n",
    "                            ),\n",
    "                            \"human_score\": row[\"consistency\"][i],\n",
    "                        }\n",
    "\n",
    "                except KeyError as e:\n",
    "                    print(\n",
    "                        f\"Key error: {e}. Please check if the keys exist in the JSON file.\"\n",
    "                    )\n",
    "                except AssertionError as e:\n",
    "                    print(\n",
    "                        f\"Assertion error: {e}. The lengths of 'machine_summaries' and 'consistency' do not match.\"\n",
    "                    )\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON decode error: {e}. Check the line format.\")\n",
    "\n",
    "\n",
    "# Snowflake IT dataset\n",
    "\n",
    "\n",
    "def generatate_snowflake_it_golden_set_groundedness(file_path):\n",
    "    res = []\n",
    "    with open(file_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        all_rows = list(reader)\n",
    "        for row in all_rows:\n",
    "            # Convert the 'golden' from a string to a list\n",
    "            try:\n",
    "                expected_chunks = ast.literal_eval(row[\"golden\"])\n",
    "                if not isinstance(expected_chunks, list):\n",
    "                    raise ValueError(\"Golden column should be a list\")\n",
    "\n",
    "                for expected_chunk in expected_chunks:\n",
    "                    # Yield the required fields\n",
    "                    res.append({\n",
    "                        \"query\": expected_chunk,  # source\n",
    "                        \"expected_response\": row[\n",
    "                            \"expected_response\"\n",
    "                        ],  # statement\n",
    "                        \"expected_score\": 1,  # retrieved chunks in the \"golden\" colum are always considered grounded\n",
    "                    })\n",
    "\n",
    "                # Generate a negative example for each query\n",
    "                # Collect all possible chunks from other queries to use as negative contexts\n",
    "                other_chunks = [\n",
    "                    chunk\n",
    "                    for other_row in all_rows\n",
    "                    if other_row != row\n",
    "                    for chunk in ast.literal_eval(other_row[\"golden\"])\n",
    "                ]\n",
    "\n",
    "                # Randomly select a negative chunk (context from another query)\n",
    "                if other_chunks:\n",
    "                    negative_chunk = random.choice(other_chunks)\n",
    "                    res.append({\n",
    "                        \"query\": negative_chunk,\n",
    "                        \"expected_response\": row[\n",
    "                            \"expected_response\"\n",
    "                        ],  # statement (not grounded by the chunk)\n",
    "                        \"expected_score\": 0,  # Negative example, score = 0\n",
    "                    })\n",
    "\n",
    "            except (ValueError, SyntaxError) as e:\n",
    "                print(f\"Error parsing golden column: {e}\")\n",
    "                continue\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def generate_snowflake_it_golden_set_answer_relevance(file_path):\n",
    "    res = []\n",
    "    with open(file_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        all_rows = list(\n",
    "            reader\n",
    "        )  # Store all rows in memory to use for negative example generation\n",
    "\n",
    "        for row in all_rows:\n",
    "            # generate a positive example for each query\n",
    "            if (\n",
    "                \"I don’t know the answer to that question.\"\n",
    "                in row[\"expected_response\"]\n",
    "            ):\n",
    "                ground_truth_score = 0  # label answer relevance as 0 for ABSTENTION \"I don’t know the answer to that question.\"\n",
    "            else:\n",
    "                ground_truth_score = (\n",
    "                    1  # label answer relevance as 1 for all other cases\n",
    "                )\n",
    "            res.append({\n",
    "                \"query\": row[\"query\"],\n",
    "                \"expected_response\": row[\"expected_response\"],\n",
    "                \"expected_score\": ground_truth_score,\n",
    "            })\n",
    "\n",
    "            # generate an easy negative example for each positive example by randomly selecting a response from another query\n",
    "            negative_response = random.choice([\n",
    "                r[\"expected_response\"] for r in all_rows if r != row\n",
    "            ])\n",
    "            res.append({\n",
    "                \"query\": row[\"query\"],\n",
    "                \"expected_response\": negative_response,  # Orthogonal response\n",
    "                \"expected_score\": 0,  # Label answer relevance as 0 for negative examples\n",
    "            })\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def generate_snowflake_it_golden_set_context_relevance(file_path):\n",
    "    res = []\n",
    "    with open(file_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        all_rows = list(reader)\n",
    "\n",
    "        # Step 1: Process each row to extract positive examples\n",
    "        for row in all_rows:\n",
    "            try:\n",
    "                expected_chunks = ast.literal_eval(row[\"golden\"])\n",
    "\n",
    "                if not isinstance(expected_chunks, list):\n",
    "                    raise ValueError(\"Golden column should be a list\")\n",
    "\n",
    "                # Generate positive examples\n",
    "                for expected_chunk in expected_chunks:\n",
    "                    res.append({\n",
    "                        \"query\": row[\"query\"],\n",
    "                        \"expected_response\": expected_chunk,\n",
    "                        \"expected_score\": 1,  # Positive example, score = 1\n",
    "                    })\n",
    "\n",
    "                # Step 2: Generate a negative example for each query\n",
    "                # Collect all possible chunks from other queries to use as negative contexts\n",
    "                other_chunks = [\n",
    "                    chunk\n",
    "                    for other_row in all_rows\n",
    "                    if other_row != row\n",
    "                    for chunk in ast.literal_eval(other_row[\"golden\"])\n",
    "                ]\n",
    "\n",
    "                # Randomly select a negative chunk (context from another query)\n",
    "                if other_chunks:\n",
    "                    negative_chunk = random.choice(other_chunks)\n",
    "                    res.append({\n",
    "                        \"query\": row[\"query\"],\n",
    "                        \"expected_response\": negative_chunk,  # Orthogonal/negative context\n",
    "                        \"expected_score\": 0,  # Negative example, score = 0\n",
    "                    })\n",
    "\n",
    "            except (ValueError, SyntaxError) as e:\n",
    "                print(\n",
    "                    f\"Error parsing golden column for query '{row['query']}': {e}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "def generate_qags_golden_set_groundedness(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            # Parse each line as a JSON object\n",
    "            data = json.loads(line)\n",
    "\n",
    "            # Extract the article as the query\n",
    "            query = data[\"article\"]\n",
    "\n",
    "            # Iterate over the summary_sentences to flatten the structure\n",
    "            for summary in data[\"summary_sentences\"]:\n",
    "                expected_response = summary[\"sentence\"]\n",
    "\n",
    "                # Calculate expected_score based on worker responses\n",
    "                responses = [\n",
    "                    response[\"response\"] for response in summary[\"responses\"]\n",
    "                ]\n",
    "                # Convert 'yes' to 1 and 'no' to 0, then calculate the average\n",
    "                expected_score = sum(\n",
    "                    1 if r.lower() == \"yes\" else 0 for r in responses\n",
    "                ) / len(responses)\n",
    "\n",
    "                # Yield the processed record\n",
    "                yield {\n",
    "                    \"query\": query,\n",
    "                    \"expected_response\": expected_response,\n",
    "                    \"expected_score\": expected_score,\n",
    "                }\n",
    "\n",
    "\n",
    "snowflake_it_file_path = \"/Users/dhuang/Documents/git/trulens/src/benchmark/trulens/benchmark/benchmark_frameworks/experiments/data/snowflake_it_v3.csv\"\n",
    "\n",
    "snowflake_it_for_answer_relevance = pd.DataFrame(\n",
    "    list(\n",
    "        generate_snowflake_it_golden_set_answer_relevance(\n",
    "            snowflake_it_file_path\n",
    "        )\n",
    "    )\n",
    ")\n",
    "snowflake_it_answer_relevance_true_labels = list(\n",
    "    snowflake_it_for_answer_relevance[\"expected_score\"]\n",
    ")\n",
    "\n",
    "snowflake_it_for_context_relevance = pd.DataFrame(\n",
    "    list(\n",
    "        generate_snowflake_it_golden_set_context_relevance(\n",
    "            snowflake_it_file_path\n",
    "        )\n",
    "    )\n",
    ")\n",
    "snowflake_it_for_context_relevance_true_labels = list(\n",
    "    snowflake_it_for_context_relevance[\"expected_score\"]\n",
    ")\n",
    "\n",
    "\n",
    "snowflake_it_for_groundness = pd.DataFrame(\n",
    "    list(\n",
    "        generatate_snowflake_it_golden_set_groundedness(snowflake_it_file_path)\n",
    "    )\n",
    ")\n",
    "snowflake_it_for_groundness_true_labels = list(\n",
    "    snowflake_it_for_groundness[\"expected_score\"]\n",
    ")\n",
    "\n",
    "summeval_list = list(\n",
    "    generate_summeval_groundedness_golden_set(\n",
    "        \"/Users/dhuang/Documents/git/trulens/src/benchmark/trulens/benchmark/benchmark_frameworks/experiments/data/summeval_test.json\"\n",
    "    )\n",
    ")\n",
    "\n",
    "summeval_true_labels = [entry[\"expected_score\"] for entry in summeval_list]\n",
    "\n",
    "summeval = pd.DataFrame(\n",
    "    list(\n",
    "        generate_summeval_groundedness_golden_set(\n",
    "            \"/Users/dhuang/Documents/git/trulens/src/benchmark/trulens/benchmark/benchmark_frameworks/experiments/data/summeval_test.json\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "qags_cnn_dm = pd.DataFrame(\n",
    "    list(\n",
    "        generate_qags_golden_set_groundedness(\n",
    "            \"/Users/dhuang/Documents/git/trulens/src/benchmark/trulens/benchmark/benchmark_frameworks/experiments/data/qags_mturk_cnndm.jsonl\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "qags_cnn_dm_true_labels = [\n",
    "    row[\"expected_score\"] for _, row in qags_cnn_dm.iterrows()\n",
    "]\n",
    "\n",
    "\n",
    "qags_xsum = pd.DataFrame(\n",
    "    list(\n",
    "        generate_qags_golden_set_groundedness(\n",
    "            \"/Users/dhuang/Documents/git/trulens/src/benchmark/trulens/benchmark/benchmark_frameworks/experiments/data/qags_mturk_xsum.jsonl\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "qqags_xsum_true_labels = [\n",
    "    row[\"expected_score\"] for _, row in qags_xsum.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(42)\n",
    "\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# ds = load_dataset(\"nixiesearch/ms-marco-hard-negatives\")\n",
    "# ms_marco_hard_neg = pd.DataFrame(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "def generate_balanced_ms_marco_hard_negatives_dataset(series: pd.Series, sample_size: int = 400):\n",
    "    # sampled_series = series.sample(n=sample_size, random_state=42)\n",
    "    sampled_series = series[:sample_size]    \n",
    "    # Step 2: Create a list for the balanced dataset\n",
    "    balanced_dataset = []\n",
    "    \n",
    "    # Step 3: Iterate over the sampled rows\n",
    "    for idx, row in sampled_series.items():\n",
    "        # \"row\" is a dictionary containing 'query', 'positive', and 'negative'\n",
    "        query = row.get(\"query\")\n",
    "        positive_list = row.get(\"positive\", [])\n",
    "        negative_list = row.get(\"negative\", [])\n",
    "        print(f'query: {query}')\n",
    "        # Select one positive example\n",
    "        if positive_list:\n",
    "            positive_example = random.choice(positive_list)\n",
    "        else:\n",
    "            continue  # Skip if no positive examples\n",
    "        \n",
    "        # Select one negative example\n",
    "        if negative_list:\n",
    "            negative_example = random.choice(negative_list)\n",
    "        else:\n",
    "            continue  # Skip if no negative examples\n",
    "        \n",
    "        print(f'positive_example: {positive_example} \\n negative_example: {negative_example}')\n",
    "        # Add a positive example to the dataset\n",
    "        balanced_dataset.append({\n",
    "            \"query\": query,\n",
    "            \"expected_response\": positive_example,\n",
    "            \"expected_score\": 1  # Positive example, label 1\n",
    "        })\n",
    "\n",
    "        # Add a negative example to the dataset\n",
    "        balanced_dataset.append({\n",
    "            \"query\": query,\n",
    "            \"expected_response\": negative_example,\n",
    "            \"expected_score\": 0  # Negative example, label 0\n",
    "        })\n",
    "    \n",
    "    # Ensure the dataset is exactly 200 positive and 200 negative examples\n",
    "    balanced_dataset = balanced_dataset[:sample_size]  # Ensure total sample size is respected\n",
    "    \n",
    "    # Convert the list to a DataFrame for convenience\n",
    "    balanced_df = pd.DataFrame(balanced_dataset)\n",
    "    \n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>expected_response</th>\n",
       "      <th>expected_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>)what was the immediate impact of the success ...</td>\n",
       "      <td>The presence of communication amid scientific ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>)what was the immediate impact of the success ...</td>\n",
       "      <td>The pivotal engineering and scientific success...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_________ justice is designed to repair the ha...</td>\n",
       "      <td>The approach is based on a theory of justice t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_________ justice is designed to repair the ha...</td>\n",
       "      <td>Retributive justice is a theory of justice whi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what color is amber urine</td>\n",
       "      <td>Colorâurine can be a variety of colors, most...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>how much does it cost to replace a foundation ...</td>\n",
       "      <td>the average cost of a foundation repair the av...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>what is glycohemoglobin</td>\n",
       "      <td>test overview a glycohemoglobin test or hemogl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>what is glycohemoglobin</td>\n",
       "      <td>The hemoglobin A1c test, also called HbA1c, gl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>stalactites definition</td>\n",
       "      <td>A stalagmite (UK /ËstaelÉÉ¡maÉªt/ ËstÃ¦lÉÉ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>stalactites definition</td>\n",
       "      <td>Stalagmites are rock formations found in caves...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 query  \\\n",
       "0    )what was the immediate impact of the success ...   \n",
       "1    )what was the immediate impact of the success ...   \n",
       "2    _________ justice is designed to repair the ha...   \n",
       "3    _________ justice is designed to repair the ha...   \n",
       "4                            what color is amber urine   \n",
       "..                                                 ...   \n",
       "395  how much does it cost to replace a foundation ...   \n",
       "396                            what is glycohemoglobin   \n",
       "397                            what is glycohemoglobin   \n",
       "398                             stalactites definition   \n",
       "399                             stalactites definition   \n",
       "\n",
       "                                     expected_response  expected_score  \n",
       "0    The presence of communication amid scientific ...               1  \n",
       "1    The pivotal engineering and scientific success...               0  \n",
       "2    The approach is based on a theory of justice t...               1  \n",
       "3    Retributive justice is a theory of justice whi...               0  \n",
       "4    Colorâurine can be a variety of colors, most...               1  \n",
       "..                                                 ...             ...  \n",
       "395  the average cost of a foundation repair the av...               0  \n",
       "396  test overview a glycohemoglobin test or hemogl...               1  \n",
       "397  The hemoglobin A1c test, also called HbA1c, gl...               0  \n",
       "398  A stalagmite (UK /ËstaelÉÉ¡maÉªt/ ËstÃ¦lÉÉ...               1  \n",
       "399  Stalagmites are rock formations found in caves...               0  \n",
       "\n",
       "[400 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ms_marco_hard_neg_balanced = generate_balanced_ms_marco_hard_negatives_dataset(ms_marco_hard_neg['train'], 400)\n",
    "# ms_marco_hard_neg_balanced.to_csv(\"ms_marco_hard_neg_balanced.csv\", index=False)\n",
    "ms_marco_hard_neg_balanced = pd.read_csv(\"ms_marco_hard_neg_balanced.csv\")\n",
    "ms_marco_hard_neg_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries with expected_score = 1: 150\n",
      "Number of entries with expected_score = 0: 150\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "\n",
    "def generate_ms_marco_context_relevance_benchmark(\n",
    "    file_path=\"/Users/dhuang/Documents/git/trulens/src/benchmark/trulens/benchmark/benchmark_frameworks/experiments/data/ms_marco_v2_1_val.parquet\",\n",
    "):\n",
    "    df = pd.read_parquet(file_path, engine=\"pyarrow\")  # or engine='fastparquet'\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        assert len(row[\"passages\"][\"is_selected\"]) == len(\n",
    "            row[\"passages\"][\"passage_text\"]\n",
    "        )\n",
    "\n",
    "        if sum(row[\"passages\"][\"is_selected\"]) < 1:\n",
    "            # currently we only consider sample with one passage marked as relevant (there are samples where zero passage_text is selected)\n",
    "            continue\n",
    "        for i, passage_text in enumerate(row[\"passages\"][\"passage_text\"]):\n",
    "            yield {\n",
    "                \"query_id\": row[\"query_id\"],\n",
    "                \"query\": row[\"query\"],\n",
    "                \"expected_response\": passage_text,\n",
    "                \"expected_score\": row[\"passages\"][\"is_selected\"][\n",
    "                    i\n",
    "                ],  # Binary relevance\n",
    "            }\n",
    "\n",
    "\n",
    "ms_marco = list(generate_ms_marco_context_relevance_benchmark())\n",
    "\n",
    "\n",
    "score_1_entries = [entry for entry in ms_marco if entry[\"expected_score\"] == 1]\n",
    "score_0_entries = [entry for entry in ms_marco if entry[\"expected_score\"] == 0]\n",
    "\n",
    "# Calculate the number of samples needed from each group\n",
    "num_samples_per_group = min(\n",
    "    len(score_1_entries), len(score_0_entries), 150\n",
    ")  # Sample 150 from each\n",
    "\n",
    "\n",
    "sampled_score_1 = random.sample(score_1_entries, num_samples_per_group)\n",
    "sampled_score_0 = random.sample(score_0_entries, num_samples_per_group)\n",
    "\n",
    "# Combine and shuffle the samples to get a balanced dataset\n",
    "balanced_sample = sampled_score_1 + sampled_score_0\n",
    "random.shuffle(balanced_sample)\n",
    "\n",
    "# Ensure the combined length is 300\n",
    "assert len(balanced_sample) == 300\n",
    "\n",
    "# Now you can use `balanced_sample` as your final dataset\n",
    "print(\n",
    "    f\"Number of entries with expected_score = 1: {len([e for e in balanced_sample if e['expected_score'] == 1])}\"\n",
    ")\n",
    "print(\n",
    "    f\"Number of entries with expected_score = 0: {len([e for e in balanced_sample if e['expected_score'] == 0])}\"\n",
    ")\n",
    "\n",
    "ms_marco_balanced_sample_300 = pd.DataFrame(balanced_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query</th>\n",
       "      <th>expected_response</th>\n",
       "      <th>expected_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>429239</td>\n",
       "      <td>is there poison ivy in oregon</td>\n",
       "      <td>Poison oak is common to western Oregon and Was...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>632923</td>\n",
       "      <td>what does bbb rating mean for a bond</td>\n",
       "      <td>Definition of Baa3 Rating in the Financial Dic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>151780</td>\n",
       "      <td>direct material price variance formula</td>\n",
       "      <td>Direct Material Price Variance: = Actual Quant...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1028902</td>\n",
       "      <td>what is vesd</td>\n",
       "      <td>VESDA is a laser based smoke detector, which m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>993874</td>\n",
       "      <td>which of the following planets has the shortes...</td>\n",
       "      <td>Neptune has the longest year( Pluto would be b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>1040065</td>\n",
       "      <td>what is the elevation of orlando</td>\n",
       "      <td>Elevation: 106 feet. Land area: 93.5 square mi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>695081</td>\n",
       "      <td>what is a plateau period</td>\n",
       "      <td>pla•teau. 1. a land area having a relatively l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>728820</td>\n",
       "      <td>what is cease fire mean</td>\n",
       "      <td>A ceasefire (or truce), also called cease fire...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>427635</td>\n",
       "      <td>is the tv show lucifer renewed</td>\n",
       "      <td>FOX Renewed TV Series Lucifer for Second Seaso...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>179298</td>\n",
       "      <td>el camino christmas cast</td>\n",
       "      <td>Watch the official movie trailer of El Camino ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     query_id                                              query  \\\n",
       "0      429239                      is there poison ivy in oregon   \n",
       "1      632923               what does bbb rating mean for a bond   \n",
       "2      151780             direct material price variance formula   \n",
       "3     1028902                                       what is vesd   \n",
       "4      993874  which of the following planets has the shortes...   \n",
       "..        ...                                                ...   \n",
       "295   1040065                   what is the elevation of orlando   \n",
       "296    695081                           what is a plateau period   \n",
       "297    728820                            what is cease fire mean   \n",
       "298    427635                     is the tv show lucifer renewed   \n",
       "299    179298                           el camino christmas cast   \n",
       "\n",
       "                                     expected_response  expected_score  \n",
       "0    Poison oak is common to western Oregon and Was...               1  \n",
       "1    Definition of Baa3 Rating in the Financial Dic...               0  \n",
       "2    Direct Material Price Variance: = Actual Quant...               1  \n",
       "3    VESDA is a laser based smoke detector, which m...               1  \n",
       "4    Neptune has the longest year( Pluto would be b...               0  \n",
       "..                                                 ...             ...  \n",
       "295  Elevation: 106 feet. Land area: 93.5 square mi...               0  \n",
       "296  pla•teau. 1. a land area having a relatively l...               1  \n",
       "297  A ceasefire (or truce), also called cease fire...               1  \n",
       "298  FOX Renewed TV Series Lucifer for Second Seaso...               1  \n",
       "299  Watch the official movie trailer of El Camino ...               1  \n",
       "\n",
       "[300 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms_marco_balanced_sample_300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load preprocessed datasets from BEIR - start w/ Hotpot QA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhuang/Documents/git/trulens/src/benchmark/trulens/benchmark/benchmark_frameworks/dataset/beir_loader.py:10: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66cf8e75ab345c987dc269a31c81504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "./hotpotqa.zip:   0%|          | 0.00/624M [00:00<?, ?iB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trulens.benchmark.benchmark_frameworks.dataset.beir_loader import (\n",
    "    TruBEIRDataLoader,\n",
    ")\n",
    "\n",
    "beir_data_loader = TruBEIRDataLoader(data_folder=\"./\", dataset_name=\"hotpotqa\")\n",
    "hotpotqa = beir_data_loader.load_dataset_to_df(download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "\n",
    "hotpotqa_raw_subset = hotpotqa.sample(n=200, random_state=42)\n",
    "\n",
    "all_responses = [\n",
    "    (row[\"query\"], row[\"expected_response\"])\n",
    "    for idx, row in hotpotqa_raw_subset.iterrows()\n",
    "]\n",
    "\n",
    "all_contexts = [\n",
    "    (row[\"query\"], context[\"text\"])\n",
    "    for idx, row in hotpotqa_raw_subset.iterrows()\n",
    "    for context in row[\"expected_chunks\"]\n",
    "]\n",
    "\n",
    "hotpotqa_subset_for_answer_relevance, hotpotqa_subset_for_context_relevance = (\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "\n",
    "for idx, row in hotpotqa_raw_subset.iterrows():\n",
    "    # Positive examples for answer relevance\n",
    "    hotpotqa_subset_for_answer_relevance.append({\n",
    "        \"query\": row[\"query\"],\n",
    "        \"expected_response\": row[\"expected_response\"],  # Positive response\n",
    "        \"expected_score\": 1,  # Positive example, score = 1\n",
    "    })\n",
    "\n",
    "    # Negative examples for answer relevance (random unrelated response)\n",
    "    negative_response = random.choice([\n",
    "        r\n",
    "        for q, r in all_responses\n",
    "        if q != row[\"query\"]  # Pick response from another query\n",
    "    ])\n",
    "\n",
    "    hotpotqa_subset_for_answer_relevance.append({\n",
    "        \"query\": row[\"query\"],\n",
    "        \"expected_response\": negative_response,  # Negative response\n",
    "        \"expected_score\": 0,  # Negative example, score = 0\n",
    "    })\n",
    "\n",
    "\n",
    "for idx, row in hotpotqa_raw_subset.iterrows():\n",
    "    positive_examples = []\n",
    "    negative_examples = []\n",
    "\n",
    "    # Generate positive examples for context relevance\n",
    "    for context in row[\"expected_chunks\"]:\n",
    "        positive_examples.append({\n",
    "            \"query\": row[\"query\"],\n",
    "            \"expected_response\": context[\"text\"],  # Positive context\n",
    "            \"expected_score\": context[\"expected_score\"],  # Should be 1\n",
    "        })\n",
    "\n",
    "    # Generate negative examples for context relevance\n",
    "    for _ in positive_examples:\n",
    "        negative_context = random.choice([\n",
    "            c\n",
    "            for q, c in all_contexts\n",
    "            if q != row[\"query\"]  # Pick context from another query\n",
    "        ])\n",
    "        negative_examples.append({\n",
    "            \"query\": row[\"query\"],\n",
    "            \"expected_response\": negative_context,  # Negative context\n",
    "            \"expected_score\": 0,  # Negative example, score = 0\n",
    "        })\n",
    "\n",
    "    # Add positive and negative examples to the result set\n",
    "    hotpotqa_subset_for_context_relevance.extend(positive_examples)\n",
    "    hotpotqa_subset_for_context_relevance.extend(negative_examples)\n",
    "\n",
    "\n",
    "hotpotqa_subset_for_context_relevance_true_labels = [\n",
    "    entry[\"expected_score\"] for entry in hotpotqa_subset_for_context_relevance\n",
    "]\n",
    "hotpotqa_subset_for_answer_relevance_true_labels = [\n",
    "    entry[\"expected_score\"] for entry in hotpotqa_subset_for_answer_relevance\n",
    "]\n",
    "\n",
    "hotpotqa_subset_for_context_relevance = pd.DataFrame(\n",
    "    hotpotqa_subset_for_context_relevance\n",
    ")\n",
    "\n",
    "hotpotqa_subset_for_answer_relevance = pd.DataFrame(\n",
    "    hotpotqa_subset_for_answer_relevance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>expected_response</th>\n",
       "      <th>expected_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VIVA Media AG changed it's name in 2004. What ...</td>\n",
       "      <td>VIVA Media GmbH (until 2004 \"VIVA Media AG\") i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VIVA Media AG changed it's name in 2004. What ...</td>\n",
       "      <td>A Gesellschaft mit beschränkter Haftung (] , a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VIVA Media AG changed it's name in 2004. What ...</td>\n",
       "      <td>Yemoja (Yoruba: \"Yemọja\" ) is a major water de...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VIVA Media AG changed it's name in 2004. What ...</td>\n",
       "      <td>Jonathan Monroe \"Jonny\" Craig (born March 26, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which of Jonny Craig and Pete Doherty has been...</td>\n",
       "      <td>Jonathan Monroe \"Jonny\" Craig (born March 26, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>The King who opened Newcastle Civic Center in ...</td>\n",
       "      <td>Walk All Over Me is a Canadian film released i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>When was the Western Germanic language spoken ...</td>\n",
       "      <td>The Leda is a river in north-western Germany i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>When was the Western Germanic language spoken ...</td>\n",
       "      <td>Old Frisian is a West Germanic language spoken...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>When was the Western Germanic language spoken ...</td>\n",
       "      <td>Bishop's Stortford Football Club is a football...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>When was the Western Germanic language spoken ...</td>\n",
       "      <td>Der Rosenkavalier (The Knight of the Rose or T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 query  \\\n",
       "0    VIVA Media AG changed it's name in 2004. What ...   \n",
       "1    VIVA Media AG changed it's name in 2004. What ...   \n",
       "2    VIVA Media AG changed it's name in 2004. What ...   \n",
       "3    VIVA Media AG changed it's name in 2004. What ...   \n",
       "4    Which of Jonny Craig and Pete Doherty has been...   \n",
       "..                                                 ...   \n",
       "795  The King who opened Newcastle Civic Center in ...   \n",
       "796  When was the Western Germanic language spoken ...   \n",
       "797  When was the Western Germanic language spoken ...   \n",
       "798  When was the Western Germanic language spoken ...   \n",
       "799  When was the Western Germanic language spoken ...   \n",
       "\n",
       "                                     expected_response  expected_score  \n",
       "0    VIVA Media GmbH (until 2004 \"VIVA Media AG\") i...               1  \n",
       "1    A Gesellschaft mit beschränkter Haftung (] , a...               1  \n",
       "2    Yemoja (Yoruba: \"Yemọja\" ) is a major water de...               0  \n",
       "3    Jonathan Monroe \"Jonny\" Craig (born March 26, ...               0  \n",
       "4    Jonathan Monroe \"Jonny\" Craig (born March 26, ...               1  \n",
       "..                                                 ...             ...  \n",
       "795  Walk All Over Me is a Canadian film released i...               0  \n",
       "796  The Leda is a river in north-western Germany i...               1  \n",
       "797  Old Frisian is a West Germanic language spoken...               1  \n",
       "798  Bishop's Stortford Football Club is a football...               0  \n",
       "799  Der Rosenkavalier (The Knight of the Rose or T...               0  \n",
       "\n",
       "[800 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotpotqa_subset_for_context_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summeval_subset = summeval.sample(n=200, random_state=42)\n",
    "summeval_subset_true_labels = [\n",
    "    row[\"expected_score\"] for _, row in summeval_subset.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up feedback LLM providers \n",
    "\n",
    "We will experiment with 2 current OpenAI models and a mix of commercial and open source models avaiable in Cortex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /opt/anaconda3/envs/trulens/lib/python3.11/site-\n",
      "[nltk_data]     packages/llama_index/core/_static/nltk_cache...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import snowflake.connector\n",
    "from trulens.providers.cortex import Cortex\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "snowflake_connection_parameters = {\n",
    "    \"account\": os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "    \"user\": os.environ[\"SNOWFLAKE_USER\"],\n",
    "    \"password\": os.environ[\"SNOWFLAKE_USER_PASSWORD\"],\n",
    "}\n",
    "snowflake_connection = snowflake.connector.connect(\n",
    "    **snowflake_connection_parameters\n",
    ")\n",
    "\n",
    "\n",
    "gpt_4o = OpenAI(model_engine=\"gpt-4o\")\n",
    "gpt_4o_mini = OpenAI(model_engine=\"gpt-4o-mini\")\n",
    "\n",
    "snowflake_arctic = Cortex(snowflake_connection, model_engine=\"snowflake-arctic\")\n",
    "mistral_large = Cortex(snowflake_connection, model_engine=\"mistral-large\")\n",
    "llama3_1_8b = Cortex(snowflake_connection, model_engine=\"llama3.1-8b\")\n",
    "\n",
    "CORTEX_PROVIDERS = [snowflake_arctic, llama3_1_8b, mistral_large]\n",
    "OPENAI_PROVIDERS = [gpt_4o, gpt_4o_mini]\n",
    "ALL_PROVIDERS = CORTEX_PROVIDERS + OPENAI_PROVIDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦑 TruSession initialized with db url sqlite:///default.sqlite .\n",
      "🛑 Secret keys may be written to the database. See the `database_redact_keys` option of `TruSession` to prevent this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating app_name and app_version in apps table: 0it [00:00, ?it/s]\n",
      "Updating app_id in records table: 0it [00:00, ?it/s]\n",
      "Updating app_json in apps table: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from trulens.core import TruSession\n",
    "\n",
    "session = TruSession()\n",
    "session.reset_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snowflake IT dataset experiment runs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "\n",
    "from trulens.apps.basic import TruBasicApp\n",
    "\n",
    "\n",
    "def write_results(\n",
    "    feedback_scores: List[float],\n",
    "    labels: List[float | int],\n",
    "    latencies: List[float],\n",
    "    file_name: str,\n",
    "):\n",
    "    assert len(feedback_scores) == len(labels)\n",
    "\n",
    "    with open(file_name, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(feedback_scores)\n",
    "        writer.writerow(labels)\n",
    "        writer.writerow(latencies)\n",
    "\n",
    "\n",
    "def read_results(\n",
    "    file_name: str,\n",
    ") -> Tuple[List[float | int], List[float | int], List[float]]:\n",
    "    with open(file_name, \"r\") as file:\n",
    "        reader = csv.reader(file)\n",
    "        for index, row in enumerate(reader):\n",
    "            if index == 0:\n",
    "                # First row contains scores\n",
    "                scores = list(map(float, row))  # Convert strings to floats\n",
    "            elif index == 1:\n",
    "                # Second row contains labels\n",
    "                labels = list(map(float, row))  # Convert strings to floats\n",
    "            elif index == 2:\n",
    "                # Third row contains latencies\n",
    "                latencies = list(map(float, row))\n",
    "    return scores, labels, latencies\n",
    "\n",
    "\n",
    "def run_feedback_experiment(\n",
    "    feedback_func_wrapper, app_name, app_version, dataset_df, true_labels\n",
    "):\n",
    "    if len(dataset_df) != len(true_labels):\n",
    "        raise ValueError(\"dataset df must have the same length as labels\")\n",
    "    tru_wrapped_basic_app = TruBasicApp(\n",
    "        feedback_func_wrapper, app_name=app_name, app_version=app_version\n",
    "    )\n",
    "\n",
    "    generated_scores, labels, latencies = [], [], []\n",
    "    for i in range(len(dataset_df)):\n",
    "        arg_1 = dataset_df.iloc[i][\"query\"]\n",
    "        arg_2 = dataset_df.iloc[i][\"expected_response\"]\n",
    "        try:\n",
    "            with tru_wrapped_basic_app as _:\n",
    "                start_time = time.time()\n",
    "                score = tru_wrapped_basic_app.app(arg_1, arg_2)\n",
    "\n",
    "                end_time = time.time()\n",
    "                true_score = true_labels[i]\n",
    "\n",
    "                if not math.isnan(score):\n",
    "                    generated_scores.append(score)\n",
    "                    labels.append(true_score)\n",
    "                    latencies.append(end_time - start_time)\n",
    "\n",
    "                    # print(f\"Generated score: {score} | true_score: {true_score} \\n\")\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error {e} in run_feedback_experiment row {i} with first arg {arg_1} and second arg {arg_2}\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "    write_results(\n",
    "        generated_scores,\n",
    "        labels,\n",
    "        latencies,\n",
    "        f\"results/{app_name}_{app_version}_results.csv\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, {'reasons': 'STATEMENT 0:\\nCriteria: Earth is not round.\\nSupporting Evidence: NOTHING FOUND.\\nScore: 0\\n'})\n",
      "(0.0, {'reasons': 'STATEMENT 0:\\nCriteria: Earth is not round.\\nSupporting Evidence: NOTHING FOUND.\\nScore: 0\\n'})\n"
     ]
    }
   ],
   "source": [
    "score = gpt_4o_mini.groundedness_measure_with_cot_reasons(\n",
    "    source=\"This is a test. Earth is round\",\n",
    "    statement=\"Earth is not not round\",\n",
    "    criteria=\"A grounded response to the query should get a score of 1, and an ungrounded response should get a score of 0. The score can only be either 0 or 1 (binary).\",\n",
    "    max_score_val=1,\n",
    ")\n",
    "print(score)\n",
    "score = gpt_4o_mini.groundedness_measure_with_cot_reasons(\n",
    "    \"This is a test. Earth is round\",\n",
    "    \"Earth is not not round\",\n",
    "    criteria=\"\"\" You should score the groundedness of the statement based on the following criteria:\n",
    "    - Statements that are directly supported by the source should be considered grounded and should get a high score.\n",
    "    - Statements that are not directly supported by the source should be considered not grounded and should get a low score.\n",
    "    - Statements of doubt, that admissions of uncertainty or not knowing the answer are considered abstention, and should be counted as the most overlap and therefore get a max score.\"\"\",\n",
    "    max_score_val=3,\n",
    ")\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.feedback.v2.feedback import Groundedness\n",
    "\n",
    "default_groundedness_criteria = Groundedness.criteria\n",
    "default_groundedness_system_prompt = Groundedness.system_prompt\n",
    "\n",
    "likert4_groundedness_criteria = \"\"\"You should score the groundedness of the statement based on the following criteria:\n",
    "    - Statements that are directly supported by the source should be considered grounded and should get a high score.\n",
    "    - Statements that are not directly supported by the source should be considered not grounded and should get a low score.\n",
    "    - Statements of doubt, that admissions of uncertainty or not knowing the answer are considered abstention, and should be counted as the most overlap and therefore get a max score.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: MS MARCO hard negatives (first 400 samples) - snowflake-arctic - context_relevance_binary\n",
      "Running experiment: MS MARCO hard negatives (first 400 samples) - llama3.1-8b - context_relevance_binary\n",
      "Running experiment: MS MARCO hard negatives (first 400 samples) - mistral-large - context_relevance_binary\n",
      "Running experiment: MS MARCO hard negatives (first 400 samples) - gpt-4o - context_relevance_binary\n",
      "Running experiment: MS MARCO hard negatives (first 400 samples) - gpt-4o-mini - context_relevance_binary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input=')what was the immediate impact of the success of the manhattan project?', output='The presence of communication amid scientific minds was equally important to the success of the Manhattan Project as scientific intellect was. The only cloud hanging over the impressive achievement of the atomic researchers and engineers is what their success truly meant; hundreds of thousands of innocent lives obliterated.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input=')what was the immediate impact of the success of the manhattan project?', output='The presence of communication amid scientific minds was equally important to the success of the Manhattan Project as scientific intellect was. The only cloud hanging over the impressive achievement of the atomic researchers and engineers is what their success truly meant; hundreds of thousands of innocent lives obliterated.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input=')what was the immediate impact of the success of the manhattan project?', output='The presence of communication amid scientific minds was equally important to the success of the Manhattan Project as scientific intellect was. The only cloud hanging over the impressive achievement of the atomic researchers and engineers is what their success truly meant; hundreds of thousands of innocent lives obliterated.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input=')what was the immediate impact of the success of the manhattan project?', output='The presence of communication amid scientific minds was equally important to the success of the Manhattan Project as scientific intellect was. The only cloud hanging over the impressive achievement of the atomic researchers and engineers is what their success truly meant; hundreds of thousands of innocent lives obliterated.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input=')what was the immediate impact of the success of the manhattan project?', output='The pivotal engineering and scientific success of the Twentieth century was the Manhattan Project. The Manhattan Project assimilated concepts and leaders from all scientific fields and engineering disciplines to construct the first two atomic bombs.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input=')what was the immediate impact of the success of the manhattan project?', output='The presence of communication amid scientific minds was equally important to the success of the Manhattan Project as scientific intellect was. The only cloud hanging over the impressive achievement of the atomic researchers and engineers is what their success truly meant; hundreds of thousands of innocent lives obliterated.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='_________ justice is designed to repair the harm to victim, the community and the offender caused by the offender criminal act. question 19 options:', output='The approach is based on a theory of justice that considers crime and wrongdoing to be an offense against an individual or community, rather than the State. Restorative justice that fosters dialogue between victim and offender has shown the highest rates of victim satisfaction and offender accountability.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='_________ justice is designed to repair the harm to victim, the community and the offender caused by the offender criminal act. question 19 options:', output='Retributive justice is a theory of justice which holds that the best response to a crime is a proportionate punishment, inflicted for its own sake rather than to serve an extrinsic social purpose, such as deterrence or rehabilitation of the offender.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what color is amber urine', output='Colorâ\\x80\\x94urine can be a variety of colors, most often shades of yellow, from very pale or colorless to very dark or amber. Unusual or abnormal urine colors can be the result of a disease process, several medications (e.g., multivitamins can turn urine bright yellow), or the result of eating certain foods.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what color is amber urine', output='Abnormal urine color could be brown, orange, dark yellow, pink or red, blue or dark green, purple, and cloudy or murky. As mentioned at the beginning of the article, pale yellow to deep amber is an identifying characteristic of normal urine.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='is autoimmune hepatitis a bile acid synthesis disorder', output='Inborn errors of bile acid synthesis can produce life-threatening cholestatic liver disease (which usually presents in infancy) and progressive neurological disease presenting later in childhood or in adult life.he neurological presentation often includes signs of upper motor neurone damage (spastic paraparesis). The most useful screening test for many of these disorders is analysis of urinary cholanoids (bile acids and bile alcohols); this is usually now achieved by electrospray ionisation tandem mass spectrometry.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='is autoimmune hepatitis a bile acid synthesis disorder', output=\"Autoimmune hepatitis is a chronic disease of the liver. The body's immune system normally makes large numbers of proteins called antibodies to help the body fight off infections. In some cases, however, the body makes autoantibodies. Certain environmental triggers can lead to autoimmunity.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='elegxo meaning', output='The word convict here (elegcw /elegxo) means to bring to light or expose error often with the idea of reproving or rebuking. It brings about knowledge of believing or doing something wrong, but it does not mean that the person will respond properly to that knowledge. Our usage of the English word, convict, is similar.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='elegxo meaning', output='The word convict here (elegcw /elegxo) means to bring to light or expose error often with the idea of reproving or rebuking. It brings about knowledge of believing or doing something wrong, but it does not mean that the person will respond properly to that knowledge. Our usage of the English word, convict, is similar.')>.\n",
      "/opt/anaconda3/envs/trulens/lib/python3.11/site-packages/trulens/feedback/llm_provider.py:289: UserWarning: No supporting evidence provided. Returning score only.\n",
      "  warnings.warn(\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how much does an average person make for tutoring', output='In-home tutors can earn anywhere from $10 to $80 an hour, depending on the type of lesson, the studentâ\\x80\\x99s skill and age level and the tutorâ\\x80\\x99s experience. Tutors often charge more for older students or those who require more advanced lessons.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how much does an average person make for tutoring', output='Tutor average salary is  $38,471, median salary is $39,988 with a salary range from $21,424 to $62,400. Tutor salaries are collected from government agencies and companies. Each salary is associated with a real job position. Tutor salary statistics is not exclusive and is for reference only.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='can you use a calculator on the compass test', output='Calculators may be used on the COMPASS Pre-Algebra, Algebra, College Algebra, Geometry, and Trigonometry tests provided they meet the requirements listed below. Electronic writing pads or pen-input devicesâ\\x80\\x94The Sharp EL 9600 is permitted. Models with paper tapesâ\\x80\\x94The paper must be removed.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='can you use a calculator on the compass test', output='The COMPASS test is a self-adjusting, multiple choice test that is taken at the computer. The. answer to your current question will determine the next question; it will stop once it has determined. your level.')>.\n",
      "/opt/anaconda3/envs/trulens/lib/python3.11/site-packages/trulens/feedback/llm_provider.py:289: UserWarning: No supporting evidence provided. Returning score only.\n",
      "  warnings.warn(\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does physical medicine do', output='Doctor Directory. A physiatrist practices in the field of physiatry - also called physical medicine and rehabilitation - which is a branch of medicine that specializes in diagnosis, treatment, and management of disease primarily using physical means, such as physical therapy and medications.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does physical medicine do', output='Physical therapy is a form of health care that prevents, identifies, corrects, or alleviates acute or prolonged movement dysfunction or pain of anatomic or physiologic origin. The practice of physical therapy includes but is not limited to: (1) measurement or testing of the function of the musculoskeletal, neurological, pulmonary, or cardiovascular system; (2) rehabilitative treatment concerned with restoring function or preventing disability caused by illness, injury, or birth defect;')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does pending mean on listing', output='Active/Pending = Usually that means the seller would like to get more offers. Savvy agents want back-up offers in place. If you made an offer to purchase a property and the seller agreed to the price, but already has a buyer in contract, you would be the 1st position back-up buyer.')>.\n",
      "/opt/anaconda3/envs/trulens/lib/python3.11/site-packages/trulens/feedback/llm_provider.py:289: UserWarning: No supporting evidence provided. Returning score only.\n",
      "  warnings.warn(\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does pending mean on listing', output='A pending status means there has been an offer accepted on the home and the buyer is in the process of working with a lender to finance the property. An option pending status means the buyer is in the proces of having the home inspected and can still change their mind on purchasing the home.I would love the opportunity to help you find a home. You can reach me at 281-546-7117.hen a house is listed as pending, that means that a buyer has already submitted a contract and is currently off the market until their loan closes. If for some reason the buyer is unable to close, then the home will be placed back as active.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='feeding rice cereal how many times per day', output=\"1. Begin with rice cereal on days 1,2 and 3, offering it twice daily at breakfast and dinnertime. Rice cereal can be mixed with water or milk(breast or formula) to make a thin oatmeal like consistency. The infant should be offered a rubberized spoon. During these first three days, offer 3-4 tablespoons at a time but be flexible. Some infants will be very hungry and want more- that's OK.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='feeding rice cereal how many times per day', output='I started my baby girl on some rice cereal at 3 1/2 months, which was recommended by her Dr. I started feeding it to her with a spoon, about 2 tablespoons at breakfast then was putting about 1tbsp in her last bottle at night. Hope this helps.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='most dependable affordable cars', output='If you can look past its bargain interior and anonymous exterior, the Suzuki SX4 is one of the most reliable and affordable all-wheel-drive cars.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='most dependable affordable cars', output=\"The high-science Chevrolet Volt extended-range electric has been the most dependable compact car among 2011 models, J.D. Power says. Toyota Corolla and Honda Civic, hardly chopped liver, were second and third. Volt was one of General Motors' eight category wins, more than any other maker. GM Wieck.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='lithophile definition', output='Lithophile elements [edit]. Lithophile elements are those that remain on or close to the surface because they combine readily with oxygen, forming compounds that do not sink into the core.anganese, iron and molybdenum do form strong bonds with oxygen but in the free state (as they existed on the primitive earth when free oxygen did not exist) can mix so easily with iron that they do not concentrate in the siliceous crust as do true lithophile elements.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='lithophile definition', output='Definition of amphiphilic : of, relating to, or being a compound (such as a surfactant) consisting of molecules having a polar water-soluble group attached to a water-insoluble hydrocarbon chain; also: being a molecule of such a compound')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is a flail chest', output='Flail Chest. Flail chest describes a situation in which a portion of the rib cage is separated from the rest of the chest wall, usually due to a severe blunt trauma, such as a serious fall or a car accident.lail chest is almost always linked to severe blunt trauma, such as a serious fall or car accident, in terms of cause. But it can develop as a result of less severe impacts.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is a flail chest', output='Flail chest. A flail chest is a life-threatening medical condition that occurs when a segment of the rib cage breaks under extreme stress and becomes detached from the rest of the chest wall. It occurs when multiple adjacent ribs are broken in multiple places, separating a segment, so a part of the chest wall moves independently.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what causes ear infection in adults', output='One of the reasons children are more likely than adults to get ear infections is that their eustachian tubes are smaller and more horizontal than the tubes in most adults. If you have small eustachian tubes or you have tubes that havenâ\\x80\\x99t developed more of a slope, youâ\\x80\\x99re at a higher risk for developing an ear infection.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what causes ear infection in adults', output='Identifying an Ear Infection. 1  1. Know who is most at risk for an ear infection. In general, children are more likely to get ear infections than adults. 2  2. Recognize the symptoms of a middle ear infection. A middle ear infection (acute otitis media) is the most common type of ear infection and is caused by a virus or bacteria.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='put yourself on child support in texas', output='1 Non-Texas residents can apply for services with the Child Support Division of the Office of the Attorney General of Texas, provided there is a Texas child support order or one of the parties or child(ren) resides in Texas.eginning September 1, 2011, custodial parents will pay a $3 monthly fee in cases that receive registry-only payment processing and record-keeping services through the State Disbursement Unit (SDU) for each month in which a child support payment of more than $3 is received.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='put yourself on child support in texas', output=\"Child Support Liens in Texas. In Texas, a lien may be placed against your property if you fail to adhere to your support obligations. If you're a parent in Texas and you're going through a divorce, or your relationship has ended, you should know that the courts are required to make sure your children receive enough financial support.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='mushrooms health benefits', output='Mushrooms are a good source of B vitamins, including riboflavin, niacin, and pantothenic acid, which help to provide energy by breaking down proteins, fats and carbohydrates2. B vitamins also play an important role in the nervous system.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='mushrooms health benefits', output='The health benefits of mushrooms include relief from high cholesterol levels, breast cancer, prostate cancer, and diabetes. They also help in weight loss and increase the strength of your immune system. Almost all of us are familiar with mushrooms and their miraculous, beneficial powers.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what happens in a wrist sprain', output='A sprained wrist is an injury to the ligaments that connect the small bones of the wrist (called carpal bones) together. The most common ligament injured in the wrist is the scapho-lunate ligament, which connects the scaphoid bone to the lunate bone.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what happens in a wrist sprain', output='A sprain in the wrist is an injury to its ligaments, the tough bands of fibrous tissue that connect bones to one another inside a joint. Although most peopleâ\\x80¦ Skip to content')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='example of culture &sensitivity results', output='Culture and Sensitivity â\\x80\\x93 â\\x80\\x9cC & Sâ\\x80\\x9d. Culture: You send a specimen to the lab and the labs job is to tell you what the organism is, a definitive ID based on gram stain, morphology and biochemical profile. Susceptibility: The lab also gives you the info on the antibiotic susceptibilities to know how to treat it.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='example of culture &sensitivity results', output='and culture was 64 (45.4%), (Table 2). Comparing PAS test alone as an alternative method of. diagnosis to combined KOH and culture (for both culture positive and culture negative cases) showed an overall sensitivity of 60.9% and specificity of 80.3%.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what are rhetorical topics', output='The rhetoric of mental health considers how language functions in the production of knowledge on topics such as mental and psychological disorders, chemical imbalances in the brain, and variations on what are considered normal mental faculties.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what are rhetorical topics', output='Rhetoric of health and medicine. The rhetoric of health and medicine is an academic discipline concerning language and symbols in health and medicine. Rhetoric most commonly refers to the persuasive element in human interactions and is often best studied in the specific situations in which it occurs. The rhetoric of health and medicine specifically creates, analyzes, and criticizes how messages are delivered and structured in medicine- and health-related contexts.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='mode of acquisition', output='Accretion also abounds as another mode of acquiring a territory under the traditional international law. Notably, accretion involves the effect of natural forces such as volcanism. For instance, in cases where natural activities such as volcanism takes place and volcanic islands emerge in a stateâ\\x80\\x99s territorial waters, it would have the right to acquire territory.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='mode of acquisition', output=\"What is an 'Acquisition' An acquisition is a corporate action in which a company buys most, if not all, of another firm's ownership stakes to assume control of it. An acquisition occurs when a buying company obtains more than 50% ownership in a target company. As part of the exchange, the acquiring company often purchases the target company's stock and other assets, which allows the acquiring company to make decisions regarding the newly acquired assets without the approval of the target companyâ\\x80\\x99s shareholders.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how beautiful on the mountain are the feet', output='Isaiah 52:7 (ASV) How beautiful upon the mountains are the feet of him that bringeth good tidings, that publisheth peace, that bringeth good tidings of good, that publisheth salvation, that saith unto Zion, Thy God reigneth!eshayah 52:7 (OJB) How beautiful upon the mountains are the feet of the Mevaser (bringer of Good Tidings, the Evangelist), that publisheth shalom; that bringeth good tidings of tov, that publisheth Yeshuah (Salvation); that saith unto Tziyon, Thy G-d reigneth!')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how beautiful on the mountain are the feet', output='Itâ\\x80\\x99s also arguably one of the most beautiful, with grand wooden lodges surrounded by majestic mountains. Get some mountain air: The Peak 2 Peak gondola, which connects Whistlerâ\\x80\\x99s two mountaintops via a 2.73-mile long, 1,427-foot tall tram-ride, allows access for both winter skiers and summer hikersâ\\x80\\x94plus phenomenal views over the valley below.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is considered early fall', output=\"Very generally, an early summer bloom could mean any time from May into June; midsummer, late June into early August and late summer anywhere from the end of July to late September. Plants don't know nuthin' about calendars and it is always a bit tricky guestimating when a specific plant is likely to bloom.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is considered early fall', output=\"First Day of Fall? In the Northern Hemisphere, the fall equinox marks the first day of fall (autumn) in what we call astronomical seasons. There's also another, more common definition of when the seasons start, namely meteorological definitions, which are based on average temperatures rather that astronomical events.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what causes elevated nitrate levels in aquariums', output='Overfeeding is the number-one cause of high nitrate levels in aquariums. Overfed fish produce much more waste than normal, and when more food is given than they can take in, the uneaten food rots and produces even more wastes. Overstocking. Keeping too many fish in a tank also leads to problems with accumulated wastes.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what causes elevated nitrate levels in aquariums', output='1 If any amount of E. coli bacteria is found in a water sample, it is an indication that human sewage or animal faeces has contaminated the water supply.  Nitrates - The presence of nitrates in well water is usually the result of farming activities like fertilizing, or seepage from septic systems. If nitrates are at levels above 10 milligrams per litre of water, an infant may suffer from a condition known as â\\x80\\x9cblue baby syndromeâ\\x80\\x9d or methaemoglobinaemia. Blue baby syndrome is caused by the nitrates interfering with the bloodâ\\x80\\x99s ability to carry oxygen throughout the body.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='is an independent contractor hired by a company entitled to the same non-compete as a normal employee', output='Persons who work as real â\\x80\\x9cindependent contractorsâ\\x80\\x9d are not entitled to the same monetary benefits and legal protections as â\\x80\\x9cemployees.â\\x80\\x9d This is because independent contractors are not subject to the same types of restrictions, level of discipline, and fixed work schedules as are employees.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='is an independent contractor hired by a company entitled to the same non-compete as a normal employee', output='As an employee or independent contractor, you may have to sign a contract that contains a non-compete provision. The main purpose of the clause is to safeguard proprietary information from similar, competing companies.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='lyme disease symptoms mood', output='Lyme disease causes depression, which can cause mood swings. Additionally, the pain associated with Lyme is enough to cause patients to become irritable. Lyme disease is a systemic infection that can affect all systems of the human body. This disease is a bacterial infection transmitted by the spirochete, Borrelia burgdorferi.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='lyme disease symptoms mood', output='Lyme disease causes depression, which can cause mood swings. Additionally, the pain associated with Lyme is enough to cause patients to become irritable. Lyme disease is a systemic infection that can affect all systems of the human body. This disease is a bacterial infection transmitted by the spirochete, Borrelia burgdorferi.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what forms the epineurium', output='The layer of the epineurium that extends within the nerve to define the fascicles is termed the internal epineurium. Together, these two layers form the epineurium, which varies in thickness along the course of a nerve. The epineurium is usually most abundant around joints, as its function is to protect the nerves from stretching and subsequent injury.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what forms the epineurium', output=\"You won't believe these 10 facts about people. The perineurium is a protective layer of tissue located around nerves in the body and the internal organs. It is composed of concentric layers of connective tissue that form a protective sheath around bundles of nerve fibers.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='an alpha helix is an example of which protein structure?', output='A level chemistry resource. Protein structure The a-helix. Secondary structure (continued) We can describe the arrangement of a')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='an alpha helix is an example of which protein structure?', output='Properties of the alpha-helix. 1  The structure repeats itself every 5.4 Angstroms along the helix axis, ie we say that the alpha-helix has a pitch of 5.4 Angstroms. Alpha-helices have 3.6 amino acid residues per turn, ie a helix 36 amino acids long would form 10 turns.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='aggregate demand curve', output='(Technically, in the context of aggregate demand, the Y on the horizontal axis represents aggregate expenditure .) As it turns out, the aggregate demand curve also slopes downwards, giving a similar negative relationship between price and quantity that exists with the demand curve for a single good.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='aggregate demand curve', output='1. The aggregate demand curve is defined as . a. the net national product. b. the sum of wages, rent, interest, and profits. c. the real GDP purchased at different possible price levels. d. the total dollar value of household expectations. ANS: c. Answers a, b, and c are not real GDP purchases at different possible price levels during a time period. 2.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what county is ackley iowa in', output='Ackley, Iowa. Ackley is a city in Franklin and Hardin Counties in the U.S. state of Iowa. The population was 1,589 at the 2010 census.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what county is ackley iowa in', output='HOCKLEY COUNTY. Hockley County is in northwestern Texas, south of the Panhandle and on the Llano Estacado, bordered on the east by Lubbock County, on the south by Terry County, on the west by Cochran County, and on the north by Lamb County.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='weather in medellin, colombia', output='Home > Colombia > Climate MedellÃ\\xadn. Medellin in Colombia officially has a tropical monsoon climate. The driest period is from December until February. Medellin has an average temperature ranging between 15 and 30 degrees Celcius (59 and 86 degrees Fahrenheit) which means lower temperatures than other cities with a tropical monsoon climate.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='weather in medellin, colombia', output='Going to Medellin? Find the best month to visit Medellin based on the weather. Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec All months. Expect 23Â°C daytime maximum temperatures in the shade with on average 4 hours of sunshine per day in Medellin in October. Check more long-term weather averages for Medellin in October before you book your next holiday to Colombia in 2018.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is adiabatic?', output='Adiabatic cooling is the process of reducing heat through a change in air pressure caused by volume expansion. In data centers and other facilities, adiabatic processes have enabled free cooling methods, which use freely available natural phenomena to regulate temperature. The principle of adiabatic cooling has been harnessed to develop cooling units that can save industry millions of liters of water.')>.\n",
      "/opt/anaconda3/envs/trulens/lib/python3.11/site-packages/trulens/feedback/llm_provider.py:289: UserWarning: No supporting evidence provided. Returning score only.\n",
      "  warnings.warn(\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is adiabatic?', output='A rising parcel of air that does NOT exchange heat with its surroundings is referred to as What is an adiabatic process? -The term from process in which no heat energy is added or removed, but temp changes')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how much does a lipid panel cost', output=\"The charge for a lipid panel ranged from $10 to $10,169. Hospital prices for a basic metabolic panel (which doctors use to measure the body's metabolism) were $35 at one facility â\\x80\\x94 and $7,303 at another.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how much does a lipid panel cost', output='Typical costs: 1  Lip augmentation typically costs about $1,500 to $3,000 per lip for a total of $3,000 to $6,000 for both lips.  Lip augmentation is considered cosmetic surgery, so it typically is not covered by health insurance.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is a nrr noise reduction rating mean', output='NRR STANDS FOR NOISE REDUCTION RATING. NRR (noise reduction rating) is a single number rating method which describes a hearing protection device (HPD) based on how much the overall noise level is reduced by the HPD. Most noise measurements in the industry are derived from the A-weighting scale.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is a nrr noise reduction rating mean', output='The effectiveness of a hearing protection earplug or earmuff is specified by a Noise Reduction Rating (NRR). The typical range for hearing protection is from 15 to 35 decibels of sound attenuation. The higher the NRR is, the more noise attenuation it provides, as long as it is also a good fit in the ear.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='fibroid symptoms after menopause', output='Some of the uterine fibroid symptoms that may still persist after menopause include: 1  Pelvic pressure: You may continue to experience severe pain similar to cramps before a period.  Urinary urgency: If the fibroids are pushing on the bladder, it can cause you to have to urinate frequently.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='fibroid symptoms after menopause', output='Often, you can have fibroids and not have any symptoms. Your health care provider may find them during a physical exam or other test. Fibroids often shrink and cause no symptoms in women who have gone through menopause. A recent study also showed that some small fibroids shrink in premenopausal women.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what are the requirements for neurologist', output='Neurologist Education Requirements. 1  One has to complete an undergraduate degree before applying for neurology courses. 2  This requires four years of medical school. 3  One can become a Medical Doctor or a Doctor of Osteopathy.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what are the requirements for neurologist', output=\"Earning a bachelor's degree is the first step toward becoming a neurologist. There is no specific major required for undergraduate study. However, aspiring neurologists may benefit from concentrating their studies in biological sciences, chemistry, physics or pre-med to meet admission requirements for medical school.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='meaning of ..bann..', output='Meaning: An official prohibition or edict against something. Classified under: Nouns denoting communicative processes and contents. Synonyms: forbidding; banning; forbiddance; ban. Hypernyms (ban is a kind of...): prohibition (refusal to approve or assent to) Hyponyms (each of the following is a kind of ban):')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='meaning of ..bann..', output='verb (used with object), banned, banning. 1. to prohibit, forbid, or bar; interdict: to ban nuclear weapons; The dictator banned all newspapers and books that criticized his regime. 2. Archaic. to pronounce an ecclesiastical curse upon. to curse; execrate.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='watts & browning engineers', output='Watts & Browning Engineers is located in Marietta, Georgia. This organization primarily operates in the Consulting Engineer business / industry within the Engineering, Accounting, Research, and Management Services sector. This organization has been operating for approximately 72 years.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='watts & browning engineers', output='1 John Moses Browning was the prolific gun designer who invented the Winchester rifle (30/30), the pump shotgun, and the Colt 45 automatic. 2  He is best known for his automatic pistols and was the first one to invent the slide, which encloses the barrel of a pistol and the firing mechanism.3  John Moses Browning. The breech-loading rifle was invented by Captain Patrick Ferguson of Pitfours, Scotland. 2  John Moses Browning-Winchester Rifle. 3  John Moses Browning was the prolific gun designer who invented the Winchester rifle (30/30), the pump shotgun, and the Colt 45 automatic.')>.\n",
      "/opt/anaconda3/envs/trulens/lib/python3.11/site-packages/trulens/feedback/llm_provider.py:289: UserWarning: No supporting evidence provided. Returning score only.\n",
      "  warnings.warn(\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how long was jack denison dorothy dandridge married', output='Dorothy Dandridge and Jack Denison. Jack Denison and Dorothy Dandridge were married for 3 years. They dated for 1 year after getting together in Jun 1958 and married on 22nd Jun 1959. 3 years later they divorced on 20th Dec 1962.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how long was jack denison dorothy dandridge married', output='By the time he was five, his parents were divorced and Jack had married actress Shirley Jones, an actress who in 1955 had just made Oklahoma! (1955). When David was about 10, his mother moved to California from New Jersey.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='literal define', output='Literal language refers to the use of words solely by their defined or primary meanings. It is contrasted with figurative language, which identifies the use of words in figures of speech to convey meanings different from their usual definitions.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='literal define', output='A literal is a value that has been hard-coded directly into your source. For example: string x = This is a literal; int y = 2; // so is 2, but not y int z = y + 4; // y and z are not literals, but 4 is int a = 1 + 2; // 1 + 2 is not a literal (it is an expression), but 1 and 2 considered separately are literals Some literals can have a special syntax, so you know what type the literal is:')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does lease to purchase on car mean?', output='Leasing a vehicle basically means you borrow the vehicle from the dealership for a fixed amount to pay every month you lease it. The most common lease I believe is 3 years. You also have a certain amount of miles you have to stay under for the time you are leasing the car.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does lease to purchase on car mean?', output='A leased car is actually owned by the leasing company. To trade in a lease, the dealer for the new car must pay off the lease termination cost to the leasing company. The dealer will allow the wholesale value of the car as a trade credit and the cost to terminate the lease will be charged against that credit.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='steering define', output='steering - the act of setting and holding a course; a new council was installed under the direction of the king guidance, direction driving - the act of controlling and steering the movement of a vehicle or animal')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='steering define', output='the discriminatory practice by a real estate agent of maneuvering a client from a minority group away from considering a home in a white neighborhood. Origin of steering.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does fb mean on a ring', output=\"Best Answer: The .925 does mean sterling silver and the FB may be a Hallmark. You did not state what is on the medallion so I don't know where it was made. On some silver items the FB stands for Franz Bibus a Czech silversmith.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does fb mean on a ring', output='Answered by The Community. Making the world better, one answer at a time. FBS stands for Football Bowl Subdivision, formerly called Division I-A. This is in contrast to the FCS, or Football Championship Subdivision, formerly known as Division I-AA.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what causes elevation in ast (sgot)', output='Chronic and acute alcohol use also can commonly cause abnormal liver blood tests. In alcoholic hepatitis, the range of liver tests can vary greatly. In chronic alcohol liver disease or alcoholic cirrhosis, slight elevation of ALT and AST may be observed, whereas in acute alcoholic hepatitis, high liver enzyme numbers are often seen.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what causes elevation in ast (sgot)', output='1 Common causes of elevated ALT and AST are. 2  viral liver infections, 3  alcohol abuse, 4  cirrhosis (from any chronic causes), 5  hemochromatosis (iron overload), shock, and/or. 6  heart failure.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is garlic used for', output='Garlic (Allium sativum), a herb used widely as a flavoring in cooking, has also been used as a medicine throughout ancient and modern history to prevent and treat a wide range of conditions and diseases. Garlic belongs to the onion genus Allium, and is closely related to the onion, rakkyo, chive, leek, and shallot.arlic is a popular ingredient in cooking and may also have some health benefits. The original Olympic athletes in Ancient Greece were given garlic-possibly the earliest example of performance enhancing agents used in sports.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is garlic used for', output='Garlic: Health Benefits, Therapeutic Benefits. Garlic (Allium sativum), a herb used widely as a flavoring in cooking, has also been used as a medicine throughout ancient and modern history to prevent and treat a wide range of conditions and diseases. Garlic belongs to the onion genus Allium, and is closely related to the onion, rakkyo, chive, leek, and shallot. It has been used by humans for thousands of years and was used in Ancient Egypt for both culinary purposes and its health and therapeutic benefits.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='definition of legal term injunctive relief', output='Injunction Law and Legal Definition. Injunctive relief consists of a court order called an injunction, requiring an individual to do or not do a specific action. It is an extraordinary remedy that courts utilize in special cases where preservation of the status quo or taking some specific action is required in order to prevent possible injustice.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='definition of legal term injunctive relief', output='Permanent Injunctions. There is no requirement that a party seeking permanent injunctive relief first request either ex parte or preliminary relief. A permanent injunction may be sought as part of the full trial on the merits in an action, regardless of the outcome of prior proceedings in the case.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how many seconds should check for breathing', output='Confidence votes 277. Breathing should be checked for 10 seconds, in that time you should be looking and checking for 3 things: 1- The rising of the chest-which would suggest the casualty is breathing. 2- Listening for breathing by the mouth-not to be confused with the alike sound of gasping in which the body sometimes does when someone is unconscious and not breathing.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how many seconds should check for breathing', output=\"That's approximately 7-8 breaths per minute. But the pause length is critical. The pause is the resting phase. It is where the body gets a moments respite. Shortened pauses mean shortened or non-existent rests. So if you have an 8 second breath per minute count you should have approximately 2-3 second pauses.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does the term bar none mean', output='Confidence votes 2.8K. Bar none means excepting nothing or no one, as in He is the best looking man I have ever seen--bar none--even actors and male models!. <i>Bar none</i> means excepting nothing or no one, as in He is the best looking man I have ever seen--bar none--even actors and male models!. Minor edit? Save Cancel.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does the term bar none mean', output='bar none. with no exceptions. (Follows an assertion.) This is the best of all, bar none. See also: bar, none. bar none. without omitting anyone or anything Terrell is the best player in the division, bar none. Usage notes: used when comparing someone or something to all others of the same type.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is a mini whoodle', output='The Whoodle is a cross between the Soft-Coated Wheaten Terrier and the Poodle which comes in three sizes â\\x80\\x93 toy, miniature, and standard. The Whoodle, then, also comes in three sizes â\\x80\\x93 miniature, medium, and standard.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is a mini whoodle', output='Description: The Whoodle is a medium cross breed known for being affectionate, energetic, friendly, intelligent, loving, and playful. It is the cross of the Soft Coated Wheaten Terrier and Poodle.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='is frida kahlo mexican', output='With slim sable brushes, Frida Kahlo painstakingly rendered her bold unibrow and mustache in dozens of self-portraits. This same Frida also shaved three years off her age, claiming 1910 to be the year she was born in Coyoac n, Mexico, instead of 1907.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='is frida kahlo mexican', output=\"Frida Kahlo Fans. Chronology. On May 12th, 1891, at age 19, Wilhelm Kahl, Frida's father, a Jew of Hungarian-German origin, sails from Germany to Mexico aboard the freighter Borussia. He changes his German name Wilhelm Kahl to a more Spanish sounding name, Guillermo Kahlo and trades his Jewish religion for atheism.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what causes everything to taste sugary', output='Diabetic neuropathy can also cause this problem of experiencing sweet sensation in the mouth. Treatment : You need to consult your doctor if you start sensing constant sweet taste in your mouth. He would find out the underlying cause for this problem which is only a symptom of some other disease.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what causes everything to taste sugary', output='Sugar-Free Desserts. Foods that have been sweetened with sugar alcohols such as mannitol, maltitol and sorbitol can cause gas (and bloating) because these alcohols are hard for your body to digest. Check the ingredients of foods labeled low sugar or sugar-free, as they often contain these sugar alcohols.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what color is venus planet', output='Mercury: grey. Venus: yellowish-white. Earth: light blue with white clouds; oceans and light scattered by the atmosphere make Earth prevailingly blue. Mars: red-orange. Jupiter: has orange and white bands; the white bands are colored by ammonia clouds, while the orange comes from ammonium hydrosulfide clouds.Saturn: pale yellow; ammonia haze covers the whole planet and obscures redder clouds below. Uranus: light blue; the color comes from methane clouds.arth: light blue with white clouds; oceans and light scattered by the atmosphere make Earth prevailingly blue. Mars: red-orange. Jupiter: has orange and white bands; the white bands are colored by ammonia clouds, while the orange comes from ammonium hydrosulfide clouds.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what color is venus planet', output='Mercury: grey. Venus: yellowish-white. Earth: light blue with white clouds; oceans and light scattered by the atmosphere make Earth prevailingly blue. Mars: red-orange. Jupiter: has orange and white bands; the white bands are colored by ammonia clouds, while the orange comes from ammonium hydrosulfide clouds.Saturn: pale yellow; ammonia haze covers the whole planet and obscures redder clouds below. Uranus: light blue; the color comes from methane clouds.arth: light blue with white clouds; oceans and light scattered by the atmosphere make Earth prevailingly blue. Mars: red-orange. Jupiter: has orange and white bands; the white bands are colored by ammonia clouds, while the orange comes from ammonium hydrosulfide clouds.')>.\n",
      "/opt/anaconda3/envs/trulens/lib/python3.11/site-packages/trulens/feedback/llm_provider.py:289: UserWarning: No supporting evidence provided. Returning score only.\n",
      "  warnings.warn(\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what class lever is a nutcracker', output='In a lever when the load is between fulcrum and effort, it is classified as a Type 2 Lever. A nutcracker is one example. Observing a nutcracker in action, it... In a lever when the load is between fulcrum and effort, it is classified as a Type 2 Lever. A nutcracker is one example. Observing a nutcracker in action, it is clear that only point that does not move, i.e. the fulcrum, is at the end of the instrument where the two blades meet..')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what class lever is a nutcracker', output='Here are some everyday examples of class 2 levers: The wheelbarrow is the classic example of a class 2 lever. (Image from Microsoft Clip Organizer) A nutcracker is a combination of two class 2 levers. (Image from Microsoft Clip Organizer)')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does the name halima mean', output='Egyptian Meaning: The name Halima is an Egyptian baby name. In Egyptian the meaning of the name Halima is: Gentle. Muslim Meaning: The name Halima is a Muslim baby name. In Muslim the meaning of the name Halima is: Gentle. Patient. Mild.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does the name halima mean', output=\"Biblical Meaning: The name Jemima is a Biblical baby name. In Biblical the meaning of the name Jemima is: Handsome as the day. Hebrew Meaning: The name Jemima is a Hebrew baby name. In Hebrew the meaning of the name Jemima is: Dove.Little dove. In the Old Testament, Jemima was one of the three beautiful daughters of Job (the other two were Keziah and Keren).he name Jemima is an American baby name. In American the meaning of the name Jemima is: Little dove. In the bible one of Job's three daughters known as the most beautiful women of their time: (the other two were Keziah and Keren).\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='meds that can cause irregular heartbeat', output='This list is incomplete and various other drugs or substances may cause your symptoms (see Irregular heartbeat). Always advise your doctor of any medications or treatments you are using, including prescription, over-the-counter, supplements, herbal or alternative treatments. 1  Aldazine. 2  Amphetamine Sulfate. 3  Anatensol.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='meds that can cause irregular heartbeat', output='Though this medication often gives great benefits to people with irregular heartbeat, it may rarely cause a serious new irregular heartbeat. Therefore, when starting treatment with this drug, your doctor may recommend that you stay in the hospital for proper monitoring.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how much does a ballet dancer make', output='Best Answer: It does vary. However in a Major Ballet company the starting figure is around $30,000 per year. *Note to anonymous..... I did not give the rate for a regional or a small national company, or for an apprentice.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how much does a ballet dancer make', output='The average annual salary of a professional dancer in 2010 is $21,600 per year. The total salary range of dancers in 2010 is between $13,000 and $54,000 per year. Most dancers earn a salary between $15,000 and $35,000 per year.As with any profession, highly skilled Professional dancers make a good living individuals can easily break the mold and earn salaries well out of these ranges. A top earning professional dancer can take in more than six figures.s with any profession, highly skilled Professional dancers make a good living individuals can easily break the mold and earn salaries well out of these ranges. A top earning professional dancer can take in more than six figures.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='highest enlisted marine female', output='Averaging all four military branches, women officers represent. 7 percent at the rank of Admiral, and 15.5 percent of officers overall. At the rank of Officer W-1 or above, women are most highly represented in the Air Force. (18.5%) and least represented in the Marine Corps (5.8%). This pattern appears the same at the rank of E9 or below. The Air Force has the largest proportion of. women at the rank of E9 or below (19.7%) and the Marine Corps has the smallest proportion of women enlisted personnel (6.4%). The positive finding is that. womenâ\\x80\\x99s percentage in officer ranks in the U.S. military services roughly equals their percentage in enlisted ranks.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='highest enlisted marine female', output=\"If 200 E-5s get out of the Marine Corps in a particular MOS, then 200 E-4s can be promoted to E-5. The Marine Corps has 154,348 enlisted members on active duty. Here's how it breaks down, by enlisted rank: 1  Private (E-1) - 9,671 (6.3%). 2  Private First Class (E-2) - 20,625 (13.4%). 3  Lance Corporal (E-3) - 43,141 (30.0%).\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='cyrillic alphabet definition', output='For national variants of the Cyrillic script, see Cyrillic alphabets. For other uses, see Cyrillic (disambiguation). The Cyrillic script /sáµ»Ë\\x88rÉªlÉªk/ is a writing system used for various alphabets across eastern Europe and north and central Asia. It is based on the Early Cyrillic, which was developed in the First Bulgarian Empire during the 9th century AD at the Preslav Literary School.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='cyrillic alphabet definition', output='The Cyrillic alphabet, which is used by many countries in Eastern Europe-like Russia, Serbia, and Bulgaria-is based on the ancient Greek alphabet. Modern Greek, Hebrew, and Arabic all have their own alphabets. (Hebrew and Arabic are written from right to left.)')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='spousal roth ira contribution after age 70', output='No contributions allowed after age 70-1/2. None. Annual Earned Income Requirements. Total traditional IRA contribution amount for both spouses cannot exceed total amount of your joint taxable income, or double the annual IRA limit, whichever is less.bility to deduct contributions to a traditional IRA may be limited or entirely eliminated if the working spouse contributes to an employer-sponsored plan. No effect on eligibility for spousal Roth IRA. Age Limits on Contributions Yes. Contributions are not permitted for a nonworking spouse beyond age 70 1/2.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='spousal roth ira contribution after age 70', output='A Roth IRA is an IRA that, except as explained below, is subject to the rules that apply to a traditional IRA. You cannot deduct contributions to a Roth IRA. If you satisfy the requirements, qualified distributions are tax-free. You can make contributions to your Roth IRA after you reach age 70 Â½.You can leave amounts in your Roth IRA as long as you live. The account or annuity must be designated as a Roth IRA when it is set up.The same combined contribution limit applies to all of your Roth and traditional IRAs.ou can make contributions to your Roth IRA after you reach age 70 Â½. You can leave amounts in your Roth IRA as long as you live. The account or annuity must be designated as a Roth IRA when it is set up.')>.\n",
      "/opt/anaconda3/envs/trulens/lib/python3.11/site-packages/trulens/feedback/llm_provider.py:289: UserWarning: No supporting evidence provided. Returning score only.\n",
      "  warnings.warn(\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='typical retirement fers', output='The Federal Employees Retirement System (FERS) is the retirement system for employees within the United States civil service. FERS became effective January 1, 1987 to replace the Civil Service Retirement System (CSRS) and to conform federal retirement plans in line with those in the private sector. Contents.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='typical retirement fers', output='1. Overview of FERS and Types of Retirement The Federal Employees Retirement System (FERS) is the newer of the two retirement systems that cover many federal and U.S. Postal Service (USPS) employees. Most of these employees began working for the federal government or USPS after Jan. 1, 1984. Some employees covered by the Civil Service Retirement System (CSRS), the older of the two retirement systems, voluntarily elected to transfer to FERS during one of the FERS â\\x80\\x9copen seasonsâ\\x80\\x9d in 1987 and 1998. FERS is categorized as a federal government-sponsored defined benefit retirement plan.')>.\n",
      "/opt/anaconda3/envs/trulens/lib/python3.11/site-packages/trulens/feedback/llm_provider.py:289: UserWarning: No supporting evidence provided. Returning score only.\n",
      "  warnings.warn(\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does the name asia mean', output=\"Asia [asia] as a girls' name is pronounced AY-zhah. It is of Greek origin, and the meaning of Asia is sunrise. May also derive from Assyrian asu meaning east. Modern name, usually used in reference to the continent. Also sometimes used as a short form of a name ending with -ia, such as Aspasia. According to the Koran, Asia was the name of the Pharoah's wife who raised the infant Moses. Asia is also a variant of Aisha, the name of Muhammad's favorite wife, one of the four perfect women. See also Fatima, Khadija, and Mary.\")>.\n",
      "/opt/anaconda3/envs/trulens/lib/python3.11/site-packages/trulens/feedback/llm_provider.py:289: UserWarning: No supporting evidence provided. Returning score only.\n",
      "  warnings.warn(\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does the name asia mean', output=\"Asia (/Ë\\x88eÉªÊ\\x92É\\x99/ or /Ë\\x88eÉªÊ\\x83É\\x99/) is the Earth's largest and most populous continent, located primarily in the eastern and northern hemispheres.sia is the largest continent on Earth. It covers 8.8% of the Earth's total surface area (or 30% of its land area), and has the largest coastline, at 62,800 kilometres (39,022 mi). Asia is generally defined as comprising the eastern four-fifths of Eurasia.\")>.\n",
      "/opt/anaconda3/envs/trulens/lib/python3.11/site-packages/trulens/feedback/llm_provider.py:289: UserWarning: No supporting evidence provided. Returning score only.\n",
      "  warnings.warn(\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what cell type is responsible for producing current?', output='A common dry cell is the zincâ\\x80\\x93carbon battery, sometimes called the dry Leclanche leclanchÃ©, cell with a nominal voltage of.1 5, volts the same as the alkaline (battery since both use the same â\\x80\\x93 zinc manganese dioxide). combination')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what cell type is responsible for producing current?', output='Electrochemical cells which generate an electric current are called voltaic cells or galvanic cells, and common batteries consist of one or more such cells. In other electrochemical cells an externally supplied electric current is used to drive a chemical reaction which would not occur spontaneously.or a battery, this is the positive terminal. In order for the voltaic cell to continue to produce an external electric current, there must be a movement of the sulfate ions in solution from the right to the left to balance the electron flow in the external circuit.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what causes excel file corruption', output='Common reasons for Ms excel corruption. Improper shutdown of computer â\\x80\\x93 Basically the files size of Excel are large therefore if there is any improper shutdown of system occur there are chances that your open Ms excel can easily corrupt. The improper shut down can occur due to power failure or any other reason.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what causes excel file corruption', output='This Microsoft Excel Error You Cannot Change Part Of An Array may be caused by a missing file of a particular program thatâ\\x80\\x99s not yet fully installed or just a missing system file. The former is usually solvable through installing the software again.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='infrax system', output='Become an Infrax Systems Partner Infrax Systems is seeking qualified, highly motivated System Integrators, Value Added Resellers (VARs) and Distributors to join the Infrax team in delivering State-of-the-art equipment and unbeatable value to the growing wireless backhaul and access markets.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='infrax system', output='Infrax Partners will have the resources and expertise to provide: 1  Design, installation and maintenance services for existing and newly installed networks. 2  Value added network and RF engineering services. 3  Field level support and RMA diagnosis. 4  Telecommunications engineering, furnishing & installation (EF&I) services.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is a virtual interface', output='A virtual interface is a pointer to an actual interface in SystemVerilog. It is most often used in classes to provide a connection point to allow classes to access the signals in the interface through the virtual interface pointer. You can see some examples of how to use virtual interfaces in the UVM Cookbook.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is a virtual interface', output='User interface (UI) is a broad term for any system, either physical or software based, that allows a user to connect with a given technology. Many different kinds of user interfaces come with various devices and software programs. Many of them have some basic similarities, although each one is unique in key ways.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input=\"what ethnicity is darren criss' mom?\", output=\"Darren's mother is from Cebu in the Philippines, and has Filipino, Chinese, and Spanish ancestry. Darren's father is from Pittsburgh, Pennsylvania, and has English, and some German and Northern Irish, ancestry.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input=\"what ethnicity is darren criss' mom?\", output='Darren Criss was born on February 5, 1987 in San Francisco, California, USA as Darren Everett Criss. He is an actor, known for Glee (2009), The Tale of the Princess Kaguya (2013) and A Very Potter Musical (2009).')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how was the esperanza fire started', output='The Esperanza Fire started on Oct. 26, 2006, in Cabazon, Calif., about two hours north of San Diego. Driven by Santa Ana winds, it burned 41,000 acres and destroyed 34 homes. An investigation later determined that the fire was set by an arsonist, and in 2009 Raymond Lee Oyler was sentenced to death for the crime.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how was the esperanza fire started', output='The Esperanza Fire is available on Amazon.com or directly from the author. When a jury returns to a packed courtroom to announce its verdict in a capital murder case, every noise -- even a scraped chair or an opening door -- resonates like a high-tension cable snap.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does whistling dixie mean', output=\"Rating Newest Oldest. Best Answer: You are correct. Whistling Dixie is a slang term that means unrealistic fantasizing. The phrase, You ain't just whistling Dixie means you're saying something truthful and serious. During the American Civil War, a popular song was Dixieland, which told of the virtues of living in the south.\")>.\n",
      "/opt/anaconda3/envs/trulens/lib/python3.11/site-packages/trulens/feedback/llm_provider.py:289: UserWarning: No supporting evidence provided. Returning score only.\n",
      "  warnings.warn(\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does whistling dixie mean', output='Dixieland is the name given to early jazz by traditional jazz revivalists, starting in the 1940s and 1950s. The name is a reference to the Old South, specifically anything south of the Mason-Dixon line.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how does a bump key work', output='Bumping a Lock. 1  1. Insert the key into the lock. Fit the key into the lock. 2  2. Begin turning the key. Try turning the key in the unlock direction (usually counterclockwise). 3  3. Strike the back of the key with a blunt object.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how does a bump key work', output=\"Scroll down for video. German engineers have revealed designs to make 3D-printed 'bump keys' (shown). These devices can be used to break into popular pin tumbler design locks. In a video a lock is opened by a bump key and a hammer in seconds. To make the key all that is needed is a picture of the lock and pin positions. Later this month at an event known as LockCon 2014 German engineer Christian Holler will present his and Jos Weyers' bump key design.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='is gwen stefani italian', output='Gwen Stefani. Gwen RenÃ©e Stefani (/É¡wÉ\\x9bn Ë\\x88stÉ\\x99Ë\\x88fÉ\\x91Ë\\x90ni/; born October 3, 1969) is an American singer, songwriter, fashion designer, actress, and television personality.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='is gwen stefani italian', output='Answer by Kayla Krueger. Confidence votes 460. Stefani Germanotta (a/k/a Lady Gaga) attended the Convent of the Sacred Heart School, a Catholic School in upper Manhattan in New York City, the same High School as Paris Hilton and Nicky Hilton.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what causes exhaust hanger to fail', output=\"Have your vehicle's exhaust system inspected periodically to keep your car safe. Loose, missing or damaged hangers or brackets can allow the exhaust system to shift around, causing a possible clunking or rubbing sound while driving. A loose exhaust system, when hot, can also melt wiring, nylon lines or rubber hoses.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what causes exhaust hanger to fail', output='The catalytic converter is sensitive to changes in the contents and temperature of the exhaust gases, so another possible cause for failure is if the cylinder head gasket is damaged, allowing oil or coolant to get into the combustion chamber and be burned in the cylinders.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what county is fine ny in', output='Fine, New York. Fine is a town in St. Lawrence County, New York, United States. The population was 1,512 at the 2010 census. The town is named after Judge John Fine, a businessman involved in settling the town. The Town of Fine is in the south part of the county, south of Canton.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what county is fine ny in', output=\"There are currently 146 red-hot tax lien listings in Niagara County NY. These tax foreclosed homes are available for pennies on the dollar - as much as 75 percent off full market price (and more)! Enjoy the pride of homeownership for less than it costs to rent before it's too late.\")>.\n",
      "/opt/anaconda3/envs/trulens/lib/python3.11/site-packages/trulens/feedback/llm_provider.py:289: UserWarning: No supporting evidence provided. Returning score only.\n",
      "  warnings.warn(\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what can cause a metallic smell', output='Metallic taste or other bad taste in the mouth. Sometimes our normal sense of taste can temporarily change, causing a metallic or other bad taste in the mouth. This may occur for a number of reasons. Tastes are detected within our mouth by special structures known as taste buds.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what can cause a metallic smell', output='It seems like it is a mix of flavor and smell that causes the metallic taste. Ferrous II iron causes the strongest metallic flavor according to the study; the amount of iron in your blood is far above the your thresholds for taste and smell. Thank you for the serious reply.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what are garlic supplements good for', output='It is best known as a flavoring for food. But over the years, garlic has been used as a medicine to prevent or treat a wide range of diseases and conditions. The fresh clove or supplements made from the clove are used for medicine. Garlic is used for many conditions related to the heart and blood system.These conditions include high blood pressure, high cholesterol, coronary heart disease, heart attack, and â\\x80\\x9chardening of the arteriesâ\\x80\\x9d (atherosclerosis).arlic has been used safely in research for up to 7 years. When taken by mouth, garlic can cause bad breath, a burning sensation in the mouth or stomach, heartburn, gas, nausea, vomiting, body odor, and diarrhea.')>.\n",
      "/opt/anaconda3/envs/trulens/lib/python3.11/site-packages/trulens/feedback/llm_provider.py:289: UserWarning: No supporting evidence provided. Returning score only.\n",
      "  warnings.warn(\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what are garlic supplements good for', output='Medical Uses for Garlic. Garlic is classified as both a food and medicinal herb. It can and should be eaten as food, but it can also be taken in supplement form to augment a healthy diet when more serious health problems arise. Garlic has many healing properties, but the most research has been done on its potential to help reduce heart disease.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what county is lochbuie co in', output='Lochbuie, Colorado. The Town of Lochbuie (/Ë\\x8clÉ\\x92kbjui/ lock-BYOU-ee) is a Statutory Town located in Weld and Adams counties in the U.S. state of Colorado. The town population was 4726 at the 2010 United States Census. Contents.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what county is lochbuie co in', output='The county is bounded by Lake Erie to the west, Niagara County and Canada to the north, Genesee County and Wyoming County to the east, and Cattaraugus and Chautauqua Counties to the south.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='vde definition', output='VDE-Computer Definition. (1) (V ideo D isplay E ditor) A WordStar and WordPerfect-compatible shareware word processor written by Eric Meyer. (2) (V erband D eutscher E lektrotechniker) The German counterpart of the U.S. Underwriters Lab.Computer Desktop Encyclopedia THIS DEFINITION IS FOR PERSONAL USE ONLY All other reproduction is strictly prohibited without permission from the publisher.2) (V erband D eutscher E lektrotechniker) The German counterpart of the U.S. Underwriters Lab. Computer Desktop Encyclopedia THIS DEFINITION IS FOR PERSONAL USE ONLY All other reproduction is strictly prohibited without permission from the publisher.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='vde definition', output='Custom Search. KDE is a graphical desktop environment for Unix and Unix-like operating systems such as Linux. In addition to a basic desktop, KDE includes many popular types of software including a text editor, a web browser, an office suite, an email client, a PIM suite, and much more. KDE itself, and all the software distributed with KDE, have been licenced under the GPL free software licence since KDE version 2.0 in late 2000.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='definition chemical', output='A chemical element is a pure substance which is composed of a single type of atom, characterized by its particular number of protons in the nuclei of its atoms, known as the atomic number and represented by the symbol Z. The mass number is the sum of the number of protons and neutrons in a nucleus.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='definition chemical', output=\"Learner's definition of CHEMICAL. 1. : of or relating to chemistry. a chemical analysis. chemical elements/bonds. They studied the compound's chemical structure/composition/properties. a chemical laboratory.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='difference between hep b and c', output='In addition to the differences in replication method, transmission risk is higher with Hep B as it can be sexually transmitted, whereas generally not with Hep C. The large majority of adult patients exposed to Hep B clear it, whereas with Hep C adults who are exposed and clear are the minority.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='difference between hep b and c', output='In addition to the differences in replication method, transmission risk is higher with Hep B as it can be sexually transmitted, whereas generally not with Hep C. The large majority of adult patients exposed to Hep B clear it, whereas with Hep C adults who are exposed and clear are the minority.')>.\n",
      "/opt/anaconda3/envs/trulens/lib/python3.11/site-packages/trulens/feedback/llm_provider.py:289: UserWarning: No supporting evidence provided. Returning score only.\n",
      "  warnings.warn(\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what causes extreme swelling in just one leg?', output='Obstruction of veins draining the leg on one side of the body can cause edema in one leg. A blood clot in the leg, known as a DVT for deep vein thrombosis, is one possible cause of venous obstruction. A DVT is more common when a person is inactive for extended periods, such as during a long flight.')>.\n",
      "/opt/anaconda3/envs/trulens/lib/python3.11/site-packages/trulens/feedback/llm_provider.py:289: UserWarning: No supporting evidence provided. Returning score only.\n",
      "  warnings.warn(\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what causes extreme swelling in just one leg?', output='Leg swelling generally occurs because of an abnormal accumulation of fluid in the tissues of the lower extremity. The medical term for leg swelling from excessive fluid in the tissues is edema. Persisting indentation of a swollen leg after pressure from a finger is known as pitting edema. Common causes of leg swelling include salt retention, cellulitis, congestive heart failure, and medication side effects.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what citation is used for outside sources', output='APA Style for Citing References. APA style requires two elements for citing outside sources: brief parenthetical citations in the text of a. paper, and a detailed References list at the end. Together, these elements allow your reader to know. and to access the sources you used. For the system to function, all sources cited in the text must appear.')>.\n",
      "/opt/anaconda3/envs/trulens/lib/python3.11/site-packages/trulens/feedback/llm_provider.py:289: UserWarning: No supporting evidence provided. Returning score only.\n",
      "  warnings.warn(\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what citation is used for outside sources', output='Verbal Citations. In writing, when you use outside research, you build your credibility by citing your sources with author lead-ins, parenthetical citations, and a Works Cited page or bibliography. In speeches, you also need to be concerned with your credibility, which you build through verbal citations.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what are the drugs used for moderate sedation', output='Lorazepam (Ativan) is another benzodiazepine that may be used for mild-to-moderate sedation; however, unlike midazolam, its onset of action does not peak until 15-20 minutes after administration. The duration of action of lorazepam is longer (ie, 6-8 h) than that of midazolam (30-60 min).')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what are the drugs used for moderate sedation', output='Oral sedation is one of the available methods of conscious sedation dentistry, along with inhalation sedation (nitrous oxide) and conscious intravenous sedation. Drugs which can be used for sedation include diazepam, triazolam, zaleplon, lorazepam, and hydroxyzine.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is hardie plank', output='Hardie plank siding is a premium brand of cement board siding (also known as fiber cement board siding) produced by the James Hardie Company.ardie plank can be purchased with a wood-grain look, a plain smooth look or a stucco type finish. Hardie plank also comes in simulated cedar shingle planks.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is hardie plank', output='Varieties. Hardie plank can be purchased with a wood-grain look, a plain smooth look or a stucco type finish. Hardie plank also comes in simulated cedar shingle planks. There are a variety of colors available, but you can also buy pre-primed white and paint it with standard house paint.ardie plank can be purchased with a wood-grain look, a plain smooth look or a stucco type finish. Hardie plank also comes in simulated cedar shingle planks.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does ISF mean fluid do', output='Interstitial fluid or tissue fluid is a solution that bathes and surrounds the tissue cells of multicellular animals.ometimes the removal of tissue fluid does not function correctly, and there is a build-up. This can cause swelling, often around the feet and ankles, which is generally known as oedema. The position of swelling is due to the effects of gravity.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does ISF mean fluid do', output='Extracellular fluid (ECF) denotes all body fluid outside the cells. Total body water in humans makes up between 45 to 75% of total body weight. About two thirds of this is intracellular fluid within cells, and one third is the extracellular fluid. The main component of the extracellular fluid is the interstitial fluid that bathes cells.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is a affidavit of support', output='The Affidavit of Support form is a legal contract required by U.S. law between the Petitioner (sponsor) of an immigrant visa applicant(s) and the U.S. government. The sponsor for the visa applicant(s) must establish he or she has certain means to financially support the visa applicant(s), if necessary.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is a affidavit of support', output='The Affidavit of Relationship is the form used to reunite refugees and asylees with close relatives who are determined to be refugees but are outside the United States.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='syracuse minor league baseball', output=\"Syracuse Chiefs. The Syracuse Chiefs are a Minor League Baseball team based in Syracuse, New York. The Triple-A affiliate of the Washington Nationals, the team plays in the International League (IL). NBT Bank Stadium, on Syracuse's northside, is the Chiefs' home stadium.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='syracuse minor league baseball', output=\"Syracuse's 2014-15 basketball schedule. Youth season tickets (age 12 and under) are $225. Full-time Syracuse University students can purchase season tickets for $199. A special package, which includes the remaining five home football games, is available for a limited time at $219.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='current fuel price in delhi', output='Petrol price hiked by 75 paise; diesel by 50 paise. Petrol in Delhi currently costs Rs 71.51 per litre and after the increase it will be priced at Rs 72.43 per litre, including VAT. Petrol price cut by Rs 1.15 per litre; diesel hiked by 50 paise.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='current fuel price in delhi', output='The Lowest Diesel Price in Faridabad is at Indian Oil : 63.32 Rs/Ltr The Highest Diesel Price in Faridabad is at Indian Oil : 63.32 Rs/Ltr Haryana Diesel Prices in other cities')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what causes eyelid itching and swelling', output='The causes are many and can have bacterial infection, seborrheic dermatitis of the face or even a scalp, rosacea, allergies, and in very rare cases, the lice. If the itchy eyelids is brought about by seborrheic dermatitis then you may develop dry, flakey patches of skin on the eyelid.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what causes eyelid itching and swelling', output='Eyelid swelling has many causes (see Table: Some Causes and Features of Eyelid Swelling). It usually results from an eyelid disorder but may result from disorders in and around the eye socket (orbit) or from disorders elsewhere in the body that cause widespread swelling.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='myers-briggs personality test what do the letters stand for', output=\"INTP (introversion, intuition, thinking, perceiving) is an abbreviation used in the publications of the Myersâ\\x80\\x93Briggs Type Indicator (MBTI) to refer to one of the MBTI's 16 personality types.NTP (introversion, intuition, thinking, perceiving) is an abbreviation used in the publications of the Myersâ\\x80\\x93Briggs Type Indicator (MBTI) to refer to one of the MBTI's 16 personality types.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='myers-briggs personality test what do the letters stand for', output='This article is about the Myers-Briggs personality type. For the Socionics ISFj, see Ethical Sensory Introvert. ISFJ (Introversion, Sensing, Feeling, Judging) is an abbreviation used in the publications of the Myers-Briggs Type Indicator (MBTI) to refer to one of sixteen personality types.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is a good home remedy for hemorrhoids', output='Note that if you incorporate some natural hemorrhoid treatments and follow the prevention tips below, your symptoms may very well disappear as the issue reverses. 6 Home Remedies for Hemorrhoids. Apple Cider Vinegar â\\x80\\x93 Using apple cider vinegar is one of the most popular natural treatments for hemorrhoids. Simply soak a cotton ball or some toilet paper with ACV and apply it to the hemorrhoid area. Repeat this process until the inflammation disappears and symptoms resolve.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is a good home remedy for hemorrhoids', output='Coconut oil is one of the good home remedies for hemorrhoids that helps to get relief from hemorrhoids pain and itching. Applying warm coconut oil directly to the infected area is the easiest way to use this remedy. No related posts.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does the last name spratt mean', output='Spratt derives either from the Old English sprit meaning a  slender person or from esprit meaning an elf. Like many Anglo-Saxon names it originated as a nickname before becoming a surname in the modern sense. The earliest records of the name come from the English counties of Dorset, Somerset and Essex.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does the last name spratt mean', output='Sharat is a Hindu baby Girl name, it is an Hindi originated name. Sharat name meaning is A Season and the lucky number associated with Sharat is --. Find all the relevant details about the Sharat Meaning, Origin, Lucky Number and Religion from this page. Sharat meaning has been search 446 four hundred and forty-six times till 25 August, 2017.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='element al', output='Aluminium (British and IUPAC spelling) or aluminum (American spelling) is a chemical element with symbol Al and atomic number 13. It is a silvery-white, soft, nonmagnetic, ductile metal in the boron group.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='element al', output='Alum is a salt that in chemistry is a combination of an alkali metal, such as sodium, potassium, or ammonium and a trivalent metal, such as aluminum, iron, or chromium. The most common form, potassium aluminum sulfate, or potash alum, is one form that has been used in food processing.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what causes feelings of detachment from reality,', output=\"Why This Occurs. Emotional detachment is usually an issue caused by severe, intense anxiety - most notably panic attacks, although any form of severe anxiety can cause emotional detachment. While it's not entirely clear what causes this detachment, it most likely is a coping mechanism for the brain.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what causes feelings of detachment from reality,', output=\"Physical Detachment - Distancing Yourself From Others. Anxiety and stress cause people to feel as though they need to be alone. In fact, one of the most common ways people react to stress is to go home, lay on the couch, and avoid fun activities. Part of that is because of emotional detachment (which we'll get to later), but a bigger part of that is simply because of how difficult it is to feel comfortable with others around you.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='concrete walkway cost per square foot', output='1 A plain concrete walkway costs $4 to $6 per square foot. 2  Adding a single color or simple finish to the concrete raises the price to $6 to $10 per square foot. Expect to pay $8 to $12 per square foot for multiple colors or more elaborate patterning such as stamping or engraving. 2  Fully-custom walkways with a high level of hand-applied details (such as chemical stains or stenciling) cost $12 to $15 and up.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='concrete walkway cost per square foot', output='The cost of a concrete walkway can vary widely depending on its size and finish. Most U.S. homeowners pay between $6 and $12 per square foot for a new walkway. Prices can go beyond this if significant grading is required or a complex decorative finish is requested. Basic: $6-10 per square foot.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='armistead meaning', output='What Does Name Armistead Mean Powerful and complete. You are good intellectually and require several outlets for your energies. You are not a builder but a planner, and you want others to carry out your plans.You are bold, independent, inquisitive and interested in research. You know what you want and why you want it.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='armistead meaning', output='At the death of Armisteads widow in 1861, the Star-Spangled Banner was bequeathed to his daughter, Georgiana Armistead Appleton, who recognized that it held national as well as familial significance. As its owner, she permitted the flag to be publicly exhibited on several occasions.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='ross cameron age', output=\"Ross Cameron is 52 years old. To be more precise (and nerdy), the current age as of right now is 18981 days or (even more geeky) 455544 hours. That's a lot of hours!\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='ross cameron age', output='Cameron Boyce (born May 28, 1999) is an American actor and dancer. Boyce starred in the 2008 films Mirrors and Eagle Eye. His first screen appearance was in the Panic at the Disco music video That Green Gentleman as mini Ryan Ross. In the past Boyce has also appeared in Kraft Mac and Cheese commercials.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='reasons why nuclear power is bad', output='The truth is nuclear power is dangerous, costly, and will create more problems that it could ever hope to solve. Here is the truth about nuclear power and seven reasons why nuclear power is bad for the environment and for the nation. http://www.examiner.com/environmental-news-in-national/7-reasons-why-nuclear-power-is-bad-for-the-environment-and-the-nation. Login to post comments.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='reasons why nuclear power is bad', output='Nuclear Energy: Good or Bad? This controversial energy source is back in the spotlight after the crisis at Japanâ\\x80\\x99s Fukushima Daiichi Power Station. Thought of as both a miracle of science and a doomsday wish, nuclear energy has become a widely used and widely contested source of electricity worldwide.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does dealer invoice price mean', output=\"The dealer invoice is the price the manufacturer bills the dealer for the vehicles they purchase, plus any options equipped. This is in essence the dealer's cost, and is not generally shared with the customer.he dealer invoice is the price the manufacturer bills the dealer for the vehicles they purchase, plus any options equipped. This is in essence the dealer's cost, and is not generally shared with the customer.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does dealer invoice price mean', output=\"When youâ\\x80\\x99re trying to find the rock-bottom price on a new car, the dealer invoice price is an enticing bogey. In theory, invoice price is the dealerâ\\x80\\x99s wholesale cost for the car. Buying a car for wholesale may make it seem like you're getting the best price possible. In reality, dealers sell cars for below invoice price every day. Nobody pays invoice price â\\x80\\x93 not you, and not the dealer. When new cars first hit the market, buyers are willing to pay top dollar because they represent the latest in technology and fashion.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='use pi as wired vpn router', output='1 2x USB WiFi dongle(s) â\\x80\\x93 for using the Pi as a wireless router (one dongle connects to the WiFi network and the other creates a personal aces point, with a bridge between them.) An Ethernet cable can be used instead of one dongle if desired.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='use pi as wired vpn router', output='Setting up a Raspberry Pi as an access point in a standalone network The Raspberry Pi can be used as a wireless access point, running a standalone network. This can be done using the inbuilt wireless features of the Raspberry Pi 3 or Raspberry Pi Zero W, or by using a suitable USB wireless dongle that supports access points.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='singular noun definition for kids', output='A singular noun is a word for one person, place, or thing. A plural noun is a word for two or more persons, places, or things. For example: 1  a chair or two chairs. 2  an idea or some ideas. 3  an experience or many experiences. 4  a building or a row of buildings.  one child or two children.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='singular noun definition for kids', output=\"A noun is a word for a person, a place, or a thing. A singular noun is a word for one person, one place, or one thing. A possessive noun is a word that indicates that somethinâ\\x80¦g belongs to that person, place, or thing. A possessive singular noun is a singular noun that indicates that something belongs to that noun. EXAMPLES The doctor's office is on the second floor.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='causes of back pain in female for a week', output=\"Back pain can be a symptom of many different illnesses and conditions. The main cause of the pain can be a problem with the back itself or by a problem in another part of the body. In many cases, doctors can't find a cause for the pain. When a cause is found, common explanations include:\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='causes of back pain in female for a week', output='Upper back pain in women is caused by strain on the muscles and ligaments from poor posture and repetitive.It is a very common health problem that affect so many people both young and old of any gender but not as serious as lower back pain. SEE ALSO: Lower Back Pain in Men, The Causes, Symptoms And Prevention.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='savannah georgia average yearly weather', output=\"Home : Climate Graphs: Georgia : Savannah. Average Temperatures for Savannah. Savannah's coldest month is January when the average temperature overnight is 38.0Â°F. In July, the warmest month, the average day time temperature rises to 92.3Â°F. Average Rainfall for Savannah.\")>.\n",
      "/opt/anaconda3/envs/trulens/lib/python3.11/site-packages/trulens/feedback/llm_provider.py:289: UserWarning: No supporting evidence provided. Returning score only.\n",
      "  warnings.warn(\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='savannah georgia average yearly weather', output='Average Temperatures in Savannah, Georgia, Usa. The mean temperature in Savannah, Georgia, Usa is somewhat warm at 19.1 degrees Celsius (66.4 degrees Fahrenheit). The range of mean monthly temperatures is 18.3 C (32.9F) which is a slightly low range. The range/ variation of daily mean temperatures is 11.4 C (20.5 F). The warmest month (July) is hot with an average temperature of 27.7 degrees Celsius (81.86 degrees Fahrenheit).')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='temperature sorrento italy september', output='Averages for Sorrento in September. September sees temperatures that are slightly milder than preceding months in Sorrento, Italy, although it is still extremely warm. The average daily temperature is 21Â°C, three degrees below the annual maximum that was recorded in July and August. Read more.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='temperature sorrento italy september', output='Averages for Sorrento in September. September sees temperatures that are slightly milder than preceding months in Sorrento, Italy, although it is still extremely warm. The average daily temperature is 21Â°C, three degrees below the annual maximum that was recorded in July and August, but comfortably warm by all standards.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what county is exeter nh', output=\"Exeter, New Hampshire. Exeter is a town in Rockingham County, New Hampshire, United States. The town's population was 14,306 at the 2010 census. Exeter was the county seat until 1997, when county offices were moved to neighboring Brentwood.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what county is exeter nh', output=\"The City of Exeter is the county town; seven other districts of East Devon, Mid Devon, North Devon, South Hams, Teignbridge, Torridge, and West Devon are under the jurisdiction of Devon County Council; Plymouth and Torbay are each a part of Devon but administered as unitary authorities.orth Devon is very rural with few major towns except Barnstaple, Great Torrington, Bideford and Ilfracombe. Devon's Exmoor coast has the highest cliffs in southern Britain, culminating in the Great Hangman, a 318 m (1,043 ft) hog's-back hill with an 250 m (820 ft) cliff-face, located near Combe Martin Bay.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is fms medical term', output='Fibromyalgia (FM) is a medical condition characterised by chronic widespread pain and a heightened pain response to pressure. Other symptoms include feeling tired to a degree that normal activities are affected, sleep problems, and troubles with memory.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is fms medical term', output='How Fibromyalgia Is Diagnosed. By MaryAnn DePietro, CRT. Medically reviewed by Tom Iarocci, MD. May 7th 2016. Fibromyalgia syndrome (FMS), a chronic pain disorder that causes not only widespread pain throughout the body but also excessive fatigue and digestive problems, affects 5 million people in the United States.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is boot screening?', output='A bootsplash, also known as a bootscreen, is a term for a graphical representation of the boot process of the operating system. A bootsplash can be a simple visualisation of the scrolling boot messages in the console, but it can also present graphics or some combinations of both.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is boot screening?', output='In computing, booting (or booting up) is the initialization of a computerized system. The system can be a computer or a computer appliance. The booting process can be hard, e.g., after electrical power to the CPU is switched from off to on (in order to diagnose particular hardware errors), or soft, when those power-on self-tests (POST) can be avoided.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='is acre capitalized', output='In 2014, the actual farmland value was $7,700 per acre while the capitalized value was $8,830 or $1,130 higher than the current value ($234 cash rent / .265 10-year rate). In other words, current cash rents and the 10-year CMT rate together suggest that Illinois farmland prices are not overvalued.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='is acre capitalized', output=\"Alternatively, you may use the converters below to convert any other values. 1 Acre = 43 560 Square Feet. Acre is an imperial and US customary system area unit. In metric system, it's replaced by hectare.The abbreviation is ac. Square foot is an imperial and US customary area unit. In metric system, it's replaced by square meters (metres). The abbreviations are sq ft and ft 2 .lternatively, you may use the converters below to convert any other values. 1 Acre = 43 560 Square Feet. Acre is an imperial and US customary system area unit. In metric system, it's replaced by hectare. The abbreviation is ac. Square foot is an imperial and US customary area unit.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='smoking patches how long should you be on them', output='If you smoke less than 10 cigarettes a day, you should start on Step 2 with a 14mg patch. Use that for six weeks, before moving to Step 3 with a 7mg patch for two weeks. If you need to use NiQuitin (nicotine) Patches for longer than 9 months (3 months for 12-17 year olds) seek advice from a healthcare professional.Wait at least half an hour after your last cigarette before applying the NiQuitin (nicotine) Patch.f you need to use NiQuitin (nicotine) Patches for longer than 9 months (3 months for 12-17 year olds) seek advice from a healthcare professional. Wait at least half an hour after your last cigarette before applying the NiQuitin (nicotine) Patch.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='smoking patches how long should you be on them', output='Start using the patch on the day you plan to stop smoking. If you smoke more than 10 cigarettes per day, start with Step 1. Use this first step for weeks 1-6, then step down to Step 2 for two weeks, and then Step 3 for two weeks. If you smoke 10 or less cigarettes per day, do not use Step 1 (21 mg).')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does it mean when the bottom of your foot is really hot', output='Lee Sanders, DPM, responds: A painfully hot or burning sensation in the feet, especially in middle-aged and older people, could be caused by small fiber neuropathy. Diabetes is the most common cause of this condition; symptoms typically start with burning feet and numb toes.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does it mean when the bottom of your foot is really hot', output='some factors may be causing this heat at the bottom of your feet, 1, there may be a growing blisters on your feet,so you need to get your foot checked by a doctor. 2, your foot is tired and needs some minutes of rest. 3, blood is not circulating properly to your foot and it needs some foot massage. Views.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='oliguric meaning', output='Oliguria is defined as a urine output that is less than 1 mL/kg/h in infants, less than 0.5 mL/kg/h in children, and less than 400 mL daily in adults. It is one of the clinical hallmarks of renal failure and has been used as a criterion for diagnosing and staging acute kidney injury (AKI), previously referred to as acute renal failure.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='oliguric meaning', output='Oliguria is the medical term for a decreased output of urine. Oliguria is considered to be a urinary output of less than 400 milliliters, which is less than about 13.5 ounces, over the course of 24 hours. The absence of urine is known as anuria. Less than... Read more.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='average annual snowfall in jersey', output='Morristown, New Jersey, gets 50 inches of rain per year. The US average is 39. Snowfall is 27 inches. The average US city gets 26 inches of snow per year. The number of days with any measurable precipitation is 80. On average, there are 209 sunny days per year in Morristown, New Jersey. The July high is around 85 degrees. The January low is 20.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='average annual snowfall in jersey', output='A total of 33 cities in the Ern U.S. reported new record low temperatures for the date, and an Atlantic coast storm spread heavy snow from GA to Srn New England. Snowfall totals in NJ ranged up to 24 in Cape May Co, with 19 reported at ACY.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='price for hardwood floor refinishing', output='According to the vendor-neutral, online home source Homewyse, the average professionally completed refinishing job costs $3.32 to $3.70 per square foot in 2014. That figures out to $2,656 to $2,960 to refinish 800 square feet of hardwood floors.his process, also known as â\\x80\\x9cpad and recoat,â\\x80\\x9d costs significantly less than total floor refinishing at around $1,000 to $1,400 for 800 square feet.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='price for hardwood floor refinishing', output='Average Wood Floor Refinishing Costs. The minimum amount that most homeowners will spend on hardwood floor refinishing is $700 up to a maximum of around $2,500.In the mid-range of wood floor refinishing, the majority of consumers will spend $1,200 to $1,600.verage Wood Floor Refinishing Costs. The minimum amount that most homeowners will spend on hardwood floor refinishing is $700 up to a maximum of around $2,500.')>.\n",
      "/opt/anaconda3/envs/trulens/lib/python3.11/site-packages/trulens/feedback/llm_provider.py:289: UserWarning: No supporting evidence provided. Returning score only.\n",
      "  warnings.warn(\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is e. pluribus unum', output='E Pluribus Unum on the great seal of the United States. E pluribus unum is the official motto of the United States. It means out of many, one in Latin. It has been placed on the U.S. dollar bill, and other related items. It is meant to suggest that the U.S. has become a melting pot, a way of speech that means that many people come together as one.')>.\n",
      "/opt/anaconda3/envs/trulens/lib/python3.11/site-packages/trulens/feedback/llm_provider.py:289: UserWarning: No supporting evidence provided. Returning score only.\n",
      "  warnings.warn(\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is e. pluribus unum', output='Answer by Darknight224. Confidence votes 314. E pluribus unum is Latin; it means Out of Many One and refers to the idea that the United States is made up of individual states and populated by individual citizens, but we come together as a single country.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how much do club set bottles cost?', output='Prices reach towards $15,000 for Nebuchadnezzar (15L) sized champagne bottles. Some clubs even have specialty large format champagne at Price $75,000 per bottle. And that is pretty much everything that you need to know about bottle service prices in NYC. Bottle service is a premium experience and that is reflected in the price.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how much do club set bottles cost?', output='Best Answer: Where do you live? Most clubs charge between $5 and $15 dollars for entry, a few are $20 and a few are free. Free clubs usually serve alcohol, non-alcohol clubs almost always have a higher door fee.These are fla prices. Most clubs let women in for free or discounted prices for obvious reasons, although some do not.ut, anywhere from free to $20. (2) You can get dinner if the club has a kitchen or restaurant. Check local listings. ...and (3) Not including dinner, expect to spend anywhere from $200-$300 on drinks, dances and stage tipping.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what causes film creasing during folding process', output='Hence, the amount of delamination initiated during the creasing operation control much of the folding behavior. When the original material properties were used, the amount of delamination initiated by creasing is quite large for all three paperboards.')>.\n",
      "/opt/anaconda3/envs/trulens/lib/python3.11/site-packages/trulens/feedback/llm_provider.py:289: UserWarning: No supporting evidence provided. Returning score only.\n",
      "  warnings.warn(\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what causes film creasing during folding process', output='When coated paperboard is printed, pressed into a groove with a creasing rule and folded, white line cracking occurs along the crease due to intensive mechanical pressure. The cracking will deteriorates product quality and waste resources. Effects of creasing pressure and ink dosage on the foldability of coated board were investigated. It was shown that applying an optimum pressure is important during creasing. When the pressure was too low, the crease formed was not sufficiently deep enough to enable precise folding. When an excess pressure was applied, fiber bonding was destroyed, resulted in unsatisfactory cracking.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is a lease?', output='A typical lease spells out all of the terms involved in a land or merchandise rental agreement, including the length of time a lessee may use it and what condition it must be in upon return to the lessor. The amount of payments and any financial penalties for late payments may also be included in a contract.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is a lease?', output='A lease is a legal document or contract between a landowner (lessor) and a company or individual (lessee) granting exploration and development rights to subsurface oil and gas deposits. Companies will usually present you with a preprinted or standard lease.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='meaning of a discrete concept', output='Discrete mathematics. Discrete mathematics is the study of mathematical structures that are fundamentally discrete rather than continuous. In contrast to real numbers that have the property of varying smoothly, the objects studied in discrete mathematics â\\x80\\x93 such as integers, graphs, and statements in logic â\\x80\\x93 do not vary smoothly in this way, but have distinct, separated values. Discrete mathematics therefore excludes topics in continuous mathematics such as calculus and analysis. Discrete objects can often ...')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='meaning of a discrete concept', output='However, there is no exact definition of the term discrete mathematics.. Indeed, discrete mathematics is described less by what is included than by what is excluded: continuously varying quantities and related notions. The set of objects studied in discrete mathematics can be finite or infinite. The term finite mathematics is sometimes applied to parts of the field of discrete mathematics that deals with finite sets, particularly those areas relevant to business.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='va choice provider phone number', output='Veterans seeking to use the Veterans Choice Program or wanting to know more about it, can call1-866-606-8198 to confirm their eligibility and to schedule an appointment. For more details about the Veterans Choice Program and VAâ\\x80\\x99s progress, visit: www.va.gov/opa/choiceact.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='va choice provider phone number', output='1 The Choice Program does not impact your existing VA health care or any other VA benefit. 2  If you did not receive a Choice Card or if you cannot find your card, please call 1-866-606-8198 to learn more about eligibility for the Veterans Choice Program.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='population nehalem or', output='Nehalem, Oregon 2017 Population Estimate â\\x80\\x93 1,240 The Portland State University projects the Nehalem 2017 population as 1,240 people. Note: this value is a projection, not a count. The 2010 Decennial Census provides the most recent count of the Nehalem population as 271 people. And the US Census Bureauâ\\x80\\x99s 2016 Population Estimates Program provides a population estimate for Nehalem as 285 people. The graph is a visualization of the best Nehalem population data available from different data sources.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='population nehalem or', output='Sequim /Ë\\x88skwÉªm/ is a city in Clallam County, Washington, United States.As of the 2010 census counted a population of 6,606. [Sequim School District population served is a little over 29,000.Sequim and the area around Sequim has a population of about 28,000] Sequim is located along the Dungeness River near the base of the Olympic Mountains.The city has been increasing in population dramatically in recent years due to the influx of retirees from the Puget Sound region and California.equim is home to a herd of Roosevelt elk, one attraction to the area. Over the past two decades, Sequim has become famous for growing lavender and holds the Sequim Lavender Weekend (always the third weekend in July).')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how much is a pint of patron', output='how much is a pint of patron tequila? The KGB Agent answer: Patron Tequila is available in 50ml, 200ml, 375ml & 750ml bottles. Patron Anejo 750ml is $59.95. Patron Reposado 750ml is $53.95. Patron Silver 750ml is $49.95.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how much is a pint of patron', output='Answered by The Community. Making the world better, one answer at a time. A pint of whiskey is dependent upon the location of the purchase and the brand of the whiskey. Whiskey prices can range from six dollars a pint to list much as $1000.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is home systems protection insurance', output='HSB HomeWorksâ\\x84¢ combines the broadest homeowners equipment breakdown protection available - Home Systems Protection - with a suite of home management tools, known as MyHomeWorks. Home Systems Protection covers the breakdown of home systems, appliances and electronics. The MyHomeWorks.com website offers home energy management tools to help increase home energy efficiency and save money. HSB HomeWorks is designed to enhance the Homeowners products offered by personal lines insurers that partner with HSB.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is home systems protection insurance', output='A Home Owners Insurance Policy is designed to protect home owners against certain perils. There is usually a deductible when filing a home insurance claim unless noted. Individual home insurance policies are determined by named perils and exclusions in a policy. Consult your agent or insurance company regarding any exceptions that may apply. Whether you buy your home owners insurance policy online or with a local agent, the typical home owners insurance policy is divided into 2 parts: 1  Part I: Home Insurance Property Protection.  Part II: Home Insurance Liability Protection.')>.\n",
      "/opt/anaconda3/envs/trulens/lib/python3.11/site-packages/trulens/feedback/llm_provider.py:289: UserWarning: No supporting evidence provided. Returning score only.\n",
      "  warnings.warn(\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what all is ferric gluconate used for', output='Sodium ferric gluconate complex. Sodium ferric gluconate complex (brand name Ferrlecit by Sanofi), is an intravenously administered iron product indicated in the treatment of iron deficiency anemia. It is frequently used in patients undergoing hemodialysis, those undergoing erythropoietin therapy, and/or patients who have chronic kidney disease. Navigation menu.')>.\n",
      "/opt/anaconda3/envs/trulens/lib/python3.11/site-packages/trulens/feedback/llm_provider.py:289: UserWarning: No supporting evidence provided. Returning score only.\n",
      "  warnings.warn(\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what all is ferric gluconate used for', output='Ferrous gluconate is a type of iron. You normally get iron from the foods you eat. In your body, iron becomes a part of your hemoglobin (HEEM o glo bin) and myoglobin (MY o glo bin). Hemoglobin carries oxygen through your blood to tissues and organs.Myoglobin helps your muscle cells store oxygen.Ferrous gluconate is used to treat iron deficiency anemia (a lack of red blood cells caused by having too little iron in the body).errous gluconate can stain your teeth, but this effect is temporary. To prevent tooth staining, mix the liquid form of ferrous gluconate with water or fruit juice (not with milk) and drink the mixture through a straw. You may also clean your teeth with baking soda once per week to treat any tooth staining.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='causes of swollen ankles and feet', output='Injury or surgery involving the leg, ankle, or foot can also cause swelling. Swelling may also occur after pelvic surgery, especially for cancer. Long airplane flights or car rides, as well as standing for long periods of time, often lead to some swelling in the feet and ankles.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='causes of swollen ankles and feet', output='Swollen ankles and swollen feet can be caused by a variety of things such as pregnancy, medications, diseases (congestive heart failure, diabetes). Treatment depends on the cause.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='is levaquin an antibiotic', output='A: Levaquin is an antibiotic that is used to treat all kinds of bacterial infections. Doctors usually prescribe this drug for treating infections of kidneys, bladder, skin, and sinuses. However, in recent years, levaquin has been linked to a number of severe side effects and birth defects.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='is levaquin an antibiotic', output='A: Levaquin is an antibiotic that is used to treat all kinds of bacterial infections. Doctors usually prescribe this drug for treating infections of kidneys, bladder, skin, and sinuses. However, in recent years, levaquin has been linked to a number of severe side effects and birth defects.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does the sphinx represent', output='In ancient Egypt, the Sphinx is a male statue of a lion with the head of a human, sometimes with wings. Most sphinxes however represent a king in his appearance as the sun god.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does the sphinx represent', output='A sphinx (Greek: Ï\\x83Ï\\x86Î¯Î³Î¾ [sfiÅ\\x8bks] , Boeotian: BÅ\\x93OTIAN [Ï\\x86Î¯Î¾] , PÊ°iks: Ø£Ø¨Ù\\x88 Ø§Ù\\x84Ù\\x87Ù\\x88 Ù\\x84,) is a mythical creature with, as is minimum, the head of a human and the body of a lion. In Greek tradition, it has the head of a human, the haunches of a lion, and sometimes the wings of a bird. It is mythicised as treacherous and mercil merciless')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='meaning of obtunded', output='Word Origin and History for obtund Expand. v. c.1400, (transitive) to render dead, make dull, used occasionally in English, especially in medical jargon; from Latin obtundere to blunt, make dull, weaken, exhaust, literally to beat against (see obtuse). Related: Obtundation; obtunded.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='meaning of obtunded', output='adj. 1. adhering fixedly to a particular opinion, attitude, course of action, etc. 2. self-willed or headstrong. 3. difficult to subdue or alleviate; persistent: an obstinate fever.[C14: from Latin obstinÄ\\x81tus, past participle of obstinÄ\\x81re to persist in, from ob- (intensive) + stin-, variant of stare to stand]. (Ë\\x88É\\x92b stÉ\\x99 nÉªt). adj.. self-willed or headstrong. 3. difficult to subdue or alleviate; persistent: an obstinate fever. [C14: from Latin obstinÄ\\x81tus, past participle of obstinÄ\\x81re to persist in, from ob- (intensive) + stin-, variant of stare to stand].')>.\n",
      "/opt/anaconda3/envs/trulens/lib/python3.11/site-packages/trulens/feedback/llm_provider.py:289: UserWarning: No supporting evidence provided. Returning score only.\n",
      "  warnings.warn(\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what county is marlboro, nj', output='This article is about the township in Monmouth County. For the community within the township see Marlboro, Monmouth County, New Jersey; For other places with the same name, see Marlboro, New Jersey (disambiguation). Marlboro Township is a township in Monmouth County, New Jersey, United States.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what county is marlboro, nj', output=\"The Upper Marlboro courthouse under renovation in 2008. Upper Marlboro, officially the Town of Upper Marlboro, is the seat of Prince George's County, Maryland in the United States. The population within the town limits was 631 at the 2010 U.S. Census, although Greater Upper Marlboro is many times larger.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='symptoms of tularemia in rabbits', output='In rabbits, the symptoms include a white-or yellow-spotted liver and the liver and/or spleen may be a dark bluish-red and appear very swollen. Additionally, you may notice external ulcerations or infected areas where the animal was bitten by a tick or deer fly.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='symptoms of tularemia in rabbits', output='Tularemia (rabbit fever). People usually get tularemia from direct contact with rabbits. A person can also get it via the bite of an infected tick or deerfly, by eating contaminated food, by drinking contaminated water, or by breathing in F. tularensis, the bacteria that causes rabbit fever.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is a health unit coordinator', output=\"Health unit coordinators are responsible for administrative aspects of patient care. They typically work in hospitals and nursing care facilities, and provide non-medical support to nurses and patients alike. Health care coordinators work with patients to determine what the patient's specific medical needs are.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is a health unit coordinator', output=\"Essential Information. A patient care coordinator's job is to ensure that a medical facility is providing high quality care services. They work with administration, staff, and patients to reach health care goals and to keep the lines of communication open. Some are registered nurses, but this often is not mandatory.he main job of a patient care coordinator is to ensure high quality health care. A coordinator helps to ensure patients understand every aspect of their care while also working with administration to create policies and make decisions that are in the best interest of the patients.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what causes flashes of light in your peripheral vision?', output='Flashes of light in the same eye as the floaters Darkness on any side or sides of your vision (peripheral vision loss) These painless symptoms could be caused by a retinal tear, with or without a retinal detachment â\\x80\\x94 a sight-threatening condition that requires immediate attention.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what causes flashes of light in your peripheral vision?', output='Causes of Light flashes in one eye: The following medical conditions are some of the possible causes of Light flashes in one eye. There are likely to be other possible causes, so ask your doctor about your symptoms. 1  Retinal detachment. 2  Aging. 3  Migraine. 4  Diabetic retinopathy.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='ibm filenet discovery', output='IBMÂ® FileNetÂ® Platform is a next-generation, unified enterprise foundation for the integrated IBM FileNet P8 products. It combines the enterprise content management with comprehensive business process management and compliance capabilities. IBM FileNet P8 addresses the most demanding compliance, content, and process management needs for your entire organization. It is a key element in creating an agile, adaptable enterprise content management (ECM) environment necessary to support a dynamic organization that must respond quickly to change.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='ibm filenet discovery', output='x Developing Applications with IBM FileNet P8 APIs 6-2 Run configuration for PEComponentQueueHelper. . . . . . . . . . . . . . . . . 183 6-3 Example for PEComponentQueueHelper parameters in Eclipse . . . . . . 184')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how to abbreviate degrees fahrenheit', output='Start by taking the number in Fahrenheit and subtracting 32. Then divide the number by 9, and then multiply it by 5. This is how you convert Fahrenheit to Celsius or use the e â\\x80¦ quation C = (F - 32) Ã\\x97 5/9   In this case, the answer is about 15.56 degrees Celsius.tart by taking the number in Fahrenheit and subtracting 32. Then divide the number by 9, and then multiply it by 5. This is how you convert Fahrenheit to Celsius or use the e â\\x80¦ quation C = (F - 32) Ã\\x97 5/9   In this case, the answer is about 15.56 degrees Celsius.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how to abbreviate degrees fahrenheit', output='To convert Celsius to Fahrenheit we have to do the reverse, so use the following: Degrees in Fahrenheit = degrees in Celsius * 1.8 + 32. For example, if we have 42 degrees in Celsius and we want to convert them into Fahrenheit we do the following:')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='inherent legal definition', output='Definition of inherent. : involved in the constitution or essential character of something : belonging by nature or habit : intrinsic risks inherent in the venture.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='inherent legal definition', output='inherent. adjective intrinsic, natural, basic, central, essential, native, fundamental, underlying, hereditary, instinctive, innate, ingrained, elemental, congenital, inborn, inbred, inbuilt, immanent, hard-wired, connate the dangers inherent in an outbreak of war.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='carbs withdrawal signs and symptoms', output='Sugar Withdrawal Symptoms and Duration: What you can Expect. Sugar withdrawal symptoms vary and sugar withdrawal duration usually lasts between two and four weeks for most people. Tiredness, depression, flu symptoms, and irritability are common problems when you first give up eating sugar. Sugar does release opioids and dopamine, so some scientists believe it can be addictive.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='carbs withdrawal signs and symptoms', output='1 The withdrawal symptoms may manifest themselves in severe and intense cravings for sugary foods, or they may include more serious conditions such as headaches or nausea. 2  By gradually reducing your sugar intake over several weeks, you can greatly reduce the severity of these sorts of sugar withdrawal symptoms.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='is turmeric curcumin', output='Curcumin is the active ingredient in Turmeric. Curcumin is the yellow-orange pigment and the most important ingredient in turmeric. Curcumin has many clinical applications, particularly as a powerful, yet safe, anti-inflammatory agent. (2, 6) Curcumin is the most important active ingredient in turmeric, and makes up about 2-6% of the spice.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='is turmeric curcumin', output='Curcumin is the active ingredient of turmeric, and is also found in limited amounts in Ginger. It is a potent anti-inflammatory and cancer preventative molecule, and similar to Fish Oil it seems to be an effective metabolic syndrome band-aid. Black Pepper greatly enhances absorption.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='is rosemary oil good for back pain?', output='Aromatherapy. Treatment with essential oils can often help relieve the painful muscle spasms that contribute to back pain. Several of these--sage, rosemary, thyme, horsebalm and mountain dittany--are rich in thymol and carvacrol, compounds that help muscles relax.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='is rosemary oil good for back pain?', output='Stress: The same actions that make rosemary essential oil so useful for memory and depression make it an excellent choice for managing stress and stress-related issues. It helps you to see things more clearly, and calms and relaxes the nerves.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what are setbacks in zoning', output='Setbacks are generally set in municipal ordinances or zoning. Setbacks along state, provincial, or federal highways may also be set in the laws of the state or province, or the federal government. Homes usually have a setback from the property boundary, so that they cannot be placed close together.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what are setbacks in zoning', output='Definition. A specified distance from the sides of a lot that denote where a building cannot be placed. Building lines are often called setbacks, because a building must be set back  a specified distance from the property line.ES NO 9 people found this helpful. â\\x80\\x9c If a house is currently over the building line that was put into effect this year, all current businesses and home buildings are not required to move their property, but may be required to in the future if a large percentage of the property is altered.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is a cleric', output='Cleric. A person who has been legitimately received into the ranks of the clergy. By clergy in the strict sense is meant the entire ecclesiastical hierarchy. Consequently a cleric is one who belongs in some sense to the hierarchy.hile cleric in its strict sense means one who has received the ecclesiastical tonsure, yet in general sense it is also employed in canon law for all to whom clerical privileges have been extended. Such are the members of religious orders: Monks and nuns, and even lay brothers and novices.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is a cleric', output='The cleric is a hybrid class in Dungeons and Dragons, capable of casting magic and capable in melee. The cleric serves a support role much of the time, with spells that heal and enhance. Clerics contribute to a party with support magic and melee staying power. They are noted as having several of the advantages of fighting-men and magic-users. The cleric class in this edition also contains the so-called anti-cleric, who casts the reverse form of several of the spells, and who has different level titles. Prime Requisite Edit')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what files do dvd players read', output='DVD-RW is a re-recordable format similar to CD-RW or DVD+RW. The data on a DVD-RW disc can be erased and recorded over numerous times without damaging the medium. DVDs created by a -R/-RW device can be read by most commercial DVD-ROM players.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what files do dvd players read', output='October 15, 2013 7:04:12 PM. it not the drive is the player you need vcl or a player and the right video and audio plug ins. with older dvd if they cant read the disk it may be a firmware issue that causes the drive not to read the media. Since you are talking about stand alone dvd players, it depends. Anything listed as DivX certified should be able to. Others will actually say MP4 on the front or tray like it says DVD/Audio CD/DivX/MP4/MP3/ect. Do not assume that all players will play it because they will not. Look for the specs.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='integrated dna technology', output=\"Research and diagnostics life sciences market. Products. Integrated DNA Technologies, Inc. (IDT), headquartered in Coralville, Iowa, is the world's largest supplier of custom nucleic acids, serving the areas of academic research, biotechnology, clinical diagnostics, and pharmaceutical development.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='integrated dna technology', output='Recombinant DNA technology enables individual fragments of DNA from any genome to be inserted into vector DNA molecules, such as plasmids, and individually amplified in bacteria. Each amplified fragment is called a DNA clone.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what happens when waves hit a concave surface', output='When sound reflects off a special curved surface called a parabola, it will bounce out in a straight line no matter where it originally hits. Many stages are designed as parabolas so the sound will go directly into the audience, instead of bouncing around on stage.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what happens when waves hit a concave surface', output='A concave meniscus, which is what you normally will see, occurs when the molecules of the liquid are attracted to those of the container. This occurs with water and a glass tube. A convex meniscus occurs when the molecules have a stronger attraction to each other than to the container, as with mercury and glass.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='does dehydration cause sleep harder', output=\"One of these can be dehydration. Most people in Britain don't drink as much fluid as they need for optimum health. Although, over time, they may inadvertently train themselves not to feel thirsty, the suppressed desire to drink can result in restlessness and a host of minor problems that make sleep difficulties worse.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='does dehydration cause sleep harder', output='you could be dehydrating while you are sleeping. I have done this a couple of times. The opinions expressed here are solely those of the User.The opinions expressed here are solely those of the User. User-generated content areas are not reviewed by a WebMD physician.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='(Metformin what is it and what does it do?)', output='A: Metformin is an oral diabetes medication that helps control blood glucose (sugar) levels. It is used to treat type 2 diabetes, either alone or in combination with insulin or other medications. Common side effects include headache, weakness, mild nausea, vomiting, diarrhea, gas, and stomach pain.: Metformin is an oral diabetes medicine that helps control blood glucose (sugar) levels. It is used to treat type 2 (non-insulin-dependent) diabetes, either alone or in combination with insulin or other diabetes medications.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='(Metformin what is it and what does it do?)', output='Metformin (also known as GlucophageÂ®) helps to regulate the amount of glucose (sugar) in your blood. It makes your body more sensitive to insulin, and decreases the amount of glucose your liver releases.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='unconscious bias definition', output=\"Unconscious Bias. Categories: Insurance, A type of employment-related claim alleging that with respect to promotions, raises, and job offers, Caucasian men inevitably slight women and minorities, because they use preconceived stereotypes in evaluating such persons, despite their conscious efforts to do otherwise.ou may have health insurance as an employee benefit from your job or, if you qualify, through the federal government's Medicare or Medicaid programs. You may also buy in dividual health insurance directly from an insurance company or be eligible through a plan offered by a group to which you belong.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='unconscious bias definition', output='Implicit bias occurs when someone consciously rejects stereotypes and supports anti-discrimination efforts but also holds negative associations in his/her mind unconsciously.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='most expensive places in the world dublin', output=\"Dublin's move up ten places in the 2014 cost of living survey primarily reflects both of these factors.. Elsewhere in Europe the strength of the Swiss franc means Zurich is the most costly European city on the list, followed by Geneva and Bern.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='most expensive places in the world dublin', output=\"Singapore has retained its position as the world's most expensive city, according to research by the Economist Intelligence Unit (EIU). The top five most expensive cities in the world remain unchanged from a year earlier and include, in descending order, Paris, Oslo, Zurich and Sydney.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what causes fluid in the ears?', output='All cases of fluid in the ear are caused by some form of auditory tube dysfunction which prevents your eustachian tube from adequately draining. Common causes for developing fluid in the ear for both adults and children includes: Allergies. Any kind of congestion, from a cold virus, similar infection, or even pregnancy.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what causes fluid in the ears?', output='What causes fluid in the ear? Answer Wiki. Fluid can be in the outer ear/ear canal or middle ear/behind ear drum. Fluid in ear canal will drain out unless it is trapped behind ear wax (unlikely) or in state of active drainage from a hole in the ear drum. Fluid behind the ear drum will drain out into the throat through the Eustachian tube unless that tube is blocked by allergies or infection.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does the legal term demur mean?', output='DEMUR definition - To present a demurrer; to take an exception to the sufficiency in point of law of a pleading or state of facts alleged. see demurrer. DEMUR definition - To present a demurrer; to take an exception to the sufficiency in point of law of a pleading or state of facts alleged. see demurrer. Skip to primary navigation')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does the legal term demur mean?', output=\"DEFINITION of 'Demurrage'. A term used in currency trading to denote the cost of currency ownership and/or storage. It is essentially considered to be the cost of carrying money, and is in some respects considered superior to interest payments, as it stimulates currency circulation and economic growth. BREAKING DOWN 'Demurrage'.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what do architectural drawings show', output=\"This section deals with the conventional views used to represent a building or structure. See the Types of architectural drawing section below for drawings classified according to their purpose. Principal floor plans of the Queen's House, Greenwich (UK).\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what do architectural drawings show', output='Architectural drawings are drawn to scale, so that relative sizes are correctly represented. The scale is chosen both to ensure the whole building will fit on the chosen sheet size, and to show the required amount of detail.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='salt water aquarium salinity', output='salinity in plain terms is the concentration of salt in aquarium water more specifically salinity is a measure of the dissolved sodium ions in the water and it can be measured in parts per thousand pptmost saltwater aquarists measure the salinity in their aquarium using a hydrometer which measures the specific gravity in the tankith the proper equipment and regular water tests it should be easy to maintain a constant level of salinity in your saltwater aquarium if you take the proper precautions you can make sure that the environment in your tank is as close to the natural environment of the ocean as possible')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='salt water aquarium salinity', output='Saline water (more commonly known as salt water) is water that contains a significant concentration of dissolved salts (mainly NaCl). The salt concentration is usually expressed in parts per thousand (permille, â\\x80°) or parts per million (ppm). The United States Geological Survey classifies saline water in three salinity categories.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does lactose do', output='Lactase is an enzyme produced by many organisms. It is located in the brush border of the small intestine of humans and other mammals. Lactase is essential to the complete digestion of whole milk; it breaks down lactose, a sugar which gives milk its sweetness.Lacking lactase, a person consuming dairy products may experience the symptoms of lactose intolerance.he optimum temperature for human lactase is about 37Â°C for its activity and has an optimum pH of 6. In metabolism, the Î² -glycosidic bond in D-lactose is hydrolyzed to form D-galactose and D-glucose, which can be absorbed through the intestinal walls and into the bloodstream.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does lactose do', output='Lactose is a disaccharide (two sugar molecule) made up of glucose and galactose, commonly found in cow, sheep, goat and all mammalsâ\\x80\\x99 milk. Normally lactose is broken down by the enzyme lactase (which is produced in the villi that line our small intestine) into its single sugars to enable its absorption and use by the body.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='prices for massages', output='Salt Glow and Massage. 60 Minutes $85. Both our salt glow and warm oil massages use therapeutic oils to enhance relaxation, purification, and renewal. Sea salt is rich in trace minerals and is mixed with warm massage oil to help exfoliate dead layers of skin, which leaves your skin moist and glowing. This combines well with a therapeutic massage.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='prices for massages', output=\"Many highly experienced independent massage therapists have set up shop for themselves, with prices usually starting around $70 - $80. 1  This is an attractive alternative for people who want to get massage regularly but don't need the robes, slippers and waiting rooms. As with Massage Envy, you usually change in the room.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how many species go extinct in an average day', output='How many species are ... then 10,000 species go extinct ... These experts calculate that between 0.01 and 0.1% of all species will become extinct each ...Read more. Positive: 95 %. ..... and because every speciesâ\\x80\\x99 extinction ... we know of approximately 1,000 species that have gone extinct, ... Nobody really knows how many species ... Read more. Positive: 72 %. ... lived are extinct. The average life-span of a species is ... (many people would say premature extinction) ...')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how many species go extinct in an average day', output='UN Environment Programme: 200 Species Extinct Every Day, Unlike Anything Since Dinosaurs Disappeared 65 Million Years Ago. According to the UN Environment Programme, the Earth is in the midst of a mass extinction of life. Scientists estimate that 150-200 species of plant, insect, bird and mammal become extinct every 24 hours. This is nearly 1,000 times the natural or background rate and, say many biologists, is greater than anything the world has experienced since the vanishing of the dinosaurs nearly 65m years ago.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is an alias name', output='Alias is a short and more popular phrase for alias dictus. The abbreviation a.k.a., also known as, is frequently used in connection with the description of a person sought by law enforcement officers to disclose the names that the person has been known to use.A fictitious name assumed by a person is popularly termed an alias.he abbreviation a.k.a., also known as, is frequently used in connection with the description of a person sought by law enforcement officers to disclose the names that the person has been known to use. A fictitious name assumed by a person is popularly termed an alias.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is an alias name', output='In general, as a noun, an alias (pronounced AY-lee-uhs) is an alternate name for someone or something.n general, as a noun, an alias (pronounced AY-lee-uhs) is an alternate name for someone or something.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what county is lawton, ok', output=\"The city of Lawton is the county seat of Comanche County, in the State of Oklahoma. Located in southwestern Oklahoma, about 87 mi (140 km) southwest of Oklahoma City, it is the principal city of the Lawton, Oklahoma Metropolitan Statistical Area. According to the 2010 census, Lawton's population was 96,867, making it the fifth-largest city in the state.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what county is lawton, ok', output=\"Sponsored Topics. The city of Lawton is the county seat of Comanche County, in the U.S. state of Oklahoma. Located in the southwestern region of Oklahoma approximately 80 miles (130 km) southwest of Oklahoma City, it is the principal city of the Lawton Oklahoma Metropolitan Statistical Area. According to the 2010 US Census, Lawton's population was 96,867, making it the fifth largest city in the state.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='pluvious definition', output='Definitions for pluvious. Here are all the possible meanings and translations of the word pluvious. Wiktionary(0.00 / 0 votes)Rate this definition: pluvious(Adjective) Of or pertaining to the condition of heavy rain; rainy. Rachael sat by the fireplace one pluvious evening.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='pluvious definition', output='impervious definition: The definition of impervious is not being able to be penetrated or not able to be hurt. (adjective) An example of impervious is a fabric that does not allow any liquid to pass through. An example of impervious is a figurine made ...')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='medicare prior authorization number for florida', output='The provider must retain copies of all documentation for five years. Mail or Fax Information to: Magellan Medicaid Administration, Inc. Prior Authorization P. O. Box 7082 Tallahassee, FL 32314-7082 Phone: 877-553-7481 Fax: 877-614-1078.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='medicare prior authorization number for florida', output='Medical Prior Authorization Form. Fax Form To: 888 647-6152. Date: ____________ ï\\x81£ *Provider Pre-service Organization Determination. *Check only if requesting a pre-service determination for a Part C Medicare Advantage beneficiary. Member:')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='does restrictive lung disease cause sleep apnea', output='In addition, obesity may cause significant restrictive lung physiology and lead to obesity hypoventilation syndrome (OHS). Restrictive lung disease is associated with nocturnal pathophysiology, including sleep disturbances and breathing and oxygenation impairments during sleep.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='does restrictive lung disease cause sleep apnea', output='Doctors classify lung disease as either obstructive or restrictive. The term obstructive lung disease includes conditions that hinder a personâ\\x80\\x99s ability to exhale all the air from their lungs. Those with restrictive lung disease experience difficulty fully expanding their lungs.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how common is the name andrew?', output='Andrew is a very prominent first name for men (#35 out of 1220, Top 3%) and also a very prominent last name for all people (#2521 out of 150436, Top 2%). (2000 u.s. census) Andrew reached its top rank of #5 in the U.S. in 2003, and is currently at #34.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how common is the name andrew?', output='In 1999, it was the 19th most common name, while in 1940, it was the 31st most common name. Andrew was the first most popular boys name in the Northern Territory in 2003 to 2015 and continuing. In Victoria, Andrew was the first most popular name for a boy in the 1970s.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is human trafficking', output='Human trafficking is a form of modern-day slavery. Victims of human trafficking are subjected to force, fraud, or coercion for the purpose of commercial sex, debt bondage, or forced labor.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is human trafficking', output='Human trafficking is a modern-day form of slavery involving the illegal trade of people for exploitation or commercial gain. Every year, millions of men, women, and children are trafficked in countries around the world, including the United States. It is estimated that human trafficking generates many billions of dollars of profit per year, second only to drug trafficking as the most profitable form of transnational crime.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how much is does it cost to insure a house', output='Estimates. According to the Federal Reserve Bureau, the average cost of an annual premium for homeowners insurance is between $300 and $1,000. For most homeowners, the annual costs for a homeowners insurance policy can be estimated by dividing the value of the home by 1,000, then multiplying the result by $3.50.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how much is does it cost to insure a house', output='Many factors influence the total annual cost of this insurance, including the extent of the coverage and the location of the property. According to the Federal Reserve Bureau, the average cost of an annual premium for homeowners insurance is between $300 and $1,000. For most homeowners, the annual costs for a homeowners insurance policy can be estimated by dividing the value of the home by 1,000, then multiplying the result by $3.50. One factor in the cost of homeowners insurance is what circumstances are covered under a particular policy.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what do you need to be certified in medical coding', output='CCA (Certified Coding Associate)â\\x80\\x94An entry-level coding certificate for which holders pass a two-hour exam and typically have completed medical coding training courses or have six months of coding experience at a medical facility.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what do you need to be certified in medical coding', output='Becoming a Certified Medical Coder. 1  1. Consider becoming certified. Certification is not required for medical coders. 2  2. Decide which exam to sit for. Both the AAPC and the AHIMA offer certifications. 3  3. Prepare for the exam. Attending an accredited program and actual work experience as a medical coder will help you do well on your exam.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='irs continuing education requirements', output='This new continuing education requirement is part of the IRSâ\\x80\\x99s efforts to eliminate problems it has experienced with unqualified preparers. The 15 hours of continuing education must include 10 hours of federal tax law, three hours of federal tax law updates, and two hours of ethics. Preparers must provide their Preparer Tax Identification Number (PTIN) to continuing education providers so their continuing education can be reported to the IRS.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='irs continuing education requirements', output='Obtain 72 hours every three years (based on your renewal cycle) Obtain a minimum of 16 hours per year (2 of which must be on ethics) Use an IRS approved continuing education provider. Review the enrolled agent CE credit chart. Review the Frequently Asked Questions.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='smart procurement definition', output='Features. Smart numbers are highly configurable and can contain organizational, temporal, or factual data that relates to the procurement document. You can define smart numbering schemes by process or by transaction type.If you define a smart numbering scheme for the process of creating procurement documents, a new serial number is generated with each newly created document.his function enables the generation of smart numbers (formerly called â\\x80\\x9clong procurement numbersâ\\x80\\x9d) as identifiers for SAP Supplier Relationship Management (SAP SRM), Procurement for Public Sector (PPS) procurement documents.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='smart procurement definition', output='(October 2010). Procurement is the act of acquiring, buying goods, services or works from an external source. It is favourable that the goods, services or works are appropriate and that they are procured at the best possible cost to meet the needs of the acquirer in terms of quality and quantity, time, and location.rocurement life cycle in modern businesses usually consists of seven steps: 1  Identification of Need: This is an internal step for a company that involves understanding of the company needs by establishing a short term strategy (three to five years) followed by defining the technical direction and requirements.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='to_date function', output=\"Term: TO_DATE. Definition: The Oracle TO_DATE function will convert either a character string or an expression into a date value. Example Syntax. These all show valid date formats for February 16th, 2009: to_date('16-Feb-09', 'DD-Mon-YY') to_date('02/16/09', 'MM/DD/YY') to_date('021609', 'MMDDYY')\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='to_date function', output=\"The TO_DATE function. 1  The TO_DATE function accepts an argument of a character data type and converts this value to a DATETIME value. The TO_DATE function evaluates a character string according to the date-formatting directive that you specify and returns a DATETIME value. For a complete list of the supported date-formatting directives, see the description of the GL_DATETIME environment variable in the IBMÂ® InformixÂ® GLS User's Guide.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what degree  is ideal flag mount', output='The Wall Mount Bracket 45 Degree is steel constructed and electro-plated to accommodate one flag at a 45 degree angle. Visit our products to view various solutions we offer namely: www.Idealdisplays.co.za and see Wall Mount Bracket 45 Degree. Outdoor Displays. Outdoor product: Ideal Displays. Outdoor Displays.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what degree  is ideal flag mount', output=\"Flagpole Mounting Methods. Ground Sleeve The most common flagpole installation is a direct burial in to the ground. Standard ground set flagpoles are available in 5' increments ranging from 15' to over 100'. The general rule is that 10% of the exposed height is buried in the ground.n the inside, at the bottom, 4 steel wedges are welded to center the pole upon installation. A 1/2 steel base plate is welded to the bottom with an 18 length x 3/4 diameter steel spike attached to act as lightning protection.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='amnesty define', output='Generally, amnesty is defined as any governmental pardon for past offenses or crimes, especially political ones. Granting amnesty goes beyond a pardon, in that it forgives the said offense completely, and typically without consequences. For purposes of conservative politics, amnesty is a political term typically associated with two major issues -- immigration and capital punishment. As it relates to immigration, amnesty is the term used for granting automatic citizenship to resident aliens, who are in the United States illegally. Amnesty for illegal immigrants is the subject of tremendous controversy since it essentially bypasses the citizenship and assimilation process essential to all legal immigration into the United States.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='amnesty define', output=\"What is Amnesty International? Amnesty International is the world's largest human rights organization. Amnesty has created a world wide movement with a mission to prevent and stop grave violations of human rights, like the rights to physical and mental integrity, freedom on conscience and expression, and freedom from discrimination.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='talk stoop cat what happened to bulldog gracie', output='Gracie The Talk Stoop Bulldog. As you may have heard, Gracie the Talk Stoop Bulldog passed away last weekend :( - We are creating a video montage (or Tribute) in memory of her life. If you knew Gracie, share a few words about her and even if you didnâ\\x80\\x99t, share some words of support for Cat and wish Gracie well in the doggy afterlife.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='talk stoop cat what happened to bulldog gracie', output='If Bulldog is your Animal Totem; You have a tendency to be a bit passive and complacent unless you are provoked. If you are provoked you become very protective and will move amazingly swiftly to protect those around you and what you perceive to be your territory and family.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how much does a teaching assistant earn', output=\"The average hourly pay for all teaching assistant roles is around $10.50 per hour, but will vary by experience and location. Teaching assistants with a significant amount of experience can expect to earn up to $16 per hour.areer paths for teacher assistants. In order to make your way up the ladder, you'll need some additional education and work experience. If you're working for a local school district, they'll probably offer tuition reimbursement to help you earn your bachelorâ\\x80\\x99s degree and get your teaching license.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how much does a teaching assistant earn', output=\"(United Kingdom). A Graduate Teaching Assistant earns an average salary of Â£12,982 per year. Pay for this job does not change much by experience, with the most experienced earning only a bit more than the least. People in this job generally don't have more than 20 years' experience.Â£8,913 - Â£18,694. Graduate Teaching Assistant earns an average salary of Â£12,982 per year. Pay for this job does not change much by experience, with the most experienced earning only a bit more than the least.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how much is Welding school', output='The price for welding school varies depending on the location and reputation of the school. The length of the course can also have an effect on how much the admission fee for the school will be. 1  On average, to complete a full welding school course will cost anywhere from $5,000 to as much as $15,000.he length of the course can also have an effect on how much the admission fee for the school will be. 1  On average, to complete a full welding school course will cost anywhere from $5,000 to as much as $15,000.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how much is Welding school', output='1 For example, Alfred State SUNY College of Technology in Alfred, N.Y., estimates a $1,000 cost for tools, $150 for safety equipment, and a $700 welding rod.  Scholarships may be available to qualified applicants attending a welding training program.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='time hawaii', output='The time is the same on all of the islands you can visit in Hawaii: Maui, Oahu (Honolulu, Waikiki), Kauai, Big Island of Hawaii, Lanai, and Molokai. Even if you spell it Hawii, the time is still the same. The State of Hawaii is in the Hawaiian Time Zone. Hawaii does NOT go on Daylight Savings Time. In the winter, Hawaii is 5 hours earlier than Eastern Standard Time, 4 hours earlier than Central Standard Time, 3 hours earlier than Mountain Standard Time, and 2 hours earlier than Pacific Standard Time.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='time hawaii', output=\"So let's begin to look at the time in Hawaii as compared to other major time zones. On the world clock Hawaii is 10 hours behind Coordinated Universal Time (abbreviated UTC) and formerly known as (GMT) or Greenwich Mean Time. However, unless you live in England or Europe, that probably means very little to you.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how to become a class b cdl driver', output='All Candidates for the Class B CDL license must complete the following requirements: 1  You must pass a General Knowledge exam. 2  You must pass the Air Brakes test. 3  You must pass the Pre-trip inspection.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how to become a class b cdl driver', output='A general knowledge test for the Class A or Class B and C license must be successfully completed byall CDL applicants. The information needed to pass these tests is contained in the CDL driverâ\\x80\\x99s manual. This manual isavailable free of charge at any PennDOT Driverâ\\x80\\x99s License Exam Center or on PennDOTâ\\x80\\x99s website.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what includes in nursing general survey', output='GENERAL SURVEY AND VITAL SIGNS. 1. General survey describes client as a whole and includes apparent age vs. chronological ages, sex, race, body type, stature, and nutritional status, posture and motor activity, mental status, speech, Nursing Assessment 1. Part of Nursing Process 2.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what includes in nursing general survey', output='The state survey, licensing, and certification agencies are responsible for surveying or inspecting nursing homes to check their compliance with the regulations, investigating complaints, and reporting the results to the CMS.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what did the nuremberg trials do explanation', output='Nuremberg Trials: Table of Contents | Photographs | Trial Indictments. On October 1, 1946, the International Military Tribunal handed down its verdicts in the trials of 22 Nazi leaders - eleven were given the death penalty, three were acquitted, three were given life imprisonment and four were given imprisonment ranging from 10 to 20 years.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what did the nuremberg trials do explanation', output='The Nuremberg Trials were a series of trials that occurred in post-World War II Germany to provide a platform for justice against accused Nazi war criminals.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what are mangroves', output='Mangroves are salt tolerant trees (halophytes) adapted to live in harsh coastal conditions. They contain a complex salt filtration system and complex root system to cope with salt water immersion and wave action. They are adapted to the low oxygen (anoxic) conditions of waterlogged mud.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what are mangroves', output='Mangroves of Florida. The term mangrove is loosely used to describe a wide variety of often unrelated tropical and subtropical trees and shrubs which share common characteristics. Globally, more than 50 species in 16 different families are considered mangroves (Tomlinson 1986). In Florida, the mangrove community consists of three main species of true mangroves: the red mangrove, Rhizophora mangle, the black mangrove, Avicennia germinans, and the white mangrove, Laguncularia racemosa.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is an open system server', output='Open Enterprise Server (OES) is a server operating system designed for shared network resources and a virtualization computing environment. Open enterprise server, a product of Novell, is a combination of Netware and Suse Linux enterprise server and can be installed like any of the two depending upon the installation.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is an open system server', output='An open system is contrasted with the concept of an isolated system which exchanges neither energy, matter, nor information with its environment. An open system is also known as a constant volume system or a flow system. The concept of an open system was formalized within a framework that enabled one to interrelate the theory of the organism, thermodynamics, and evolutionary theory. This concept was expanded upon with the advent of information theory and subsequently systems theory.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how much do pediatric nurse practioners make a year', output='Average pediatric nurse practitioner salary. A Pediatric Nurse Practitioner in the United States gets to collect an average income from between $59,352 to $109,355 per year. A Pediatric Nurse Practitioner can earn up to around $120,000 per annum including profit sharing and benefits/perks. newly practicing Pediatric Nurse Practitioner in the US can receive a starting salary of around $69,500 per annum. Pediatric nurse practitioner can collect a starting hourly pay of around $39.38.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how much do pediatric nurse practioners make a year', output='A newly practicing Pediatric Nurse Practitioner in the US can receive a starting salary of around $69,500 per annum. Pediatric nurse practitioner can collect a starting hourly pay of around $39.38.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='meaning of material requisition', output='Wiktionary(0.00 / 0 votes)Rate this definition: requisition(Noun) A request for something, especially a formal written request on a pre-printed form. requisition(Noun) A requirement. requisition(Verb) To demand something, especially for a military need of personnel, supplies or transport.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='meaning of material requisition', output='noun requisitioner the act of requiring or demanding. 1; noun requisitioner a demand made. 1; noun requisitioner an authoritative or formal demand for something to be done, given, supplied, etc.: The general issued a requisition to the townspeople for eight trucks. 1; noun requisitioner a written request or order for something, as supplies. 1')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is a prostate rub', output='Prostate massage. Male genital anatomy. Prostate massage is the massage or stimulation of the male prostate gland for sexual stimulation or medical purposes. The prostate takes part in the sexual response cycle, and is essential for the production of semen.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is a prostate rub', output='Prostate massage. Prostate massage is the massage or stimulation of the male prostate gland for sexual stimulation or medical purposes. The prostate takes part in the sexual response cycle, and is essential for the production of semen.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='population brigham city utah', output='The 2016 Brigham City, Utah, population is 18,752. There are 765 people per square mile (population density).')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='population brigham city utah', output='In 2016, the estimated population of Utah is 2,995,919, up significantly from the 2010 Census-confirmed population of 2.76 million. Utah currently enjoys a growth rate of 1.64%, which ranks 4th in the country. The last census in the USA was carried out in 2010, and this revealed that the Utah population stood at 2,763,885. This is a significant increase of over 20% from the numbers declared in 2000.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='price clips', output='Great Clips Prices. Great Clips prices start at $14.00 for adult haircuts and $12.00 for kids haircuts. For a detailed look at Great Clips prices and services, continue reading below. Salons have always been the solution of many men and women when it comes to hair care and maintenance.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='price clips', output='Buy the selected items together. This item:1/8 offset mounting canvas Z clips for picture framing -- #6 x 3/8 pan-head screws included -- 144â\\x80¦ $13.40. In Stock. Sold by Gittels Art Supply and ships from Amazon Fulfillment. FREE Shipping on orders over $49.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='team uniform color schemes', output='That year, the league also assigned jersey and cap colors, but by player position rather than by club. Traditionally, when playing at home, teams wore uniforms that were mostly white with trim in team colors and when playing away, they wore uniforms that were mostly gray with trim in team colors.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='team uniform color schemes', output='That year, the league also assigned jersey and cap colors, but by player position rather than by club. Traditionally, when playing at home, teams wore uniforms that were mostly white with trim in team colors and when playing away, they wore uniforms that were mostly gray with trim in team colors.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what causes gall bladder infection', output='Even sepsis can cause gall bladder infection. In children, gall stones are formed over a period of time when viral infections are not treated properly. If the bodyâ\\x80\\x99s immunity is lowered or if the immune system is not functioning well, it can affect the gallbladder causing infection.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what causes gall bladder infection', output='Most bladder infections are caused by various strains of E. coli, bacteria that normally live in the gut. Women sometimes get bladder infections after sex.Vaginal intercourse makes it easier for bacteria to reach the bladder through the urethra. The risk for bladder infection -- dubbed honeymoon cystitis -- increases with frequent sex.Pregnant women, whose urinary tracts change in response to hormones and increased kidney function, are also prone to infections.ost bladder infections are caused by various strains of E. coli, bacteria that normally live in the gut. Women sometimes get bladder infections after sex.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='volcano hawaii stats', output=\"Whatâ\\x80\\x99s so special about Hawaiiâ\\x80\\x99s volcanoes? Hawaii emerged from the sea millions of years ago, forged by the power of volcanoes. Today, Hawaii Volcanoes National Park on Hawaii Island is one of the few places in the world where visitors can come face to face with an active volcano. This once in a lifetime chance to safely explore Kilauea, one of the world's most active volcanoes, makes Hawaii a destination unlike any other.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='volcano hawaii stats', output='Kilauea is one of the worldâ\\x80\\x99s most active volcanoes. It is a shield-type volcano that makes up the southeastern side of the Big Island of Hawaii.The volcano rises 4,190 feet (1,227 meters) above sea level and is about 14 percent of the land area of the Big Island.t is a shield-type volcano that makes up the southeastern side of the Big Island of Hawaii. The volcano rises 4,190 feet (1,227 meters) above sea level and is about 14 percent of the land area of the Big Island.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='does vinegar help poison ivy', output='Rinse with apple cider vinegar. Among the many medical treatments apple cider vinegar can be used for, expediting the healing process of poison ivy exposure is one of them. Use a cotton pad to gently apply the vinegar to the area, or rinse it with a mixture of equal parts vinegar and water.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='does vinegar help poison ivy', output='Poison Ivy Vegetation Killer. 1  1 cup salt. 2  8 drops liquid detergent. 3  1 gallon vinegar.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='is the pill nabumetone a narcotic', output='Best Answer: No. Nabumetone is a NSAID (Non-steroidal anti-inflammatory drug). It is RX only, but it is not a controlled substance (by controlled substance it is not listed as a scheduled drug by the FDA or DEA.)')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='is the pill nabumetone a narcotic', output='Nabumetone belongs to a group of medicines called non-steroidal anti-inflammatory drugs (NSAIDs), which are used to reduce inflammation and pain in joints and muscles. Nabumetone tablets are for diseases of the joints such as rheumatoid arthritis or osteoarthritis.abumetone belongs to a group of medicines called non-steroidal anti-inflammatory drugs (NSAIDs), which are used to reduce inflammation and pain in joints and muscles. Nabumetone tablets are for diseases of the joints such as rheumatoid arthritis or osteoarthritis.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='symptoms of strep throat for an adult', output='Some people experience mild symptoms like a sore throat, whereas other people have more severe symptoms including fever and difficulty swallowing. Common symptoms of strep throat include: 1  sudden fever. 2  sore throat (red throat with white patches). 3  headache. 4  chills. 5  loss of appetite. 6  swollen lymph nodes in the neck.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='symptoms of strep throat for an adult', output='Common symptoms of strep throat include: 1  sudden fever. 2  sore throat (red throat with white patches). 3  headache. 4  chills. 5  loss of appetite. 6  swollen lymph nodes in the neck.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='if i contribute to a roth ira is it taxable', output='Roth IRA contributions are not taxed at the time you contribute the funds to your Roth IRA. However, your contributions come from post-tax income. You pay taxes on your income today, but not in the future.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='if i contribute to a roth ira is it taxable', output='Roth IRA contributions are not taxed at the time you contribute the funds to your Roth IRA. However, your contributions come from post-tax income. You pay taxes on your income today, but not in the future.ithdrawals made after age 59 and 1/2 are normal retirement withdrawals and are not taxed. You paid tax on that income when you first contributed to the Roth IRA. If the withdrawal is made before age 59 and 1/2 and is only up to the amount that has been contributed to the Roth IRA then no income tax is charged.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what are voles?', output='Appearance. Voles, sometimes commonly called meadow mice, are small, ground-dwelling rodents that range from about 5-8 inches long with a body that is more robust than a mouse. They have relatively short legs and tails, and their body fur is brownish and black. The voleâ\\x80\\x99s head is round shaped and the snout is blunt.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what are voles?', output='Not to be confused with Mole (animal) . A vole is a small rodent resembling a mouse but with a stouter body, a shorter, hairy tail, a slightly more round head, smaller ears and eyes, and differently formed molars (high-crowned and with angular cusps instead of low-crowned and with rounded cusps).ot to be confused with Mole (animal) . A vole is a small rodent resembling a mouse but with a stouter body, a shorter, hairy tail, a slightly more round head, smaller ears and eyes, and differently formed molars (high-crowned and with angular cusps instead of low-crowned and with rounded cusps).')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how many people died at the battle of gettysburg', output='Aftermath at Gettysburg and the American Civil War. The Battle of Gettysburg changed the lives of not only the soldiers, but also Gettysburgâ\\x80\\x99s residents. The Battle of Gettysburg resulted in 51,000 casualties (killed, wounded, captured and missing).')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how many people died at the battle of gettysburg', output='When the Battle of Gettysburg was over on July 3, 1863, 51,000 soldiers were casualties (killed, wounded, captured or missing) in what remains the largest battle ever fought in North America. The Battle of Gettysburg: Details of the Fighting.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what causes hair to break', output='Over Processing. A common result of over processing is heat damage. Heat damage from excessive heat styling is a major cause of breakage, and comes from over-employing your straightener, hair dryer, and other tools. It causes the ends to â\\x80\\x9cfryâ\\x80\\x9d and change texture. Over processing can mean more than just using your flat iron too frequently, however.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what causes hair to break', output='Diseases that can cause hair loss, thinning, or breakage include: 1  Lupus, in which hair tends to become brittle and may fall out in patches. Short, broken hairs (lupus hairs) commonly appear above the forehead. 2  Thyroid problems, which are a common cause of scattered hair loss.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what causes gall bladder infections?', output=\"The gallbladder holds a digestive fluid that's released into your small intestine (bile). In most cases, gallstones blocking the tube leading out of your gallbladder cause cholecystitis. This results in a bile buildup that can cause inflammation. Other causes of cholecystitis include bile duct problems, tumors, serious illness and certain infections.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what causes gall bladder infections?', output='Simple Bladder Infection. A bladder infection causes inflammation (irritation and swelling) of the bladder and urethra. This causes the sudden development of a predictable group of symptoms. Many women who have had a bladder infection in the past can identify the symptoms easily when they develop another infection.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='meaning of a navigate', output=\"navigates; navigated; navigating. Learner's definition of NAVIGATE. 1. : to find the way to get to a place when you are traveling in a ship, airplane, car, etc. [no object] For thousands of years, sailors navigated by the stars.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='meaning of a navigate', output='A visitor has landed on your website in search of something. Navigation is often the guide to what it is your visitor seeks. Something as simple as word choice can be the difference between a visitor finding what theyâ\\x80\\x99re looking for or leaving in frustration. Effective navigation can facilitate a fruitful visit and increase the likelihood of a return visit in the future.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does a spider bute sore look like', output='Mite bites do not usually spread disease, but they can irritate the skin and cause intense itching. Itch mites usually feed on insects, but will bite other animals, including people. The bites usually go unnoticed until itchy, red marks develop that may look like a skin rash. You may notice itching, hives, and swelling around an injury or sore, or a rash of small, red bumps that may or may not bleed. 2  Flea bites are more common on your ankles, in your armpits, around your waist, and in the bends of your knees and elbows.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what does a spider bute sore look like', output='Varicose (VAR-i-kos) veins are enlarged veins that can be blue, red, or flesh-colored. They often look like cords and appear twisted and bulging. Spider veins are like varicose veins, but smaller. Varicose veins and spider veins usually appear in the legs and can cause pain, swelling, or an itchy rash. Varicose veins and spider veins are treatable. Expand all')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='is china bigger than the united states', output=\"I'm assuming that by America you mean The United States. There's no country called America. Even when excluding Taiwan and other disputed territories, China is larger than the United States, both in terms of land area, and in terms of land area including inland waterbodies. By land area only: China: 9,326,410 sq km. United States: 9,161,966 sq km, 1.8% smaller than China.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='is china bigger than the united states', output=\"Russia, Canada, and the United States of America Russia, USA, Canada are all larger than China in land area. But in terms of population, I believe China is the most populated.o, it is not. in terms of population, yes. in terms of land area, no. Improvement: Well, this gives you the answer, but I feel that people need more than that. In fact, th â\\x80¦ e reason that the U.S. is larger is because you can't count forth the disputed territories in China.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what are trap cards', output=\"Trap Cards (Japanese: ã\\x82«ã\\x83¼ã\\x83\\x89 Torappu KÄ\\x81do) are cards with purple-coloured borders that have various effects. Unless stated otherwise, a Trap Card must first be Set and can only be activated after the current turn has finished.After that, it may be activated during either player's turn.Trap Cards are Spell Speed 2, with the exception of Counter Trap Cards, which are Spell Speed 3. Trap Cards may be chained to Spell Cards, other Trap Cards, or to the effects of Effect Monsters.rap Cards (Japanese: ã\\x82«ã\\x83¼ã\\x83\\x89 Torappu KÄ\\x81do) are cards with purple-coloured borders that have various effects. Unless stated otherwise, a Trap Card must first be Set and can only be activated after the current turn has finished.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what are trap cards', output=\"trap 1. (trÄ\\x83p) n. 1. A contrivance for catching and holding animals, as a concealed pit or a clamplike device that springs shut suddenly. 2. A stratagem for catching or tricking an unwary person. 3. A confining or undesirable circumstance from which escape or relief is difficult: fell into poverty's trap.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what degree sunburn causes blisters', output='First-degree burns cause red skin, but second-degree burns cause blisters. Anything that gives skin a second-degree burn will cause burn blisters. Common examples include scalding from boiling water or hot oil, or even very bad sunburn.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what degree sunburn causes blisters', output='The study, published in the journal Cancer Epidemiology, Biomarkers & Prevention, showed that people who experienced five or more blistering sunburns between ages 15 and 20 had a 68 percent increased risk of two kinds of skin cancer, basal cell carcinoma and squamous cell carcinoma, as well as an 80 percent higher melanoma risk.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='average age of nhl teams', output=\"And how the players stack up. 1  The average NHLer is 73.33 (6' 1 1/3) tall, weighs 204.42 lbs, and is 27.36 years old. 2  The average forward is 72.93 (6' 1) tall, weighs 202.4 lbs, and is 27.1 years old. 3  The average defenseman is 73.87 (6' 1 7/8) tall, weighs 209.66 lbs, and is 27.49 years old.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='average age of nhl teams', output='DIII programs use other forms of scholarship and financial aid for hockey players. 95% of college hockey programs are in the Northeast, mid-Atlantic and Midwestern parts of the USA. Junior leagues are college hockeyâ\\x80\\x99s minor leagues. The average age of college hockey freshman is 20 years old.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what are the symptoms of a bad O2 sensor', output='Symptoms of a Bad Oxygen Sensor. When you have a bad oxygen sensor, your vehicle will run less efficiently, it can sometimes have a poor idle, erratic jerking at steady throttle, hard starting problems, cause the check engine light to come on, and will cause high fuel consumption.owever, replacing an oxygen sensor is a job that you can do yourself, (as long as the O2 sensor is not completely rotted into the pipe or catalytic converter) thereby saving you precious time and money. Letâ\\x80\\x99s take a closer look at just what an oxygen sensor does and the steps you can take to do the work yourself.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what are the symptoms of a bad O2 sensor', output='If your vehicleâ\\x80\\x99s O2 sensor is faulty, chances are there will be signs! Check this list to see if itâ\\x80\\x99s time to replace YOUR vehicleâ\\x80\\x99s oxygen sensors. Sudden decrease in fuel mileage. A defective O2 sensor will likely cause your air-fuel mixture to be too rich and directly affect your fuel economy.f your vehicleâ\\x80\\x99s O2 sensor is faulty, chances are there will be signs! Check this list to see if itâ\\x80\\x99s time to replace YOUR vehicleâ\\x80\\x99s oxygen sensors. Sudden decrease in fuel mileage. A defective O2 sensor will likely cause your air-fuel mixture to be too rich and directly affect your fuel economy.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what are the three major groups of protists', output='Protists were traditionally subdivided into several groups based on similarities to the higher kingdoms: the unicellular animal-like protozoa, the plant-like protophyta (mostly unicellular algae), and the fungus-like slime molds and water molds.mong the pioneers in the study of the protists, which were almost ignored by Linnaeus except for some algae, were Leeuwenhoek, O. F. Mueller, MÃ¼ller. C. G ehrenberg And Felix. FÃ©lix dujardin the first groups used to classify microscopic organism Were the animalcules And. the infusoria')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what are the three major groups of protists', output='Protists are eukaryotes, but beyond that, they are difficult to classify. The group of organisms that fit into the â\\x80\\x9cprotistâ\\x80\\x9d category has more than 7 kingdoms and 10,000 species. About the only thing they all have truly in common is that they are all eukaryotes.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how much does it cost to replace a foundation of a house?', output=\"Many Americans are looking to raise their house foundations due to new restrictions from FEMA. If you want to improve your home and raise a foundation for this or any other reasons, it's an expensive but worthwhile process. Overall, you can expect to spend anywhere between $30,000 and $100,000 depending on the size of your house, number of floors and the size of your house's footprint.\")>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='how much does it cost to replace a foundation of a house?', output='the average cost of a foundation repair the average cost to repair a foundation is $ 3097 most homeowners paid between $ 2546 and $ 3648 to have their foundations fixedhomeowners reported spending anywhere between $ 890 and $ 5100 for a foundation repairthese cost reports are only a guidelinehe average cost of a foundation repair the average cost to repair a foundation is $ 3097 most homeowners paid between $ 2546 and $ 3648 to have their foundations fixed')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is glycohemoglobin', output='test overview a glycohemoglobin test or hemoglobin a1c is a blood test that checks the amount of sugar glucose bound to the hemoglobin in the red blood cells when hemoglobin and glucose bond a coat of sugar forms on the hemoglobinthat coat gets thicker when there s more sugar in the blood1c is a blood test that checks the amount of sugar glucose bound to hemoglobin the result is shown as a percentage the result of your a1c test can also be used to estimate your average blood sugar level')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='what is glycohemoglobin', output='The hemoglobin A1c test, also called HbA1c, glycated hemoglobin test, or glycohemoglobin, is an important blood test that shows how well your diabetes is being controlled.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='stalactites definition', output='A stalagmite (UK /Ë\\x88staelÉ\\x99É¡maÉªt/ Ë\\x88stÃ¦lÉ\\x99É¡maÉªt OR /us/ ; stÉ\\x99Ë\\x88laeÉ¡maÉªt stÉ\\x99Ë\\x88lÃ¦É¡maÉªt From the - greek, Ï\\x83Ï\\x84Î±Î»Î±Î³Î¼Î¯Ï\\x84Î·Ï\\x82 stalagmites - stalagmitÃªs, from, Ï\\x83Ï\\x84Î±Î»Î±Î³Î¼Î¯Î±Ï\\x82) stalagmias dropping trickling is a type of rock formation that rises from the floor of a cave due to the accumulation of material deposited on the floor. from ceiling drippingshis stalagmite formation occurs only under certain pH conditions within the underground cavern. They form through deposition of calcium carbonate and other minerals, which is precipitated from mineralized water solutions.')>.\n",
      "WARNI [trulens.core.app] Unsure what the main input string is for the call to _call with args <BoundArguments (input='stalactites definition', output='Stalagmites are rock formations found in caves. Along with stalactites, stalagmites are speleothems, or cave deposits that form slowly over hundreds of years. People should not touch stalagmites, else they could disrupt the deposition of mineral and halting their growth. Stalactites and stalagmites are found inside most caves.')>.\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import traceback\n",
    "\n",
    "from trulens.feedback.v2.feedback import Groundedness\n",
    "\n",
    "\n",
    "def runn_all_experiments_for_provider(provider):\n",
    "    \"\"\"\n",
    "    Runs all experiments for a given provider.\n",
    "    \"\"\"\n",
    "\n",
    "    def context_relevance_binary(input, output) -> float:\n",
    "        return provider.context_relevance_with_cot_reasons(\n",
    "            question=input,\n",
    "            context=output,\n",
    "            criteria=\"A relevant context to the question should get a score of 1, and an irrelevant context should get a score of 0. The score can only be either 0 or 1 (binary).\",\n",
    "            min_score_val=0,\n",
    "            max_score_val=1,\n",
    "        )[0]\n",
    "\n",
    "    # Run context relevance binary experiment\n",
    "    # context_relevance_binary_run = create_benchmark_experiment_app(\n",
    "    #     app_name=f\"Snowflake IT  - {provider.model_engine}\",\n",
    "    #     app_version='context_relevance_binary',\n",
    "    #     benchmark_experiment=TruBenchmarkExperiment(\n",
    "    #         feedback_fn=context_relevance_binary,\n",
    "    #         agg_funcs=snowflake_it_metrics,\n",
    "    #         benchmark_params=benchmark_params\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    # with context_relevance_binary_run as recording:\n",
    "    #     feedback_res = context_relevance_binary_run.app(snowflake_it_for_context_relevance)\n",
    "    #     print(f'feedback results: {feedback_res}')\n",
    "    #     write_results(feedback_scores=feedback_res, file_name=f\"results/{provider.model_engine}-context_relevance_binary_feedback_scores.csv\")\n",
    "\n",
    "    # # Similar pattern for answer relevance binary experiment\n",
    "    def answer_relevance_binary(input, output) -> float:\n",
    "        return provider.relevance(\n",
    "            prompt=input,\n",
    "            response=output,\n",
    "            criteria=\"A relevant response to the prompt should get a score of 1, and an irrelevant response should get a score of 0. The score can only be either 0 or 1 (binary).\",\n",
    "            min_score_val=0,\n",
    "            max_score_val=1,\n",
    "        )\n",
    "\n",
    "    # answer_relevance_binary_run = create_benchmark_experiment_app(\n",
    "    #     app_name=f\"Snowflake IT - {provider.model_engine}\",\n",
    "    #     app_version='answer_relevance_binary',\n",
    "    #     benchmark_experiment=TruBenchmarkExperiment(\n",
    "    #         feedback_fn=answer_relevance_binary,\n",
    "    #         agg_funcs=snowflake_it_metrics,\n",
    "    #         benchmark_params=benchmark_params\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    # with answer_relevance_binary_run as recording:\n",
    "    #     feedback_res = answer_relevance_binary_run.app(snowflake_it_for_answer_relevance)\n",
    "    #     write_results(feedback_scores=feedback_res, file_name=f\"results/{provider.model_engine}-answer_relevance_binary_feedback_scores.csv\")\n",
    "\n",
    "    def groundedness_binary(input, output) -> float:\n",
    "        return provider.groundedness_measure_with_cot_reasons(\n",
    "            source=input,\n",
    "            statement=output,\n",
    "            criteria=\"A grounded response to the query should get a score of 1, and an ungrounded response should get a score of 0. The score can only be either 0 or 1 (binary).\",\n",
    "            min_score_val=0,\n",
    "            max_score_val=1,\n",
    "            use_sent_tokenize=True,\n",
    "        )[0]\n",
    "\n",
    "    def groundedness_likert_4(input, output) -> float:\n",
    "        return provider.groundedness_measure_with_cot_reasons(\n",
    "            source=input,\n",
    "            statement=output,\n",
    "            use_sent_tokenize=True,\n",
    "            min_score_val=0,\n",
    "            max_score_val=3,\n",
    "            criteria=likert4_groundedness_criteria,\n",
    "        )[0]\n",
    "\n",
    "    # Define a function to wrap the run_feedback_experiment call\n",
    "    def run_experiment(\n",
    "        feedback_fn, app_name, app_version, dataset_df, true_labels\n",
    "    ):\n",
    "        run_feedback_experiment(\n",
    "            feedback_func_wrapper=feedback_fn,\n",
    "            app_name=app_name,\n",
    "            app_version=app_version,\n",
    "            dataset_df=dataset_df,\n",
    "            true_labels=true_labels,\n",
    "        )\n",
    "\n",
    "    context_relevance_experiments = [\n",
    "        # {\n",
    "        #     \"feedback_fn\": context_relevance_binary,\n",
    "        #     \"app_name\": f\"Snowflake IT balanced - {provider.model_engine}\",\n",
    "        #     \"app_version\": \"context_relevance_binary\",\n",
    "        #     \"dataset_df\": snowflake_it_for_context_relevance,\n",
    "        #     \"true_labels\": snowflake_it_for_context_relevance_true_labels,\n",
    "        # },\n",
    "        # {\n",
    "        #     \"feedback_fn\": context_relevance_binary,\n",
    "        #     \"app_name\": f\"Hotpot QA (800 samples) - {provider.model_engine}\",\n",
    "        #     \"app_version\": \"context_relevance_binary\",\n",
    "        #     \"dataset_df\": hotpotqa_subset_for_context_relevance,\n",
    "        #     \"true_labels\": hotpotqa_subset_for_context_relevance_true_labels,\n",
    "        # },\n",
    "        \n",
    "        {\n",
    "            \"feedback_fn\": context_relevance_binary,\n",
    "            \"app_name\": f\"MS MARCO hard negatives (first 400 samples) - {provider.model_engine}\",\n",
    "            \"app_version\": \"context_relevance_binary\",\n",
    "            \"dataset_df\": ms_marco_hard_neg_balanced,\n",
    "            \"true_labels\": [row[\"expected_score\"] for _, row in ms_marco_hard_neg_balanced.iterrows()],\n",
    "        },\n",
    "        # {\n",
    "        #     # MS MARCO V2 for context relevance\n",
    "        #     'feedback_fn': context_relevance_binary,\n",
    "        #     'app_name': f\"MS MARCO V2 balanced (300 samples) - {provider.model_engine}\",\n",
    "        #     'app_version': 'context_relevance_binary',\n",
    "        #     'dataset_df': ms_marco_balanced_sample_300,\n",
    "        #     'true_labels': [row[\"expected_score\"] for _, row in ms_marco_balanced_sample_300.iterrows()]\n",
    "        # },\n",
    "    ]\n",
    "\n",
    "    groundedness_experiments = [\n",
    "        # {\n",
    "        #     \"feedback_fn\": groundedness_binary,\n",
    "        #     \"app_name\": f\"Snowflake IT balanced - {provider.model_engine}\",\n",
    "        #     \"app_version\": \"groundedness_binary\",\n",
    "        #     \"dataset_df\": snowflake_it_for_groundness,\n",
    "        #     \"true_labels\": snowflake_it_for_groundness_true_labels,\n",
    "        # },\n",
    "        # {\n",
    "        #     'feedback_fn': groundedness_likert_4,\n",
    "        #     'app_name': f\"SummEval (200 samples) - {provider.model_engine}\",\n",
    "        #     'app_version': 'groundedness_likert4',\n",
    "        #     'dataset_df': summeval_subset,\n",
    "        #     'true_labels': [row[\"expected_score\"] for _, row in summeval_subset.iterrows()]\n",
    "        # },\n",
    "        # {\n",
    "        #     'feedback_fn': groundedness_likert_4,\n",
    "        #     'app_name': f\"QAGS CNN_DM - {provider.model_engine}\",\n",
    "        #     'app_version': 'groundedness_likert4',\n",
    "        #     'dataset_df': qags_cnn_dm,\n",
    "        #     'true_labels': qags_cnn_dm_true_labels\n",
    "        # },\n",
    "        # {\n",
    "        #     'feedback_fn': groundedness_likert_4,\n",
    "        #     'app_name': f\"QAGS XSum - {provider.model_engine}\",\n",
    "        #     'app_version': 'groundedness_likert4',\n",
    "        #     'dataset_df': qags_xsum,\n",
    "        #     'true_labels': qqags_xsum_true_labels\n",
    "        # }\n",
    "    ]\n",
    "    answer_relevance_experiments = [\n",
    "        # {\n",
    "        #     \"feedback_fn\": answer_relevance_binary,\n",
    "        #     \"app_name\": f\"Snowflake IT balanced - {provider.model_engine}\",\n",
    "        #     \"app_version\": \"answer_relevance_binary\",\n",
    "        #     \"dataset_df\": snowflake_it_for_answer_relevance,\n",
    "        #     \"true_labels\": snowflake_it_answer_relevance_true_labels,\n",
    "        # },\n",
    "        # {\n",
    "        #     \"feedback_fn\": answer_relevance_binary,\n",
    "        #     \"app_name\": f\"Hotpot QA (400 samples) - {provider.model_engine}\",\n",
    "        #     \"app_version\": \"answer_relevance_binary\",\n",
    "        #     \"dataset_df\": hotpotqa_subset_for_answer_relevance,\n",
    "        #     \"true_labels\": hotpotqa_subset_for_answer_relevance_true_labels,\n",
    "        # },\n",
    "    ]\n",
    "\n",
    "    for exp in (\n",
    "        answer_relevance_experiments\n",
    "        + context_relevance_experiments\n",
    "        + groundedness_experiments\n",
    "    ):\n",
    "        print(f\"Running experiment: {exp['app_name']} - {exp['app_version']}\")\n",
    "        if \"groundedness\" in exp[\"app_version\"]:\n",
    "            print(f\"Groundedness system prompt: {Groundedness.system_prompt}\")\n",
    "\n",
    "        run_experiment(\n",
    "            exp[\"feedback_fn\"],\n",
    "            exp[\"app_name\"],\n",
    "            exp[\"app_version\"],\n",
    "            exp[\"dataset_df\"],\n",
    "            exp[\"true_labels\"],\n",
    "        )\n",
    "\n",
    "\n",
    "# for provider in ALL_PROVIDERS:\n",
    "#     runn_all_experiments_for_provider(provider)\n",
    "#     # Run the experiments in parallel using ThreadPoolExecutor\n",
    "#     with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#         # Submit tasks to the executor\n",
    "#         futures = [\n",
    "#             executor.submit(run_experiment,\n",
    "#                             exp['feedback_fn'],\n",
    "#                             exp['app_name'],\n",
    "#                             exp['app_version'],\n",
    "#                             exp['dataset_df'],\n",
    "#                             exp['true_labels'])\n",
    "#             for exp in experiments\n",
    "#         ]\n",
    "\n",
    "#         # Optionally, gather results or handle exceptions\n",
    "#         for future in concurrent.futures.as_completed(futures):\n",
    "#             try:\n",
    "#                 future.result()  # This will re-raise any exceptions caught during execution\n",
    "#             except Exception as e:\n",
    "#                 traceback.print_exc()\n",
    "#                 print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Submit the run_experiment_for_provider function for each provider\n",
    "    futures = [\n",
    "        executor.submit(runn_all_experiments_for_provider, provider)\n",
    "        for provider in ALL_PROVIDERS\n",
    "    ]\n",
    "\n",
    "    # Optionally, gather results or exceptions\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            result = (\n",
    "                future.result()\n",
    "            )  # This will re-raise any exceptions caught during execution\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(true_labels, predicted_scores, threshold=0.5, title=\"Confusion Matrix\"):\n",
    "    # Binarize the predicted scores based on the threshold\n",
    "    predicted_labels = [1 if score >= threshold else 0 for score in predicted_scores]\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    \n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "                yticklabels=['True 0', 'True 1'])\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini: recall: 0.7800, precision: 0.9811, f1: 0.8691, Cohen's Kappa: 0.7650, Matthews: 0.7816, AUC: 0.8825, avg_latency: 0.6331\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o: recall: 0.8200, precision: 0.9880, f1: 0.8962, Cohen's Kappa: 0.8100, Matthews: 0.8220, AUC: 0.9050, avg_latency: 0.5660\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snowflake-arctic: recall: 0.9350, precision: 0.8821, f1: 0.9078, Cohen's Kappa: 0.8100, Matthews: 0.8115, AUC: 0.9050, avg_latency: 1.2559\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama3.1-8b: recall: 0.7800, precision: 0.9571, f1: 0.8595, Cohen's Kappa: 0.7450, Matthews: 0.7581, AUC: 0.8725, avg_latency: 1.1540\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistral-large: recall: 0.8200, precision: 0.9591, f1: 0.8841, Cohen's Kappa: 0.7850, Matthews: 0.7934, AUC: 0.8925, avg_latency: 1.1704\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhAAAAGJCAYAAADbgQqfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA1/0lEQVR4nO3df3xP9f//8ftrY6+N/SKb+Tljbz+GEvWWVljkRyW1fryRbH6VqBgr1Mebjaz0Q/J+FyqS9Aslv0ryK6RfmIiW+ZFihDHNmNme3z/67vX2so0d2c7kdr1cdnF5Pc/znPM4Z6+X3V/nPM85DmOMEQAAgAUedhcAAAAuPwQIAABgGQECAABYRoAAAACWESAAAIBlBAgAAGAZAQIAAFhGgAAAAJYRIAAAgGUECFxxduzYoQ4dOiggIEAOh0Pz58+/pMvfs2ePHA6H3nrrrUu63MtZ27Zt1bZtW7vLuCixsbGqU6eO3WW4Ke7+XLVqlRwOh1atWlXiNeHKQ4CALXbu3KmHH35YdevWlbe3t/z9/RUZGalJkybp5MmTJbrumJgYbdmyRc8884xmzZql6667rkTXV5piY2PlcDjk7+9f6H7csWOHHA6HHA6HXnjhBcvL379/v8aMGaPk5ORLUO3f17Zt2zRmzBjt2bPH7lKAElPO7gJw5Vm8eLHuu+8+OZ1O9erVS02aNNHp06e1du1aPfHEE/rxxx81bdq0Eln3yZMntX79ej399NN69NFHS2QdoaGhOnnypMqXL18iy7+QcuXKKSsrSwsXLtT999/vNm327Nny9vbWqVOnLmrZ+/fvV0JCgurUqaNmzZoVe77PP//8otZXFrz++uvKy8uzNM+2bduUkJCgtm3blrmjF8ClQoBAqdq9e7e6deum0NBQrVixQtWqVXNNGzRokFJTU7V48eISW/+hQ4ckSYGBgSW2DofDIW9v7xJb/oU4nU5FRkbqvffeKxAg3n33Xd1+++2aN29eqdSSlZWlChUqyMvLq1TWVxJKOggaY3Tq1Cn5+PiU6HouhRMnTqhixYp2l4EyglMYKFUTJkxQZmam3nzzTbfwkC88PFyDBw92vT5z5ozGjh2revXqyel0qk6dOnrqqaeUnZ3tNl+dOnV0xx13aO3atfrnP/8pb29v1a1bV2+//barz5gxYxQaGipJeuKJJ+RwOFzfDos6zz1mzBg5HA63tmXLlummm25SYGCgfH191aBBAz311FOu6UWNgVixYoVuvvlmVaxYUYGBgeratau2b99e6PpSU1MVGxurwMBABQQEqHfv3srKyip6x56jR48e+vTTT3Xs2DFX23fffacdO3aoR48eBfqnp6crPj5eTZs2la+vr/z9/dW5c2dt3rzZ1WfVqlW6/vrrJUm9e/d2nQrJ3862bduqSZMm2rBhg1q3bq0KFSq49su55+xjYmLk7e1dYPs7duyoSpUqaf/+/cXe1gtxOBx69NFHNWfOHEVERMjHx0etWrXSli1bJElTp05VeHi4vL291bZt2wKnHQp7b7z//vtq0aKF/Pz85O/vr6ZNm2rSpEmSpLfeekv33XefJCkqKsq1n/LHIeS/V5cuXarrrrtOPj4+mjp1qiRpxowZuuWWWxQcHCyn06mIiAi99tprl2xfSNKaNWt03333qXbt2nI6napVq5bi4uIKnPKKjY2Vr6+vdu7cqdtuu01+fn564IEHJP15JO/xxx9XlSpV5OfnpzvvvFP79u2Tw+HQmDFj3Jazb98+9enTR1WrVpXT6VTjxo01ffr0S7pNsAdHIFCqFi5cqLp16+rGG28sVv9+/fpp5syZuvfeezVs2DB98803SkpK0vbt2/Xxxx+79U1NTdW9996rvn37KiYmRtOnT1dsbKxatGihxo0bKzo6WoGBgYqLi1P37t112223ydfX11L9P/74o+644w5dffXVSkxMlNPpVGpqqtatW3fe+b744gt17txZdevW1ZgxY3Ty5ElNnjxZkZGR2rhxY4E/UPfff7/CwsKUlJSkjRs36o033lBwcLCee+65YtUZHR2tAQMG6KOPPlKfPn0k/Xn0oWHDhmrevHmB/rt27dL8+fN13333KSwsTAcPHtTUqVPVpk0bbdu2TdWrV1ejRo2UmJiof//733rooYd08803S5Lb7/LIkSPq3LmzunXrpp49e6pq1aqF1jdp0iStWLFCMTExWr9+vTw9PTV16lR9/vnnmjVrlqpXr16s7SyuNWvWaMGCBRo0aJAkKSkpSXfccYeefPJJvfrqqxo4cKCOHj2qCRMmqE+fPlqxYkWRy1q2bJm6d++udu3auX4f27dv17p16zR48GC1bt1ajz/+uF555RU99dRTatSokSS5/pWklJQUde/eXQ8//LD69++vBg0aSJJee+01NW7cWHfeeafKlSunhQsXauDAgcrLy3PV/lfNmTNHWVlZeuSRR3TVVVfp22+/1eTJk/Xbb79pzpw5bn3PnDmjjh076qabbtILL7ygChUqSPozXHz44Yd68MEHdcMNN2j16tW6/fbbC6zr4MGDuuGGG1whLigoSJ9++qn69u2r48ePa8iQIZdkm2ATA5SSjIwMI8l07dq1WP2Tk5ONJNOvXz+39vj4eCPJrFixwtUWGhpqJJkvv/zS1fb7778bp9Nphg0b5mrbvXu3kWSef/55t2XGxMSY0NDQAjWMHj3anP0xmThxopFkDh06VGTd+euYMWOGq61Zs2YmODjYHDlyxNW2efNm4+HhYXr16lVgfX369HFb5t13322uuuqqItd59nZUrFjRGGPMvffea9q1a2eMMSY3N9eEhISYhISEQvfBqVOnTG5uboHtcDqdJjEx0dX23XffFdi2fG3atDGSzJQpUwqd1qZNG7e2pUuXGklm3LhxZteuXcbX19fcddddF9xGqyQZp9Npdu/e7WqbOnWqkWRCQkLM8ePHXe0jR440ktz6nvveGDx4sPH39zdnzpwpcp1z5swxkszKlSsLTMt/r3722WcFpmVlZRVo69ixo6lbt65bW2H7szArV64sUEdh60hKSjIOh8P88ssvrraYmBgjyYwYMcKt74YNG4wkM2TIELf22NhYI8mMHj3a1da3b19TrVo1c/jwYbe+3bp1MwEBAYXWgssHpzBQao4fPy5J8vPzK1b/JUuWSJKGDh3q1j5s2DBJKjBWIiIiwvWtWJKCgoLUoEED7dq166JrPlf+2IlPPvmk2APr0tLSlJycrNjYWFWuXNnVfvXVV+vWW291befZBgwY4Pb65ptv1pEjR1z7sDh69OihVatW6cCBA1qxYoUOHDhQ6OkL6c9xEx4ef/53kJubqyNHjrhOz2zcuLHY63Q6nerdu3ex+nbo0EEPP/ywEhMTFR0dLW9vb9eh/EutXbt2bkd5WrZsKUm655573N6P+e3ne88EBgbqxIkTWrZs2UXXExYWpo4dOxZoP3scREZGhg4fPqw2bdpo165dysjIuOj1FbWOEydO6PDhw7rxxhtljNGmTZsK9H/kkUfcXn/22WeSpIEDB7q1P/bYY26vjTGaN2+eunTpImOMDh8+7Prp2LGjMjIyLL23UPYQIFBq/P39JUl//PFHsfr/8ssv8vDwUHh4uFt7SEiIAgMD9csvv7i1165du8AyKlWqpKNHj15kxQX961//UmRkpPr166eqVauqW7du+vDDD88bJvLrzD9MfbZGjRrp8OHDOnHihFv7udtSqVIlSbK0LfnnrT/44APNnj1b119/fYF9mS8vL08TJ07UP/7xDzmdTlWpUkVBQUH64YcfLP3hqlGjhqUBky+88IIqV66s5ORkvfLKKwoODr7gPIcOHdKBAwdcP5mZmRec59z9GRAQIEmqVatWoe3n288DBw5U/fr11blzZ9WsWVN9+vRx/VEtrrCwsELb161bp/bt27vGyQQFBbnGkZzv93D2/jhw4MB5L4Xeu3evK8z6+voqKChIbdq0KXQd5cqVU82aNd3a8j+X527Due+tQ4cO6dixY5o2bZqCgoLcfvJD5u+//15knSj7GAOBUuPv76/q1atr69atluY7dxBjUTw9PQttN8Zc9Dpyc3PdXvv4+OjLL7/UypUrtXjxYn322Wf64IMPdMstt+jzzz8vsgar/sq25HM6nYqOjtbMmTO1a9euAoPbzjZ+/HiNGjVKffr00dixY1W5cmV5eHhoyJAhli5htHolwaZNm1x/RLZs2aLu3btfcJ7rr7/eLTyOHj36vNsmFb0/L2Y/BwcHKzk5WUuXLtWnn36qTz/9VDNmzFCvXr00c+bMC9YvFb6fdu7cqXbt2qlhw4Z66aWXVKtWLXl5eWnJkiWaOHHieX8P5w5InjFjhmJjYwv0y83N1a233qr09HQNHz5cDRs2VMWKFbVv3z7FxsYWWMfZR6asyl9Wz549FRMTU2ifq6+++qKWjbKBAIFSdccdd2jatGlav369WrVqdd6+oaGhysvL044dO9wGoB08eFDHjh1zXVFxKVSqVMntioV85x7lkCQPDw+1a9dO7dq100svvaTx48fr6aef1sqVK9W+fftCt0P6c+DcuX766SdVqVKlxC6N69Gjh6ZPny4PDw9169atyH5z585VVFSU3nzzTbf2Y8eOqUqVKq7XxQ1zxXHixAn17t1bERERuvHGGzVhwgTdfffdris9ijJ79my3b9h169a9ZDUVl5eXl7p06aIuXbooLy9PAwcO1NSpUzVq1CiFh4df1H5auHChsrOztWDBArcjJitXrrzgvOeeTmncuHGh/bZs2aKff/5ZM2fOVK9evYqc/3zyP5e7d+/WP/7xD1d7amqqW7+goCD5+fkpNze30M8FLn+cwkCpevLJJ1WxYkX169dPBw8eLDB9586drsvhbrvtNknSyy+/7NbnpZdekqRCR31frHr16ikjI0M//PCDqy0tLa3AlR7p6ekF5s2/odK5l5bmq1atmpo1a6aZM2e6hZStW7fq888/d21nSYiKitLYsWP1n//8RyEhIUX28/T0LPCte86cOdq3b59bW37QKSxsWTV8+HDt3btXM2fO1EsvvaQ6deooJiamyP2YLzIyUu3bt3f9lHaAOHLkiNtrDw8P1zfp/NovZj/lHw05+/eQkZGhGTNmXHDes/dH+/btC71Euqh1GGNcn7niyB+78eqrr7q1T548ucC67rnnHs2bN6/Qo47592TB5YsjEChV9erV07vvvqt//etfatSokdudKL/66ivNmTPHdej1mmuuUUxMjKZNm6Zjx46pTZs2+vbbbzVz5kzdddddioqKumR1devWTcOHD9fdd9+txx9/XFlZWXrttddUv359t4FeiYmJ+vLLL3X77bcrNDRUv//+u1599VXVrFlTN910U5HLf/7559W5c2e1atVKffv2dV3GGRAQcMHD73+Fh4eH/u///u+C/e644w4lJiaqd+/euvHGG7VlyxbNnj27wB/nevXqKTAwUFOmTJGfn58qVqyoli1bFnlOvygrVqzQq6++qtGjR7suK50xY4batm2rUaNGacKECZaWV5r69eun9PR03XLLLapZs6Z++eUXTZ48Wc2aNXMdKWvWrJk8PT313HPPKSMjQ06n03V/h6J06NDBdWTj4YcfVmZmpl5//XUFBwcrLS3tktTesGFD1atXT/Hx8dq3b5/8/f01b948S2NrWrRooXvuuUcvv/yyjhw54rqM8+eff5bkfpTq2Wef1cqVK9WyZUv1799fERERSk9P18aNG/XFF18UGshxGbHr8g9c2X7++WfTv39/U6dOHePl5WX8/PxMZGSkmTx5sjl16pSrX05OjklISDBhYWGmfPnyplatWmbkyJFufYz589K422+/vcB6zr3crajLOI0x5vPPPzdNmjQxXl5epkGDBuadd94pcBnn8uXLTdeuXU316tWNl5eXqV69uunevbv5+eefC6zj3Esdv/jiCxMZGWl8fHyMv7+/6dKli9m2bZtbn/z1nXuZ6IwZMwpcXliYsy/jLEpRl3EOGzbMVKtWzfj4+JjIyEizfv36Qi8X/OSTT0xERIQpV66c23a2adPGNG7cuNB1nr2c48ePm9DQUNO8eXOTk5Pj1i8uLs54eHiY9evXn3cbrJBkBg0a5NZW1Psg/7LHOXPmuNrOvYxz7ty5pkOHDiY4ONh4eXmZ2rVrm4cfftikpaW5Lev11183devWNZ6enm6XUhb1XjXGmAULFpirr77aeHt7mzp16pjnnnvOTJ8+vcDv/q9cxrlt2zbTvn174+vra6pUqWL69+9vNm/eXOA9e7730okTJ8ygQYNM5cqVXZffpqSkGEnm2Wefdet78OBBM2jQIFOrVi1Tvnx5ExISYtq1a2emTZt2wfpRtjmMsTAqCwCAQiQnJ+vaa6/VO++847pjJf7eGAMBALCksMtEX375ZXl4eKh169Y2VAQ7MAYCAGDJhAkTtGHDBkVFRalcuXKuy1kfeuihAvfWwN8XpzAAAJYsW7ZMCQkJ2rZtmzIzM1W7dm09+OCDevrpp1WuHN9LrxQECAAAYBljIAAAgGUECAAAYBkBAgAAWPa3HO3ic+2jdpcA4DyOfvcfu0sAUATvYiYDjkAAAADLCBAAAMAyAgQAALCMAAEAACwjQAAAAMsIEAAAwDICBAAAsIwAAQAALCNAAAAAywgQAADAMgIEAACwjAABAAAsI0AAAADLCBAAAMAyAgQAALCMAAEAACwjQAAAAMsIEAAAwDICBAAAsIwAAQAALCNAAAAAywgQAADAMgIEAACwjAABAAAsI0AAAADLCBAAAMAyAgQAALCMAAEAACwjQAAAAMsIEAAAwDICBAAAsIwAAQAALCNAAAAAywgQAADAMgIEAACwjAABAAAsI0AAAADLCBAAAMAyAgQAALCMAAEAACwjQAAAAMsIEAAAwDICBAAAsIwAAQAALCNAAAAAywgQAADAMgIEAACwjAABAAAsI0AAAADLCBAAAMAyAgQAALCMAAEAACwjQAAAAMsIEAAAwDICBAAAsIwAAQAALCNAAAAAy8rZufLTp09r/vz5Wr9+vQ4cOCBJCgkJ0Y033qiuXbvKy8vLzvIAAEARbDsCkZqaqkaNGikmJkabNm1SXl6e8vLytGnTJvXq1UuNGzdWamqqXeUBAIDzcBhjjB0rvvXWW1WxYkW9/fbb8vf3d5t2/Phx9erVSydPntTSpUstL9vn2kcvVZkASsDR7/5jdwkAiuBdzHMTtp3CWLdunb799tsC4UGS/P39NXbsWLVs2dKGygAAwIXYdgojMDBQe/bsKXL6nj17FBgYWGr1AACA4rPtCES/fv3Uq1cvjRo1Su3atVPVqlUlSQcPHtTy5cs1btw4PfbYY3aVBwAAzsO2MRCS9Nxzz2nSpEk6cOCAHA6HJMkYo5CQEA0ZMkRPPvnkRS2XMRBA2cYYCKDsKu4YCFsDRL7du3e7XcYZFhb2l5ZHgADKNgIEUHaV+UGUZwsLC/vLoQEAAJQe7kQJAAAsI0AAAADLCBAAAMAyAgQAALCsTAyiXLNmjaZOnaqdO3dq7ty5qlGjhmbNmqWwsDDddNNNdpeHEhDZvJ7ierVX84jaqhYUoPvjpmnhqh9c04Mr+2nc4K5q36qRAnx9tHZjqoZOmKOdew+5+kx+uptuadlA1YIClHkyW19v3q3/m/SJft5z0I5NAq4onW+9Rfv37yvQ/q9uPfTUqNE2VITSZvsRiHnz5qljx47y8fHRpk2blJ2dLUnKyMjQ+PHjba4OJaWij1Nbft6nIUkfFDr9w4kPKaxmFd03ZKpu6P6s9qala8mUx1TB+39PaN20/Vc9NOYdNYsepzsH/lcOh0OLXh0kDw9HaW0GcMWa/cFcLV+11vUz9Y0ZkqRbO3ayuTKUFtsDxLhx4zRlyhS9/vrrKl++vKs9MjJSGzdutLEylKTP121TwquLtGDlDwWmhdcOVsurw/T4M+9rw7a92vHL73p8/AfydpbX/Z1buPpN/2id1m3cqb1p6Ur+6Tcl/HehalWrrNDqV5XmpgBXpMqVK6tKUJDr58tVK1WrVm1dd/0/7S4NpcT2AJGSkqLWrVsXaA8ICNCxY8dKvyDYzun155m1U6fPuNqMMTp9+oxubFav0HkqeHup1503aPdvh/XbgaOlUieAP+WcPq3Fixboruh7XHcVxt+f7QEiJCREqampBdrXrl2runXr2lAR7Jay54D2pqVr7GN3KtDPR+XLeWpYbHvVDKmkkCoBbn0fuu9mHVr3oo6sf0kdIiN0+yP/Uc6ZXJsqB65MK1Z8oT/++EN33nW33aWgFNkeIPr376/Bgwfrm2++kcPh0P79+zV79mzFx8frkUceueD82dnZOn78uNuPyeMPyOXszJk8dRv2usJDg5X25fNKX/+SWl9XX5+t/VF5Js+t7/uffqcbuj+r9n0nasfeQ3rnuT6uIxgASsfH8+Yp8qbWCg6uancpKEW2/087YsQI5eXlqV27dsrKylLr1q3ldDoVHx9frKdxJiUlKSEhwa3Ns+r1Kl+N83CXs03bf9UN3Z6Vv6+3vMqX0+Gjmfry7Xht2LbXrd/xzFM6nnlKO/ce0rc/7FHalxPU9ZZr9OFnG2yqHLiy7N+/T998/ZVemjTZ7lJQymw/AuFwOPT0008rPT1dW7du1ddff61Dhw5p7NixxZp/5MiRysjIcPspV7XFhWfEZeF45ikdPpqperWD1DyithatKjjoMp/D4ZBDDnmVtz0XA1eMTz7+SJUrX6WbW7e1uxSUsjLzP62Xl5ciIiIsz+d0OuV0Ot3aHB6el6oslJCKPl6qVyvI9bpOjat0df0aOno8S78eOKro9tfq0NFM/XogXU3+UV0vPHGvFq76Qcu//snV/96OLbR8/XYdPpqpGlUDNax3B53MztHStT/atVnAFSUvL0+ffPyRunS9S+XKlZk/Jygltv/Go6Kizjtqd8WKFaVYDUpL84hQff7GYNfrCfH3SJJmLfhaD41+RyFB/npuWLSCr/LTgcPHNXvRN0qa9pmrf/bpM4q8tp4e7dFWlfwr6Pcjf2jtxlRFxb6oQ0czS317gCvR1+u/Ulraft0VfY/dpcAGDmOMsbOAuLg4t9c5OTlKTk7W1q1bFRMTo0mTJlleps+1j16q8gCUgKPf/cfuEgAUwbuYhxZsPwIxceLEQtvHjBmjzEy+SQIAUBbZPoiyKD179tT06dPtLgMAABSizAaI9evXy9vb2+4yAABAIWw/hREdHe322hijtLQ0ff/99xo1apRNVQEAgPOxPUAEBLjfmtjDw0MNGjRQYmKiOnToYFNVAADgfGwNELm5uerdu7eaNm2qSpUq2VkKAACwwNYxEJ6enurQoQNP3QQA4DJj+yDKJk2aaNeuXXaXAQAALLA9QIwbN07x8fFatGiR0tLSCjxZEwAAlD223YkyMTFRw4YNk5+f3/+KOeuW1sYYORwO5eZafzQ3d6IEyjbuRAmUXcW9E6VtAcLT01NpaWnavn37efu1adPG8rIJEEDZRoAAyq4yfyvr/NxyMQEBAADYy9YxEOd7CicAACi7bL0PRP369S8YItLT00upGgAAUFy2BoiEhIQCd6IEAABln60Bolu3bgoODrazBAAAcBFsGwPB+AcAAC5ftgUIm64eBQAAl4BtpzDy8vLsWjUAAPiLbL+VNQAAuPwQIAAAgGUECAAAYBkBAgAAWEaAAAAAlhEgAACAZQQIAABgGQECAABYRoAAAACWESAAAIBlBAgAAGAZAQIAAFhGgAAAAJYRIAAAgGUECAAAYBkBAgAAWEaAAAAAlhEgAACAZQQIAABgGQECAABYRoAAAACWESAAAIBlBAgAAGAZAQIAAFhGgAAAAJYRIAAAgGUECAAAYBkBAgAAWEaAAAAAlhEgAACAZQQIAABgGQECAABYRoAAAACWESAAAIBlBAgAAGAZAQIAAFhGgAAAAJYRIAAAgGUECAAAYBkBAgAAWHZRAWLNmjXq2bOnWrVqpX379kmSZs2apbVr117S4gAAQNlkOUDMmzdPHTt2lI+PjzZt2qTs7GxJUkZGhsaPH3/JCwQAAGWP5QAxbtw4TZkyRa+//rrKly/vao+MjNTGjRsvaXEAAKBsshwgUlJS1Lp16wLtAQEBOnbs2KWoCQAAlHGWA0RISIhSU1MLtK9du1Z169a9JEUBAICyzXKA6N+/vwYPHqxvvvlGDodD+/fv1+zZsxUfH69HHnmkJGoEAABlTDmrM4wYMUJ5eXlq166dsrKy1Lp1azmdTsXHx+uxxx4riRoBAEAZ4zDGmIuZ8fTp00pNTVVmZqYiIiLk6+t7qWu7aD7XPmp3CQDO4+h3/7G7BABF8C7moQXLRyDyeXl5KSIi4mJnBwAAlzHLASIqKkoOh6PI6StWrPhLBQEAgLLPcoBo1qyZ2+ucnBwlJydr69atiomJuVR1AQCAMsxygJg4cWKh7WPGjFFmZuZfLggAAJR9l+xhWj179tT06dMv1eIAAEAZdtGDKM+1fv16eXt7X6rF/SWbP51gdwkAzqPqg7PsLgFAETLee7BY/SwHiOjoaLfXxhilpaXp+++/16hRo6wuDgAAXIYsB4iAgAC31x4eHmrQoIESExPVoUOHS1YYAAAouywFiNzcXPXu3VtNmzZVpUqVSqomAABQxlkaROnp6akOHTrw1E0AAK5wlq/CaNKkiXbt2lUStQAAgMuE5QAxbtw4xcfHa9GiRUpLS9Px48fdfgAAwN9fscdAJCYmatiwYbrtttskSXfeeafbLa2NMXI4HMrNzb30VQIAgDKl2AEiISFBAwYM0MqVK0uyHgAAcBkodoDIf+p3mzZtSqwYAABwebA0BuJ8T+EEAABXDkv3gahfv/4FQ0R6evpfKggAAJR9lgJEQkJCgTtRAgCAK4+lANGtWzcFBweXVC0AAOAyUewxEIx/AAAA+YodIPKvwgAAACj2KYy8vLySrAMAAFxGLN/KGgAAgAABAAAsI0AAAADLCBAAAMAyAgQAALCMAAEAACwjQAAAAMsIEAAAwDICBAAAsIwAAQAALCNAAAAAywgQAADAMgIEAACwjAABAAAsI0AAAADLCBAAAMAyAgQAALCMAAEAACwjQAAAAMsIEAAAwDICBAAAsIwAAQAALCNAAAAAywgQAADAMgIEAACwjAABAAAsI0AAAADLCBAAAMAyAgQAALCMAAEAACwjQAAAAMsIEAAAwDICBAAAsIwAAQAALCNAAAAAywgQAADAMgIEAACwjAABAAAsI0AAAADLCBAAAMAyAgQAALCszAaIgwcPKjEx0e4yAABAIcpsgDhw4IASEhLsLgMAABSinF0r/uGHH847PSUlpZQqAQAAVtkWIJo1ayaHwyFjTIFp+e0Oh8OGygAAwIXYFiAqV66sCRMmqF27doVO//HHH9WlS5dSrgoAABSHbQGiRYsW2r9/v0JDQwudfuzYsUKPTgAAAPvZFiAGDBigEydOFDm9du3amjFjRilWBAAAisth/oZf838+kGV3CQDO4/q4eXaXAKAIGe89WKx+ZfYyTgAAUHYRIAAAgGUECAAAYBkBAgAAWEaAAAAAltl2GefZ1qxZo6lTp2rnzp2aO3euatSooVmzZiksLEw33XST3eWhFCyZ/6E+/WSuDh7YL0mqXaeuusU8pOtu+N/v/6etmzXrjf8qZfsWeXh4qm54fSW88KqcTm+7ygb+lm5sGKzH72isZnUrq1qlCurx4iot/v5Xtz71q/sroUdzRTaqqnIeHkrZd0wPTlyt344UvApu7vBbdGuzGoUuB5cv2wPEvHnz9OCDD+qBBx7Qpk2blJ2dLUnKyMjQ+PHjtWTJEpsrRGmoElRVMQ8/puo1a8sYaflnC/XM03F6+Y33FRpWTz9t3azRTz6qex/orYcGD5enp6d2p/4sDwcH0YBLrYKznLbuPap3VqVq9rC2BaaHBftq6ZhOmrUqVUlzN+uPrBw1rBWoUzl5BfoO7NxIf7+bBUAqAwFi3LhxmjJlinr16qX333/f1R4ZGalx48bZWBlK0z8j27i97tX/UX36yRylbPtBoWH19MZ/X1SXe7rpvgf6uPrUrF2nlKsErgxfbN6vLzbvL3L6qH9dq8+T9+nf7250te3+PbNAv6ahlfTo7Y3U9ukl2jHlvhKpFfax/etbSkqKWrduXaA9ICBAx44dK/2CYLvc3Fx9ufwznTp1Ug0bX61jR9OVsm2LAgIr64mBMXrwrnYa8Xhf/fjDJrtLBa44DofU4doaSk07ro9GtFPqlPu0fGxn3X5dLbd+Pl6eeuPRmxQ/41v9nnHKpmpRkmwPECEhIUpNTS3QvnbtWtWtW9eGimCXPTt36L5ONyr61pZ69aVn9PS4F1W7Tj0d2P+bJOm9t6aq4x3RGjPhv6pXv5H+b+jD2v/bLzZXDVxZgvy95edTXnF3NtEXm/fr7qQvtOi7vXonro0iGwW7+iU9eJ2+/fmQlmz4zcZqUZJsP4XRv39/DR48WNOnT5fD4dD+/fu1fv16xcfHa9SoURecPzs72zVuIt/p7Fx5OZ0lVTJKSI3adTTpjfeVdSJT61Z/oYnj/62kV96QMX+eV+3U5R61v62rJKle/Yb6YcO3WrbkE8U89LidZQNXFA8PhyRpyYZf9eqn2yVJW345qn/WD1Kf9vW1bvvv6tyiplo3DtHNIxfbWSpKmO0BYsSIEcrLy1O7du2UlZWl1q1by+l0Kj4+Xo899tgF509KSlJCQoJb26PDntJj8U+XVMkoIeXLl1f1mrUlSeENIrTjpx+1YO57uveB3pKkWnXcj0jVDA3ToYMHSr1O4Ep25Hi2cs7k6ad9GW7tP+/L0A0N/jwC0bpxiMKq+mnvm/9y6zMrrrW++ul33TF2WanVi5Jje4BwOBx6+umn9cQTTyg1NVWZmZmKiIiQr69vseYfOXKkhg4d6ta292huSZSKUmbyjHJyTqtqSHVVrhKkfb/ucZu+/9df1KJlpD3FAVeonNw8bdx1WP+o5u/WXq+av349/OcTlid+slVvr3A/Nf3181008u0N+mwjpzT+LmwPEPm8vLwUERFheT6n0ynnOacrvLJ4GuflZua0V9SiZaSCgqvpZNYJrV7+qbYkf6+E51+Vw+FQdLcYvTtjisLq1VdYeAOtWLpQv+3doxGJz9tdOvC3U9FZTnVD/FyvQ4N81TS0ko5mZuu3I1l6ZeE2zRh8s7766aDW/HhQ7a6prs7Na+r2sZ9Lkn7POFXowMnfjpzQL4cKXq2By5PtASIqKkoOh6PI6StWrCjFamCXjKPpmjh+lNKPHFbFir6qU+8fSnj+VV17/Q2SpK73PaDTp7P1xn9e1B9/ZCisXn0lvviaqtWodYElA7Dq2rpXafG/O7heJ/W6TpI0e/VODZzylRZ9/6vi3vxGQ+9soudirteO/cf14MTV+jrlkF0lwwYOY+y9xUdcXJzb65ycHCUnJ2vr1q2KiYnRpEmTLC/z5wMcgQDKsuvj5tldAoAiZLz3YLH62X4EYuLEiYW2jxkzRpmZHOoCAKAssv0+EEXp2bOnpk+fbncZAACgEGU2QKxfv17e3jwkCQCAssj2UxjR0dFur40xSktL0/fff1+sG0kBAIDSZ3uACAgIcHvt4eGhBg0aKDExUR06dChiLgAAYCdbA0Rubq569+6tpk2bqlKlSnaWAgAALLB1DISnp6c6dOjAUzcBALjM2D6IskmTJtq1a5fdZQAAAAtsDxDjxo1TfHy8Fi1apLS0NB0/ftztBwAAlD22jYFITEzUsGHDdNttt0mS7rzzTrdbWhtj5HA4lJvLg7EAAChrbAsQCQkJGjBggFauXGlXCQAA4CLZFiDyH8HRpk0bu0oAAAAXydYxEOd7CicAACi7bL0PRP369S8YItLT00upGgAAUFy2BoiEhIQCd6IEAABln60Bolu3bgoODrazBAAAcBFsGwPB+AcAAC5ftgWI/KswAADA5ce2Uxh5eXl2rRoAAPxFtt/KGgAAXH4IEAAAwDICBAAAsIwAAQAALCNAAAAAywgQAADAMgIEAACwjAABAAAsI0AAAADLCBAAAMAyAgQAALCMAAEAACwjQAAAAMsIEAAAwDICBAAAsIwAAQAALCNAAAAAywgQAADAMgIEAACwjAABAAAsI0AAAADLCBAAAMAyAgQAALCMAAEAACwjQAAAAMsIEAAAwDICBAAAsIwAAQAALCNAAAAAywgQAADAMgIEAACwjAABAAAsI0AAAADLCBAAAMAyAgQAALCMAAEAACwjQAAAAMsIEAAAwDICBAAAsIwAAQAALCNAAAAAywgQAADAMgIEAACwjAABAAAsI0AAAADLCBAAAMAyAgQAALCMAAEAACwjQAAAAMsIEAAAwDICBAAAsIwAAQAALCNAAAAAyxzGGGN3EcD5ZGdnKykpSSNHjpTT6bS7HABn4fN55SJAoMw7fvy4AgIClJGRIX9/f7vLAXAWPp9XLk5hAAAAywgQAADAMgIEAACwjACBMs/pdGr06NEM0ALKID6fVy4GUQIAAMs4AgEAACwjQAAAAMsIEAAAwDICBGwRGxuru+66y/W6bdu2GjJkSKnXsWrVKjkcDh07dqzU1w2UZXxGcSEECLjExsbK4XDI4XDIy8tL4eHhSkxM1JkzZ0p83R999JHGjh1brL6l/R/KqVOnNGjQIF111VXy9fXVPffco4MHD5bKuoGz8Rkt3LRp09S2bVv5+/sTNkoRAQJuOnXqpLS0NO3YsUPDhg3TmDFj9Pzzzxfa9/Tp05dsvZUrV5afn98lW96lFBcXp4ULF2rOnDlavXq19u/fr+joaLvLwhWKz2hBWVlZ6tSpk5566im7S7miECDgxul0KiQkRKGhoXrkkUfUvn17LViwQNL/Dmk+88wzql69uho0aCBJ+vXXX3X//fcrMDBQlStXVteuXbVnzx7XMnNzczV06FAFBgbqqquu0pNPPqlzrx4+9/Bodna2hg8frlq1asnpdCo8PFxvvvmm9uzZo6ioKElSpUqV5HA4FBsbK0nKy8tTUlKSwsLC5OPjo2uuuUZz5851W8+SJUtUv359+fj4KCoqyq3OwmRkZOjNN9/USy+9pFtuuUUtWrTQjBkz9NVXX+nrr7++iD0M/DV8RgsaMmSIRowYoRtuuMHi3sRfQYDAefn4+Lh9i1m+fLlSUlK0bNkyLVq0SDk5OerYsaP8/Py0Zs0arVu3Tr6+vurUqZNrvhdffFFvvfWWpk+frrVr1yo9PV0ff/zxedfbq1cvvffee3rllVe0fft2TZ06Vb6+vqpVq5bmzZsnSUpJSVFaWpomTZokSUpKStLbb7+tKVOm6Mcff1RcXJx69uyp1atXS/rzP9Ho6Gh16dJFycnJ6tevn0aMGHHeOjZs2KCcnBy1b9/e1dawYUPVrl1b69evt75DgUvsSv+MwkYG+P9iYmJM165djTHG5OXlmWXLlhmn02ni4+Nd06tWrWqys7Nd88yaNcs0aNDA5OXludqys7ONj4+PWbp0qTHGmGrVqpkJEya4pufk5JiaNWu61mWMMW3atDGDBw82xhiTkpJiJJlly5YVWufKlSuNJHP06FFX26lTp0yFChXMV1995da3b9++pnv37sYYY0aOHGkiIiLcpg8fPrzAss42e/Zs4+XlVaD9+uuvN08++WSh8wAlhc/o+RW2XpSccjZmF5RBixYtkq+vr3JycpSXl6cePXpozJgxrulNmzaVl5eX6/XmzZuVmppa4NzoqVOntHPnTmVkZCgtLU0tW7Z0TStXrpyuu+66AodI8yUnJ8vT01Nt2rQpdt2pqanKysrSrbfe6tZ++vRpXXvttZKk7du3u9UhSa1atSr2OoCygM8oygoCBNxERUXptddek5eXl6pXr65y5dzfIhUrVnR7nZmZqRYtWmj27NkFlhUUFHRRNfj4+FieJzMzU5K0ePFi1ahRw23aX7lHf0hIiE6fPq1jx44pMDDQ1X7w4EGFhIRc9HKBi8VnFGUFAQJuKlasqPDw8GL3b968uT744AMFBwfL39+/0D7VqlXTN998o9atW0uSzpw5ow0bNqh58+aF9m/atKny8vK0evVqt7EH+fK/XeXm5rraIiIi5HQ6tXfv3iK/FTVq1Mg12CzfhQZCtmjRQuXLl9fy5ct1zz33SPrzvO7evXv5ZgRb8BlFWcEgSvwlDzzwgKpUqaKuXbtqzZo12r17t1atWqXHH39cv/32myRp8ODBevbZZzV//nz99NNPGjhw4Hmv065Tp45iYmLUp08fzZ8/37XMDz/8UJIUGhoqh8OhRYsW6dChQ8rMzJSfn5/i4+MVFxenmTNnaufOndq4caMmT56smTNnSpIGDBigHTt26IknnlBKSoreffddvfXWW+fdvoCAAPXt21dDhw7VypUrtWHDBvXu3VutWrVixDcuC3/3z6gkHThwQMnJyUpNTZUkbdmyRcnJyUpPT/9rOw/nZ/cgDJQdZw/QsjI9LS3N9OrVy1SpUsU4nU5Tt25d079/f5ORkWGM+XNA1uDBg42/v78JDAw0Q4cONb169SpygJYxxpw8edLExcWZatWqGS8vLxMeHm6mT5/ump6YmGhCQkKMw+EwMTExxpg/B5W9/PLLpkGDBqZ8+fImKCjIdOzY0axevdo138KFC014eLhxOp3m5ptvNtOnT7/goKuTJ0+agQMHmkqVKpkKFSqYu+++26SlpZ13XwIlgc9o4UaPHm0kFfiZMWPG+XYn/iIe5w0AACzjFAYAALCMAAEAACwjQAAAAMsIEAAAwDICBAAAsIwAAQAALCNAAAAAywgQAADAMgIEgBITGxuru+66y/W6bdu2GjJkSKnXsWrVKjkcjvPenhmANQQI4AoUGxsrh8Mhh8MhLy8vhYeHKzExUWfOnCnR9X700UcaO3ZssfryRx8o23gaJ3CF6tSpk2bMmKHs7GwtWbJEgwYNUvny5TVy5Ei3fqdPn3Y9XfGvqly58iVZDgD7cQQCuEI5nU6FhIQoNDRUjzzyiNq3b68FCxa4Tjs888wzql69uho0aCBJ+vXXX3X//fcrMDBQlStXVteuXbVnzx7X8nJzczV06FAFBgbqqquu0pNPPqlzH7Vz7imM7OxsDR8+XLVq1ZLT6VR4eLjefPNN7dmzR1FRUZKkSpUqyeFwKDY2VpKUl5enpKQkhYWFycfHR9dcc43mzp3rtp4lS5aofv368vHxUVRUlFudAC4NAgQASZKPj49Onz4tSVq+fLlSUlK0bNkyLVq0SDk5OerYsaP8/Py0Zs0arVu3Tr6+vurUqZNrnhdffFFvvfWWpk+frrVr1yo9PV0ff/zxedfZq1cvvffee3rllVe0fft2TZ06Vb6+vqpVq5bmzZsnSUpJSVFaWpomTZokSUpKStLbb7+tKVOm6Mcff1RcXJx69uyp1atXS/oz6ERHR6tLly5KTk5Wv379NGLEiJLabcCVy+angQKwwdmPfc7LyzPLli0zTqfTxMfHm5iYGFO1alWTnZ3t6j9r1izToEEDk5eX52rLzs42Pj4+ZunSpcYYY6pVq2YmTJjgmp6Tk2Nq1qxZ5COhU1JSjCSzbNmyQmtcuXJlgcc4nzp1ylSoUMF89dVXbn379u1runfvbowxZuTIkSYiIsJt+vDhwy/4SGgA1jAGArhCLVq0SL6+vsrJyVFeXp569OihMWPGaNCgQWratKnbuIfNmzcrNTVVfn5+bss4deqUdu7cqYyMDKWlpally5auaeXKldN1111X4DRGvuTkZHl6eqpNmzbFrjk1NVVZWVm69dZb3dpPnz6ta6+9VpK0fft2tzokqVWrVsVeB4DiIUAAV6ioqCi99tpr8vLyUvXq1VWu3P/+O6hYsaJb38zMTLVo0UKzZ88usJygoKCLWr+Pj4/leTIzMyVJixcvVo0aNdymOZ3Oi6oDwMUhQABXqIoVKyo8PLxYfZs3b64PPvhAwcHB8vf3L7RPtWrV9M0336h169aSpDNnzmjDhg1q3rx5of2bNm2qvLw8rV69Wu3bty8wPf8ISG5urqstIiJCTqdTe/fuLfLIRaNGjbRgwQK3tq+//vrCGwnAEgZRArigBx54QFWqVFHXrl21Zs0a7d69W6tWrdLjjz+u3377TZI0ePBgPfvss5o/f75++uknDRw48Lz3cKhTp45iYmLUp08fzZ8/37XMDz/8UJIUGhoqh8OhRYsW6dChQ8rMzJSfn5/i4+MVFxenmTNnaufOndq4caMmT56smTNnSpIGDBigHTt26IknnlBKSoreffddvfXWWyW9i4ArDgECwAVVqFBBX375pWrXrq3o6Gg1atRIffv21alTp1xHJIYNG6YHH3xQMTExatWqlfz8/HT33Xefd7mvvfaa7r33Xg0cOFANGzZU//79deLECUlSjRo1lJCQoBEjRqhq1ap69NFHJUljx47VqFGjlJSUpEaNGqlTp05avHixwsLCJEm1a9fWvHnzNH/+fF1zzTWaMmWKxo8fX4J7B7gyOUxRI5wAAACKwBEIAABgGQECAABYRoAAAACWESAAAIBlBAgAAGAZAQIAAFhGgAAAAJYRIAAAgGUECAAAYBkBAgAAWEaAAAAAlv0/s2dPt0bvouwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trulens.feedback import GroundTruthAggregator\n",
    "\n",
    "for provider_name in [\n",
    "    \"gpt-4o-mini\",\n",
    "    \"gpt-4o\",\n",
    "    \"snowflake-arctic\",\n",
    "    \"llama3.1-8b\",\n",
    "    \"mistral-large\",\n",
    "]:\n",
    "    file_path = f\"/Users/dhuang/Documents/git/trulens/src/benchmark/trulens/benchmark/benchmark_frameworks/experiments/results/Hotpot QA (400 samples) - {provider_name}_answer_relevance_binary_results.csv\"\n",
    "    scores, labels, latencies = read_results(file_path)\n",
    "    f_recall = GroundTruthAggregator(labels).recall\n",
    "    f_precision = GroundTruthAggregator(labels).precision\n",
    "    f_f1_score = GroundTruthAggregator(labels).f1_score\n",
    "\n",
    "    binary_labels = []\n",
    "    for label in labels:\n",
    "        if label >= 0.5:\n",
    "            binary_labels.append(1)\n",
    "        else:\n",
    "            binary_labels.append(0)\n",
    "    \n",
    "    binary_scores = []\n",
    "    for score in scores:\n",
    "        if score >= 0.5:\n",
    "            binary_scores.append(1)\n",
    "        else:\n",
    "            binary_scores.append(0)\n",
    "\n",
    "    f_cohens_kappa = GroundTruthAggregator(binary_labels).cohens_kappa\n",
    "    f_auc = GroundTruthAggregator(labels).auc\n",
    "\n",
    "    f_mae = GroundTruthAggregator(labels).mae\n",
    "    f_pearson = GroundTruthAggregator(labels).pearson_correlation\n",
    "    f_spearman = GroundTruthAggregator(labels).spearman_correlation\n",
    "    f_matthews = GroundTruthAggregator(binary_labels).matthews_correlation\n",
    "    \n",
    "    recall = f_recall(scores)\n",
    "    precision = f_precision(scores)\n",
    "    f1_score = f_f1_score(scores)\n",
    "    mae = f_mae(scores)\n",
    "    pearson = f_pearson(scores)\n",
    "    spearman = f_spearman(scores)\n",
    "    cohens_kappa = f_cohens_kappa(scores)\n",
    "    auc = f_auc(scores)\n",
    "    matthews = f_matthews(binary_scores)\n",
    "    \n",
    "\n",
    "    for latency in latencies:\n",
    "        if latency > 20:\n",
    "            # print(f\"Warning: latency is greater than 10 seconds: {latency}\")\n",
    "            latencies.remove(latency)\n",
    "    avg_latency = sum(latencies) / len(latencies)\n",
    "\n",
    "    # print(f\"{provider_name}: mae: {mae:.4f}, pearson: {pearson:.4f}, spearman: {spearman:.4f}, Cohen's Kappa: {cohens_kappa:.4f}\")\n",
    "    print(\n",
    "        f\"{provider_name}: recall: {recall:.4f}, precision: {precision:.4f}, f1: {f1_score:.4f}, Cohen's Kappa: {cohens_kappa:.4f}, Matthews: {matthews:.4f}, AUC: {auc:.4f}, avg_latency: {avg_latency:.4f}\"\n",
    "    )\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    \n",
    "    plot_confusion_matrix(labels, scores, title=f\"Confusion Matrix - {provider_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cortex_eval_df = pd.read_csv(\"/Users/dhuang/Documents/git/trulens/src/benchmark/trulens/benchmark/benchmark_frameworks/experiments/eval_scrape_mistral-large_output_1727118011.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['query', 'expected_response', 'golden', 'context_file',\n",
       "       'rephrased_queries', 'session', 'llm_answer', 'adjusted_llm_answer',\n",
       "       'rag_context', 'input_token_count', 'debug_signals', 'citations',\n",
       "       'rewritten_queries', 'accuracy_llm', 'llm_judge_answer',\n",
       "       'llm_citation_f1', 'llm_response_citation', 'llm_judge', 'search_table',\n",
       "       'is_match', 'anls', 'citation_match', 'citation_syntax',\n",
       "       'citation_error', 'citation_specific_reason', 'citation_precision',\n",
       "       'gt_citation_f1', 'gris_anls', 'retrieval_anls', 'retrieval_ndcg_at_1',\n",
       "       'retrieval_hit_at_1', 'retrieval_ndcg_at_3', 'retrieval_hit_at_3',\n",
       "       'gris_llm_answer'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cortex_eval_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROW 0: query: What tool do I use to engage sales prospects\n",
      " , llm_response: You can use the Outreach Sales Engagement Platform to engage sales prospects 【†1†】.\n",
      "g, context_chunks: ['Requesting Outreach Access The Outreach Sales Engagement Platform helps efficiently and effectively engage prospects to drive more pipeline and close more deals.'] \n",
      "\n",
      "\n",
      "ROW 1: query: How can I get grammarly\n",
      " , llm_response: You can request access to Grammarly in two ways: 1. Submit a request ticket using the Lift Self Service catalog item and select the appropriate role, or 2. Go to SnowBot and type “Grammarly Access”. Then, complete the “Grammarly Access Request” form. Once your request is processed, you will receive an email from Grammarly. Open the email and click on “Accept Invitation”, then download and add Grammarly to your Chrome browser 【†1†】 【†2†】.\n",
      "g, context_chunks: ['Requesting Grammarly Access Grammarly is a cross-platform cloud-based writing assistant that reviews spelling, grammar, punctuation, clarity, engagement, and delivery mistakes. It uses AI to identify and search for an appropriate replacement for the mistake it locates. How to Request Grammarly Access: There are two ways how you may request access to Grammarly: 1. Lift Self Service: Submit a request ticket using the Lift catalog iue item and sect the appropriate role. Grammarly Access Request (https://lift.snowflake.com/lift?'] \n",
      "\n",
      "\n",
      "ROW 2: query: How to enable encyryption\n",
      " , llm_response: To enable encryption on a MacOS device, you can use FileVault disk encryption. Here are the steps:\n",
      "1. On your Mac, choose Apple menu > System Settings, then click Privacy & Security in the sidebar.\n",
      "2. Click Turn On FileVault, and type in your laptop password 【†1†】.\n",
      "\n",
      "Alternatively, you can use Vera for file encryption. Here are the steps to install and use Vera:\n",
      "1. Go to https://snowbiz.okta.com and open VERA.\n",
      "2. In the bottom left corner, download the Vera installer.\n",
      "3. Run the downloaded Vera installer.dmg on your MacBook.\n",
      "4. To install, double-click the installer icon.\n",
      "5. Install for all users on this computer.\n",
      "6. Don't change the install location, click install.\n",
      "7. You might need to close applications before Vera can be installed. Close all applications and allow Vera to access data from other apps.\n",
      "8. After successful installation, you can now use the right-mouse button / control-click to secure with VERA 【†2†】.\n",
      "g, context_chunks: ['Enable Filevault disk encryption for MacOS 1. On your Mac, choose Apple menu > System Settings, click Privacy & Security in the sidebar, then click FileVault on the right. 2. Click Turn On, and type in your laptop password.'] \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error removing trivial statements: invalid syntax (<string>, line 1). Proceeding with all statements.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROW 3: query: What tool do I use to set up alerting list for slack?\n",
      " , llm_response: To set up an alerting list for Slack, you can use the Datadog app. Invite the Datadog app to a Slack channel and it will be added automatically. After setting up the Slack integration, type @slack in your notification message to see the available list of channels to send your notification to. 【†1†】\n",
      "g, context_chunks: ['Setup your team slack channel/mailing list for Alerting 1. Invite the Datadog app to a Slack channel and it will be added automatically. 2. After setting up the Slack integration, type @slack in your notification message to see the available 2. After setting up the Slack integration, type @slack in your notification message to see the available list of channels to send your notification to. 3. Wrap the @username in < > in your monitor’s message template to @-notify the defined user,within Slack notifications. For example: @slack-Snowflake'] \n",
      "\n",
      "\n",
      "ROW 4: query: How can I set up my 2FA to auto approve\n",
      " , llm_response: I don't know the answer to that question.\n",
      "g, context_chunks: [] \n",
      "\n",
      "\n",
      "ROW 5: query: Can I add an external collaborator to in internal slack channel?\n",
      " , llm_response: Based on the provided excerpts, it seems that to collaborate with an external partner in Slack, you need to create a new channel in your External Slack Workspace and invite them to that channel. It is not mentioned that you can add an external collaborator directly to an internal Slack channel 【†1†】 【†2†】.\n",
      "g, context_chunks: ['Slack Shared Channels give us the ability as a company to correspond with other companies in real time using the Slack Application, as if they were in our own Slack instance! Pre-requisites: 1. You must be part of the Snowflake External Workspace (info on how to join Snowflake External Here (/lift? id=kb _ article&sys _ id=860cd3031bbbd05486e485d8cd4bcb6f)) How to set up a Shared Channel in Slack: 1. Navigate to Snowflake External by clicking on the white snowflake with the blue background on your left sidebar (if you have not joined, see instructions at the bottom of this page in the \"Common Questions\" section). You must create these channels in Snowflake External or your request will go unseen! Scroll down to the \"Channels\" section and click the \"+\" button next to the header'] \n",
      "\n",
      "\n",
      "ROW 6: query: What is the email address which I should have gotten an activation email for printing from? \n",
      " , llm_response: The email address from which you should have received the activation email for printing is info@na.accounts.ricoh.com 【†1†】.\n",
      "g, context_chunks: ['Please follow the instructions to 1. Confirm your Ricoh registration 2. Install the printer drivers for your laptop type If you forgot your badge at home please submit a ticket to have your temp badge added to your account. Guests will not be able to print. Whomever is working with the guest will need to print for them. You will need to be on the \"Snowflake\" WiFi NOT \"SnowflakeGuest\" Ricoh Registration Email 1. Look for an email from info@na.accounts.ricoh.com (mailto:info@na.accounts.ricoh). It will be titled [RICOH Account] Confirm registration and set password. Once you have located the email, click on the link received'] \n",
      "\n",
      "\n",
      "ROW 7: query: Can I use touch screen outside of meeting room to schedule meeting\n",
      " , llm_response: Yes, you can use the touch screen outside of the meeting room to schedule a meeting. This can be done by going to the room scheduling panel of the room which is available at the time, clicking on \"Reserve\", entering the time and subject of the meeting, and specifying the email IDs of the participants. You can also set a passcode and waiting room for the meeting if desired 【†1†】.\n",
      "g, context_chunks: ['How to use it?\\nIt can be done using any one of the ways below:\\nUsing Touch panel Outside room:\\nGo to the room scheduling panel of the room which is available at the time. (Green color-> Available).'] \n",
      "\n",
      "\n",
      "ROW 8: query: What do I need to have on my iPhone before I can use slack?\n",
      " , llm_response: Before you can use Slack on your iPhone, you need to have FIDO2, Okta Verify, and Beyond Identity configured on your mobile phone. If you don't have any of these, you can refer to the respective KB articles for setup instructions. Additionally, you will need to download the Slack app from the App Store 【†1†】 【†2†】.\n",
      "g, context_chunks: [\"Setting up Slack on Mobile Prerequisite: 1. Must have the FIDO2, Okta Verify and Beyond Identity Configured on your Mobile Phone. 2. If you don't have any of these, refer to how to set up FIDO2 KB0014203 (https://lift.snowflake.com/lift? sys _ kb _ id=10951aacdb6e219087642d89139619b1&id=kb _ article _ view&sysparm _ rank=3&sysparm _ tsqueryId=404cec931b52b594a752ffbf034bcb90), Okta Verify KB0 (https://lift.snowflake.com/lift? sys _ kb _ id=bbd1b8901b686450995e8487dc4bcb7b&id=kb _ article _ view&sysparm _ rank=2&sysparm _ tsqueryId=54b00597dbee155887642d8913961903)010714 (https://lift.snowflake.com/lift? sys _ kb _ id=bbd1b8901b686450995e8487dc4bcb7b&id=kb _ article _ view&sysparm _ rank=2&sysparm _ tsqueryId=54b00597dbee155887642d8913961903), and Beyond Identity KB0010814 (https://lift.snowflake.com/lift? sys _ kb _ id=e14c244f1bed59500615c802604bcb4e&id=kb _ article _ view&sysparm _ rank=3&sysparm _ tsqueryId=2cc001d7dbee155887642d89139619e2).\"] \n",
      "\n",
      "\n",
      "ROW 9: query: Who do I email to schedule a meeting with Emily\n",
      "Hernandez in Bellevue Office?\n",
      " , llm_response: You can email Katarzyna Nowak at katarzyna.nowak@snowflake.com to schedule a meeting with Emily Hernandez in the Bellevue office 【†1†】.\n",
      "g, context_chunks: ['Emily Hernandez Bellevue Katarzyna Nowak katarzyna.nowak@snowflake.com'] \n",
      "\n",
      "\n",
      "ROW 10: query: Is it bad to at channel on slack?\n",
      " , llm_response: It is not necessarily bad to at a channel on Slack, but it can draw attention more than necessary. It is appropriate to point out inconsiderate Slack use respectfully and be open to receiving feedback about your own Slack habits. The goal is to make each other the best Slack users. 【†1†】\n",
      "g, context_chunks: ['In particular,think twice before using @channel or @here. These tags are for emergencies only and should only be used\\nwhen your message requires the urgent attention of absolutely everyone in a channel. Slack overload often starts with\\nsomeone sending a message to way more people than the message merits. Lean towards starting with a small group when initiating a conversation and then pull in more people (https://slack.com/help/articles/217555437-Move-group-direct-\\nmessages-to-a-private-channel) as necessary.'] \n",
      "\n",
      "\n",
      "ROW 11: query: I have a work from home setup expense. How do I get reimbused?\n",
      " , llm_response: To get reimbursed for your work from home setup expense, follow these steps: Log into Okta, click on 'Create New Expense', type 'WFH' in the Expense Item Field and select the appropriate Expense Item. Then, click on 'Workday', 'Credit Card Transactions' or 'Quick Expenses'. Add a memo line (mandatory) and attach a receipt. Make sure the receipt is itemized and includes the name of the merchant. The reimbursement process must be completed within 90 days of your start date. For employees in India, the invoices/receipts must be under the company name and registered address, and should contain the GST number to receive reimbursement. If the invoices/receipts are not submitted as per these instructions, the reimbursement will be treated as a perquisite under income tax laws and income tax will be deducted on the reimbursement amount at the applicable slab rate and subsequently recovered from the employee’s payroll. The country limit for this expense can be found on the Snowflake Global T&E Policy Addendum. 【†1†】 【†2†】\n",
      "g, context_chunks: ['Follow the instructions provided: Login to Okta, Click on Work day, Click on Expenses, Click on Create Expense Report, Create New Expense Report from Spend Authorization and Click OK, Click Create New Expense -> Type WFH in Expense Item Field and select the appropriate Expense Item'] \n",
      "\n",
      "\n",
      "ROW 12: query: I work in Warsaw office. How much can I expense for work from home setup? And what is the deadline for reporting?\n",
      " , llm_response: I'm sorry, but the provided excerpts do not contain information about the work-from-home budget for employees working in the Warsaw office or the deadline for reporting expenses. Therefore, I don't know the answer to that question.\n",
      "g, context_chunks: ['Country Work-from-home budget (USD) Welcome period (mo)\\nPoland $350 12 '] \n",
      "\n",
      "\n",
      "ROW 13: query: I accidentally deleted my google calendar meeting. Is it recoverable?\n",
      " , llm_response: Yes, you can restore deleted Google calendar events. To do this, go to Google Calendar from your computer, click on the Settings wheel in the top-right corner, and select Trash from the dropdown menu. From the list of deleted files, select the event you want to restore by checking the box beside it, and click on the Restore arrow from the right side 【†1†】.\n",
      "g, context_chunks: ['How to restore Google calendar events\\nRestoring deleted events from the trash'] \n",
      "\n",
      "\n",
      "ROW 14: query: How do I join a team on Workday\n",
      " , llm_response: To join a team on Workday, you need to submit a Workday ticket to get access. The link for the ticket is provided in the Workday Team Management access article 【†1†】. The article does not mention how to join an existing team, so I'm assuming you need to request access to join a team. If you're looking for Workday related team management access, you can submit the Workday ticket at the link provided in the article 【†1†】. Please note that this is managed by the HR team, not the Cloud Operations team 【†1†】.\n",
      "g, context_chunks: ['Please submit below Workday ticket to get access\\nWorkday Ticket (https://lift.snowflake.com/lift?\\nid=sc_cat_item&sys_id=491ba199db3c58502f1c9eb6db961946)'] \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error removing trivial statements: invalid character '【' (U+3010) (<string>, line 1). Proceeding with all statements.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROW 15: query: Can I get a French keyboard\n",
      " , llm_response: The provided excerpts do not mention anything about getting a French keyboard. Therefore, I don't know the answer to that question.\n",
      "g, context_chunks: [] \n",
      "\n",
      "\n",
      "ROW 16: query: I'm going on a personal trip to China. Can I bring my laptop?\n",
      " , llm_response: The provided excerpts do not contain information about taking your laptop on a personal trip to China. Therefore, I don't know the answer to that question.\n",
      "g, context_chunks: ['China & High-Risk Country Travel Process. Do not bring your Snowflake corporate laptop with you.'] \n",
      "\n",
      "\n",
      "ROW 17: query: BI:MADf beyond identity error\n",
      " , llm_response: The error BI:MADf indicates that the Mac device does not have FileVault disk encryption enabled. To resolve this issue, enable FileVault on the Mac device 【†1†】.\n",
      "g, context_chunks: ['Mac device does not have FileVault disk encryption enabled. Enable FileVault. https://lift.snowflake.com/shorturl.do?\\nnumber=KB0014839'] \n",
      "\n",
      "\n",
      "ROW 18: query: email retention policy\n",
      " , llm_response: The email retention policy within Gmail is 1 year. To prevent an email necessary for business operations from being deleted permanently once it ages beyond the retention period, it must be marked with the specific Gmail label: “DND” (which stands for “Do Not Delete”) 【†1†】.\n",
      "g, context_chunks: ['Within Gmail, our retention period is 1 year. To prevent an email that is necessary for business operations from being deleted permanently once it ages beyond the retention period, you must mark that email with the specific Gmail label: “DND” (which stands for “Do Not Delete”).'] \n",
      "\n",
      "\n",
      "ROW 19: query: How can i preserve emails from not getting deleted automatically?\n",
      " , llm_response: To preserve emails from being deleted automatically, you can create a specific label called \"DND\" (Do Not Delete) in Gmail. To create the label, go to Gmail settings, click on \"Create New Label\" under the \"Labels\" heading, type \"DND\" into the \"Label Name\" box, and click \"Create\" 【†1†】. Then, mark the necessary emails with this label to prevent them from being deleted once they age beyond the retention period.\n",
      "g, context_chunks: ['Within Gmail, our retention period is 1 year. To prevent an email that is necessary for business operations from being deleted permanently once it ages beyond the retention period, you must mark that email with the specific Gmail label: “DND” (which stands for “Do Not Delete”).'] \n",
      "\n",
      "\n",
      "ROW 20: query: san mateo IT desk hours of operation\n",
      " , llm_response: The San Mateo Ice Bar, which is the IT desk in San Mateo, is open Monday through Friday, from 8 AM to 5 PM 【†1†】.\n",
      "g, context_chunks: ['Hours of Operation: We are open Monday through Friday, from 8 AM to 5 PM.'] \n",
      "\n",
      "\n",
      "ROW 21: query: poland wfh budget\n",
      " , llm_response: The work-from-home (WFH) budget for employees in Poland is $350, which must be spent within 12 months after joining Snowflake 【†1†】. The budget can be used for approved items such as monitors, keyboards, mice, and dongles/USB hubs 【†2†】.\n",
      "g, context_chunks: ['Country Work-from-home budget (USD) Welcome period (mo)\\nOman $400 12\\nPanama $300 3\\nPapua New Guinea $300 12\\nParaguay $300 9\\nPeru $300 3\\nPhilippines $300 6\\nPoland $350 12\\nPortugal $400 6\\nQatar $450 3\\nRomania $350 12\\nRwanda $300 9\\nSaint Kitts and Nevis $300 6\\nSaint Lucia $300 6\\nSaint Vincent and the\\nGrenadines $300 12\\nSamoa $300 9\\nSan Marino $450 6\\nSaudi Arabia $450'] \n",
      "\n",
      "\n",
      "ROW 22: query: how long to i have to spend my wfh stipend in US \n",
      " , llm_response: You have 90 days from your start date to purchase and expense your equipment in the US 【†2†】.\n",
      "g, context_chunks: ['Work-from-home (WFH) IT budget\\nEach Snowflake employee gets a work-from-home budget based on their country of residence. The\\ntable below shows the budget in USD per employee, which must be spent within the Welcome\\nPeriod after joining Snowflake. For example, employees in Australia have 12 months to spend their\\nwork-from-home IT budget of $500.\\n\\nCountry Work-from-home budget (USD) Welcome period (mo)\\nOman $400 12\\nPanama $300 3\\nPapua New Guinea $300 12\\nParaguay $300 9\\nPeru $300 3\\nPhilippines $300 6\\nPoland $350 12\\nPortugal $400 6\\nQatar $450 3\\nRomania $350 12\\nRwanda $300 9\\nSaint Kitts and Nevis $300 6\\nSaint Lucia $300 6\\nSaint Vincent and the\\nGrenadines $300 12\\nSamoa $300 9\\nSan Marino $450 6\\nSaudi Arabia $450 3\\nSerbia $300 12\\nSeychelles $350 3\\nSingapore $450 9\\nSlovakia $350 6\\nSlovenia $350 9\\nSolomon Islands $300 6\\nSouth Africa $350 3\\nSpain $450 12\\nSri Lanka $300 6\\nSuriname $300 3\\nSweden $450 9\\nSwitzerland $500 3\\nTaiwan $450 12\\nTanzania $300 6\\nThailand $350 9\\nTrinidad and Tobago $350 12\\nTunisia $300 6\\nTurkey $350 9\\nUganda $300 6\\nUkraine $300 3\\nUnited Arab Emirates $450 12\\nUnited Kingdom $450 3\\nUnited States $500 9'] \n",
      "\n",
      "\n",
      "ROW 23: query: globalprotect change defaul portal\n",
      " , llm_response: To change the default portal in GlobalProtect, you can follow these steps:\n",
      "1. Click the GlobalProtect icon in your system tray.\n",
      "2. If the auto-selected gateway is not what you want to select as your preferred gateway, click on \"Change Gateway\".\n",
      "3. Once connected to your desired gateway, click the star icon (⭐️) to set it as your preferred gateway 【†1†】.\n",
      "\n",
      "If GlobalProtect is unable to connect to your preferred gateway, it will fall back to the best available gateway 【†1†】. This process applies to all VPN gateways, including Dev and Corp 【†1†】.\n",
      "g, context_chunks: [] \n",
      "\n",
      "\n",
      "ROW 24: query: Can i use loom?\n",
      " , llm_response: The provided excerpts do not mention anything about using Loom, so I don't know the answer to that question.\n",
      "g, context_chunks: ['Current List of Applications that Should Not be Used\\nThis list is constantly evolving and will be updated periodically. Refer to the above definitions for Not Approved and Explicitly\\nDisallowed. Approved potential alternatives are listed.\\nApplication Not\\nApproved\\nExplicitly\\nDisallowed Justification Approved Potential\\nAlternatives\\n\\nAcronis True Image x Unapproved image and\\ndata back-up. Code42\\n\\nAhnLab x\\n\\nUnapproved AVs can\\ninterfere or block Company\\ninstalled AV\\n\\nCrowdstrike\\nAirmail x Unapproved email client. GMail, Outlook, MacOS\\n\\nMail\\n\\nAppCleaner x\\n\\nFile deletion can interfere\\nwith AV / Crowdstrike.\\n\\nOS-Native software\\nremoval procedures on\\nMac / Windows\\n\\nBoxy for Gmail x\\n\\nIntegrates with sensitive\\napps.\\n\\nGmail, Outlook, MacOS\\nMail\\n\\nCleanMyMac x\\n\\nUnapproved AVs can\\ninterfere or block Company\\ninstalled AV.\\n\\nCrowdstrike\\n\\nClickShare x\\n\\nUnapproved conferencing\\nsolution; requires formal\\nVSA submission.\\n\\nNone identified\\n\\nCloudApp X Unauthorized Screen\\nrecording app. Zoom\\n\\nCryptomator x\\n\\nUnauthorized file\\nencryption for Box. Vera\\n\\nExpressVPN x\\n\\nUnauthorized VPN\\nproduct; not approved for\\ngeneral use; exception\\nmay be granted on case by\\ncase basis.\\n\\nOpenVPN Connect\\n\\nFantastical x Not gone through a VSA. Google Calendar.\\nFreeFileSync x Unapproved file sharing\\n\\napplication. Google Drive\\n\\n5/10/24, 2:59 PM Information Technology (IT) - Approved Corporate Applications & FAQ v1.0\\n\\nhttps://lift.snowflake.com/kb_view.do?sys_kb_id=549199921b0ace9c04ad33fe034bcb46 2/4\\n\\nKarabiner-\\nElements x\\n\\nThis automation can be\\nleveraged by malware. ivangreene/keymap\\n\\nKeePassX x\\n\\n1Password is the only\\npassword storage vault\\nwhich is approved.\\n\\n1Password\\n\\nLoom x Unapproved video\\n\\nconferencing application Zoom\\n\\nMalwarebytes x\\n\\nUnapproved AVs can\\ninterfere or block Company\\ninstalled AV\\n\\nCrowdstrike\\n\\nMcAfee x\\n\\nUnapproved AVs can\\ninterfere or block Company\\ninstalled AV\\n\\nCrowdstrike\\n\\nNextDNS x\\n\\nNo product is approved\\nwhich interferes with\\nnetwork communication.\\n\\nCrowdstrike\\n\\nNordPass x\\n\\nUnauthorized VPN\\nproduct; Exception\\nrequires initiation of VSA\\nfrom business sponsor.\\n\\nOpenVPN Connect\\n\\nNordVPN IKE x\\n\\nUnauthorized VPN\\nproduct; Exception\\nrequires initiation of VSA\\nfrom business sponsor.\\n\\nOpenVPN Connect'] \n",
      "\n",
      "\n",
      "ROW 25: query: what is the IT analytics oncall slack channel\n",
      " , llm_response: The IT analytics oncall slack channel is #oncall-it-analytics-engineering 【†1†】.\n",
      "g, context_chunks: ['Current On-calls (PagerDuty)\\n\\nAnalytics\\nEngineeri\\nng\\nFinance Automation Projects Analytics\\nEngineering:\\nFinance Automation\\n\\n#oncall-it-\\nanalytics-\\nengineering\\n\\n#xfn-de-finance-\\npod\\n\\nAnalytics\\nEngineeri\\nng\\nCatch all service that can be used by\\nany projects supported by Analytics\\nEngineering\\n\\nAnalytics\\nEngineering: IT\\n\\n#oncall-it-\\nanalytics-\\nengineering\\n\\npd_analytics_eng\\n_it\\n\\nAnalytics\\nEngineeri\\nng\\nSales Analytics\\nEngineering: Sales\\n\\n#oncall-it-\\nanalytics-\\nengineering\\n\\n#sales-data-\\nissues\\n\\npd_analytics_eng\\n_sales\\n\\nTeam Scope Pager Duty Service On-call Slack\\nChannel\\n\\nXFN Slack\\nChannel\\n\\nSnow Routing\\nKey\\n\\nHow to reach on-call person?\\nPlease go to the on-call slack channel listed in the table above and type this command:\\n\\nIt will prompt you to select Service from the dropdown:\\n\\nHow to open a ticket (RITM)...\\nBefore opening a ticket, please check if it was asked in Q&A by subscribing to this channel #it-analytics-engineering-faq\\nEnter a ticket on The Lift for Data Applications: Login - The Lift\\nOur SLA process for tickets (TBD)\\nHow to open a project request\\nHow to get access to Snowhouse data\\nSnowhouse Roles Access request - End Use Guide (Servicenow)\\n\\nLogging Customer Calls\\nThe DE teams are often pulled into customer calls to show Snow on Snow examples. It is a team metric that we are collecting to\\nunderstand how much time we are spending and with whom for org impact.\\nThe call logging application is housed here (IT_MODELING_RL): IT DE Call Logger\\n\\nImportant Docs\\nFY23 Roadmap Data Engineering Roadmap\\nSales Attainment Datasets\\nData Engineering Career Ladder\\n\\nOMR Design Document\\nKnow your Product ❄️\\nKnow your Colleague ❄️\\nAnalytics\\nEngineeri\\nng\\nSnowHub, Master Data Analytics\\nEngineering:\\nSnowHub\\n\\n#oncall-it-\\nanalytics-\\nengineering\\n\\npd_analytics_eng\\n_snowhub\\n\\nAnalytics\\nEngineeri\\nng\\nThird Party Compliance Screening\\nprocesses: Sanctions Screening,\\nPartner Diligence\\n\\nAnalytics\\nEngineering: TPCS\\n\\n#oncall-it-\\nanalytics-\\nengineering\\n\\npd_analytics_eng\\n_tpcs'] \n",
      "\n",
      "\n",
      "ROW 26: query: which sites are eligible for it site leadership engagement \n",
      " , llm_response: The sites eligible for IT Site Leadership engagement are those with 40+ headcounts and critical locations such as Bozeman, MT where there is no IT presence 【†1†】.\n",
      "g, context_chunks: ['IT Site Leadership Program Eligible Sites 40+ Headcounts Critical Locations (ex: Bozeman, MT)'] \n",
      "\n",
      "\n",
      "ROW 27: query: what is the work from home budget for japan\n",
      " , llm_response: The work from home budget for Japan is $500 【†1†】.\n",
      "g, context_chunks: ['Japan\\n$500\\n6'] \n",
      "\n",
      "\n",
      "ROW 28: query: Can i use meetingbar?\n",
      " , llm_response: I don't know the answer to that question.\n",
      "g, context_chunks: [] \n",
      "\n",
      "\n",
      "ROW 29: query: slack ttl policy\n",
      " , llm_response: Direct Messages in Slack are wiped after 7 days due to the retention policy 【†1†】. Additionally, for custom Slack Apps, the Bot user token must be rotated at least every 90 days 【†2†】.\n",
      "g, context_chunks: [\"Use and Respect DND\\nSlack allows you to pause notifications by manually enabling Do Not Disturb (https://slack.com/help/articles/214908388-\\nPause-notifications-with-Do-Not-Disturb) (DND) or by setting a notification schedule\\n(https://slack.com/help/articles/214908388-Pause-notifications-with-Do-Not-Disturb#set-a-notification-schedule) to\\nonly get notifications on days you choose during certain hours. Notification schedules are quite flexible: you can use them\\n\\nto only get notifications on weekdays, or you can define different hours for different days of the week, e.g., Monday-\\nThursday from 9am to 6pm and Friday from 8am to 4pm.\\n\\nTake advantage of these features as appropriate; they will allow you to avoid unwelcome notifications when you’re\\nconcentrating or when you’re away from work, e.g., late on weekdays or on weekends.\\n\\uf1f2 \\uf1f2 \\uf1f2 \\uf1f2 \\uf1f2\\n\\n5/10/24, 2:58 PM Information Technology (IT) - Slack Etiquette\\n\\nhttps://lift.snowflake.com/kb_view.do?sys_kb_id=31f0236f8779ca18b7daea8f8bbb3526 2/4\\nConversely, when messaging someone who has paused notifications, don’t override DND to force-send your notification\\nunless it’s an emergency. And if you do override DND, make sure to explain why it’s urgent.\\nAlso, if appropriate, consider sending an email instead; it’s less intrusive than a Slack message and doesn’t demand an\\nimmediate response. Direct Messages are wiped after 7 days due to our retention policy. If someone will be on vacation for\\nlonger than your message will persist, and it's important, send an email instead!\\nFinally, continue to exercise good judgment when messaging someone who has disabled notifications. Even though the\\nreceiver won’t be notified immediately, the guidelines about crafting messages thoughtfully\\n(https://docs.google.com/document/d/1e819uqzXRrxyQxfD881dwNDgIwrNHhoMQ8TXZphD7GI/edit#heading=h.g0tg91ifc7jr)\\nstill apply.\"] \n",
      "\n",
      "\n",
      "ROW 30: query: where is the ice bar in san mateo\n",
      " , llm_response: The San Mateo Ice Bar is located on the second floor of the South Tower, right next to the IT Department 【†1†】.\n",
      "g, context_chunks: ['What We Offer: At the San Mateo Ice Bar, our friendly and knowledgeable technicians are\\nhere to assist you with any hardware, software, access, and peripherals issues you may\\nencounter.\\nLocation: You can find us on the second floor of the South Tower, right next to the IT Department.\\nHours of Operation: We are open Monday through Friday, from 8 AM to 5 PM.'] \n",
      "\n",
      "\n",
      "ROW 31: query: can I still see my bookmarks after turning off google chrome sync\n",
      " , llm_response: Yes, if you turn off sync in Google Chrome, you can still see your bookmarks on your computer. However, any changes you make to your bookmarks after turning off sync will not be saved to your Google Account and synced to your other devices 【†1†】.\n",
      "g, context_chunks: [\"If you turn off sync, you can still see your bookmarks, history, passwords, and\\nother settings on your computer. If you make any changes, they won't be saved\\nto your Google Account and synced to your other devices.\\nWhen you turn off sync, you’ll also be signed out of other Google services, like\\nGmail.\"] \n",
      "\n",
      "\n",
      "ROW 32: query: how to enable zoom caption translation\n",
      " , llm_response: To enable Zoom caption translation, first go to the Zoom tile in Snowbiz Okta, select Settings in the left column, then select In Meeting (Advanced). Here, you'll be able to toggle ON the Translated Captions 【†1†】. If a presenter is speaking in English and another language is desired, select the carrot icon in the bottom toolbar of the Webinar in the box for “Closed Captions” 【†1†】. Also, make sure the (Speaking Language) is correct. The Translated Closed Captions will be displayed at the bottom of your screen once enabled 【†1†]].\n",
      "g, context_chunks: [\"Translated Captions in Zoom\\nThe new Translated Caption feature to allow teams from around the world to translate speech into different languages.\\nT o first enable this, go to the Zoom tile in Snowbiz Okta, select Settings in the left column, then select In Meeting (Advanced), here you'll be able\\nto toggle ON the Translated Captions.\"] \n",
      "\n",
      "\n",
      "ROW 33: query: when will snowflake okta password expired\n",
      " , llm_response: The Snowflake Okta password will expire after 90 days 【†1†】.\n",
      "g, context_chunks: ['Snowbiz Okta\\n· Password expiration = 365 days\\n· Exception for Service Account: Service account are set to not expire.\\n· Password minimum length = 14 characters\\nWith Beyond Identity password is not\\nused by users anymore'] \n",
      "\n",
      "\n",
      "ROW 34: query: how to delete chrome profile\n",
      " , llm_response: The provided excerpts do not contain information on how to delete a Chrome profile. Therefore, I don't know the answer to that question.\n",
      "g, context_chunks: [] \n",
      "\n",
      "\n",
      "ROW 35: query: list all snowflake services that requires Global Protect VPN\n",
      " , llm_response: The excerpts provided do not specify which Snowflake services require Global Protect VPN. Therefore, I don't know the answer to that question.\n",
      "g, context_chunks: [] \n",
      "\n",
      "\n",
      "ROW 36: query: which team can request 5k monitor\n",
      " , llm_response: The Design team members under Michal Kopec or one of Michal's direct reports can request the 5k monitor 【†1†】.\n",
      "g, context_chunks: [\"5k Monitors are approved for all Design team members under Michal Kopec or one of Michal's direct\\nreports.\\nThis has been approved by Sean Moua. Please use this knowledge base article as a reference if you need\\napproval to place the order with CDW\"] \n",
      "\n",
      "\n",
      "ROW 37: query: What's the budget for purchasing ergonomic mouse and keyboard\n",
      " , llm_response: The budget for purchasing an ergonomic mouse and keyboard is $300, with a limit of $150 for a keyboard and $150 for a mouse 【†1†】.\n",
      "g, context_chunks: ['For your common ergonomic mouse and keyboard needs such as some of the\\noptions listed below, you can purchase and submit a reimbursement request\\nthrough Workday. The items below are examples of what you could purchase, but\\nplease find something that will work for your needs. This will require your\\nmanager’s approval as an expense under your cost center, so please run it by\\nthem before ordering. The limit for this will be $150 for a keyboard and $150 for\\na mouse for a total limit of $300. This is for Snowflake employees only and you\\nwill only be able to submit this once as an employee at Snowflake.'] \n",
      "\n",
      "\n",
      "ROW 38: query: what's the company rule for creating github username when requesting github access\n",
      " , llm_response: The company rule for creating a GitHub username when requesting GitHub access is that the ID must be in the format \"sfc-gh-<ldap>\", where \"<ldap>\" is the username part of your Prod VPN login ID. For example, if your Prod VPN login ID is \"ymatsuzaki@int.snowflakecomputing.com\", then your GitHub username should be \"sfc-gh-ymatsuzaki\". This rule is mandatory and requests for access from GitHub user accounts not following this format will not be approved 【†1†】.\n",
      "g, context_chunks: ['1. Create an independent GitHub account with Snowflake corporate email\\nGo to https://github.com/ (https://github.com/) and sign out if you have logged in with your personal account\\nSign up with your corporate email (@snowflake.com)\\nID must be sfc-gh-<ldap> (E.g. sfc-gh-ymatsuzaki), this is a company rule you must comply with\\nYour LDAP name is the username part of your Prod VPN login ID (<this-part>@int.snowflakecomputing.com)\\nRequests for access from GitHub user accounts not following the above will not be approved\\nAfter signing up and logging in, change the Name property to your full name (E.g., Yoshi Matsuzaki)\\nYou can change your name from the top-right icon > Your profile > Edit profile > Name > Save.'] \n",
      "\n",
      "\n",
      "ROW 39: query: Can I set up face recognition for Talon browser in 1Password?\n",
      " , llm_response: The provided excerpts do not mention setting up face recognition for Talon browser in 1Password, only biometric authentication is mentioned. Therefore, I don't know the answer to that question.\n",
      "g, context_chunks: [] \n",
      "\n",
      "\n",
      "ROW 40: query: Why can't I use app cleaner on mac? \n",
      " , llm_response: You cannot use AppCleaner on your Mac because it can interfere with or block the company-installed antivirus software. The recommended method for software removal is to follow the OS-native procedures for Mac and Windows 【†1†】.\n",
      "g, context_chunks: ['Application | Not Approved | Explicitly Disallowed | Justification | Approved Potential Alternatives \\nAppCleaner | | x | File deletion can interfere with AV / CrowdStrike. | OS-Native software removal procedures on Mac / Windows'] \n",
      "\n",
      "\n",
      "ROW 41: query: How can I request approval for a new paid app?\n",
      " , llm_response: To request approval for a new paid app, you need to initiate a vendor and application security review. You can do this by filling out a new request form at the provided link (https://snowflake.ziphq.com/login) 【†1†】. Once the request is submitted, it will be pending your approval. You can approve it via Snowbot in Slack by typing \"My Approvals\" and reviewing the request details, then clicking \"Approve\". Alternatively, you can approve it via Slack by searching for \"pending approval request from\" and following a similar process 【†2†】.\n",
      "g, context_chunks: ['How do I obtain approval for a new business / commercial application (not on the list)?\\nIf there is a commercial / paid license or use fee associated with your application request, please initiate a vendor and application\\nsecurity review by filling out a new request form here (https://snowflake.ziphq.com/login).'] \n",
      "\n",
      "\n",
      "ROW 42: query: What are the steps needed to use Talon with 1Password?\n",
      " , llm_response: To use Talon with 1Password, follow these steps:\n",
      "1. Install the 1Password Extension on Talon Enterprise Browser by clicking on the extension's icon (Looks like a Jigsaw puzzle piece) and selecting 1Password 【†1†】.\n",
      "2. Sign in using your full Snowflake email address and select \"Continue\" 【†1†】.\n",
      "3. Open the 1Password Application and Select \"Settings...\" 【†1†】.\n",
      "4. Select the Browser tab and select, Add Browser 【†1†】.\n",
      "5. Authorize the browser Talon Enterprise Browser to access 1Password using the biometric authenticator (TouchID \\ Windows Hello) 【†1†】.\n",
      "6. To set Talon as a trusted browser in 1Password to allow biometric unlocking, open your 1Password desktop application and access its \"Settings\" 【†2†】.\n",
      "7. Choose the “Browser” category in your 1Password settings 【†2†】.\n",
      "8. Click on the “Add Browser” button. You’ll receive a pop up window to select the desired browser application. Locate your Talon browser in your application list to select before clicking on “Open” 【†2†】.\n",
      "9. Authenticate with your touch ID/Biometric to complete the process 【†2†]].\n",
      "10. Confirm Talon is a trusted browser back in your 1Password settings “Browser” category 【†2†]].\n",
      "g, context_chunks: ['3. Click on the \"Add Browser\" button. You\\'ll receive a pop up window to select the desired browser application. Locate your Talon browser in your application list to select before clicking on \"Open\". \\n\\nConnect to additional browsers\\n1Password supports Chrome, Edge, Firefox, or Safari with extensions. If you want to use Password with another browser, you can give explicit permission for 1Password to connect with it.\\n\\nTalon Enterprise Browser.app\\n\\n4. You will be prompted to authenticate with your touch ID/Biometric to complete the add\\n\\n'] \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error removing trivial statements: invalid syntax (<string>, line 1). Proceeding with all statements.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROW 43: query: I lost my device with okta verify. What do I do?\n",
      " , llm_response: If you do not have access to the device that is currently setup with Okta Verify, you should reach out to the IT team in #team-it in Slack to have a tech manually reset it for you. They will need to verify your identity by calling you and asking you some questions before resetting 【†2†】. If you are completely locked out of Slack, Gmail and Service Portal, you can open a ticket from your personal email by emailing servicedesk@snowflake.com 【†2†】.\n",
      "g, context_chunks: [\"If you DO NOT have access to the device that is currently setup with Okta Verify, you\\nprobably won't be able to login to the Service Portal to open a ticket, so please reach out to us in\\n#team-it (https://snowflake.slack.com/archives/C4X35RJ2F) in Slack to have a tech manually\\nreset it for you.\\n1.IT will need to verify your identity by calling you and asking you some questions before\\nresetting.\\n2. If you are completely locked out of Slack, Gmail and Service Portal, you can open a\\nticket from your personal email by emailing us at servicedesk@snowflake.com\\n(mailto:servicedesk@snowflake.com).\"] \n",
      "\n",
      "\n",
      "ROW 44: query: when can i get a laptop upgrade?\n",
      " , llm_response: You are eligible for a tech refresh/upgrade at the end of your laptop's 3rd year, as per Snowflake Inc.'s Laptop Refresh Policy 【†1†】.\n",
      "g, context_chunks: [\"All full-time employees are eligible for a tech refresh/upgrade at the end\\nof the laptop's 3rd year.\\nIT equipment will be refreshed when they are determined to be obsolete\\nor end-of-life (operating beyond its determined life span) or when deemed\\nnon-functional by IT. All IT equipment will have a life span and refresh rate\\nof 3 years.\\nThe laptop will not be refreshed or replaced with minor wear and tear\\nwithin the stipulated lifecycle if still deemed functional by IT.\\nAll contractors to full time employment conversions are eligible for a\\nlaptop refresh if they received a slightly used laptop on the first day of\\nemployment.\"] \n",
      "\n",
      "\n",
      "ROW 45: query: what are the different reactions available for a video call? \n",
      " , llm_response: The different reactions available for a video call include showing red hearts, balloons, a thumbs up emoji, a thumbs down emoji, stormy rain, confetti, fireworks, and laser beams. These can be activated using specific hand gestures or by clicking the corresponding button in the Reactions submenu 【†1†】.\n",
      "g, context_chunks: ['Hearts: To show red hearts, use two hands to make a heart sign or click the Hearts button in the Reactions submenu.\\nBalloons: To show balloons, make a victory sign (peace sign) or click the Balloons button in the Reactions submenu.\\nThumbs Up: To show a thumbs up emoji, hold one thumb up or click the Thumbs Up button in the Reactions submenu.\\nThumbs Down: To show a thumbs down emoji, hold one thumb down or click the Thumbs Down button in the Reactions submenu.\\nRain: To show stormy rain, hold two thumbs down or click the Rain button in the Reactions submenu.\\nConfetti: To show confetti, make two victory signs (peace signs) or click the Confetti button in the Reactions submenu.\\nFireworks: To show fireworks, hold two thumbs up or click the Fireworks button in the Reactions submenu.\\nLasers: To show laser beams, make the sign of the horns (✌️) on each hand or click the Lasers button in the Reactions submenu.'] \n",
      "\n",
      "\n",
      "ROW 46: query: How can employees request a brand of laptop outside of the standard offerings?\n",
      " , llm_response: The provided excerpts do not mention how employees can request a brand of laptop outside of the standard offerings. Therefore, I don't know the answer to that question.\n",
      "g, context_chunks: [] \n",
      "\n",
      "\n",
      "ROW 47: query: What to do if my laptop is getting way too hot for no reason?\n",
      " , llm_response: The excerpts provided do not give a clear solution to the problem of a laptop getting too hot for no reason. Therefore, I don't know the answer to that question.\n",
      "g, context_chunks: ['What is an SMC reset?\\nA system management controller (SMC) reset is only on Macs and is responsible for managing behaviors\\nrelated to these. You can perform the SMC reset if you are having these issues:\\nPower, including the power button and power to the USB ports issues\\nBattery and charging issues\\nFans and other thermal-management features\\nIndicators or sensors such as status indicator lights (sleep status, battery charging status and\\nothers), the sudden motion sensor, the ambient light sensor and keyboard back-lighting\\nopening and closing the lid of a notebook computer screen issues\\n\\n1. Press and hold the power button/ touch ID on the device until powered off. Release the button once\\npowered off.\\n2. Press and hold all of the following keys below. Your Mac might turn on.\\nControl on the left side of your keyboard\\nOption (Alt) on the left side of your keyboard\\nShift on the right side of your keyboard\\n4. Keep holding allthree keys for 7 seconds, then press and hold the power button as well. If your Mac\\nis on, it will turn off as you hold the keys.\\n5. Keep holding all four keys for another 7 seconds, then release them.\\n6. Wait a few seconds, then press the power button to turn on your Mac.'] \n",
      "\n",
      "\n",
      "ROW 48: query: How do i use golinks? \n",
      " , llm_response: To use GoLinks, first clear your browser cache and type “go/” in your browser. This should redirect you to Snowflake Links. If it doesn't work, open an Incognito Window and type in \"go/\" in your browser. If you're using the GoLinks Chrome extension, remove the old one as it is no longer working. You can download and install the new Snowflake Links Extension from the link provided in the article. After installation, restart your browser and you should see the new Snowflake Links extension. 【†1†】\n",
      "g, context_chunks: ['Snowflake Links has now replaced GoLinks\\nThis article will walk through some troubleshooting steps on how to use the new Snowflake\\nLinks and also how to install the extension if you do not already have it.\\n\\nInitial usage:\\nFirst, you\\'ll want to clear your browser cache and type “go/” in your browser. This should redirect you\\nto Snowflake links.\\nThen, you\\'ll want to open an Incognito Window and type in \"go/\" in your browser\\nIf you\\'re using the GoLinks Chrome extension, you may see two extensions in your browser. Please\\nremove the old one as it is no longer working.\\nYou\\'ll want to right-click on the icon and remove it from Chrome\\n\\nNew Snowflake Links Extension should look like below:\\n\\nFuther Troubleshooting:\\nAttached to this article is the SnowflakeLinks extension install file. It can be downloaded and used to install the\\nextension for your computer.\\n1. Download the zip file and unzip it. \\n2. Go to your Chrome/Talon browser. Click on 3 vertical dots to the right. Click on Extensions -->\\nManage Extensions\\n3. On the top right, enable Devloper Mode --> On the left under \"Extensions\", click on Load Unpacked --\\n> Select the unzip folder called Snowflakelinks that you did in Step 1.\\n4. Restart you browser and you should see the new SnowflakeLinks extension.'] \n",
      "\n",
      "\n",
      "ROW 49: query: What are the steps to get Snowflake email on iOS?\n",
      " , llm_response: To set up Snowflake email on your iPhone, follow these steps:\n",
      "1. Go to Settings > Mail > Accounts > Add Account.\n",
      "2. Select Google and accept the terms.\n",
      "3. Log in to Google with your Snowflake email.\n",
      "4. Enter your Okta Username and Password on the Okta sign-in page.\n",
      "5. Authenticate using the biometric passkey on your device.\n",
      "6. Select the options you want to sync from your Snowflake email account and tap \"Save\". 【†1†】\n",
      "\n",
      "For a new device, you also need to:\n",
      "1. Go to the App Store and install the Google Gmail app.\n",
      "2. Sign in to Gmail with your Snowflake email address and Snowbiz Okta authentication. 【†2†】\n",
      "g, context_chunks: ['1. On your iPhone, go to Settings > Mail > Accounts > Add Account\\n2. Select Google and click on \"continue\"to accept the terms\\n3. Log in to Google with your Snowflake email and tap Next\\n4. You\\'ll be prompted for the Okta sign-in page. Enter your Okta Username and Password\\nand tap Next\\n5. Beyond identity authentication should start after clicking next.\\n6.Click settings arrow in the top left to return to authentication process.\\n7. Next you will authenticate using the biometric passkey on your device, if you have not\\nset this up please follow the steps in KB0014203 (https://lift.snowflake.com/lift?\\nid=kb_article&sys_id=b2aa58411ba2fd18a752ffbf034bcb0e&table=kb_knowledge).\\n8.Click verify and then continue which will trigger a biometric authentication through\\n9. Select the options you want to sync from your Snowflake email account and tap \"Save\"\\non the top right\\n'] \n",
      "\n",
      "\n",
      "ROW 50: query: what is thoughtspot\n",
      " , llm_response: ThoughtSpot is an AI-powered analytics platform designed to enable users to query data using natural language and obtain precise results. It provides a user-friendly interface for querying and analyzing data from various sources, including data warehouses 【†1†】.\n",
      "g, context_chunks: ['ThoughtSpot is an AI-powered analytics platform designed to enable users to: Query data using\\nnatural language and obtain precise results. Provide a user-friendly interface for querying and\\nanalyzing data from various sources, including data warehouses.'] \n",
      "\n",
      "\n",
      "ROW 51: query: There's going to be a planned power outage. As part of the GCS team, what do I need to do?\n",
      " , llm_response: As part of the GCS team, during a planned power outage, you need to run a sweep of all conference/Zoom rooms and MDF/IDF and other relevant areas to ensure normalcy. You should work with Network/AV/WPS to restore services. The checks you need to perform include checking MDF/IDF rooms for temperature & Humidity, UPS/PDU for power availability and Network devices for LED status, alarm sounds. You should also connect to different wireless access points on the floor and run speed tests to confirm network functionality, check for LED indicators on wireless access points for any faults, and check the status of all zoom rooms to ensure functionality. A visual walkthrough and inspection of the floor/s should be done to affirm service restoration and flag any inconsistencies to the Network/AV/WPS team. Post verification of devices by GCS should be done at least 12 Hrs prior to next business day or immediately after the window if it is nearing business hours to allow for sufficient time for troubleshooting 【†1†】.\n",
      "g, context_chunks: ['GCS\\n\\nCommunication in local channels, site leaders.\\nFlag any risks, concerns as identified.\\nVerify status of Zoom rooms, MDF/IDF and other assets onsite pre and post maintenance activity.\\nCommunicate with WPS, Network, AV and other teams as necessary.\\n\\n**AND**\\n\\nLocal GCS team will run a sweep of all conference/Zoom rooms and MDF/IDF and other relevant areas to ensure normalcy and work with Network/AV/WPS to\\nrestore services.\\nGCS would be conducting following checks:\\na. Check MDF/IDF rooms for temperature & Humidity (check after 2 hrs to have it settle down), UPS/PDU for power availability and Network devices for LED\\nstatus, alarm sounds.\\nb. Connect to different wireless access points on the floor and run speed tests to confirm network functionality. Check for LED indicators on wireless access\\npoints for any faults.\\nc. Check the status of all zoom rooms to ensure functionality of all of them.\\nd. A visual walkthrough and inspection of the floor/s to affirm service restoration and flag any inconsistencies to the Network/AV/WPS team.\\ne. Post verification of devices by GCS should be done atleast 12 Hrs prior to next business day or immediately after the window if itis nearing business\\nhours to allow for sufficienttime for troubleshooting.'] \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error removing trivial statements: invalid syntax (<string>, line 1). Proceeding with all statements.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROW 52: query: How do I print to a toshiba printer?\n",
      " , llm_response: To print to a Toshiba printer, follow these steps: Open the Self Service Application on your laptop and click on Toshiba Printer Driver. Then, open System Preferences by clicking on the Apple icon, and select System Preferences. Go to Printers & Scanners, click the [+] below the printer list, then Add. Select the TOSHIBA TEC TODEBA e-STUDIO350SAC printer. If you already have the printer in your list but still cannot print to it, you will want to delete the printer and then add the printer again 【†1†】.\n",
      "g, context_chunks: ['<image>'] \n",
      "\n",
      "\n",
      "ROW 53: query: Is setup for Ricoh printers different than for Toshiba printers?\n",
      " , llm_response: The provided excerpts do not specify the setup process for Toshiba printers, they only detail the setup for Ricoh and non-Ricoh printers. Therefore, it's unclear if the setup for Ricoh printers is different than for Toshiba printers 【†1†】 【†2†】 【†3†】.\n",
      "g, context_chunks: ['**From the Toshiba KB article**: <image>\\n---------------------------------------------\\n\\n**From the Ricoh article**:\\n\\nInstall the Ricoh Print Drivers from Self Service for Mac Laptop\\n\\n1. Open the Self Service application\\n2. In Self Service select the option for \"Setup Ricoh Cloud Print\"\\n\\n3. You may be prompted to allow Jamf access to control finder and system events. click\\n\"OK\" on both prompts\\n4. After installation, you will need to do a search to pull up the Ricoh virtual printer driver\\napplication\\n5. Click on the magnify glass at the top right of the screen and search for\\nvirtualprinterapplication\\n\\n6. You should now see the Ricoh icon in the menu bar\\n\\n7. If this icon does not display please restart your computer, or reach out to the SnowDesk\\nfrom the Lift Portal (https://lift.snowflake.com/lift).\\n8. Once the icon is displaying you will see a new printer in your print windows, Ricoh Print\\nCloud Virtual Printer. Select your new Virtual Printer and print as normal.\\n9. You should be able to print from any applications now.'] \n",
      "\n",
      "\n",
      "ROW 54: query: How do I install the Ricoh driver on Mac?\n",
      " , llm_response: To install the Ricoh driver on a Mac, follow these steps:\n",
      "1. Open the Self Service application.\n",
      "2. In Self Service, select the option for \"Setup Ricoh Cloud Print\".\n",
      "3. You may be prompted to allow Jamf access to control finder and system events. Click \"OK\" on both prompts.\n",
      "4. After installation, search for the Ricoh virtual printer driver application by clicking on the magnifying glass at the top right of the screen and searching for \"virtualprinterapplication\".\n",
      "5. You should now see the Ricoh icon in the menu bar. If this icon does not display, please restart your computer or reach out to the SnowDesk from the Lift Portal.\n",
      "6. Once the icon is displaying, you will see a new printer in your print windows, \"Ricoh Print Cloud Virtual Printer\". Select your new Virtual Printer and print as normal. 【†1†】\n",
      "g, context_chunks: ['Install the Ricoh Print Drivers from Self Service for Mac Laptop\\n\\n1. Open the Self Service application\\n2. In Self Service select the option for \"Setup Ricoh Cloud Print\"\\n\\n3. You may be prompted to allow Jamf access to control finder and system events. click\\n\"OK\" on both prompts\\n4. After installation, you will need to do a search to pull up the Ricoh virtual printer driver\\napplication\\n5. Click on the magnify glass at the top right of the screen and search for\\nvirtualprinterapplication\\n\\n6. You should now see the Ricoh icon in the menu bar\\n\\n7. If this icon does not display please restart your computer, or reach out to the SnowDesk\\nfrom the Lift Portal (https://lift.snowflake.com/lift).\\n8. Once the icon is displaying you will see a new printer in your print windows, Ricoh Print\\nCloud Virtual Printer. Select your new Virtual Printer and print as normal.\\n9. You should be able to print from any applications now.'] \n",
      "\n",
      "\n",
      "ROW 55: query: Which team checks the Zoom room iPads after a power outage?\n",
      " , llm_response: The GCS team is responsible for checking the status of all Zoom rooms, including the iPads, to ensure their functionality after a power outage 【†1†】.\n",
      "g, context_chunks: ['GCS\\n\\nCommunication in local channels, site leaders.\\nFlag any risks, concerns as identified.\\nVerify status of Zoom rooms, MDF/IDF and other assets onsite pre and post maintenance activity.\\nCommunicate with WPS, Network, AV and other teams as necessary.\\n\\n**AND/OR**\\n\\nLocal GCS team will run a sweep of all conference/Zoom rooms and MDF/IDF and other relevant areas to ensure normalcy and work with Network/AV/WPS to\\nrestore services.\\nGCS would be conducting following checks:\\na. Check MDF/IDF rooms for temperature & Humidity (check after 2 hrs to have it settle down), UPS/PDU for power availability and Network devices for LED\\nstatus, alarm sounds.\\nb. Connect to different wireless access points on the floor and run speed tests to confirm network functionality. Check for LED indicators on wireless access\\npoints for any faults.\\nc. Check the status of all zoom rooms to ensure functionality of all of them.\\nd. A visual walkthrough and inspection of the floor/s to affirm service restoration and flag any inconsistencies to the Network/AV/WPS team.\\ne. Post verification of devices by GCS should be done atleast 12 Hrs prior to next business day or immediately after the window if itis nearing business\\nhours to allow for sufficienttime for troubleshooting.'] \n",
      "\n",
      "\n",
      "ROW 56: query: What are the main set of applications I should set up on my iphone?\n",
      " , llm_response: The main set of applications you should set up on your iPhone, based on the provided excerpts, are Gmail, Snowflake apps such as Slack, Salesforce, Okta verify, and Duo, and your iOS native calendar (CalDav) 【†1†】 【†3†】.\n",
      "g, context_chunks: ['1. On your iPhone, go to Settings > Mail > Accounts > Add Account\\n2. Select Google and click on \"continue\" to accept the terms\\n3. Log in to Google with your Snowflake email and tap Next,On your Snowflake laptop, open the Beyond Identity application and click“Set up other devices”. For information on how to enroll your devices, please click here (/nav_to.do?uri=%2Fkb_view.do%3Fsysparm_article%3DKB0010716) .', '1. Download Slack application on your phone from App Store or Play Store\\n2. Once application is installed Open it\\n3. Click on “Sign in”\\n4. After you have selected to sign in, you will next be asked how you would like to sign in. Select the link\\nat the bottom “Sign in with Workplace URL.”\\n5. You will see a box where you can enter your workplace URL. Type in Snowflake so that it looks like this\\n“snowflake.slack.com”', 'Below is a collection of articles and tips related to the most commonly used applications and questions at Snowflake.'] \n",
      "\n",
      "\n",
      "ROW 57: query: How do I setup instagram\n",
      " , llm_response: I don't know the answer to that question.\n",
      "g, context_chunks: [] \n",
      "\n",
      "\n",
      "ROW 58: query: How do I install slack\n",
      " , llm_response: To install Slack on your mobile device, first ensure you have FIDO2, Okta Verify, and Beyond Identity configured on your phone. If not, refer to the respective KB articles for setup instructions. Once these are set up, download Slack from the App Store or Play Store. Open the Slack app and click 'Sign in', then select the 'Sign in with workspace URL' option. Enter your Slack workspace URL, which would be 'snowflake.slack.com', and click 'Continue'. Finally, select the 'Sign in with SnowBiz Okta' option and enter your Snowbiz Okta Username/Email Address. Click 'Next' to complete the setup process 【†1†】 【†2†】.\n",
      "g, context_chunks: ['1. Download Slack application on your phone from App Store or Play Store\\n2. Once application is installed Open it\\n3. Click on “Sign in”\\n4. After you have selected to sign in, you will next be asked how you would like to sign in. Select the link\\nat the bottom “Sign in with Workplace URL.”\\n5. You will see a box where you can enter your workplace URL. Type in Snowflake so that it looks like this\\n“snowflake.slack.com”'] \n",
      "\n",
      "\n",
      "ROW 59: query: How do I book travel to Snowflake Summit?\n",
      " , llm_response: To book travel to Snowflake Summit, follow these steps:\n",
      "\n",
      "1. Access the Corporate Travel Tool, MyTripPlanner, through Okta and log in to TripActions using your Snowflake credentials. Ensure your profile is up to date with your travel preferences and loyalty program numbers 【†1†】.\n",
      "2. For booking flights, search for flights by entering your departure and destination locations, trip dates, and any other relevant preferences. Choose the least expensive airfare option that complies with the travel policy and confirm your booking. To confirm your flights, you must enter the proper expense code for your trip. The appropriate expense code for travel to Snowflake Summit is SNOW-K841 【†3†】.\n",
      "3. For booking accommodations, search for hotels and select a hotel from Snowflake’s preferred hotels whenever possible. These hotels offer negotiated rates and include amenities like internet access, breakfast, and transportation services. Confirm your booking by selecting the \"Pay at Hotel\" option if flexibility is required. Always book refundable rate rooms to avoid non-reimbursable expenses if plans change 【†1†】.\n",
      "4. For booking car rentals and local transportation, search for car rentals and select an intermediate category car or lower unless a justified reason is provided. Avoid car rentals for short trips under 25 miles. Review and confirm your car rental reservation. Ensure to refuel the rental car before returning it to avoid excessive refueling charges from the rental agency 【†1†】.\n",
      "5. Before finalizing travel arrangements, discuss the business objectives and estimated costs with your manager and obtain approval 【†1†】.\n",
      "6. After your trip, gather itemized receipts for all travel-related expenses and submit your expense report using the appropriate expense codes 【†3†】.\n",
      "g, context_chunks: ['Step 1 - Access the Corporate Travel Tool\\n● All business travel arrangements must be made through the Corporate Travel Tool,\\nMyTripPlanner, which is accessible through Okta. Log in to TripActions using your\\nSnowflake credentials. Ensure that your profile is up to date with your travel preferences\\nand loyalty program numbers.\\nStep 2 - Booking Flights\\n● Search for Flights: Enter your departure and destination locations, travel dates, and any other relevant preferences.\\n● Compare Options: TripActions will present you with various flight options. Choose the least expensive airfare option that complies with the travel policy.\\n● Select Flights: Select the flights that best suit your schedule and budget. Lower-cost alternatives, such as non-refundable fares, flights with connections, and alternate airports,\\nmay be suggested.\\n● Confirm Booking: Review your flight details and confirm your booking. Ensure to plan\\nyour trips at least fourteen days in advance to benefit from lower fares.\\n○ To confirm your flights, you must enter the proper expense code for your trip.\\nTo find the appropriate expense code, please see KB0016213.Step 3 - Booking Accommodations\\n● Search for Hotels: Enter your destination, check-in and check-out dates, and any other\\nrelevant preferences.\\n● Select Hotel: Choose from Snowflake’s preferred hotels whenever possible. These hotels\\noffer negotiated rates and include amenities like internet access, breakfast, and\\ntransportation services.\\n● Confirm Booking: Select the \"Pay at Hotel\" option if flexibility is required. Always book\\nrefundable rate rooms to avoid non-reimbursable expenses if plans change.\\n● Review and Confirm: Review your hotel booking details and confirm your reservation.\\nStep 4 - Booking Car Rentals and Local Transportation\\n● Search for Car Rentals: Enter your pick-up and drop-off locations, dates, and times.\\n● Select Car: Choose an intermediate category car or lower unless a justified reason is\\nprovided. Avoid car rentals for short trips under 25 miles.\\n● Confirm Booking: Review and confirm your car rental reservation.\\n● Refuel Rental Car: Ensure to refuel the rental car before returning it to avoid excessive\\nrefueling charges from the rental agency.\\nStep 5 - Travel Authorizations\\n● Discuss with Manager: Before finalizing travel arrangements, discuss the business\\nobjectives and estimated costs with your manager.\\n● Obtain Approval: Ensure that the trip is essential and that all expenses are justified.', 'Air Travel SNOW-1A2B Expenses related to airline tickets, booking fees'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "gpt_4o = OpenAI(model_engine=\"gpt-4o-mini\")\n",
    "\n",
    "context_relevant_likert_4_criteria = \"\"\"\n",
    "        - CONTEXT that is IRRELEVANT to the QUESTION should score 0.\n",
    "        - CONTEXT that is RELEVANT to some of the QUESTION should score of 1.\n",
    "        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 2.\n",
    "        - CONTEXT that is RELEVANT to the entirety of the QUESTION should get a score of 3, which is the full mark.\n",
    "        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 3.\n",
    "        \"\"\"\n",
    "def trulens_context_relevance(query, context) -> float:\n",
    "    try:\n",
    "        return gpt_4o.context_relevance_with_cot_reasons(\n",
    "                question=query,\n",
    "                context=context,\n",
    "                max_score_val=3,\n",
    "                min_score_val=0,\n",
    "                criteria=context_relevant_likert_4_criteria\n",
    "        )[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in trulens_context_relevance: {e}\")\n",
    "        return -1\n",
    "answer_relevant_likert_4_criteria = \"\"\"\n",
    "        - RESPONSE must be relevant to the entire PROMPT to get a score of 4.\n",
    "        - RELEVANCE score should increase as the RESPONSE provides RELEVANT context to more parts of the PROMPT.\n",
    "        - RESPONSE that is RELEVANT to none of the PROMPT should get a score of 0.\n",
    "        - RESPONSE that is RELEVANT to some of the PROMPT should get as score of 1 or 2. Higher score indicates more RELEVANCE.\n",
    "        - RESPONSE that is RELEVANT to the entire PROMPT should get a score of 3.\n",
    "        - RESPONSE that is RELEVANT and answers the entire PROMPT completely should get a score of 3.\n",
    "        - RESPONSE that confidently FALSE should get a score of 0.\n",
    "        - RESPONSE that is only seemingly RELEVANT should get a score of 0.\n",
    "        - Answers that intentionally do not answer the question, such as 'I don't know' and model refusals, should also be counted as the least RELEVANT and get a score of 0.\n",
    "    \"\"\"\n",
    "def trulens_answer_relevance(query, response) -> float:\n",
    "    try:\n",
    "        return gpt_4o.relevance(\n",
    "                prompt=query,\n",
    "                response=response,\n",
    "                min_score_val=0,\n",
    "                max_score_val=3,\n",
    "                criteria=answer_relevant_likert_4_criteria\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error in trulens_answer_relevance: {e}\")\n",
    "        return -1\n",
    "\n",
    "def trulens_answer_relevance_cot(query, response) -> float:\n",
    "    try:\n",
    "        return gpt_4o.relevance_with_cot_reasons(\n",
    "                prompt=query,\n",
    "                response=response,\n",
    "                min_score_val=0,\n",
    "                max_score_val=3,\n",
    "                criteria=answer_relevant_likert_4_criteria\n",
    "        )[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in trulens_answer_relevance: {e}\")\n",
    "        return -1\n",
    "\n",
    "likert4_groundedness_criteria = \"\"\"You should score the groundedness of the statement based on the following criteria:\n",
    "    - Statements that are directly supported by the source should be considered grounded and should get a high score.\n",
    "    - Statements that are not directly supported by the source should be considered not grounded and should get a low score.\n",
    "    - Statements of doubt, that admissions of uncertainty or not knowing the answer are considered abstention, and should be counted as the most overlap and therefore get a max score.\"\"\"\n",
    "def trulens_groundedness(context, response) -> float:\n",
    "    try:\n",
    "        return gpt_4o.groundedness_measure_with_cot_reasons(\n",
    "                source=context,\n",
    "                statement=response,\n",
    "                use_sent_tokenize=True,\n",
    "                min_score_val=0,\n",
    "                max_score_val=3,\n",
    "                criteria=likert4_groundedness_criteria,\n",
    "        )[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in trulens_groundedness: {e}\")\n",
    "        return -1\n",
    "        \n",
    "context_relevance_scores, answer_relevance_scores, groundedness_scores, answer_relevance_cot_scores = [], [], [], []\n",
    "\n",
    "for i, row in cortex_eval_df.iterrows():\n",
    "    query = row[\"query\"]\n",
    "    context_chunks = [chunk for chunk in ast.literal_eval(row[\"golden\"])]\n",
    "    llm_response = row[\"llm_answer\"]\n",
    "    \n",
    "    assert query and llm_response, \"query and llm_response should not be empty\"\n",
    "\n",
    "    print(f\"ROW {i}: query: {query}\\n , llm_response: {llm_response}\\ng, context_chunks: {context_chunks} \\n\\n\")\n",
    "    answer_relevance_score = trulens_answer_relevance(query, llm_response)\n",
    "    answer_relevance_cot_score = trulens_answer_relevance_cot(query, llm_response)\n",
    "    answer_relevance_scores.append(answer_relevance_score)\n",
    "    answer_relevance_cot_scores.append(answer_relevance_cot_score)\n",
    "    if len(context_chunks) > 0:\n",
    "        _context_relevance_scores_per_query = []\n",
    "        _groundedness_scores_per_query = []\n",
    "        for context in context_chunks:\n",
    "            if context == \"Country Work-from-home budget (USD) Welcome period (mo)\\nPoland $350 12 \":\n",
    "                contex = \"Country Work-from-home budget (USD) Welcome period in Poland $350 12\"\n",
    "            _context_relevance_scores_per_query.append(trulens_context_relevance(query, context))\n",
    "            _groundedness_scores_per_query.append(trulens_groundedness(context, llm_response))\n",
    "            \n",
    "        context_relevance_scores.append(sum(_context_relevance_scores_per_query)/len(_context_relevance_scores_per_query))\n",
    "        groundedness_scores.append(sum(_groundedness_scores_per_query)/len(_groundedness_scores_per_query))\n",
    "    else:\n",
    "        context_relevance_scores.append(0)\n",
    "        groundedness_scores.append(0)\n",
    "assert len(context_relevance_scores) == len(answer_relevance_scores) == len(groundedness_scores) == len(cortex_eval_df) == len(answer_relevance_cot_scores)\n",
    "\n",
    "# save scores to csv\n",
    "cortex_eval_df[\"context_relevance_scores\"] = context_relevance_scores\n",
    "cortex_eval_df[\"answer_relevance_scores\"] = answer_relevance_scores\n",
    "cortex_eval_df[\"groundedness_scores\"] = groundedness_scores\n",
    "cortex_eval_df[\"answer_relevance_scores_cot\"] = answer_relevance_cot_scores\n",
    "cortex_eval_df.to_csv(\"cortex_eval_df_with_trulens_scores_relevance_cot.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.000000\n",
       "1     0.000000\n",
       "2     0.000000\n",
       "3     0.000000\n",
       "4     0.000000\n",
       "5     0.000000\n",
       "6     0.000000\n",
       "7     0.000000\n",
       "8     0.000000\n",
       "9     0.000000\n",
       "10    0.000000\n",
       "11    0.000000\n",
       "12    0.000000\n",
       "13    0.000000\n",
       "14    0.333333\n",
       "15    0.000000\n",
       "16    0.000000\n",
       "17   -0.333333\n",
       "18    0.333333\n",
       "19    0.000000\n",
       "20    0.666667\n",
       "21    0.000000\n",
       "22    0.000000\n",
       "23    0.000000\n",
       "24    0.000000\n",
       "25    0.000000\n",
       "26    0.000000\n",
       "27    0.000000\n",
       "28    0.000000\n",
       "29    0.000000\n",
       "30    0.000000\n",
       "31    0.000000\n",
       "32    0.000000\n",
       "33    0.000000\n",
       "34    0.000000\n",
       "35    0.000000\n",
       "36    0.000000\n",
       "37    0.000000\n",
       "38    0.000000\n",
       "39    0.000000\n",
       "40    0.000000\n",
       "41    0.000000\n",
       "42    0.000000\n",
       "43    0.000000\n",
       "44    0.000000\n",
       "45    0.000000\n",
       "46    0.000000\n",
       "47    0.000000\n",
       "48    0.000000\n",
       "49    0.000000\n",
       "50    0.000000\n",
       "51    0.000000\n",
       "52    0.000000\n",
       "53    0.000000\n",
       "54    0.000000\n",
       "55    0.000000\n",
       "56    0.000000\n",
       "57    0.000000\n",
       "58   -0.333333\n",
       "59    0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cortex_eval_df['answer_relevance_scores'] - cortex_eval_df['answer_relevance_scores_cot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer Relevance Mean Score: 73.89%\n",
      "Context Relevance Mean Score: 68.33%\n",
      "Groundedness Mean Score: 48.00%\n"
     ]
    }
   ],
   "source": [
    "answer_relevance_mean = cortex_eval_df['answer_relevance_scores_cot'].mean() * 100\n",
    "context_relevance_mean = cortex_eval_df['context_relevance_scores'].mean() * 100\n",
    "groundedness_mean = cortex_eval_df['groundedness_scores'].mean() * 100\n",
    "\n",
    "print(f\"Answer Relevance Mean Score: {answer_relevance_mean:.2f}%\")\n",
    "print(f\"Context Relevance Mean Score: {context_relevance_mean:.2f}%\")\n",
    "print(f\"Groundedness Mean Score: {groundedness_mean:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation test with Cortex's GT based metrics\n",
    "#### \n",
    "Cortex GT-based metrics:\n",
    "\n",
    "accuracy_llm: {-1, 0, 1, 2}, llm_citation_f1 [-1, 1.0], gt_citation_f1 [-1, 1.0], gris_llm_answer [0.0, 1.0] <-> answer relevance, context relevance, and groundedness (Likert 4)\n",
    "\n",
    "anls, gris_anls [0.0, 1.0] \n",
    "retrieval_ndcg_at_1 [0.0, 1.0], retrieval_hit_rate_at_1 BINARY, retrieval_ndcg_at_3 [0.0, 1.0], retrieval_hit_rate_at_3 BINARY <-> context relevance \n",
    "\n",
    "adjusted_llm_answer vs llm_answer?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['query', 'expected_response', 'golden', 'context_file',\n",
       "       'rephrased_queries', 'session', 'llm_answer', 'adjusted_llm_answer',\n",
       "       'rag_context', 'input_token_count', 'debug_signals', 'citations',\n",
       "       'rewritten_queries', 'accuracy_llm', 'llm_judge_answer',\n",
       "       'llm_citation_f1', 'llm_response_citation', 'llm_judge', 'search_table',\n",
       "       'is_match', 'anls', 'citation_match', 'citation_syntax',\n",
       "       'citation_error', 'citation_specific_reason', 'citation_precision',\n",
       "       'gt_citation_f1', 'gris_anls', 'retrieval_anls', 'retrieval_ndcg_at_1',\n",
       "       'retrieval_hit_at_1', 'retrieval_ndcg_at_3', 'retrieval_hit_at_3',\n",
       "       'gris_llm_answer', 'context_relevance_scores',\n",
       "       'answer_relevance_scores', 'groundedness_scores',\n",
       "       'context_relevance_scores_binary', 'answer_relevance_scores_binary',\n",
       "       'groundedness_scores_binary', 'accuracy_llm_normalized',\n",
       "       'answer_relevance_scores_cot'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cortex_eval_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "cortex_eval_df['context_relevance_scores_binary'] = cortex_eval_df['context_relevance_scores'].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "cortex_eval_df['answer_relevance_scores_binary'] = cortex_eval_df['answer_relevance_scores_cot'].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "cortex_eval_df['groundedness_scores_binary'] = cortex_eval_df['groundedness_scores'].apply(lambda x: 1 if x >= 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation Coefficient (retrieval_hit_at_1 vs context_relevance_scores_binary): 0.5229\n",
      "Cohen's Kappa (retrieval_hit_at_1 vs context_relevance_scores_binary): 0.5197\n",
      "Matthews Correlation Coefficient (retrieval_hit_at_3 vs context_relevance_scores_binary): 0.5585\n",
      "cohen's Kappa (retrieval_hit_at_3 vs context_relevance_scores_binary): 0.5522\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef, cohen_kappa_score\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "\n",
    "# Calculate Matthews correlation coefficient for retrieval_hit_at_1 and context_relevance_scores_binary\n",
    "mcc_hit_at_1_context_relevance = matthews_corrcoef(cortex_eval_df['retrieval_hit_at_1'], cortex_eval_df['context_relevance_scores_binary'])\n",
    "cohens_kappa_context_relevance = cohen_kappa_score(cortex_eval_df['retrieval_hit_at_1'], cortex_eval_df['context_relevance_scores_binary'])\n",
    "print(f\"Matthews Correlation Coefficient (retrieval_hit_at_1 vs context_relevance_scores_binary): {mcc_hit_at_1_context_relevance:.4f}\")\n",
    "print(f\"Cohen's Kappa (retrieval_hit_at_1 vs context_relevance_scores_binary): {cohens_kappa_context_relevance:.4f}\")\n",
    "\n",
    "# Calculate Matthews correlation coefficient for retrieval_hit_at_3 and context_relevance_scores_binary\n",
    "mcc_hit_at_3_context_relevance = matthews_corrcoef(cortex_eval_df['retrieval_hit_at_3'], cortex_eval_df['context_relevance_scores_binary'])\n",
    "cohens_kappa_context_relevance = cohen_kappa_score(cortex_eval_df['retrieval_hit_at_3'], cortex_eval_df['context_relevance_scores_binary'])\n",
    "print(f\"Matthews Correlation Coefficient (retrieval_hit_at_3 vs context_relevance_scores_binary): {mcc_hit_at_3_context_relevance:.4f}\")\n",
    "print(f\"cohen's Kappa (retrieval_hit_at_3 vs context_relevance_scores_binary): {cohens_kappa_context_relevance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "cortex_eval_df['accuracy_llm_normalized'] = cortex_eval_df['accuracy_llm'].apply(lambda x: (x - 0)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "REAL_VALUED_COLS = ['accuracy_llm_normalized', 'llm_citation_f1', 'gt_citation_f1', 'gris_llm_answer', 'anls', 'gris_anls', 'retrieval_ndcg_at_3', 'retrieval_ndcg_at_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cortex GT-based metrics</th>\n",
       "      <th>Spearman correlation with context_relevance_scores</th>\n",
       "      <th>Spearman correlation with answer_relevance_scores</th>\n",
       "      <th>Spearman correlation with groundedness_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy_llm_normalized</td>\n",
       "      <td>0.049536</td>\n",
       "      <td>-0.034905</td>\n",
       "      <td>0.241457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llm_citation_f1</td>\n",
       "      <td>0.006387</td>\n",
       "      <td>-0.133313</td>\n",
       "      <td>-0.017840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gt_citation_f1</td>\n",
       "      <td>0.590637</td>\n",
       "      <td>0.463307</td>\n",
       "      <td>0.453587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gris_llm_answer</td>\n",
       "      <td>0.046310</td>\n",
       "      <td>-0.091365</td>\n",
       "      <td>0.223963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anls</td>\n",
       "      <td>-0.128156</td>\n",
       "      <td>-0.037991</td>\n",
       "      <td>0.057499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gris_anls</td>\n",
       "      <td>-0.093176</td>\n",
       "      <td>-0.088607</td>\n",
       "      <td>0.061236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>retrieval_ndcg_at_3</td>\n",
       "      <td>0.618186</td>\n",
       "      <td>0.581630</td>\n",
       "      <td>0.567309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>retrieval_ndcg_at_1</td>\n",
       "      <td>0.567153</td>\n",
       "      <td>0.600265</td>\n",
       "      <td>0.512126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cortex GT-based metrics  \\\n",
       "0  accuracy_llm_normalized   \n",
       "1          llm_citation_f1   \n",
       "2           gt_citation_f1   \n",
       "3          gris_llm_answer   \n",
       "4                     anls   \n",
       "5                gris_anls   \n",
       "6      retrieval_ndcg_at_3   \n",
       "7      retrieval_ndcg_at_1   \n",
       "\n",
       "   Spearman correlation with context_relevance_scores  \\\n",
       "0                                           0.049536    \n",
       "1                                           0.006387    \n",
       "2                                           0.590637    \n",
       "3                                           0.046310    \n",
       "4                                          -0.128156    \n",
       "5                                          -0.093176    \n",
       "6                                           0.618186    \n",
       "7                                           0.567153    \n",
       "\n",
       "   Spearman correlation with answer_relevance_scores  \\\n",
       "0                                          -0.034905   \n",
       "1                                          -0.133313   \n",
       "2                                           0.463307   \n",
       "3                                          -0.091365   \n",
       "4                                          -0.037991   \n",
       "5                                          -0.088607   \n",
       "6                                           0.581630   \n",
       "7                                           0.600265   \n",
       "\n",
       "   Spearman correlation with groundedness_scores  \n",
       "0                                       0.241457  \n",
       "1                                      -0.017840  \n",
       "2                                       0.453587  \n",
       "3                                       0.223963  \n",
       "4                                       0.057499  \n",
       "5                                       0.061236  \n",
       "6                                       0.567309  \n",
       "7                                       0.512126  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# Create a dictionary to store the results\n",
    "results = {\n",
    "    \"Cortex GT-based metrics\": [],\n",
    "    \"Spearman correlation with context_relevance_scores\": [],\n",
    "    \"Spearman correlation with answer_relevance_scores\": [],\n",
    "    \"Spearman correlation with groundedness_scores\": []\n",
    "}\n",
    "\n",
    "# Calculate Spearman correlations and store them in the dictionary\n",
    "for col_name in REAL_VALUED_COLS:\n",
    "    spearman_corr_context, _ = spearmanr(cortex_eval_df[col_name], cortex_eval_df['context_relevance_scores'])\n",
    "    \n",
    "    spearman_corr_answer, _ = spearmanr(cortex_eval_df[col_name], cortex_eval_df['answer_relevance_scores_cot'])\n",
    "    \n",
    "    spearman_corr_groundedness, _ = spearmanr(cortex_eval_df[col_name], cortex_eval_df['groundedness_scores'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    results[\"Cortex GT-based metrics\"].append(col_name)\n",
    "    results[\"Spearman correlation with context_relevance_scores\"].append(spearman_corr_context)\n",
    "    results[\"Spearman correlation with answer_relevance_scores\"].append(spearman_corr_answer)\n",
    "    results[\"Spearman correlation with groundedness_scores\"].append(spearman_corr_groundedness)\n",
    "\n",
    "# Convert the dictionary to a pandas DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(cortex_eval_df['answer_relevance_scores_binary'], cortex_eval_df['anls'].apply(lambda x: 1 if x >= 0.5 else 0), title=\"Confusion Matrix - Answer Relevance vs ANLS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(cortex_eval_df['answer_relevance_scores_binary'], cortex_eval_df['accuracy_llm_normalized'].apply(lambda x: 1 if x >= 0.5 else 0), title=\"Confusion Matrix - Answer Relevance vs Accuracy LLM Normalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with Phoenix / Arize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m provider_name \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral-large\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m ]:\n\u001b[1;32m     11\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/dhuang/Documents/git/trulens/src/benchmark/trulens/benchmark/benchmark_frameworks/experiments/results/Snowflake IT balanced - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprovider_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_context_relevance_binary_results.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m     scores, labels, latencies \u001b[38;5;241m=\u001b[39m \u001b[43mread_results\u001b[49m(file_path)\n\u001b[1;32m     14\u001b[0m     scores \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m score \u001b[38;5;129;01min\u001b[39;00m scores]\n\u001b[1;32m     16\u001b[0m     binary_labels \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'read_results' is not defined"
     ]
    }
   ],
   "source": [
    "from trulens.feedback import GroundTruthAggregator\n",
    "import csv\n",
    "\n",
    "for provider_name in [\n",
    "    \"gpt-4o-mini\",\n",
    "    \"gpt-4o\",\n",
    "    \"snowflake-arctic\",\n",
    "    \"llama3.1-8b\",\n",
    "    \"mistral-large\",\n",
    "]:\n",
    "    file_path = f\"/Users/dhuang/Documents/git/trulens/src/benchmark/trulens/benchmark/benchmark_frameworks/experiments/results/Snowflake IT balanced - {provider_name}_context_relevance_binary_results.csv\"\n",
    "    scores, labels, latencies = read_results(file_path)\n",
    "    \n",
    "    scores = [1 if score >= 0.5 else 0 for score in scores]\n",
    "    \n",
    "    binary_labels = []\n",
    "    for label in labels:\n",
    "        if label >= 0.5:\n",
    "            binary_labels.append(1)\n",
    "        else:\n",
    "            binary_labels.append(0)\n",
    "    f_recall = GroundTruthAggregator(binary_labels).recall\n",
    "    f_precision = GroundTruthAggregator(binary_labels).precision\n",
    "    f_f1_score = GroundTruthAggregator(binary_labels).f1_score\n",
    "\n",
    "   \n",
    "\n",
    "    f_cohens_kappa = GroundTruthAggregator(binary_labels).cohens_kappa\n",
    "    # f_auc = GroundTruthAggregator(labels).auc\n",
    "\n",
    "    # f_mae = GroundTruthAggregator(labels).mae\n",
    "    # f_pearson = GroundTruthAggregator(labels).pearson_correlation\n",
    "    # f_spearman = GroundTruthAggregator(labels).spearman_correlation\n",
    "\n",
    "    recall = f_recall(scores)\n",
    "    precision = f_precision(scores)\n",
    "    f1_score = f_f1_score(scores)\n",
    "    # mae = f_mae(scores)\n",
    "    # pearson = f_pearson(scores)\n",
    "    # spearman = f_spearman(scores)\n",
    "    cohens_kappa = f_cohens_kappa(scores)\n",
    "    # auc = f_auc(scores)\n",
    "\n",
    "    for latency in latencies:\n",
    "        if latency > 20:\n",
    "            # print(f\"Warning: latency is greater than 10 seconds: {latency}\")\n",
    "            latencies.remove(latency)\n",
    "    avg_latency = sum(latencies) / len(latencies)\n",
    "\n",
    "    # print(f\"{provider_name}: mae: {mae:.4f}, pearson: {pearson:.4f}, spearman: {spearman:.4f}, Cohen's Kappa: {cohens_kappa:.4f}\")\n",
    "    print(\n",
    "        f\"{provider_name}: recall: {recall:.4f}, precision: {precision:.4f}, f1: {f1_score:.4f}, Cohen's Kappa: {cohens_kappa:.4f}, avg_latency: {avg_latency:.4f}\"\n",
    "    )\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    \n",
    "    plot_confusion_matrix(binary_labels, scores, title=f\"Confusion Matrix - {provider_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
