{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets preprocessing:\n",
    "\n",
    "Datasets that need some preprocessing before they can be used in `TruBenchmarkExperiment` class:\n",
    "1. Snowflake IT (internal): both rephrased and regular?, this should be used for all 3 in the triad\n",
    "2. SummEval (CNN and DailyMail summarizations with annotation) for groundedness\n",
    "3. QAGS (CNN and DailyMail with Turkers' annotation) for groundedness\n",
    "4. QAGS (XSUM with Turkers' annotation) for groundedness\n",
    "5. MSMARCO V2 for context relevance\n",
    "6. HotPot QA for answer relevance \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    generate_qags_golden_set_groundedness,\n",
    ")\n",
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    generate_snowflake_it_golden_set_answer_relevance,\n",
    ")\n",
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    generate_snowflake_it_golden_set_context_relevance,\n",
    ")\n",
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    generate_snowflake_it_golden_set_groundedness,\n",
    ")\n",
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    generate_summeval_groundedness_golden_set,\n",
    ")\n",
    "\n",
    "# Pin random seed\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "snowflake_it_file_path = \"data/snowflake_it_v3.csv\"\n",
    "\n",
    "snowflake_it_for_answer_relevance = pd.DataFrame(\n",
    "    list(\n",
    "        generate_snowflake_it_golden_set_answer_relevance(\n",
    "            snowflake_it_file_path\n",
    "        )\n",
    "    )\n",
    ")\n",
    "snowflake_it_answer_relevance_true_labels = list(\n",
    "    snowflake_it_for_answer_relevance[\"expected_score\"]\n",
    ")\n",
    "\n",
    "snowflake_it_for_context_relevance = pd.DataFrame(\n",
    "    list(\n",
    "        generate_snowflake_it_golden_set_context_relevance(\n",
    "            snowflake_it_file_path\n",
    "        )\n",
    "    )\n",
    ")\n",
    "snowflake_it_for_context_relevance_true_labels = list(\n",
    "    snowflake_it_for_context_relevance[\"expected_score\"]\n",
    ")\n",
    "\n",
    "\n",
    "snowflake_it_for_groundness = pd.DataFrame(\n",
    "    list(generate_snowflake_it_golden_set_groundedness(snowflake_it_file_path))\n",
    ")\n",
    "snowflake_it_for_groundness_true_labels = list(\n",
    "    snowflake_it_for_groundness[\"expected_score\"]\n",
    ")\n",
    "\n",
    "summeval_list = list(\n",
    "    generate_summeval_groundedness_golden_set(\"data/summeval_test.json\")\n",
    ")\n",
    "\n",
    "summeval_true_labels = [entry[\"expected_score\"] for entry in summeval_list]\n",
    "\n",
    "summeval = pd.DataFrame(\n",
    "    list(generate_summeval_groundedness_golden_set(\"data/summeval_test.json\"))\n",
    ")\n",
    "\n",
    "qags_cnn_dm = pd.DataFrame(\n",
    "    list(generate_qags_golden_set_groundedness(\"data/qags_mturk_cnndm.jsonl\"))\n",
    ")\n",
    "\n",
    "qags_cnn_dm_true_labels = [\n",
    "    row[\"expected_score\"] for _, row in qags_cnn_dm.iterrows()\n",
    "]\n",
    "\n",
    "\n",
    "qags_xsum = pd.DataFrame(\n",
    "    list(generate_qags_golden_set_groundedness(\"data/qags_mturk_xsum.jsonl\"))\n",
    ")\n",
    "\n",
    "qqags_xsum_true_labels = [\n",
    "    row[\"expected_score\"] for _, row in qags_xsum.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(42)\n",
    "\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# ds = load_dataset(\"nixiesearch/ms-marco-hard-negatives\")\n",
    "# ms_marco_hard_neg = pd.DataFrame(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "# ms_marco_hard_neg_balanced = generate_balanced_ms_marco_hard_negatives_dataset(ms_marco_hard_neg['train'], 400)\n",
    "# ms_marco_hard_neg_balanced.to_csv(\"ms_marco_hard_neg_balanced.csv\", index=False)\n",
    "ms_marco_hard_neg_balanced = pd.read_csv(\"data/ms_marco_hard_neg_balanced.csv\")\n",
    "ms_marco_hard_neg_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    generate_ms_marco_context_relevance_benchmark,\n",
    ")\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "ms_marco = list(generate_ms_marco_context_relevance_benchmark())\n",
    "\n",
    "\n",
    "score_1_entries = [entry for entry in ms_marco if entry[\"expected_score\"] == 1]\n",
    "score_0_entries = [entry for entry in ms_marco if entry[\"expected_score\"] == 0]\n",
    "\n",
    "# Calculate the number of samples needed from each group\n",
    "num_samples_per_group = min(\n",
    "    len(score_1_entries), len(score_0_entries), 150\n",
    ")  # Sample 150 from each\n",
    "\n",
    "\n",
    "sampled_score_1 = random.sample(score_1_entries, num_samples_per_group)\n",
    "sampled_score_0 = random.sample(score_0_entries, num_samples_per_group)\n",
    "\n",
    "# Combine and shuffle the samples to get a balanced dataset\n",
    "balanced_sample = sampled_score_1 + sampled_score_0\n",
    "random.shuffle(balanced_sample)\n",
    "\n",
    "# Ensure the combined length is 300\n",
    "assert len(balanced_sample) == 300\n",
    "\n",
    "# Now you can use `balanced_sample` as your final dataset\n",
    "print(\n",
    "    f\"Number of entries with expected_score = 1: {len([e for e in balanced_sample if e['expected_score'] == 1])}\"\n",
    ")\n",
    "print(\n",
    "    f\"Number of entries with expected_score = 0: {len([e for e in balanced_sample if e['expected_score'] == 0])}\"\n",
    ")\n",
    "\n",
    "ms_marco_balanced_sample_300 = pd.DataFrame(balanced_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_marco_balanced_sample_300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load preprocessed datasets from BEIR - start w/ Hotpot QA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.benchmark.benchmark_frameworks.dataset.beir_loader import (\n",
    "    TruBEIRDataLoader,\n",
    ")\n",
    "\n",
    "beir_data_loader = TruBEIRDataLoader(data_folder=\"./\", dataset_name=\"hotpotqa\")\n",
    "hotpotqa = beir_data_loader.load_dataset_to_df(download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "\n",
    "hotpotqa_raw_subset = hotpotqa.sample(n=200, random_state=42)\n",
    "\n",
    "all_responses = [\n",
    "    (row[\"query\"], row[\"expected_response\"])\n",
    "    for idx, row in hotpotqa_raw_subset.iterrows()\n",
    "]\n",
    "\n",
    "all_contexts = [\n",
    "    (row[\"query\"], context[\"text\"])\n",
    "    for idx, row in hotpotqa_raw_subset.iterrows()\n",
    "    for context in row[\"expected_chunks\"]\n",
    "]\n",
    "\n",
    "hotpotqa_subset_for_answer_relevance, hotpotqa_subset_for_context_relevance = (\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "\n",
    "for idx, row in hotpotqa_raw_subset.iterrows():\n",
    "    # Positive examples for answer relevance\n",
    "    hotpotqa_subset_for_answer_relevance.append({\n",
    "        \"query\": row[\"query\"],\n",
    "        \"expected_response\": row[\"expected_response\"],  # Positive response\n",
    "        \"expected_score\": 1,  # Positive example, score = 1\n",
    "    })\n",
    "\n",
    "    # Negative examples for answer relevance (random unrelated response)\n",
    "    negative_response = random.choice([\n",
    "        r\n",
    "        for q, r in all_responses\n",
    "        if q != row[\"query\"]  # Pick response from another query\n",
    "    ])\n",
    "\n",
    "    hotpotqa_subset_for_answer_relevance.append({\n",
    "        \"query\": row[\"query\"],\n",
    "        \"expected_response\": negative_response,  # Negative response\n",
    "        \"expected_score\": 0,  # Negative example, score = 0\n",
    "    })\n",
    "\n",
    "\n",
    "for idx, row in hotpotqa_raw_subset.iterrows():\n",
    "    positive_examples = []\n",
    "    negative_examples = []\n",
    "\n",
    "    # Generate positive examples for context relevance\n",
    "    for context in row[\"expected_chunks\"]:\n",
    "        positive_examples.append({\n",
    "            \"query\": row[\"query\"],\n",
    "            \"expected_response\": context[\"text\"],  # Positive context\n",
    "            \"expected_score\": context[\"expected_score\"],  # Should be 1\n",
    "        })\n",
    "\n",
    "    # Generate negative examples for context relevance\n",
    "    for _ in positive_examples:\n",
    "        negative_context = random.choice([\n",
    "            c\n",
    "            for q, c in all_contexts\n",
    "            if q != row[\"query\"]  # Pick context from another query\n",
    "        ])\n",
    "        negative_examples.append({\n",
    "            \"query\": row[\"query\"],\n",
    "            \"expected_response\": negative_context,  # Negative context\n",
    "            \"expected_score\": 0,  # Negative example, score = 0\n",
    "        })\n",
    "\n",
    "    # Add positive and negative examples to the result set\n",
    "    hotpotqa_subset_for_context_relevance.extend(positive_examples)\n",
    "    hotpotqa_subset_for_context_relevance.extend(negative_examples)\n",
    "\n",
    "\n",
    "hotpotqa_subset_for_context_relevance_true_labels = [\n",
    "    entry[\"expected_score\"] for entry in hotpotqa_subset_for_context_relevance\n",
    "]\n",
    "hotpotqa_subset_for_answer_relevance_true_labels = [\n",
    "    entry[\"expected_score\"] for entry in hotpotqa_subset_for_answer_relevance\n",
    "]\n",
    "\n",
    "hotpotqa_subset_for_context_relevance = pd.DataFrame(\n",
    "    hotpotqa_subset_for_context_relevance\n",
    ")\n",
    "\n",
    "hotpotqa_subset_for_answer_relevance = pd.DataFrame(\n",
    "    hotpotqa_subset_for_answer_relevance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotpotqa_subset_for_context_relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up feedback LLM providers \n",
    "\n",
    "We will experiment with 2 current OpenAI models and a mix of commercial and open source models avaiable in Cortex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from snowflake.snowpark import Session\n",
    "from trulens.providers.cortex import Cortex\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "snowflake_connection_parameters = {\n",
    "    \"account\": os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "    \"user\": os.environ[\"SNOWFLAKE_USER\"],\n",
    "    \"password\": os.environ[\"SNOWFLAKE_USER_PASSWORD\"],\n",
    "}\n",
    "\n",
    "snowpark_session = Session.builder.configs(\n",
    "    snowflake_connection_parameters\n",
    ").create()\n",
    "\n",
    "gpt_4o = OpenAI(model_engine=\"gpt-4o\")\n",
    "gpt_4o_mini = OpenAI(model_engine=\"gpt-4o-mini\")\n",
    "\n",
    "snowflake_arctic = Cortex(snowpark_session, model_engine=\"snowflake-arctic\")\n",
    "mistral_large = Cortex(snowpark_session, model_engine=\"mistral-large\")\n",
    "llama3_1_8b = Cortex(snowpark_session, model_engine=\"llama3.1-8b\")\n",
    "\n",
    "CORTEX_PROVIDERS = [snowflake_arctic, llama3_1_8b, mistral_large]\n",
    "OPENAI_PROVIDERS = [gpt_4o, gpt_4o_mini]\n",
    "ALL_PROVIDERS = CORTEX_PROVIDERS + OPENAI_PROVIDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import TruSession\n",
    "\n",
    "session = TruSession()\n",
    "session.reset_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snowflake IT dataset experiment runs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "from trulens.apps.basic import TruBasicApp\n",
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    read_results,\n",
    ")\n",
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    write_results,\n",
    ")\n",
    "\n",
    "\n",
    "def run_feedback_experiment(\n",
    "    feedback_func_wrapper, app_name, app_version, dataset_df, true_labels\n",
    "):\n",
    "    if len(dataset_df) != len(true_labels):\n",
    "        raise ValueError(\"dataset df must have the same length as labels\")\n",
    "    tru_wrapped_basic_app = TruBasicApp(\n",
    "        feedback_func_wrapper, app_name=app_name, app_version=app_version\n",
    "    )\n",
    "\n",
    "    generated_scores, labels, latencies = [], [], []\n",
    "    for i in range(len(dataset_df)):\n",
    "        arg_1 = dataset_df.iloc[i][\"query\"]\n",
    "        arg_2 = dataset_df.iloc[i][\"expected_response\"]\n",
    "        try:\n",
    "            with tru_wrapped_basic_app as _:\n",
    "                start_time = time.time()\n",
    "                score = tru_wrapped_basic_app.app(arg_1, arg_2)\n",
    "\n",
    "                end_time = time.time()\n",
    "                true_score = true_labels[i]\n",
    "\n",
    "                if not math.isnan(score):\n",
    "                    generated_scores.append(score)\n",
    "                    labels.append(true_score)\n",
    "                    latencies.append(end_time - start_time)\n",
    "\n",
    "                    # print(f\"Generated score: {score} | true_score: {true_score} \\n\")\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error {e} in run_feedback_experiment row {i} with first arg {arg_1} and second arg {arg_2}\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "    write_results(\n",
    "        generated_scores,\n",
    "        labels,\n",
    "        latencies,\n",
    "        f\"results/{app_name}_{app_version}_results.csv\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import traceback\n",
    "\n",
    "from trulens.feedback.v2.feedback import Groundedness\n",
    "\n",
    "\n",
    "def runn_all_experiments_for_provider(provider):\n",
    "    \"\"\"\n",
    "    Runs all experiments for a given provider.\n",
    "    \"\"\"\n",
    "\n",
    "    def context_relevance_binary(input, output) -> float:\n",
    "        return provider.context_relevance_with_cot_reasons(\n",
    "            question=input,\n",
    "            context=output,\n",
    "            criteria=\"A relevant context to the question should get a score of 1, and an irrelevant context should get a score of 0. The score can only be either 0 or 1 (binary).\",\n",
    "            min_score_val=0,\n",
    "            max_score_val=1,\n",
    "        )[0]\n",
    "\n",
    "    # Run context relevance binary experiment\n",
    "    # context_relevance_binary_run = create_benchmark_experiment_app(\n",
    "    #     app_name=f\"Snowflake IT  - {provider.model_engine}\",\n",
    "    #     app_version='context_relevance_binary',\n",
    "    #     benchmark_experiment=TruBenchmarkExperiment(\n",
    "    #         feedback_fn=context_relevance_binary,\n",
    "    #         agg_funcs=snowflake_it_metrics,\n",
    "    #         benchmark_params=benchmark_params\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    # with context_relevance_binary_run as recording:\n",
    "    #     feedback_res = context_relevance_binary_run.app(snowflake_it_for_context_relevance)\n",
    "    #     print(f'feedback results: {feedback_res}')\n",
    "    #     write_results(feedback_scores=feedback_res, file_name=f\"results/{provider.model_engine}-context_relevance_binary_feedback_scores.csv\")\n",
    "\n",
    "    # # Similar pattern for answer relevance binary experiment\n",
    "    def answer_relevance_binary(input, output) -> float:\n",
    "        return provider.relevance(\n",
    "            prompt=input,\n",
    "            response=output,\n",
    "            criteria=\"A relevant response to the prompt should get a score of 1, and an irrelevant response should get a score of 0. The score can only be either 0 or 1 (binary).\",\n",
    "            min_score_val=0,\n",
    "            max_score_val=1,\n",
    "        )\n",
    "\n",
    "    # answer_relevance_binary_run = create_benchmark_experiment_app(\n",
    "    #     app_name=f\"Snowflake IT - {provider.model_engine}\",\n",
    "    #     app_version='answer_relevance_binary',\n",
    "    #     benchmark_experiment=TruBenchmarkExperiment(\n",
    "    #         feedback_fn=answer_relevance_binary,\n",
    "    #         agg_funcs=snowflake_it_metrics,\n",
    "    #         benchmark_params=benchmark_params\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    # with answer_relevance_binary_run as recording:\n",
    "    #     feedback_res = answer_relevance_binary_run.app(snowflake_it_for_answer_relevance)\n",
    "    #     write_results(feedback_scores=feedback_res, file_name=f\"results/{provider.model_engine}-answer_relevance_binary_feedback_scores.csv\")\n",
    "\n",
    "    def groundedness_binary(input, output) -> float:\n",
    "        return provider.groundedness_measure_with_cot_reasons(\n",
    "            source=input,\n",
    "            statement=output,\n",
    "            criteria=\"A grounded response to the query should get a score of 1, and an ungrounded response should get a score of 0. The score can only be either 0 or 1 (binary).\",\n",
    "            min_score_val=0,\n",
    "            max_score_val=1,\n",
    "            use_sent_tokenize=True,\n",
    "        )[0]\n",
    "\n",
    "    def groundedness_likert_4(input, output) -> float:\n",
    "        return provider.groundedness_measure_with_cot_reasons(\n",
    "            source=input, statement=output, use_sent_tokenize=True\n",
    "        )[0]\n",
    "\n",
    "    # Define a function to wrap the run_feedback_experiment call\n",
    "    def run_experiment(\n",
    "        feedback_fn, app_name, app_version, dataset_df, true_labels\n",
    "    ):\n",
    "        run_feedback_experiment(\n",
    "            feedback_func_wrapper=feedback_fn,\n",
    "            app_name=app_name,\n",
    "            app_version=app_version,\n",
    "            dataset_df=dataset_df,\n",
    "            true_labels=true_labels,\n",
    "        )\n",
    "\n",
    "    context_relevance_experiments = [\n",
    "        {\n",
    "            \"feedback_fn\": context_relevance_binary,\n",
    "            \"app_name\": f\"Snowflake IT balanced - {provider.model_engine}\",\n",
    "            \"app_version\": \"context_relevance_binary\",\n",
    "            \"dataset_df\": snowflake_it_for_context_relevance,\n",
    "            \"true_labels\": snowflake_it_for_context_relevance_true_labels,\n",
    "        },\n",
    "        {\n",
    "            \"feedback_fn\": context_relevance_binary,\n",
    "            \"app_name\": f\"Hotpot QA (800 samples) - {provider.model_engine}\",\n",
    "            \"app_version\": \"context_relevance_binary\",\n",
    "            \"dataset_df\": hotpotqa_subset_for_context_relevance,\n",
    "            \"true_labels\": hotpotqa_subset_for_context_relevance_true_labels,\n",
    "        },\n",
    "        {\n",
    "            \"feedback_fn\": context_relevance_binary,\n",
    "            \"app_name\": f\"MS MARCO hard negatives (first 400 samples) - {provider.model_engine}\",\n",
    "            \"app_version\": \"context_relevance_binary\",\n",
    "            \"dataset_df\": ms_marco_hard_neg_balanced,\n",
    "            \"true_labels\": [\n",
    "                row[\"expected_score\"]\n",
    "                for _, row in ms_marco_hard_neg_balanced.iterrows()\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            # MS MARCO V2 for context relevance\n",
    "            \"feedback_fn\": context_relevance_binary,\n",
    "            \"app_name\": f\"MS MARCO V2 balanced (300 samples) - {provider.model_engine}\",\n",
    "            \"app_version\": \"context_relevance_binary\",\n",
    "            \"dataset_df\": ms_marco_balanced_sample_300,\n",
    "            \"true_labels\": [\n",
    "                row[\"expected_score\"]\n",
    "                for _, row in ms_marco_balanced_sample_300.iterrows()\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    groundedness_experiments = [\n",
    "        {\n",
    "            \"feedback_fn\": groundedness_binary,\n",
    "            \"app_name\": f\"Snowflake IT balanced - {provider.model_engine}\",\n",
    "            \"app_version\": \"groundedness_binary\",\n",
    "            \"dataset_df\": snowflake_it_for_groundness,\n",
    "            \"true_labels\": snowflake_it_for_groundness_true_labels,\n",
    "        },\n",
    "        {\n",
    "            \"feedback_fn\": groundedness_likert_4,\n",
    "            \"app_name\": f\"SummEval (bucketed samples) - {provider.model_engine}\",\n",
    "            \"app_version\": \"groundedness_likert4\",\n",
    "            \"dataset_df\": summeval,\n",
    "            \"true_labels\": [\n",
    "                row[\"expected_score\"] for _, row in summeval.iterrows()\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"feedback_fn\": groundedness_likert_4,\n",
    "            \"app_name\": f\"QAGS CNN_DM (bucketed samples) - {provider.model_engine}\",\n",
    "            \"app_version\": \"groundedness_likert4\",\n",
    "            \"dataset_df\": qags_cnn_dm,\n",
    "            \"true_labels\": qags_cnn_dm_true_labels,\n",
    "        },\n",
    "        {\n",
    "            \"feedback_fn\": groundedness_likert_4,\n",
    "            \"app_name\": f\"QAGS XSum (bucketed samples) - {provider.model_engine}\",\n",
    "            \"app_version\": \"groundedness_likert4\",\n",
    "            \"dataset_df\": qags_xsum,\n",
    "            \"true_labels\": qqags_xsum_true_labels,\n",
    "        },\n",
    "    ]\n",
    "    answer_relevance_experiments = [\n",
    "        {\n",
    "            \"feedback_fn\": answer_relevance_binary,\n",
    "            \"app_name\": f\"Snowflake IT balanced - {provider.model_engine}\",\n",
    "            \"app_version\": \"answer_relevance_binary\",\n",
    "            \"dataset_df\": snowflake_it_for_answer_relevance,\n",
    "            \"true_labels\": snowflake_it_answer_relevance_true_labels,\n",
    "        },\n",
    "        {\n",
    "            \"feedback_fn\": answer_relevance_binary,\n",
    "            \"app_name\": f\"Hotpot QA (400 samples) - {provider.model_engine}\",\n",
    "            \"app_version\": \"answer_relevance_binary\",\n",
    "            \"dataset_df\": hotpotqa_subset_for_answer_relevance,\n",
    "            \"true_labels\": hotpotqa_subset_for_answer_relevance_true_labels,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for exp in (\n",
    "        answer_relevance_experiments\n",
    "        + context_relevance_experiments\n",
    "        + groundedness_experiments\n",
    "    ):\n",
    "        print(f\"Running experiment: {exp['app_name']} - {exp['app_version']}\")\n",
    "        if \"groundedness\" in exp[\"app_version\"]:\n",
    "            print(f\"Groundedness system prompt: {Groundedness.system_prompt}\")\n",
    "\n",
    "        run_experiment(\n",
    "            exp[\"feedback_fn\"],\n",
    "            exp[\"app_name\"],\n",
    "            exp[\"app_version\"],\n",
    "            exp[\"dataset_df\"],\n",
    "            exp[\"true_labels\"],\n",
    "        )\n",
    "\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Submit the run_experiment_for_provider function for each provider\n",
    "    futures = [\n",
    "        executor.submit(runn_all_experiments_for_provider, provider)\n",
    "        for provider in ALL_PROVIDERS\n",
    "    ]\n",
    "\n",
    "    # Optionally, gather results or exceptions\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            result = (\n",
    "                future.result()\n",
    "            )  # This will re-raise any exceptions caught during execution\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    plot_confusion_matrix,\n",
    ")\n",
    "from trulens.feedback import GroundTruthAggregator\n",
    "\n",
    "for provider_name in [\n",
    "    \"gpt-4o-mini\",\n",
    "    \"gpt-4o\",\n",
    "    \"snowflake-arctic\",\n",
    "    \"llama3.1-8b\",\n",
    "    \"mistral-large\",\n",
    "]:\n",
    "    file_path = f\"results/QAGS XSum (bucketed samples) - {provider_name}_groundedness_likert4_results.csv\"\n",
    "    scores, labels, latencies = read_results(file_path)\n",
    "\n",
    "    print(len(scores), len(labels), len(latencies))\n",
    "\n",
    "    binary_labels = []\n",
    "    for label in labels:\n",
    "        if label >= 0.5:\n",
    "            binary_labels.append(1)\n",
    "        else:\n",
    "            binary_labels.append(0)\n",
    "    f_recall = GroundTruthAggregator(binary_labels).recall\n",
    "    f_precision = GroundTruthAggregator(binary_labels).precision\n",
    "    f_f1_score = GroundTruthAggregator(binary_labels).f1_score\n",
    "\n",
    "    binary_scores = []\n",
    "    for score in scores:\n",
    "        if score >= 0.5:\n",
    "            binary_scores.append(1)\n",
    "        else:\n",
    "            binary_scores.append(0)\n",
    "\n",
    "    f_cohens_kappa = GroundTruthAggregator(binary_labels).cohens_kappa\n",
    "    f_auc = GroundTruthAggregator(labels).auc\n",
    "\n",
    "    f_mae = GroundTruthAggregator(labels).mae\n",
    "    f_pearson = GroundTruthAggregator(labels).pearson_correlation\n",
    "    f_spearman = GroundTruthAggregator(labels).spearman_correlation\n",
    "    f_matthews = GroundTruthAggregator(binary_labels).matthews_correlation\n",
    "\n",
    "    recall = f_recall(scores)\n",
    "    precision = f_precision(scores)\n",
    "    f1_score = f_f1_score(scores)\n",
    "    mae = f_mae(scores)\n",
    "    pearson = f_pearson(scores)\n",
    "    spearman = f_spearman(scores)\n",
    "    cohens_kappa = f_cohens_kappa(scores)\n",
    "    # auc = f_auc(scores)\n",
    "    matthews = f_matthews(binary_scores)\n",
    "\n",
    "    for latency in latencies:\n",
    "        if latency > 20:\n",
    "            # print(f\"Warning: latency is greater than 10 seconds: {latency}\")\n",
    "            latencies.remove(latency)\n",
    "    avg_latency = sum(latencies) / len(latencies)\n",
    "\n",
    "    print(\n",
    "        f\"{provider_name}: mae: {mae:.4f}, pearson: {pearson:.4f}, spearman: {spearman:.4f}, Cohen's Kappa: {cohens_kappa:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"{provider_name}: recall: {recall:.4f}, precision: {precision:.4f}, f1: {f1_score:.4f},  Matthews: {matthews:.4f}, Cohen's Kappa: {cohens_kappa:.4f}, avg_latency: {avg_latency:.4f}\"\n",
    "    )\n",
    "    print(\"\\n\")\n",
    "\n",
    "    plot_confusion_matrix(\n",
    "        binary_labels,\n",
    "        binary_scores,\n",
    "        title=f\"Confusion Matrix - {provider_name}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading results of internal evaluation runs scraped from Cortex Chat orchestrator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cortex_eval_df = pd.read_csv(\"eval_scrape_mistral-large_output_1727118011.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cortex_eval_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "gpt_4o = OpenAI(model_engine=\"gpt-4o-mini\")\n",
    "\n",
    "context_relevant_likert_4_criteria = \"\"\"\n",
    "        - CONTEXT that is IRRELEVANT to the QUESTION should score 0.\n",
    "        - CONTEXT that is RELEVANT to some of the QUESTION should score of 1.\n",
    "        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 2.\n",
    "        - CONTEXT that is RELEVANT to the entirety of the QUESTION should get a score of 3, which is the full mark.\n",
    "        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 3.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "def trulens_context_relevance(query, context) -> float:\n",
    "    try:\n",
    "        return gpt_4o.context_relevance_with_cot_reasons(\n",
    "            question=query,\n",
    "            context=context,\n",
    "            max_score_val=3,\n",
    "            min_score_val=0,\n",
    "            criteria=context_relevant_likert_4_criteria,\n",
    "        )[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in trulens_context_relevance: {e}\")\n",
    "        return -1\n",
    "\n",
    "\n",
    "answer_relevant_likert_4_criteria = \"\"\"\n",
    "        - RESPONSE must be relevant to the entire PROMPT to get a score of 4.\n",
    "        - RELEVANCE score should increase as the RESPONSE provides RELEVANT context to more parts of the PROMPT.\n",
    "        - RESPONSE that is RELEVANT to none of the PROMPT should get a score of 0.\n",
    "        - RESPONSE that is RELEVANT to some of the PROMPT should get as score of 1 or 2. Higher score indicates more RELEVANCE.\n",
    "        - RESPONSE that is RELEVANT to the entire PROMPT should get a score of 3.\n",
    "        - RESPONSE that is RELEVANT and answers the entire PROMPT completely should get a score of 3.\n",
    "        - RESPONSE that confidently FALSE should get a score of 0.\n",
    "        - RESPONSE that is only seemingly RELEVANT should get a score of 0.\n",
    "        - Answers that intentionally do not answer the question, such as 'I don't know' and model refusals, should also be counted as the least RELEVANT and get a score of 0.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def trulens_answer_relevance(query, response) -> float:\n",
    "    try:\n",
    "        return gpt_4o.relevance(\n",
    "            prompt=query,\n",
    "            response=response,\n",
    "            min_score_val=0,\n",
    "            max_score_val=3,\n",
    "            criteria=answer_relevant_likert_4_criteria,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error in trulens_answer_relevance: {e}\")\n",
    "        return -1\n",
    "\n",
    "\n",
    "def trulens_answer_relevance_cot(query, response) -> float:\n",
    "    try:\n",
    "        return gpt_4o.relevance_with_cot_reasons(\n",
    "            prompt=query,\n",
    "            response=response,\n",
    "            min_score_val=0,\n",
    "            max_score_val=3,\n",
    "            criteria=answer_relevant_likert_4_criteria,\n",
    "        )[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in trulens_answer_relevance: {e}\")\n",
    "        return -1\n",
    "\n",
    "\n",
    "def trulens_groundedness(context, response) -> float:\n",
    "    try:\n",
    "        return gpt_4o.groundedness_measure_with_cot_reasons(\n",
    "            source=context, statement=response, use_sent_tokenize=True\n",
    "        )[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in trulens_groundedness: {e}\")\n",
    "        return -1\n",
    "\n",
    "\n",
    "(\n",
    "    context_relevance_scores,\n",
    "    answer_relevance_scores,\n",
    "    groundedness_scores,\n",
    "    answer_relevance_cot_scores,\n",
    ") = [], [], [], []\n",
    "\n",
    "for i, row in cortex_eval_df.iterrows():\n",
    "    query = row[\"query\"]\n",
    "    context_chunks = [chunk for chunk in ast.literal_eval(row[\"golden\"])]\n",
    "    llm_response = row[\"llm_answer\"]\n",
    "\n",
    "    assert query and llm_response, \"query and llm_response should not be empty\"\n",
    "\n",
    "    print(\n",
    "        f\"ROW {i}: query: {query}\\n , llm_response: {llm_response}\\ng, context_chunks: {context_chunks} \\n\\n\"\n",
    "    )\n",
    "    answer_relevance_score = trulens_answer_relevance(query, llm_response)\n",
    "    answer_relevance_cot_score = trulens_answer_relevance_cot(\n",
    "        query, llm_response\n",
    "    )\n",
    "    answer_relevance_scores.append(answer_relevance_score)\n",
    "    answer_relevance_cot_scores.append(answer_relevance_cot_score)\n",
    "    if len(context_chunks) > 0:\n",
    "        _context_relevance_scores_per_query = []\n",
    "        _groundedness_scores_per_query = []\n",
    "        for context in context_chunks:\n",
    "            if (\n",
    "                context\n",
    "                == \"Country Work-from-home budget (USD) Welcome period (mo)\\nPoland $350 12 \"\n",
    "            ):\n",
    "                contex = \"Country Work-from-home budget (USD) Welcome period in Poland $350 12\"\n",
    "            _context_relevance_scores_per_query.append(\n",
    "                trulens_context_relevance(query, context)\n",
    "            )\n",
    "            _groundedness_scores_per_query.append(\n",
    "                trulens_groundedness(context, llm_response)\n",
    "            )\n",
    "\n",
    "        context_relevance_scores.append(\n",
    "            sum(_context_relevance_scores_per_query)\n",
    "            / len(_context_relevance_scores_per_query)\n",
    "        )\n",
    "        groundedness_scores.append(\n",
    "            sum(_groundedness_scores_per_query)\n",
    "            / len(_groundedness_scores_per_query)\n",
    "        )\n",
    "    else:\n",
    "        context_relevance_scores.append(0)\n",
    "        groundedness_scores.append(0)\n",
    "assert (\n",
    "    len(context_relevance_scores)\n",
    "    == len(answer_relevance_scores)\n",
    "    == len(groundedness_scores)\n",
    "    == len(cortex_eval_df)\n",
    "    == len(answer_relevance_cot_scores)\n",
    ")\n",
    "\n",
    "# save scores to csv\n",
    "cortex_eval_df[\"context_relevance_scores\"] = context_relevance_scores\n",
    "cortex_eval_df[\"answer_relevance_scores\"] = answer_relevance_scores\n",
    "cortex_eval_df[\"groundedness_scores\"] = groundedness_scores\n",
    "cortex_eval_df[\"answer_relevance_scores_cot\"] = answer_relevance_cot_scores\n",
    "cortex_eval_df.to_csv(\n",
    "    \"cortex_eval_df_with_trulens_scores_relevance_cot.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    cortex_eval_df[\"answer_relevance_scores\"]\n",
    "    - cortex_eval_df[\"answer_relevance_scores_cot\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_relevance_mean = (\n",
    "    cortex_eval_df[\"answer_relevance_scores_cot\"].mean() * 100\n",
    ")\n",
    "context_relevance_mean = cortex_eval_df[\"context_relevance_scores\"].mean() * 100\n",
    "groundedness_mean = cortex_eval_df[\"groundedness_scores\"].mean() * 100\n",
    "\n",
    "print(f\"Answer Relevance Mean Score: {answer_relevance_mean:.2f}%\")\n",
    "print(f\"Context Relevance Mean Score: {context_relevance_mean:.2f}%\")\n",
    "print(f\"Groundedness Mean Score: {groundedness_mean:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation test with Cortex's GT based metrics\n",
    "#### \n",
    "Cortex GT-based metrics:\n",
    "\n",
    "accuracy_llm: {-1, 0, 1, 2}, llm_citation_f1 [-1, 1.0], gt_citation_f1 [-1, 1.0], gris_llm_answer [0.0, 1.0] <-> answer relevance, context relevance, and groundedness (Likert 4)\n",
    "\n",
    "anls, gris_anls [0.0, 1.0] \n",
    "retrieval_ndcg_at_1 [0.0, 1.0], retrieval_hit_rate_at_1 BINARY, retrieval_ndcg_at_3 [0.0, 1.0], retrieval_hit_rate_at_3 BINARY <-> context relevance \n",
    "\n",
    "adjusted_llm_answer vs llm_answer?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cortex_eval_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cortex_eval_df[\"context_relevance_scores_binary\"] = cortex_eval_df[\n",
    "    \"context_relevance_scores\"\n",
    "].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "cortex_eval_df[\"answer_relevance_scores_binary\"] = cortex_eval_df[\n",
    "    \"answer_relevance_scores_cot\"\n",
    "].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "cortex_eval_df[\"groundedness_scores_binary\"] = cortex_eval_df[\n",
    "    \"groundedness_scores\"\n",
    "].apply(lambda x: 1 if x >= 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "# Calculate Matthews correlation coefficient for retrieval_hit_at_1 and context_relevance_scores_binary\n",
    "mcc_hit_at_1_context_relevance = matthews_corrcoef(\n",
    "    cortex_eval_df[\"retrieval_hit_at_1\"],\n",
    "    cortex_eval_df[\"context_relevance_scores_binary\"],\n",
    ")\n",
    "cohens_kappa_context_relevance = cohen_kappa_score(\n",
    "    cortex_eval_df[\"retrieval_hit_at_1\"],\n",
    "    cortex_eval_df[\"context_relevance_scores_binary\"],\n",
    ")\n",
    "print(\n",
    "    f\"Matthews Correlation Coefficient (retrieval_hit_at_1 vs context_relevance_scores_binary): {mcc_hit_at_1_context_relevance:.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Cohen's Kappa (retrieval_hit_at_1 vs context_relevance_scores_binary): {cohens_kappa_context_relevance:.4f}\"\n",
    ")\n",
    "\n",
    "# Calculate Matthews correlation coefficient for retrieval_hit_at_3 and context_relevance_scores_binary\n",
    "mcc_hit_at_3_context_relevance = matthews_corrcoef(\n",
    "    cortex_eval_df[\"retrieval_hit_at_3\"],\n",
    "    cortex_eval_df[\"context_relevance_scores_binary\"],\n",
    ")\n",
    "cohens_kappa_context_relevance = cohen_kappa_score(\n",
    "    cortex_eval_df[\"retrieval_hit_at_3\"],\n",
    "    cortex_eval_df[\"context_relevance_scores_binary\"],\n",
    ")\n",
    "print(\n",
    "    f\"Matthews Correlation Coefficient (retrieval_hit_at_3 vs context_relevance_scores_binary): {mcc_hit_at_3_context_relevance:.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"cohen's Kappa (retrieval_hit_at_3 vs context_relevance_scores_binary): {cohens_kappa_context_relevance:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cortex_eval_df[\"accuracy_llm_normalized\"] = cortex_eval_df[\n",
    "    \"accuracy_llm\"\n",
    "].apply(lambda x: (x - 0) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REAL_VALUED_COLS = [\n",
    "    \"accuracy_llm_normalized\",\n",
    "    \"llm_citation_f1\",\n",
    "    \"gt_citation_f1\",\n",
    "    \"gris_llm_answer\",\n",
    "    \"anls\",\n",
    "    \"gris_anls\",\n",
    "    \"retrieval_ndcg_at_3\",\n",
    "    \"retrieval_ndcg_at_1\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the results\n",
    "results = {\n",
    "    \"Cortex GT-based metrics\": [],\n",
    "    \"Spearman correlation with context_relevance_scores\": [],\n",
    "    \"Spearman correlation with answer_relevance_scores\": [],\n",
    "    \"Spearman correlation with groundedness_scores\": [],\n",
    "}\n",
    "\n",
    "# Calculate Spearman correlations and store them in the dictionary\n",
    "for col_name in REAL_VALUED_COLS:\n",
    "    spearman_corr_context, _ = spearmanr(\n",
    "        cortex_eval_df[col_name], cortex_eval_df[\"context_relevance_scores\"]\n",
    "    )\n",
    "\n",
    "    spearman_corr_answer, _ = spearmanr(\n",
    "        cortex_eval_df[col_name], cortex_eval_df[\"answer_relevance_scores_cot\"]\n",
    "    )\n",
    "\n",
    "    spearman_corr_groundedness, _ = spearmanr(\n",
    "        cortex_eval_df[col_name], cortex_eval_df[\"groundedness_scores\"]\n",
    "    )\n",
    "\n",
    "    results[\"Cortex GT-based metrics\"].append(col_name)\n",
    "    results[\"Spearman correlation with context_relevance_scores\"].append(\n",
    "        spearman_corr_context\n",
    "    )\n",
    "    results[\"Spearman correlation with answer_relevance_scores\"].append(\n",
    "        spearman_corr_answer\n",
    "    )\n",
    "    results[\"Spearman correlation with groundedness_scores\"].append(\n",
    "        spearman_corr_groundedness\n",
    "    )\n",
    "\n",
    "# Convert the dictionary to a pandas DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\n",
    "    cortex_eval_df[\"answer_relevance_scores_binary\"],\n",
    "    cortex_eval_df[\"anls\"].apply(lambda x: 1 if x >= 0.5 else 0),\n",
    "    title=\"Confusion Matrix - Answer Relevance vs ANLS\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\n",
    "    cortex_eval_df[\"answer_relevance_scores_binary\"],\n",
    "    cortex_eval_df[\"accuracy_llm_normalized\"].apply(\n",
    "        lambda x: 1 if x >= 0.5 else 0\n",
    "    ),\n",
    "    title=\"Confusion Matrix - Answer Relevance vs Accuracy LLM Normalized\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with Phoenix / Arize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.feedback import GroundTruthAggregator\n",
    "\n",
    "for provider_name in [\n",
    "    \"gpt-4o-mini\",\n",
    "    \"gpt-4o\",\n",
    "    \"snowflake-arctic\",\n",
    "    \"llama3.1-8b\",\n",
    "    \"mistral-large\",\n",
    "]:\n",
    "    file_path = f\"results/Snowflake IT balanced - {provider_name}_context_relevance_binary_results.csv\"\n",
    "    scores, labels, latencies = read_results(file_path)\n",
    "\n",
    "    scores = [1 if score >= 0.5 else 0 for score in scores]\n",
    "\n",
    "    binary_labels = []\n",
    "    for label in labels:\n",
    "        if label >= 0.5:\n",
    "            binary_labels.append(1)\n",
    "        else:\n",
    "            binary_labels.append(0)\n",
    "    f_recall = GroundTruthAggregator(binary_labels).recall\n",
    "    f_precision = GroundTruthAggregator(binary_labels).precision\n",
    "    f_f1_score = GroundTruthAggregator(binary_labels).f1_score\n",
    "\n",
    "    f_cohens_kappa = GroundTruthAggregator(binary_labels).cohens_kappa\n",
    "    f_auc = GroundTruthAggregator(labels).auc\n",
    "\n",
    "    f_mae = GroundTruthAggregator(labels).mae\n",
    "    f_pearson = GroundTruthAggregator(labels).pearson_correlation\n",
    "    f_spearman = GroundTruthAggregator(labels).spearman_correlation\n",
    "\n",
    "    recall = f_recall(scores)\n",
    "    precision = f_precision(scores)\n",
    "    f1_score = f_f1_score(scores)\n",
    "    mae = f_mae(scores)\n",
    "    pearson = f_pearson(scores)\n",
    "    spearman = f_spearman(scores)\n",
    "    cohens_kappa = f_cohens_kappa(scores)\n",
    "    auc = f_auc(scores)\n",
    "\n",
    "    for latency in latencies:\n",
    "        if latency > 20:\n",
    "            # print(f\"Warning: latency is greater than 10 seconds: {latency}\")\n",
    "            latencies.remove(latency)\n",
    "    avg_latency = sum(latencies) / len(latencies)\n",
    "\n",
    "    print(\n",
    "        f\"{provider_name}: mae: {mae:.4f}, pearson: {pearson:.4f}, spearman: {spearman:.4f}, Cohen's Kappa: {cohens_kappa:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"{provider_name}: recall: {recall:.4f}, precision: {precision:.4f}, f1: {f1_score:.4f}, Cohen's Kappa: {cohens_kappa:.4f}, avg_latency: {avg_latency:.4f}\"\n",
    "    )\n",
    "    print(\"\\n\")\n",
    "\n",
    "    plot_confusion_matrix(\n",
    "        binary_labels, scores, title=f\"Confusion Matrix - {provider_name}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
