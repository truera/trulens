{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets preprocessing:\n",
    "\n",
    "Datasets that need some preprocessing before they can be used in `TruBenchmarkExperiment` class:\n",
    "1. Snowflake IT (internal): both rephrased and regular?, this should be used for all 3 in the triad\n",
    "2. SummEval (CNN and DailyMail summarizations with annotation) for groundedness\n",
    "3. QAGS (CNN and DailyMail with Turkers' annotation) for groundedness\n",
    "4. QAGS (XSUM with Turkers' annotation) for groundedness\n",
    "5. MSMARCO V2 for context relevance\n",
    "6. HotPot QA for answer relevance \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import csv\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# SummEval\n",
    "def generate_summeval_groundedness_golden_set(file_path):\n",
    "    def calculate_expected_score(normalized_metrics_lst, weights_lst):\n",
    "        assert len(normalized_metrics_lst) == len(weights_lst)\n",
    "        return round(\n",
    "            sum(\n",
    "                normalized_metrics_lst[i] * weights_lst[i]\n",
    "                for i in range(len(normalized_metrics_lst))\n",
    "            )\n",
    "            / sum(weights_lst),\n",
    "            2,\n",
    "        )\n",
    "\n",
    "    with open(file_path) as f:\n",
    "        for line in f:\n",
    "            # Each line is a separate JSON object\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "\n",
    "                # Ensure the expected keys exist in the JSON\n",
    "                try:\n",
    "                    row = data\n",
    "                    assert (\n",
    "                        len(row[\"machine_summaries\"]) == len(row[\"consistency\"])\n",
    "                    ), \"Mismatch in lengths of machine_summaries and consistency\"\n",
    "\n",
    "                    # Iterate over the summaries and create the desired dictionary structure\n",
    "                    for i in range(len(row[\"machine_summaries\"])):\n",
    "                        yield {\n",
    "                            \"query\": row.get(\n",
    "                                \"text\", \"\"\n",
    "                            ),  # Default to empty string if key not found\n",
    "                            \"expected_response\": row[\"machine_summaries\"][i],\n",
    "                            \"expected_score\": calculate_expected_score(\n",
    "                                [\n",
    "                                    (row[\"consistency\"][i] - 1)\n",
    "                                    / 4,  # Normalize from [1, 5] to [0, 1]\n",
    "                                ],\n",
    "                                [1.0],\n",
    "                            ),\n",
    "                            \"human_score\": row[\"consistency\"][i],\n",
    "                        }\n",
    "\n",
    "                except KeyError as e:\n",
    "                    print(\n",
    "                        f\"Key error: {e}. Please check if the keys exist in the JSON file.\"\n",
    "                    )\n",
    "                except AssertionError as e:\n",
    "                    print(\n",
    "                        f\"Assertion error: {e}. The lengths of 'machine_summaries' and 'consistency' do not match.\"\n",
    "                    )\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON decode error: {e}. Check the line format.\")\n",
    "\n",
    "\n",
    "# Snowflake IT dataset\n",
    "\n",
    "\n",
    "def generatate_snowflake_it_golden_set_groundedness(file_path):\n",
    "    with open(file_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)  # Read the CSV file as a dictionary\n",
    "        for row in reader:\n",
    "            # Convert the 'golden' from a string to a list\n",
    "            try:\n",
    "                expected_chunks = ast.literal_eval(row[\"golden\"])\n",
    "                if not isinstance(expected_chunks, list):\n",
    "                    raise ValueError(\"Golden column should be a list\")\n",
    "\n",
    "                for expected_chunk in expected_chunks:\n",
    "                    # Yield the required fields\n",
    "                    yield {\n",
    "                        \"query\": expected_chunk,\n",
    "                        \"expected_response\": row[\"expected_response\"],\n",
    "                        \"expected_score\": 1,  # Static score as per the requirement\n",
    "                    }\n",
    "            except (ValueError, SyntaxError) as e:\n",
    "                print(f\"Error parsing golden column: {e}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "def generate_snowflake_it_golden_set_answer_relevance(file_path):\n",
    "    with open(file_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)  # Read the CSV file as a dictionary\n",
    "        for row in reader:\n",
    "            # Extract data and yield the required fields\n",
    "            yield {\n",
    "                \"query\": row[\"query\"],\n",
    "                \"expected_response\": row[\"expected_response\"],\n",
    "                \"expected_score\": 1,  # always positive example for answer relevance\n",
    "            }\n",
    "\n",
    "\n",
    "def generate_snowflake_it_golden_set_context_relevance(file_path):\n",
    "    with open(file_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)  # Read the CSV file as a dictionary\n",
    "        for row in reader:\n",
    "            # Convert the 'golden' from a string to a list\n",
    "            try:\n",
    "                expected_chunks = ast.literal_eval(row[\"golden\"])\n",
    "                # if len(expected_chunks) > 1:\n",
    "                #     print(\n",
    "                #         f'query w/ more than one golden contexts: {row[\"query\"]}'\n",
    "                #     )\n",
    "                #     print(expected_chunks)\n",
    "                if not isinstance(expected_chunks, list):\n",
    "                    raise ValueError(\"Golden column should be a list\")\n",
    "\n",
    "                for expected_chunk in expected_chunks:\n",
    "                    # Yield the required fields\n",
    "                    yield {\n",
    "                        \"query\": row[\"query\"],\n",
    "                        \"expected_response\": expected_chunk,\n",
    "                        \"expected_score\": 1,  # Static score as per the requirement\n",
    "                    }\n",
    "            except (ValueError, SyntaxError) as e:\n",
    "                print(f\"Error parsing golden column: {e}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "def generate_qags_golden_set_groundedness(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            # Parse each line as a JSON object\n",
    "            data = json.loads(line)\n",
    "\n",
    "            # Extract the article as the query\n",
    "            query = data[\"article\"]\n",
    "\n",
    "            # Iterate over the summary_sentences to flatten the structure\n",
    "            for summary in data[\"summary_sentences\"]:\n",
    "                expected_response = summary[\"sentence\"]\n",
    "\n",
    "                # Calculate expected_score based on worker responses\n",
    "                responses = [\n",
    "                    response[\"response\"] for response in summary[\"responses\"]\n",
    "                ]\n",
    "                # Convert 'yes' to 1 and 'no' to 0, then calculate the average\n",
    "                expected_score = sum(\n",
    "                    1 if r.lower() == \"yes\" else 0 for r in responses\n",
    "                ) / len(responses)\n",
    "\n",
    "                # Yield the processed record\n",
    "                yield {\n",
    "                    \"query\": query,\n",
    "                    \"expected_response\": expected_response,\n",
    "                    \"expected_score\": expected_score,\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summeval_list = list(\n",
    "    generate_summeval_groundedness_golden_set(\n",
    "        \"/Users/dhuang/Documents/git/trulens/src/benchmark/trulens/benchmark/benchmark_frameworks/experiments/data/summeval_test.json\"\n",
    "    )\n",
    ")\n",
    "\n",
    "summeval_true_labels = [entry[\"expected_score\"] for entry in summeval_list]\n",
    "\n",
    "summeval = pd.DataFrame(\n",
    "    list(\n",
    "        generate_summeval_groundedness_golden_set(\n",
    "            \"/Users/dhuang/Documents/git/trulens/src/benchmark/trulens/benchmark/benchmark_frameworks/experiments/data/summeval_test.json\"\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowflake_it_file_path = \"/Users/dhuang/Documents/git/trulens/src/benchmark/trulens/benchmark/benchmark_frameworks/experiments/data/snowflake_it_v3.csv\"\n",
    "\n",
    "snowflake_it_for_answer_relevance = pd.DataFrame(\n",
    "    list(\n",
    "        generate_snowflake_it_golden_set_answer_relevance(\n",
    "            snowflake_it_file_path\n",
    "        )\n",
    "    )\n",
    ")\n",
    "snowflake_it_answer_relevance_true_labels = [\n",
    "    1 for _ in range(len(snowflake_it_for_answer_relevance))\n",
    "]\n",
    "\n",
    "snowflake_it_for_context_relevance = pd.DataFrame(\n",
    "    list(\n",
    "        generate_snowflake_it_golden_set_context_relevance(\n",
    "            snowflake_it_file_path\n",
    "        )\n",
    "    )\n",
    ")\n",
    "snowflake_it_for_context_relevance_true_labels = [\n",
    "    1 for _ in range(len(snowflake_it_for_context_relevance))\n",
    "]\n",
    "\n",
    "snowflake_it_for_groundness = pd.DataFrame(\n",
    "    list(\n",
    "        generatate_snowflake_it_golden_set_groundedness(snowflake_it_file_path)\n",
    "    )\n",
    ")\n",
    "snowflake_it_for_groundness_true_labels = [\n",
    "    1 for _ in range(len(snowflake_it_for_groundness))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qags_cnn_dm = pd.DataFrame(\n",
    "    list(\n",
    "        generate_qags_golden_set_groundedness(\n",
    "            \"/Users/dhuang/Documents/git/trulens/src/benchmark/trulens/benchmark/benchmark_frameworks/experiments/data/qags_mturk_cnndm.jsonl\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "qags_cnn_dm_true_labels = [\n",
    "    row[\"expected_score\"] for _, row in qags_cnn_dm.iterrows()\n",
    "]\n",
    "\n",
    "\n",
    "qags_xsum = pd.DataFrame(\n",
    "    list(\n",
    "        generate_qags_golden_set_groundedness(\n",
    "            \"/Users/dhuang/Documents/git/trulens/src/benchmark/trulens/benchmark/benchmark_frameworks/experiments/data/qags_mturk_xsum.jsonl\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "qqags_xsum_true_labels = [\n",
    "    row[\"expected_score\"] for _, row in qags_xsum.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up feedback LLM providers \n",
    "\n",
    "We will experiment with 2 current OpenAI models and a mix of commercial and open source models avaiable in Cortex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import snowflake.connector\n",
    "from trulens.providers.cortex import Cortex\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "# OPENAI_MODELS = [\"gpt-4o\", \"gpt-4o-mini\"]\n",
    "# CORTEX_MODELS = [\"snowflake-arctic\", \"llama3.1-8b\", \"llama3.1-70b\", \"mistral-large\"]\n",
    "\n",
    "gpt_4o = OpenAI(\"gpt-4o\")\n",
    "gpt_4o_mini = OpenAI(\"gpt-4o-mini\")\n",
    "\n",
    "snowflake_connection_parameters = {\n",
    "    \"account\": os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "    \"user\": os.environ[\"SNOWFLAKE_USER\"],\n",
    "    \"password\": os.environ[\"SNOWFLAKE_USER_PASSWORD\"],\n",
    "}\n",
    "snowflake_connection = snowflake.connector.connect(\n",
    "    **snowflake_connection_parameters\n",
    ")\n",
    "\n",
    "snowflake_arctic = Cortex(snowflake_connection, model_engine=\"snowflake-arctic\")\n",
    "llama3_1_8b = Cortex(snowflake_connection, model_engine=\"llama3.1-8b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o.groundedness_measure_with_cot_reasons(\n",
    "    summeval[0][\"query\"],\n",
    "    summeval[0][\"expected_response\"],\n",
    "    criteria=\"Grounded should get a socre of 10, and non-grounded should get a score of 0. The score should be between 0 to 10\",\n",
    "    max_score_val=10,\n",
    "    use_sent_tokenize=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_mini.context_relevance_with_cot_reasons(\n",
    "    snowflake_it_for_context_relevance[0][\"query\"],\n",
    "    snowflake_it_for_context_relevance[0][\"expected_response\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from trulens.core import TruSession\n",
    "\n",
    "session = TruSession()\n",
    "session.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_relevance_likert_4(provider, input, output) -> float:\n",
    "    return provider.context_relevance_with_cot_reasons(\n",
    "        question=input,\n",
    "        context=output,\n",
    "    )\n",
    "\n",
    "\n",
    "def context_relevance_binary(provider, input, output) -> float:\n",
    "    return provider.context_relevance(\n",
    "        question=input,\n",
    "        context=output,\n",
    "        criteria=\"A relevant context to the question should get a score of 1, and an irrelevant context should get a score of 0. The score should be either 0 or 1 (binary).\",\n",
    "        min_score_val=0,\n",
    "        max_score_val=1,\n",
    "    )\n",
    "\n",
    "\n",
    "def answer_relevance_binary(provider, input, output) -> float:\n",
    "    return provider.relevance(\n",
    "        prompt=input,\n",
    "        response=output,\n",
    "        criteria=\"A relevant response to the prompt should get a score of 1, and an irrelevant response should get a score of 0. The score should be either 0 or 1 (binary).\",\n",
    "        min_score_val=0,\n",
    "        max_score_val=1,\n",
    "    )\n",
    "\n",
    "\n",
    "def groundedness_likert_4(provider, input, output) -> float:\n",
    "    return provider.groundedness_measure_with_cot_reasons(\n",
    "        source=input, statement=output\n",
    "    )\n",
    "\n",
    "\n",
    "def groudedness_binary(provider, input, output) -> float:\n",
    "    return provider.groundedness_measure_with_cot_reasons(\n",
    "        source=input,\n",
    "        statement=output,\n",
    "        criteria=\"A grounded response based on the source should get a socre of 1, and non-grounded one should get a score of 0. The score should be either 0 or 1 (binary).\",\n",
    "        min_score_val=0,\n",
    "        max_score_val=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snowflake_it_benchmark = TruBenchmarkExperiment("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
