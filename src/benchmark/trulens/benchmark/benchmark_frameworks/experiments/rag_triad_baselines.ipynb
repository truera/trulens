{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets preprocessing:\n",
    "\n",
    "Datasets that need some preprocessing before they can be used in `TruBenchmarkExperiment` class:\n",
    "1. Snowflake IT (internal): both rephrased and regular?, this should be used for all 3 in the triad\n",
    "2. SummEval (CNN and DailyMail summarizations with annotation) for groundedness\n",
    "3. QAGS (CNN and DailyMail with Turkers' annotation) for groundedness\n",
    "4. QAGS (XSUM with Turkers' annotation) for groundedness\n",
    "5. MSMARCO V2 for context relevance\n",
    "6. HotPot QA for answer relevance \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Pin random seed\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "# SummEval\n",
    "def generate_summeval_groundedness_golden_set(file_path):\n",
    "    def calculate_expected_score(normalized_metrics_lst, weights_lst):\n",
    "        assert len(normalized_metrics_lst) == len(weights_lst)\n",
    "        return round(\n",
    "            sum(\n",
    "                normalized_metrics_lst[i] * weights_lst[i]\n",
    "                for i in range(len(normalized_metrics_lst))\n",
    "            )\n",
    "            / sum(weights_lst),\n",
    "            2,\n",
    "        )\n",
    "\n",
    "    with open(file_path) as f:\n",
    "        for line in f:\n",
    "            # Each line is a separate JSON object\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "\n",
    "                # Ensure the expected keys exist in the JSON\n",
    "                try:\n",
    "                    row = data\n",
    "                    assert (\n",
    "                        len(row[\"machine_summaries\"]) == len(row[\"consistency\"])\n",
    "                    ), \"Mismatch in lengths of machine_summaries and consistency\"\n",
    "\n",
    "                    # Iterate over the summaries and create the desired dictionary structure\n",
    "                    for i in range(len(row[\"machine_summaries\"])):\n",
    "                        yield {\n",
    "                            \"query\": row.get(\n",
    "                                \"text\", \"\"\n",
    "                            ),  # Default to empty string if key not found\n",
    "                            \"expected_response\": row[\"machine_summaries\"][i],\n",
    "                            \"expected_score\": calculate_expected_score(\n",
    "                                [\n",
    "                                    (row[\"consistency\"][i] - 1)\n",
    "                                    / 4,  # Normalize from [1, 5] to [0, 1]\n",
    "                                ],\n",
    "                                [1.0],\n",
    "                            ),\n",
    "                            \"human_score\": row[\"consistency\"][i],\n",
    "                        }\n",
    "\n",
    "                except KeyError as e:\n",
    "                    print(\n",
    "                        f\"Key error: {e}. Please check if the keys exist in the JSON file.\"\n",
    "                    )\n",
    "                except AssertionError as e:\n",
    "                    print(\n",
    "                        f\"Assertion error: {e}. The lengths of 'machine_summaries' and 'consistency' do not match.\"\n",
    "                    )\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON decode error: {e}. Check the line format.\")\n",
    "\n",
    "\n",
    "# Snowflake IT dataset\n",
    "\n",
    "\n",
    "def generatate_snowflake_it_golden_set_groundedness(file_path):\n",
    "    res = []\n",
    "    with open(file_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        all_rows = list(reader)\n",
    "        for row in all_rows:\n",
    "            # Convert the 'golden' from a string to a list\n",
    "            try:\n",
    "                expected_chunks = ast.literal_eval(row[\"golden\"])\n",
    "                if not isinstance(expected_chunks, list):\n",
    "                    raise ValueError(\"Golden column should be a list\")\n",
    "\n",
    "                for expected_chunk in expected_chunks:\n",
    "                    # Yield the required fields\n",
    "                    res.append({\n",
    "                        \"query\": expected_chunk,  # source\n",
    "                        \"expected_response\": row[\n",
    "                            \"expected_response\"\n",
    "                        ],  # statement\n",
    "                        \"expected_score\": 1,  # retrieved chunks in the \"golden\" colum are always considered grounded\n",
    "                    })\n",
    "\n",
    "                # Generate a negative example for each query\n",
    "                # Collect all possible chunks from other queries to use as negative contexts\n",
    "                other_chunks = [\n",
    "                    chunk\n",
    "                    for other_row in all_rows\n",
    "                    if other_row != row\n",
    "                    for chunk in ast.literal_eval(other_row[\"golden\"])\n",
    "                ]\n",
    "\n",
    "                # Randomly select a negative chunk (context from another query)\n",
    "                if other_chunks:\n",
    "                    negative_chunk = random.choice(other_chunks)\n",
    "                    res.append({\n",
    "                        \"query\": negative_chunk,\n",
    "                        \"expected_response\": row[\n",
    "                            \"expected_response\"\n",
    "                        ],  # statement (not grounded by the chunk)\n",
    "                        \"expected_score\": 0,  # Negative example, score = 0\n",
    "                    })\n",
    "\n",
    "            except (ValueError, SyntaxError) as e:\n",
    "                print(f\"Error parsing golden column: {e}\")\n",
    "                continue\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def generate_snowflake_it_golden_set_answer_relevance(file_path):\n",
    "    res = []\n",
    "    with open(file_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        all_rows = list(\n",
    "            reader\n",
    "        )  # Store all rows in memory to use for negative example generation\n",
    "\n",
    "        for row in all_rows:\n",
    "            # generate a positive example for each query\n",
    "            if (\n",
    "                \"I don’t know the answer to that question.\"\n",
    "                in row[\"expected_response\"]\n",
    "            ):\n",
    "                ground_truth_score = 0  # label answer relevance as 0 for ABSTENTION \"I don’t know the answer to that question.\"\n",
    "            else:\n",
    "                ground_truth_score = (\n",
    "                    1  # label answer relevance as 1 for all other cases\n",
    "                )\n",
    "            res.append({\n",
    "                \"query\": row[\"query\"],\n",
    "                \"expected_response\": row[\"expected_response\"],\n",
    "                \"expected_score\": ground_truth_score,\n",
    "            })\n",
    "\n",
    "            # generate an easy negative example for each positive example by randomly selecting a response from another query\n",
    "            negative_response = random.choice([\n",
    "                r[\"expected_response\"] for r in all_rows if r != row\n",
    "            ])\n",
    "            res.append({\n",
    "                \"query\": row[\"query\"],\n",
    "                \"expected_response\": negative_response,  # Orthogonal response\n",
    "                \"expected_score\": 0,  # Label answer relevance as 0 for negative examples\n",
    "            })\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def generate_snowflake_it_golden_set_context_relevance(file_path):\n",
    "    res = []\n",
    "    with open(file_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        all_rows = list(reader)\n",
    "\n",
    "        # Step 1: Process each row to extract positive examples\n",
    "        for row in all_rows:\n",
    "            try:\n",
    "                expected_chunks = ast.literal_eval(row[\"golden\"])\n",
    "\n",
    "                if not isinstance(expected_chunks, list):\n",
    "                    raise ValueError(\"Golden column should be a list\")\n",
    "\n",
    "                # Generate positive examples\n",
    "                for expected_chunk in expected_chunks:\n",
    "                    res.append({\n",
    "                        \"query\": row[\"query\"],\n",
    "                        \"expected_response\": expected_chunk,\n",
    "                        \"expected_score\": 1,  # Positive example, score = 1\n",
    "                    })\n",
    "\n",
    "                # Step 2: Generate a negative example for each query\n",
    "                # Collect all possible chunks from other queries to use as negative contexts\n",
    "                other_chunks = [\n",
    "                    chunk\n",
    "                    for other_row in all_rows\n",
    "                    if other_row != row\n",
    "                    for chunk in ast.literal_eval(other_row[\"golden\"])\n",
    "                ]\n",
    "\n",
    "                # Randomly select a negative chunk (context from another query)\n",
    "                if other_chunks:\n",
    "                    negative_chunk = random.choice(other_chunks)\n",
    "                    res.append({\n",
    "                        \"query\": row[\"query\"],\n",
    "                        \"expected_response\": negative_chunk,  # Orthogonal/negative context\n",
    "                        \"expected_score\": 0,  # Negative example, score = 0\n",
    "                    })\n",
    "\n",
    "            except (ValueError, SyntaxError) as e:\n",
    "                print(\n",
    "                    f\"Error parsing golden column for query '{row['query']}': {e}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "def generate_qags_golden_set_groundedness(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            # Parse each line as a JSON object\n",
    "            data = json.loads(line)\n",
    "\n",
    "            # Extract the article as the query\n",
    "            query = data[\"article\"]\n",
    "\n",
    "            # Iterate over the summary_sentences to flatten the structure\n",
    "            for summary in data[\"summary_sentences\"]:\n",
    "                expected_response = summary[\"sentence\"]\n",
    "\n",
    "                # Calculate expected_score based on worker responses\n",
    "                responses = [\n",
    "                    response[\"response\"] for response in summary[\"responses\"]\n",
    "                ]\n",
    "                # Convert 'yes' to 1 and 'no' to 0, then calculate the average\n",
    "                expected_score = sum(\n",
    "                    1 if r.lower() == \"yes\" else 0 for r in responses\n",
    "                ) / len(responses)\n",
    "\n",
    "                # Yield the processed record\n",
    "                yield {\n",
    "                    \"query\": query,\n",
    "                    \"expected_response\": expected_response,\n",
    "                    \"expected_score\": expected_score,\n",
    "                }\n",
    "\n",
    "\n",
    "snowflake_it_file_path = \"/Users/dhuang/Documents/git/trulens/src/benchmark/trulens/benchmark/benchmark_frameworks/experiments/data/snowflake_it_v3.csv\"\n",
    "\n",
    "snowflake_it_for_answer_relevance = pd.DataFrame(\n",
    "    list(\n",
    "        generate_snowflake_it_golden_set_answer_relevance(\n",
    "            snowflake_it_file_path\n",
    "        )\n",
    "    )\n",
    ")\n",
    "snowflake_it_answer_relevance_true_labels = list(\n",
    "    snowflake_it_for_answer_relevance[\"expected_score\"]\n",
    ")\n",
    "\n",
    "snowflake_it_for_context_relevance = pd.DataFrame(\n",
    "    list(\n",
    "        generate_snowflake_it_golden_set_context_relevance(\n",
    "            snowflake_it_file_path\n",
    "        )\n",
    "    )\n",
    ")\n",
    "snowflake_it_for_context_relevance_true_labels = list(\n",
    "    snowflake_it_for_context_relevance[\"expected_score\"]\n",
    ")\n",
    "\n",
    "\n",
    "snowflake_it_for_groundness = pd.DataFrame(\n",
    "    list(\n",
    "        generatate_snowflake_it_golden_set_groundedness(snowflake_it_file_path)\n",
    "    )\n",
    ")\n",
    "snowflake_it_for_groundness_true_labels = list(\n",
    "    snowflake_it_for_groundness[\"expected_score\"]\n",
    ")\n",
    "\n",
    "summeval_list = list(\n",
    "    generate_summeval_groundedness_golden_set(\n",
    "        \"/Users/dhuang/Documents/git/trulens/src/benchmark/trulens/benchmark/benchmark_frameworks/experiments/data/summeval_test.json\"\n",
    "    )\n",
    ")\n",
    "\n",
    "summeval_true_labels = [entry[\"expected_score\"] for entry in summeval_list]\n",
    "\n",
    "summeval = pd.DataFrame(\n",
    "    list(\n",
    "        generate_summeval_groundedness_golden_set(\n",
    "            \"/Users/dhuang/Documents/git/trulens/src/benchmark/trulens/benchmark/benchmark_frameworks/experiments/data/summeval_test.json\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "qags_cnn_dm = pd.DataFrame(\n",
    "    list(\n",
    "        generate_qags_golden_set_groundedness(\n",
    "            \"/Users/dhuang/Documents/git/trulens/src/benchmark/trulens/benchmark/benchmark_frameworks/experiments/data/qags_mturk_cnndm.jsonl\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "qags_cnn_dm_true_labels = [\n",
    "    row[\"expected_score\"] for _, row in qags_cnn_dm.iterrows()\n",
    "]\n",
    "\n",
    "\n",
    "qags_xsum = pd.DataFrame(\n",
    "    list(\n",
    "        generate_qags_golden_set_groundedness(\n",
    "            \"/Users/dhuang/Documents/git/trulens/src/benchmark/trulens/benchmark/benchmark_frameworks/experiments/data/qags_mturk_xsum.jsonl\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "qqags_xsum_true_labels = [\n",
    "    row[\"expected_score\"] for _, row in qags_xsum.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "\n",
    "def generate_ms_marco_context_relevance_benchmark(\n",
    "    file_path=\"/Users/dhuang/Documents/git/trulens/src/benchmark/trulens/benchmark/benchmark_frameworks/experiments/data/ms_marco_v2_1_val.parquet\",\n",
    "):\n",
    "    df = pd.read_parquet(file_path, engine=\"pyarrow\")  # or engine='fastparquet'\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        assert len(row[\"passages\"][\"is_selected\"]) == len(\n",
    "            row[\"passages\"][\"passage_text\"]\n",
    "        )\n",
    "\n",
    "        if sum(row[\"passages\"][\"is_selected\"]) < 1:\n",
    "            # currently we only consider sample with one passage marked as relevant (there are samples where zero passage_text is selected)\n",
    "            continue\n",
    "        for i, passage_text in enumerate(row[\"passages\"][\"passage_text\"]):\n",
    "            yield {\n",
    "                \"query_id\": row[\"query_id\"],\n",
    "                \"query\": row[\"query\"],\n",
    "                \"expected_response\": passage_text,\n",
    "                \"expected_score\": row[\"passages\"][\"is_selected\"][\n",
    "                    i\n",
    "                ],  # Binary relevance\n",
    "            }\n",
    "\n",
    "\n",
    "ms_marco = list(generate_ms_marco_context_relevance_benchmark())\n",
    "\n",
    "\n",
    "score_1_entries = [entry for entry in ms_marco if entry[\"expected_score\"] == 1]\n",
    "score_0_entries = [entry for entry in ms_marco if entry[\"expected_score\"] == 0]\n",
    "\n",
    "# Calculate the number of samples needed from each group\n",
    "num_samples_per_group = min(\n",
    "    len(score_1_entries), len(score_0_entries), 150\n",
    ")  # Sample 150 from each\n",
    "\n",
    "\n",
    "sampled_score_1 = random.sample(score_1_entries, num_samples_per_group)\n",
    "sampled_score_0 = random.sample(score_0_entries, num_samples_per_group)\n",
    "\n",
    "# Combine and shuffle the samples to get a balanced dataset\n",
    "balanced_sample = sampled_score_1 + sampled_score_0\n",
    "random.shuffle(balanced_sample)\n",
    "\n",
    "# Ensure the combined length is 300\n",
    "assert len(balanced_sample) == 300\n",
    "\n",
    "# Now you can use `balanced_sample` as your final dataset\n",
    "print(\n",
    "    f\"Number of entries with expected_score = 1: {len([e for e in balanced_sample if e['expected_score'] == 1])}\"\n",
    ")\n",
    "print(\n",
    "    f\"Number of entries with expected_score = 0: {len([e for e in balanced_sample if e['expected_score'] == 0])}\"\n",
    ")\n",
    "\n",
    "ms_marco_balanced_sample_300 = pd.DataFrame(balanced_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load preprocessed datasets from BEIR - start w/ Hotpot QA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.benchmark.benchmark_frameworks.dataset.beir_loader import (\n",
    "    TruBEIRDataLoader,\n",
    ")\n",
    "\n",
    "beir_data_loader = TruBEIRDataLoader(data_folder=\"./\", dataset_name=\"hotpotqa\")\n",
    "hotpotqa = beir_data_loader.load_dataset_to_df(download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "\n",
    "hotpotqa_raw_subset = hotpotqa.sample(n=200, random_state=42)\n",
    "\n",
    "all_responses = [\n",
    "    (row[\"query\"], row[\"expected_response\"])\n",
    "    for idx, row in hotpotqa_raw_subset.iterrows()\n",
    "]\n",
    "\n",
    "all_contexts = [\n",
    "    (row[\"query\"], context[\"text\"])\n",
    "    for idx, row in hotpotqa_raw_subset.iterrows()\n",
    "    for context in row[\"expected_chunks\"]\n",
    "]\n",
    "\n",
    "hotpotqa_subset_for_answer_relevance, hotpotqa_subset_for_context_relevance = (\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "\n",
    "for idx, row in hotpotqa_raw_subset.iterrows():\n",
    "    # Positive examples for answer relevance\n",
    "    hotpotqa_subset_for_answer_relevance.append({\n",
    "        \"query\": row[\"query\"],\n",
    "        \"expected_response\": row[\"expected_response\"],  # Positive response\n",
    "        \"expected_score\": 1,  # Positive example, score = 1\n",
    "    })\n",
    "\n",
    "    # Negative examples for answer relevance (random unrelated response)\n",
    "    negative_response = random.choice([\n",
    "        r\n",
    "        for q, r in all_responses\n",
    "        if q != row[\"query\"]  # Pick response from another query\n",
    "    ])\n",
    "\n",
    "    hotpotqa_subset_for_answer_relevance.append({\n",
    "        \"query\": row[\"query\"],\n",
    "        \"expected_response\": negative_response,  # Negative response\n",
    "        \"expected_score\": 0,  # Negative example, score = 0\n",
    "    })\n",
    "\n",
    "\n",
    "for idx, row in hotpotqa_raw_subset.iterrows():\n",
    "    positive_examples = []\n",
    "    negative_examples = []\n",
    "\n",
    "    # Generate positive examples for context relevance\n",
    "    for context in row[\"expected_chunks\"]:\n",
    "        positive_examples.append({\n",
    "            \"query\": row[\"query\"],\n",
    "            \"expected_response\": context[\"text\"],  # Positive context\n",
    "            \"expected_score\": context[\"expected_score\"],  # Should be 1\n",
    "        })\n",
    "\n",
    "    # Generate negative examples for context relevance\n",
    "    for _ in positive_examples:\n",
    "        negative_context = random.choice([\n",
    "            c\n",
    "            for q, c in all_contexts\n",
    "            if q != row[\"query\"]  # Pick context from another query\n",
    "        ])\n",
    "        negative_examples.append({\n",
    "            \"query\": row[\"query\"],\n",
    "            \"expected_response\": negative_context,  # Negative context\n",
    "            \"expected_score\": 0,  # Negative example, score = 0\n",
    "        })\n",
    "\n",
    "    # Add positive and negative examples to the result set\n",
    "    hotpotqa_subset_for_context_relevance.extend(positive_examples)\n",
    "    hotpotqa_subset_for_context_relevance.extend(negative_examples)\n",
    "\n",
    "\n",
    "hotpotqa_subset_for_context_relevance_true_labels = [\n",
    "    entry[\"expected_score\"] for entry in hotpotqa_subset_for_context_relevance\n",
    "]\n",
    "hotpotqa_subset_for_answer_relevance_true_labels = [\n",
    "    entry[\"expected_score\"] for entry in hotpotqa_subset_for_answer_relevance\n",
    "]\n",
    "\n",
    "hotpotqa_subset_for_context_relevance = pd.DataFrame(\n",
    "    hotpotqa_subset_for_context_relevance\n",
    ")\n",
    "\n",
    "hotpotqa_subset_for_answer_relevance = pd.DataFrame(\n",
    "    hotpotqa_subset_for_answer_relevance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotpotqa_subset_for_context_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summeval_subset = summeval.sample(n=200, random_state=42)\n",
    "summeval_subset_true_labels = [\n",
    "    row[\"expected_score\"] for _, row in summeval_subset.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up feedback LLM providers \n",
    "\n",
    "We will experiment with 2 current OpenAI models and a mix of commercial and open source models avaiable in Cortex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import snowflake.connector\n",
    "from trulens.providers.cortex import Cortex\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "snowflake_connection_parameters = {\n",
    "    \"account\": os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "    \"user\": os.environ[\"SNOWFLAKE_USER\"],\n",
    "    \"password\": os.environ[\"SNOWFLAKE_USER_PASSWORD\"],\n",
    "}\n",
    "snowflake_connection = snowflake.connector.connect(\n",
    "    **snowflake_connection_parameters\n",
    ")\n",
    "\n",
    "\n",
    "gpt_4o = OpenAI(model_engine=\"gpt-4o\")\n",
    "gpt_4o_mini = OpenAI(model_engine=\"gpt-4o-mini\")\n",
    "\n",
    "snowflake_arctic = Cortex(snowflake_connection, model_engine=\"snowflake-arctic\")\n",
    "mistral_large = Cortex(snowflake_connection, model_engine=\"mistral-large\")\n",
    "llama3_1_8b = Cortex(snowflake_connection, model_engine=\"llama3.1-8b\")\n",
    "\n",
    "CORTEX_PROVIDERS = [snowflake_arctic, llama3_1_8b, mistral_large]\n",
    "OPENAI_PROVIDERS = [gpt_4o, gpt_4o_mini]\n",
    "ALL_PROVIDERS = CORTEX_PROVIDERS + OPENAI_PROVIDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import TruSession\n",
    "\n",
    "session = TruSession()\n",
    "session.reset_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snowflake IT dataset experiment runs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "\n",
    "from trulens.apps.basic import TruBasicApp\n",
    "\n",
    "\n",
    "def write_results(\n",
    "    feedback_scores: List[float],\n",
    "    labels: List[float | int],\n",
    "    latencies: List[float],\n",
    "    file_name: str,\n",
    "):\n",
    "    assert len(feedback_scores) == len(labels)\n",
    "\n",
    "    with open(file_name, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(feedback_scores)\n",
    "        writer.writerow(labels)\n",
    "        writer.writerow(latencies)\n",
    "\n",
    "\n",
    "def read_results(\n",
    "    file_name: str,\n",
    ") -> Tuple[List[float | int], List[float | int], List[float]]:\n",
    "    with open(file_name, \"r\") as file:\n",
    "        reader = csv.reader(file)\n",
    "        for index, row in enumerate(reader):\n",
    "            if index == 0:\n",
    "                # First row contains scores\n",
    "                scores = list(map(float, row))  # Convert strings to floats\n",
    "            elif index == 1:\n",
    "                # Second row contains labels\n",
    "                labels = list(map(float, row))  # Convert strings to floats\n",
    "            elif index == 2:\n",
    "                # Third row contains latencies\n",
    "                latencies = list(map(float, row))\n",
    "    return scores, labels, latencies\n",
    "\n",
    "\n",
    "def run_feedback_experiment(\n",
    "    feedback_func_wrapper, app_name, app_version, dataset_df, true_labels\n",
    "):\n",
    "    if len(dataset_df) != len(true_labels):\n",
    "        raise ValueError(\"dataset df must have the same length as labels\")\n",
    "    tru_wrapped_basic_app = TruBasicApp(\n",
    "        feedback_func_wrapper, app_name=app_name, app_version=app_version\n",
    "    )\n",
    "\n",
    "    generated_scores, labels, latencies = [], [], []\n",
    "    for i in range(len(dataset_df)):\n",
    "        arg_1 = dataset_df.iloc[i][\"query\"]\n",
    "        arg_2 = dataset_df.iloc[i][\"expected_response\"]\n",
    "        try:\n",
    "            with tru_wrapped_basic_app as _:\n",
    "                start_time = time.time()\n",
    "                score = tru_wrapped_basic_app.app(arg_1, arg_2)\n",
    "\n",
    "                end_time = time.time()\n",
    "                true_score = true_labels[i]\n",
    "\n",
    "                if not math.isnan(score):\n",
    "                    generated_scores.append(score)\n",
    "                    labels.append(true_score)\n",
    "                    latencies.append(end_time - start_time)\n",
    "\n",
    "                    # print(f\"Generated score: {score} | true_score: {true_score} \\n\")\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error {e} in run_feedback_experiment row {i} with first arg {arg_1} and second arg {arg_2}\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "    write_results(\n",
    "        generated_scores,\n",
    "        labels,\n",
    "        latencies,\n",
    "        f\"results/{app_name}_{app_version}_results.csv\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = gpt_4o_mini.groundedness_measure_with_cot_reasons(\n",
    "    source=\"This is a test. Earth is round\",\n",
    "    statement=\"Earth is not not round\",\n",
    "    criteria=\"A grounded response to the query should get a score of 1, and an ungrounded response should get a score of 0. The score can only be either 0 or 1 (binary).\",\n",
    "    max_score_val=1,\n",
    ")\n",
    "print(score)\n",
    "score = gpt_4o_mini.groundedness_measure_with_cot_reasons(\n",
    "    \"This is a test. Earth is round\",\n",
    "    \"Earth is not not round\",\n",
    "    criteria=\"\"\" You should score the groundedness of the statement based on the following criteria:\n",
    "    - Statements that are directly supported by the source should be considered grounded and should get a high score.\n",
    "    - Statements that are not directly supported by the source should be considered not grounded and should get a low score.\n",
    "    - Statements of doubt, that admissions of uncertainty or not knowing the answer are considered abstention, and should be counted as the most overlap and therefore get a max score.\"\"\",\n",
    "    max_score_val=3,\n",
    ")\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.feedback.v2.feedback import Groundedness\n",
    "\n",
    "default_groundedness_criteria = Groundedness.criteria\n",
    "default_groundedness_system_prompt = Groundedness.system_prompt\n",
    "\n",
    "likert4_groundedness_criteria = \"\"\"You should score the groundedness of the statement based on the following criteria:\n",
    "    - Statements that are directly supported by the source should be considered grounded and should get a high score.\n",
    "    - Statements that are not directly supported by the source should be considered not grounded and should get a low score.\n",
    "    - Statements of doubt, that admissions of uncertainty or not knowing the answer are considered abstention, and should be counted as the most overlap and therefore get a max score.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_groundedness_system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import traceback\n",
    "\n",
    "from trulens.feedback.v2.feedback import Groundedness\n",
    "\n",
    "\n",
    "def runn_all_experiments_for_provider(provider):\n",
    "    \"\"\"\n",
    "    Runs all experiments for a given provider.\n",
    "    \"\"\"\n",
    "\n",
    "    def context_relevance_binary(input, output) -> float:\n",
    "        return provider.context_relevance_with_cot_reasons(\n",
    "            question=input,\n",
    "            context=output,\n",
    "            criteria=\"A relevant context to the question should get a score of 1, and an irrelevant context should get a score of 0. The score can only be either 0 or 1 (binary).\",\n",
    "            min_score_val=0,\n",
    "            max_score_val=1,\n",
    "        )[0]\n",
    "\n",
    "    # Run context relevance binary experiment\n",
    "    # context_relevance_binary_run = create_benchmark_experiment_app(\n",
    "    #     app_name=f\"Snowflake IT  - {provider.model_engine}\",\n",
    "    #     app_version='context_relevance_binary',\n",
    "    #     benchmark_experiment=TruBenchmarkExperiment(\n",
    "    #         feedback_fn=context_relevance_binary,\n",
    "    #         agg_funcs=snowflake_it_metrics,\n",
    "    #         benchmark_params=benchmark_params\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    # with context_relevance_binary_run as recording:\n",
    "    #     feedback_res = context_relevance_binary_run.app(snowflake_it_for_context_relevance)\n",
    "    #     print(f'feedback results: {feedback_res}')\n",
    "    #     write_results(feedback_scores=feedback_res, file_name=f\"results/{provider.model_engine}-context_relevance_binary_feedback_scores.csv\")\n",
    "\n",
    "    # # Similar pattern for answer relevance binary experiment\n",
    "    def answer_relevance_binary(input, output) -> float:\n",
    "        return provider.relevance(\n",
    "            prompt=input,\n",
    "            response=output,\n",
    "            criteria=\"A relevant response to the prompt should get a score of 1, and an irrelevant response should get a score of 0. The score can only be either 0 or 1 (binary).\",\n",
    "            min_score_val=0,\n",
    "            max_score_val=1,\n",
    "        )\n",
    "\n",
    "    # answer_relevance_binary_run = create_benchmark_experiment_app(\n",
    "    #     app_name=f\"Snowflake IT - {provider.model_engine}\",\n",
    "    #     app_version='answer_relevance_binary',\n",
    "    #     benchmark_experiment=TruBenchmarkExperiment(\n",
    "    #         feedback_fn=answer_relevance_binary,\n",
    "    #         agg_funcs=snowflake_it_metrics,\n",
    "    #         benchmark_params=benchmark_params\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    # with answer_relevance_binary_run as recording:\n",
    "    #     feedback_res = answer_relevance_binary_run.app(snowflake_it_for_answer_relevance)\n",
    "    #     write_results(feedback_scores=feedback_res, file_name=f\"results/{provider.model_engine}-answer_relevance_binary_feedback_scores.csv\")\n",
    "\n",
    "    def groundedness_binary(input, output) -> float:\n",
    "        return provider.groundedness_measure_with_cot_reasons(\n",
    "            source=input,\n",
    "            statement=output,\n",
    "            criteria=\"A grounded response to the query should get a score of 1, and an ungrounded response should get a score of 0. The score can only be either 0 or 1 (binary).\",\n",
    "            min_score_val=0,\n",
    "            max_score_val=1,\n",
    "            use_sent_tokenize=True,\n",
    "        )[0]\n",
    "\n",
    "    def groundedness_likert_4(input, output) -> float:\n",
    "        return provider.groundedness_measure_with_cot_reasons(\n",
    "            source=input,\n",
    "            statement=output,\n",
    "            use_sent_tokenize=True,\n",
    "            min_score_val=0,\n",
    "            max_score_val=3,\n",
    "            criteria=likert4_groundedness_criteria,\n",
    "        )[0]\n",
    "\n",
    "    # Define a function to wrap the run_feedback_experiment call\n",
    "    def run_experiment(\n",
    "        feedback_fn, app_name, app_version, dataset_df, true_labels\n",
    "    ):\n",
    "        run_feedback_experiment(\n",
    "            feedback_func_wrapper=feedback_fn,\n",
    "            app_name=app_name,\n",
    "            app_version=app_version,\n",
    "            dataset_df=dataset_df,\n",
    "            true_labels=true_labels,\n",
    "        )\n",
    "\n",
    "    context_relevance_experiments = [\n",
    "        {\n",
    "            \"feedback_fn\": context_relevance_binary,\n",
    "            \"app_name\": f\"Snowflake IT balanced - {provider.model_engine}\",\n",
    "            \"app_version\": \"context_relevance_binary\",\n",
    "            \"dataset_df\": snowflake_it_for_context_relevance,\n",
    "            \"true_labels\": snowflake_it_for_context_relevance_true_labels,\n",
    "        },\n",
    "        {\n",
    "            \"feedback_fn\": context_relevance_binary,\n",
    "            \"app_name\": f\"Hotpot QA (800 samples) - {provider.model_engine}\",\n",
    "            \"app_version\": \"context_relevance_binary\",\n",
    "            \"dataset_df\": hotpotqa_subset_for_context_relevance,\n",
    "            \"true_labels\": hotpotqa_subset_for_context_relevance_true_labels,\n",
    "        },\n",
    "        # {\n",
    "        #     # MS MARCO V2 for context relevance\n",
    "        #     'feedback_fn': context_relevance_binary,\n",
    "        #     'app_name': f\"MS MARCO V2 balanced (300 samples) - {provider.model_engine}\",\n",
    "        #     'app_version': 'context_relevance_binary',\n",
    "        #     'dataset_df': ms_marco_balanced_sample_300,\n",
    "        #     'true_labels': [row[\"expected_score\"] for _, row in ms_marco_balanced_sample_300.iterrows()]\n",
    "        # },\n",
    "    ]\n",
    "\n",
    "    groundedness_experiments = [\n",
    "        {\n",
    "            \"feedback_fn\": groundedness_binary,\n",
    "            \"app_name\": f\"Snowflake IT balanced - {provider.model_engine}\",\n",
    "            \"app_version\": \"groundedness_binary\",\n",
    "            \"dataset_df\": snowflake_it_for_groundness,\n",
    "            \"true_labels\": snowflake_it_for_groundness_true_labels,\n",
    "        },\n",
    "        # {\n",
    "        #     'feedback_fn': groundedness_likert_4,\n",
    "        #     'app_name': f\"SummEval (200 samples) - {provider.model_engine}\",\n",
    "        #     'app_version': 'groundedness_likert4',\n",
    "        #     'dataset_df': summeval_subset,\n",
    "        #     'true_labels': [row[\"expected_score\"] for _, row in summeval_subset.iterrows()]\n",
    "        # },\n",
    "        # {\n",
    "        #     'feedback_fn': groundedness_likert_4,\n",
    "        #     'app_name': f\"QAGS CNN_DM - {provider.model_engine}\",\n",
    "        #     'app_version': 'groundedness_likert4',\n",
    "        #     'dataset_df': qags_cnn_dm,\n",
    "        #     'true_labels': qags_cnn_dm_true_labels\n",
    "        # },\n",
    "        # {\n",
    "        #     'feedback_fn': groundedness_likert_4,\n",
    "        #     'app_name': f\"QAGS XSum - {provider.model_engine}\",\n",
    "        #     'app_version': 'groundedness_likert4',\n",
    "        #     'dataset_df': qags_xsum,\n",
    "        #     'true_labels': qqags_xsum_true_labels\n",
    "        # }\n",
    "    ]\n",
    "    answer_relevance_experiments = [\n",
    "        {\n",
    "            \"feedback_fn\": answer_relevance_binary,\n",
    "            \"app_name\": f\"Snowflake IT balanced - {provider.model_engine}\",\n",
    "            \"app_version\": \"answer_relevance_binary\",\n",
    "            \"dataset_df\": snowflake_it_for_answer_relevance,\n",
    "            \"true_labels\": snowflake_it_answer_relevance_true_labels,\n",
    "        },\n",
    "        {\n",
    "            \"feedback_fn\": answer_relevance_binary,\n",
    "            \"app_name\": f\"Hotpot QA (400 samples) - {provider.model_engine}\",\n",
    "            \"app_version\": \"answer_relevance_binary\",\n",
    "            \"dataset_df\": hotpotqa_subset_for_answer_relevance,\n",
    "            \"true_labels\": hotpotqa_subset_for_answer_relevance_true_labels,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for exp in (\n",
    "        answer_relevance_experiments\n",
    "        + context_relevance_experiments\n",
    "        + groundedness_experiments\n",
    "    ):\n",
    "        print(f\"Running experiment: {exp['app_name']} - {exp['app_version']}\")\n",
    "        if \"groundedness\" in exp[\"app_version\"]:\n",
    "            print(f\"Groundedness system prompt: {Groundedness.system_prompt}\")\n",
    "\n",
    "        run_experiment(\n",
    "            exp[\"feedback_fn\"],\n",
    "            exp[\"app_name\"],\n",
    "            exp[\"app_version\"],\n",
    "            exp[\"dataset_df\"],\n",
    "            exp[\"true_labels\"],\n",
    "        )\n",
    "\n",
    "\n",
    "# for provider in ALL_PROVIDERS:\n",
    "#     runn_all_experiments_for_provider(provider)\n",
    "#     # Run the experiments in parallel using ThreadPoolExecutor\n",
    "#     with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#         # Submit tasks to the executor\n",
    "#         futures = [\n",
    "#             executor.submit(run_experiment,\n",
    "#                             exp['feedback_fn'],\n",
    "#                             exp['app_name'],\n",
    "#                             exp['app_version'],\n",
    "#                             exp['dataset_df'],\n",
    "#                             exp['true_labels'])\n",
    "#             for exp in experiments\n",
    "#         ]\n",
    "\n",
    "#         # Optionally, gather results or handle exceptions\n",
    "#         for future in concurrent.futures.as_completed(futures):\n",
    "#             try:\n",
    "#                 future.result()  # This will re-raise any exceptions caught during execution\n",
    "#             except Exception as e:\n",
    "#                 traceback.print_exc()\n",
    "#                 print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Submit the run_experiment_for_provider function for each provider\n",
    "    futures = [\n",
    "        executor.submit(runn_all_experiments_for_provider, provider)\n",
    "        for provider in ALL_PROVIDERS\n",
    "    ]\n",
    "\n",
    "    # Optionally, gather results or exceptions\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            result = (\n",
    "                future.result()\n",
    "            )  # This will re-raise any exceptions caught during execution\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.feedback import GroundTruthAggregator\n",
    "\n",
    "for provider_name in [\n",
    "    \"gpt-4o-mini\",\n",
    "    \"gpt-4o\",\n",
    "    \"snowflake-arctic\",\n",
    "    \"llama3.1-8b\",\n",
    "    \"mistral-large\",\n",
    "]:\n",
    "    file_path = f\"/Users/dhuang/Documents/git/trulens/src/benchmark/trulens/benchmark/benchmark_frameworks/experiments/results/Snowflake IT balanced - {provider_name}_context_relevance_binary_results.csv\"\n",
    "    scores, labels, latencies = read_results(file_path)\n",
    "    f_recall = GroundTruthAggregator(labels).recall\n",
    "    f_precision = GroundTruthAggregator(labels).precision\n",
    "    f_f1_score = GroundTruthAggregator(labels).f1_score\n",
    "\n",
    "    binary_labels = []\n",
    "    for label in labels:\n",
    "        if label >= 0.5:\n",
    "            binary_labels.append(1)\n",
    "        else:\n",
    "            binary_labels.append(0)\n",
    "\n",
    "    f_cohens_kappa = GroundTruthAggregator(binary_labels).cohens_kappa\n",
    "    f_auc = GroundTruthAggregator(labels).auc\n",
    "\n",
    "    f_mae = GroundTruthAggregator(labels).mae\n",
    "    f_pearson = GroundTruthAggregator(labels).pearson_correlation\n",
    "    f_spearman = GroundTruthAggregator(labels).spearman_correlation\n",
    "\n",
    "    recall = f_recall(scores)\n",
    "    precision = f_precision(scores)\n",
    "    f1_score = f_f1_score(scores)\n",
    "    mae = f_mae(scores)\n",
    "    pearson = f_pearson(scores)\n",
    "    spearman = f_spearman(scores)\n",
    "    cohens_kappa = f_cohens_kappa(scores)\n",
    "    # auc = f_auc(scores)\n",
    "\n",
    "    for latency in latencies:\n",
    "        if latency > 20:\n",
    "            # print(f\"Warning: latency is greater than 10 seconds: {latency}\")\n",
    "            latencies.remove(latency)\n",
    "    avg_latency = sum(latencies) / len(latencies)\n",
    "\n",
    "    # print(f\"{provider_name}: mae: {mae:.4f}, pearson: {pearson:.4f}, spearman: {spearman:.4f}, Cohen's Kappa: {cohens_kappa:.4f}\")\n",
    "    print(\n",
    "        f\"{provider_name}: recall: {recall:.4f}, precision: {precision:.4f}, f1: {f1_score:.4f}, Cohen's Kappa: {cohens_kappa:.4f}, avg_latency: {avg_latency:.4f}\"\n",
    "    )\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TruLens vs RAGAS comparison\n",
    "\n",
    "RAGAS vs TruLens' equivalents\n",
    "\n",
    "faithfulness <-> groundedness\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.cost import get_token_usage_for_openai\n",
    "from ragas.llms import llm_factory\n",
    "from ragas.metrics import faithfulness\n",
    "\n",
    "langchain_llm = llm_factory(model=\"gpt-4o-mini\")\n",
    "\n",
    "faithfulness.llm = langchain_llm\n",
    "\n",
    "\n",
    "# data_samples = {\n",
    "#     'question': ['When was the first super bowl?', 'Who won the most super bowls?'],\n",
    "#     'answer': ['The first superbowl was held on Jan 15, 1967', 'The most super bowls have been won by The New England Patriots'],\n",
    "#     'contexts' : [['The First AFL–NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles,'],\n",
    "#     ['The Green Bay Packers...Green Bay, Wisconsin.','The Packers compete...Football Conference']],\n",
    "# }\n",
    "\n",
    "# dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "# score = evaluate(dataset,metrics=[faithfulness], llm=langchain_llm,  token_usage_parser=get_token_usage_for_openai,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.total_cost(cost_per_input_token=5 / 1e6, cost_per_output_token=15 / 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trulens_groundedness(input, output) -> float:\n",
    "    return gpt_4o_mini.groundedness_measure_with_cot_reasons(\n",
    "        source=input,\n",
    "        statement=output,\n",
    "        use_sent_tokenize=True,\n",
    "        min_score_val=0,\n",
    "        max_score_val=3,\n",
    "        criteria=likert4_groundedness_criteria,\n",
    "    )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ragas_experiment(\n",
    "    dataset_df,\n",
    "):\n",
    "    data_samples = {\"question\": [], \"answer\": [], \"contexts\": []}\n",
    "    for i, row in dataset_df.iterrows():\n",
    "        data_samples[\"question\"].append(str(i))\n",
    "        data_samples[\"answer\"].append(row[\"expected_response\"])\n",
    "        data_samples[\"contexts\"].append([row[\"query\"]])\n",
    "\n",
    "    ragas_dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "    score = evaluate(\n",
    "        ragas_dataset,\n",
    "        metrics=[faithfulness],\n",
    "        llm=langchain_llm,\n",
    "        token_usage_parser=get_token_usage_for_openai,\n",
    "    )\n",
    "    avg_cost = (\n",
    "        score.total_cost(\n",
    "            cost_per_input_token=0.15 / 1e6, cost_per_output_token=0.6 / 1e6\n",
    "        )\n",
    "        / 200\n",
    "    )\n",
    "    print(f\"Average cost per sample: {avg_cost}\")\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def trulens_experiment(\n",
    "    dataset_df,\n",
    "):\n",
    "    data_samples = {\"question\": [], \"answer\": [], \"contexts\": []}\n",
    "    for i, row in dataset_df.iterrows():\n",
    "        data_samples[\"question\"].append(str(i))\n",
    "        data_samples[\"answer\"].append(row[\"expected_response\"])\n",
    "        data_samples[\"contexts\"].append([row[\"query\"]])\n",
    "\n",
    "    ff_scores = []\n",
    "    for i in range(len(data_samples[\"contexts\"])):\n",
    "        ff_scores.append(\n",
    "            trulens_groundedness(\n",
    "                data_samples[\"contexts\"][i][0], data_samples[\"answer\"][i]\n",
    "            )\n",
    "        )\n",
    "    import numpy as np\n",
    "\n",
    "    ff_scores = np.array(ff_scores)\n",
    "    return ff_scores\n",
    "\n",
    "\n",
    "ragas_cnn_score = ragas_experiment(qags_cnn_dm)\n",
    "ragas_xsum_score = ragas_experiment(qags_xsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_cnn_score.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trulens_cnn_scores, cnn_labels, latencies = read_results(\n",
    "    \"/Users/dhuang/Documents/git/trulens/src/benchmark/trulens/benchmark/benchmark_frameworks/experiments/results/QAGS CNN_DM - gpt-4o-mini_groundedness_likert4_results.csv\"\n",
    ")\n",
    "trulens_xsum_scores, xsum_labels, latencies = read_results(\n",
    "    \"/Users/dhuang/Documents/git/trulens/src/benchmark/trulens/benchmark/benchmark_frameworks/experiments/results/QAGS XSum - gpt-4o-mini_groundedness_likert4_results.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cnn_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "true_scores = np.array(cnn_labels)\n",
    "mae_trulens = np.mean(np.abs(trulens_cnn_scores - true_scores))\n",
    "mae_ragas = np.mean(\n",
    "    np.abs(\n",
    "        ragas_cnn_score.to_pandas()[\"faithfulness\"] - qags_cnn_dm_true_labels\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Trulens MAE: {mae_trulens:.4f}, Ragas MAE: {mae_ragas:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summeval_ragas_data_samples = {\"question\": [], \"answer\": [], \"contexts\": []}\n",
    "for i, row in summeval_subset.iterrows():\n",
    "    summeval_ragas_data_samples[\"question\"].append(str(i))\n",
    "    summeval_ragas_data_samples[\"answer\"].append(row[\"expected_response\"])\n",
    "    summeval_ragas_data_samples[\"contexts\"].append([row[\"query\"]])\n",
    "\n",
    "summeval_ragas_dataset = Dataset.from_dict(summeval_ragas_data_samples)\n",
    "\n",
    "score = evaluate(\n",
    "    summeval_ragas_dataset,\n",
    "    metrics=[faithfulness],\n",
    "    llm=langchain_llm,\n",
    "    token_usage_parser=get_token_usage_for_openai,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_cost = (\n",
    "    score.total_cost(\n",
    "        cost_per_input_token=0.15 / 1e6, cost_per_output_token=0.6 / 1e6\n",
    "    )\n",
    "    / 200\n",
    ")\n",
    "avg_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(summeval_ragas_data_samples[\"contexts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_scores = []\n",
    "for i in range(len(summeval_ragas_data_samples[\"contexts\"])):\n",
    "    ff_scores.append(\n",
    "        trulens_groundedness(\n",
    "            summeval_ragas_data_samples[\"contexts\"][i][0],\n",
    "            summeval_ragas_data_samples[\"answer\"][i],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ff_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ff_scores = np.array(ff_scores)\n",
    "ragas_scores = np.array(score.to_pandas()[\"faithfulness\"])\n",
    "\n",
    "true_scores = np.array(summeval_subset_true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_trulens = np.mean(np.abs(ff_scores - true_scores))\n",
    "mae_ragas = np.mean(np.abs(ragas_scores - true_scores))\n",
    "\n",
    "print(f\"Trulens MAE: {mae_trulens:.4f}, Ragas MAE: {mae_ragas:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
