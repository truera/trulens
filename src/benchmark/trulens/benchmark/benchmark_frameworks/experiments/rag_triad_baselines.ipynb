{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets preprocessing:\n",
    "\n",
    "Datasets that need some preprocessing before they can be used in `TruBenchmarkExperiment` class:\n",
    "1. Snowflake IT (internal): both rephrased and regular?, this should be used for all 3 in the triad\n",
    "2. SummEval (CNN and DailyMail summarizations with annotation) for groundedness\n",
    "3. QAGS (CNN and DailyMail with Turkers' annotation) for groundedness\n",
    "4. QAGS (XSUM with Turkers' annotation) for groundedness\n",
    "5. MSMARCO V2 for context relevance\n",
    "6. HotPot QA for answer relevance \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Pin random seed\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "# SummEval\n",
    "def generate_summeval_groundedness_golden_set(file_path):\n",
    "    def calculate_expected_score(normalized_metrics_lst, weights_lst):\n",
    "        assert len(normalized_metrics_lst) == len(weights_lst)\n",
    "        return round(\n",
    "            sum(\n",
    "                normalized_metrics_lst[i] * weights_lst[i]\n",
    "                for i in range(len(normalized_metrics_lst))\n",
    "            )\n",
    "            / sum(weights_lst),\n",
    "            2,\n",
    "        )\n",
    "\n",
    "    with open(file_path) as f:\n",
    "        for line in f:\n",
    "            # Each line is a separate JSON object\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "\n",
    "                # Ensure the expected keys exist in the JSON\n",
    "                try:\n",
    "                    row = data\n",
    "                    assert (\n",
    "                        len(row[\"machine_summaries\"]) == len(row[\"consistency\"])\n",
    "                    ), \"Mismatch in lengths of machine_summaries and consistency\"\n",
    "\n",
    "                    # Iterate over the summaries and create the desired dictionary structure\n",
    "                    for i in range(len(row[\"machine_summaries\"])):\n",
    "                        yield {\n",
    "                            \"query\": row.get(\n",
    "                                \"text\", \"\"\n",
    "                            ),  # Default to empty string if key not found\n",
    "                            \"expected_response\": row[\"machine_summaries\"][i],\n",
    "                            \"expected_score\": calculate_expected_score(\n",
    "                                [\n",
    "                                    (row[\"consistency\"][i] - 1)\n",
    "                                    / 4,  # Normalize from [1, 5] to [0, 1]\n",
    "                                ],\n",
    "                                [1.0],\n",
    "                            ),\n",
    "                            \"human_score\": row[\"consistency\"][i],\n",
    "                        }\n",
    "\n",
    "                except KeyError as e:\n",
    "                    print(\n",
    "                        f\"Key error: {e}. Please check if the keys exist in the JSON file.\"\n",
    "                    )\n",
    "                except AssertionError as e:\n",
    "                    print(\n",
    "                        f\"Assertion error: {e}. The lengths of 'machine_summaries' and 'consistency' do not match.\"\n",
    "                    )\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON decode error: {e}. Check the line format.\")\n",
    "\n",
    "\n",
    "# Snowflake IT dataset\n",
    "\n",
    "\n",
    "def generatate_snowflake_it_golden_set_groundedness(file_path):\n",
    "    res = []\n",
    "    with open(file_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        all_rows = list(reader)\n",
    "        for row in all_rows:\n",
    "            # Convert the 'golden' from a string to a list\n",
    "            try:\n",
    "                expected_chunks = ast.literal_eval(row[\"golden\"])\n",
    "                if not isinstance(expected_chunks, list):\n",
    "                    raise ValueError(\"Golden column should be a list\")\n",
    "\n",
    "                for expected_chunk in expected_chunks:\n",
    "                    # Yield the required fields\n",
    "                    res.append({\n",
    "                        \"query\": expected_chunk,  # source\n",
    "                        \"expected_response\": row[\n",
    "                            \"expected_response\"\n",
    "                        ],  # statement\n",
    "                        \"expected_score\": 1,  # retrieved chunks in the \"golden\" colum are always considered grounded\n",
    "                    })\n",
    "\n",
    "                # Generate a negative example for each query\n",
    "                # Collect all possible chunks from other queries to use as negative contexts\n",
    "                other_chunks = [\n",
    "                    chunk\n",
    "                    for other_row in all_rows\n",
    "                    if other_row != row\n",
    "                    for chunk in ast.literal_eval(other_row[\"golden\"])\n",
    "                ]\n",
    "\n",
    "                # Randomly select a negative chunk (context from another query)\n",
    "                if other_chunks:\n",
    "                    negative_chunk = random.choice(other_chunks)\n",
    "                    res.append({\n",
    "                        \"query\": negative_chunk,\n",
    "                        \"expected_response\": row[\n",
    "                            \"expected_response\"\n",
    "                        ],  # statement (not grounded by the chunk)\n",
    "                        \"expected_score\": 0,  # Negative example, score = 0\n",
    "                    })\n",
    "\n",
    "            except (ValueError, SyntaxError) as e:\n",
    "                print(f\"Error parsing golden column: {e}\")\n",
    "                continue\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def generate_snowflake_it_golden_set_answer_relevance(file_path):\n",
    "    res = []\n",
    "    with open(file_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        all_rows = list(\n",
    "            reader\n",
    "        )  # Store all rows in memory to use for negative example generation\n",
    "\n",
    "        for row in all_rows:\n",
    "            # generate a positive example for each query\n",
    "            if (\n",
    "                \"I don’t know the answer to that question.\"\n",
    "                in row[\"expected_response\"]\n",
    "            ):\n",
    "                ground_truth_score = 0  # label answer relevance as 0 for ABSTENTION \"I don’t know the answer to that question.\"\n",
    "            else:\n",
    "                ground_truth_score = (\n",
    "                    1  # label answer relevance as 1 for all other cases\n",
    "                )\n",
    "            res.append({\n",
    "                \"query\": row[\"query\"],\n",
    "                \"expected_response\": row[\"expected_response\"],\n",
    "                \"expected_score\": ground_truth_score,\n",
    "            })\n",
    "\n",
    "            # generate an easy negative example for each positive example by randomly selecting a response from another query\n",
    "            negative_response = random.choice([\n",
    "                r[\"expected_response\"] for r in all_rows if r != row\n",
    "            ])\n",
    "            res.append({\n",
    "                \"query\": row[\"query\"],\n",
    "                \"expected_response\": negative_response,  # Orthogonal response\n",
    "                \"expected_score\": 0,  # Label answer relevance as 0 for negative examples\n",
    "            })\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def generate_snowflake_it_golden_set_context_relevance(file_path):\n",
    "    res = []\n",
    "    with open(file_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        all_rows = list(reader)\n",
    "\n",
    "        # Step 1: Process each row to extract positive examples\n",
    "        for row in all_rows:\n",
    "            try:\n",
    "                expected_chunks = ast.literal_eval(row[\"golden\"])\n",
    "\n",
    "                if not isinstance(expected_chunks, list):\n",
    "                    raise ValueError(\"Golden column should be a list\")\n",
    "\n",
    "                # Generate positive examples\n",
    "                for expected_chunk in expected_chunks:\n",
    "                    res.append({\n",
    "                        \"query\": row[\"query\"],\n",
    "                        \"expected_response\": expected_chunk,\n",
    "                        \"expected_score\": 1,  # Positive example, score = 1\n",
    "                    })\n",
    "\n",
    "                # Step 2: Generate a negative example for each query\n",
    "                # Collect all possible chunks from other queries to use as negative contexts\n",
    "                other_chunks = [\n",
    "                    chunk\n",
    "                    for other_row in all_rows\n",
    "                    if other_row != row\n",
    "                    for chunk in ast.literal_eval(other_row[\"golden\"])\n",
    "                ]\n",
    "\n",
    "                # Randomly select a negative chunk (context from another query)\n",
    "                if other_chunks:\n",
    "                    negative_chunk = random.choice(other_chunks)\n",
    "                    res.append({\n",
    "                        \"query\": row[\"query\"],\n",
    "                        \"expected_response\": negative_chunk,  # Orthogonal/negative context\n",
    "                        \"expected_score\": 0,  # Negative example, score = 0\n",
    "                    })\n",
    "\n",
    "            except (ValueError, SyntaxError) as e:\n",
    "                print(\n",
    "                    f\"Error parsing golden column for query '{row['query']}': {e}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "def generate_qags_golden_set_groundedness(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            # Parse each line as a JSON object\n",
    "            data = json.loads(line)\n",
    "\n",
    "            # Extract the article as the query\n",
    "            query = data[\"article\"]\n",
    "\n",
    "            # Iterate over the summary_sentences to flatten the structure\n",
    "            for summary in data[\"summary_sentences\"]:\n",
    "                expected_response = summary[\"sentence\"]\n",
    "\n",
    "                # Calculate expected_score based on worker responses\n",
    "                responses = [\n",
    "                    response[\"response\"] for response in summary[\"responses\"]\n",
    "                ]\n",
    "                # Convert 'yes' to 1 and 'no' to 0, then calculate the average\n",
    "                expected_score = sum(\n",
    "                    1 if r.lower() == \"yes\" else 0 for r in responses\n",
    "                ) / len(responses)\n",
    "\n",
    "                # Yield the processed record\n",
    "                yield {\n",
    "                    \"query\": query,\n",
    "                    \"expected_response\": expected_response,\n",
    "                    \"expected_score\": expected_score,\n",
    "                }\n",
    "\n",
    "\n",
    "snowflake_it_file_path = \"data/snowflake_it_v3.csv\"\n",
    "\n",
    "snowflake_it_for_answer_relevance = pd.DataFrame(\n",
    "    list(\n",
    "        generate_snowflake_it_golden_set_answer_relevance(\n",
    "            snowflake_it_file_path\n",
    "        )\n",
    "    )\n",
    ")\n",
    "snowflake_it_answer_relevance_true_labels = list(\n",
    "    snowflake_it_for_answer_relevance[\"expected_score\"]\n",
    ")\n",
    "\n",
    "snowflake_it_for_context_relevance = pd.DataFrame(\n",
    "    list(\n",
    "        generate_snowflake_it_golden_set_context_relevance(\n",
    "            snowflake_it_file_path\n",
    "        )\n",
    "    )\n",
    ")\n",
    "snowflake_it_for_context_relevance_true_labels = list(\n",
    "    snowflake_it_for_context_relevance[\"expected_score\"]\n",
    ")\n",
    "\n",
    "\n",
    "snowflake_it_for_groundness = pd.DataFrame(\n",
    "    list(\n",
    "        generatate_snowflake_it_golden_set_groundedness(snowflake_it_file_path)\n",
    "    )\n",
    ")\n",
    "snowflake_it_for_groundness_true_labels = list(\n",
    "    snowflake_it_for_groundness[\"expected_score\"]\n",
    ")\n",
    "\n",
    "summeval_list = list(\n",
    "    generate_summeval_groundedness_golden_set(\"data/summeval_test.json\")\n",
    ")\n",
    "\n",
    "summeval_true_labels = [entry[\"expected_score\"] for entry in summeval_list]\n",
    "\n",
    "summeval = pd.DataFrame(\n",
    "    list(generate_summeval_groundedness_golden_set(\"data/summeval_test.json\"))\n",
    ")\n",
    "\n",
    "qags_cnn_dm = pd.DataFrame(\n",
    "    list(generate_qags_golden_set_groundedness(\"data/qags_mturk_cnndm.jsonl\"))\n",
    ")\n",
    "\n",
    "qags_cnn_dm_true_labels = [\n",
    "    row[\"expected_score\"] for _, row in qags_cnn_dm.iterrows()\n",
    "]\n",
    "\n",
    "\n",
    "qags_xsum = pd.DataFrame(\n",
    "    list(generate_qags_golden_set_groundedness(\"data/qags_mturk_xsum.jsonl\"))\n",
    ")\n",
    "\n",
    "qqags_xsum_true_labels = [\n",
    "    row[\"expected_score\"] for _, row in qags_xsum.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(42)\n",
    "\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# ds = load_dataset(\"nixiesearch/ms-marco-hard-negatives\")\n",
    "# ms_marco_hard_neg = pd.DataFrame(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "\n",
    "def generate_balanced_ms_marco_hard_negatives_dataset(\n",
    "    series: pd.Series, sample_size: int = 400\n",
    "):\n",
    "    # sampled_series = series.sample(n=sample_size, random_state=42)\n",
    "    sampled_series = series[:sample_size]\n",
    "    # Step 2: Create a list for the balanced dataset\n",
    "    balanced_dataset = []\n",
    "\n",
    "    # Step 3: Iterate over the sampled rows\n",
    "    for idx, row in sampled_series.items():\n",
    "        # \"row\" is a dictionary containing 'query', 'positive', and 'negative'\n",
    "        query = row.get(\"query\")\n",
    "        positive_list = row.get(\"positive\", [])\n",
    "        negative_list = row.get(\"negative\", [])\n",
    "        print(f\"query: {query}\")\n",
    "        # Select one positive example\n",
    "        if positive_list:\n",
    "            positive_example = random.choice(positive_list)\n",
    "        else:\n",
    "            continue  # Skip if no positive examples\n",
    "\n",
    "        # Select one negative example\n",
    "        if negative_list:\n",
    "            negative_example = random.choice(negative_list)\n",
    "        else:\n",
    "            continue  # Skip if no negative examples\n",
    "\n",
    "        print(\n",
    "            f\"positive_example: {positive_example} \\n negative_example: {negative_example}\"\n",
    "        )\n",
    "        # Add a positive example to the dataset\n",
    "        balanced_dataset.append({\n",
    "            \"query\": query,\n",
    "            \"expected_response\": positive_example,\n",
    "            \"expected_score\": 1,  # Positive example, label 1\n",
    "        })\n",
    "\n",
    "        # Add a negative example to the dataset\n",
    "        balanced_dataset.append({\n",
    "            \"query\": query,\n",
    "            \"expected_response\": negative_example,\n",
    "            \"expected_score\": 0,  # Negative example, label 0\n",
    "        })\n",
    "\n",
    "    # Ensure the dataset is exactly 200 positive and 200 negative examples\n",
    "    balanced_dataset = balanced_dataset[\n",
    "        :sample_size\n",
    "    ]  # Ensure total sample size is respected\n",
    "\n",
    "    # Convert the list to a DataFrame for convenience\n",
    "    balanced_df = pd.DataFrame(balanced_dataset)\n",
    "\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ms_marco_hard_neg_balanced = generate_balanced_ms_marco_hard_negatives_dataset(ms_marco_hard_neg['train'], 400)\n",
    "# ms_marco_hard_neg_balanced.to_csv(\"ms_marco_hard_neg_balanced.csv\", index=False)\n",
    "ms_marco_hard_neg_balanced = pd.read_csv(\"ms_marco_hard_neg_balanced.csv\")\n",
    "ms_marco_hard_neg_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "\n",
    "def generate_ms_marco_context_relevance_benchmark(\n",
    "    file_path=\"data/ms_marco_v2_1_val.parquet\",\n",
    "):\n",
    "    df = pd.read_parquet(file_path, engine=\"pyarrow\")  # or engine='fastparquet'\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        assert len(row[\"passages\"][\"is_selected\"]) == len(\n",
    "            row[\"passages\"][\"passage_text\"]\n",
    "        )\n",
    "\n",
    "        if sum(row[\"passages\"][\"is_selected\"]) < 1:\n",
    "            # currently we only consider sample with one passage marked as relevant (there are samples where zero passage_text is selected)\n",
    "            continue\n",
    "        for i, passage_text in enumerate(row[\"passages\"][\"passage_text\"]):\n",
    "            yield {\n",
    "                \"query_id\": row[\"query_id\"],\n",
    "                \"query\": row[\"query\"],\n",
    "                \"expected_response\": passage_text,\n",
    "                \"expected_score\": row[\"passages\"][\"is_selected\"][\n",
    "                    i\n",
    "                ],  # Binary relevance\n",
    "            }\n",
    "\n",
    "\n",
    "ms_marco = list(generate_ms_marco_context_relevance_benchmark())\n",
    "\n",
    "\n",
    "score_1_entries = [entry for entry in ms_marco if entry[\"expected_score\"] == 1]\n",
    "score_0_entries = [entry for entry in ms_marco if entry[\"expected_score\"] == 0]\n",
    "\n",
    "# Calculate the number of samples needed from each group\n",
    "num_samples_per_group = min(\n",
    "    len(score_1_entries), len(score_0_entries), 150\n",
    ")  # Sample 150 from each\n",
    "\n",
    "\n",
    "sampled_score_1 = random.sample(score_1_entries, num_samples_per_group)\n",
    "sampled_score_0 = random.sample(score_0_entries, num_samples_per_group)\n",
    "\n",
    "# Combine and shuffle the samples to get a balanced dataset\n",
    "balanced_sample = sampled_score_1 + sampled_score_0\n",
    "random.shuffle(balanced_sample)\n",
    "\n",
    "# Ensure the combined length is 300\n",
    "assert len(balanced_sample) == 300\n",
    "\n",
    "# Now you can use `balanced_sample` as your final dataset\n",
    "print(\n",
    "    f\"Number of entries with expected_score = 1: {len([e for e in balanced_sample if e['expected_score'] == 1])}\"\n",
    ")\n",
    "print(\n",
    "    f\"Number of entries with expected_score = 0: {len([e for e in balanced_sample if e['expected_score'] == 0])}\"\n",
    ")\n",
    "\n",
    "ms_marco_balanced_sample_300 = pd.DataFrame(balanced_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_marco_balanced_sample_300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load preprocessed datasets from BEIR - start w/ Hotpot QA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.benchmark.benchmark_frameworks.dataset.beir_loader import (\n",
    "    TruBEIRDataLoader,\n",
    ")\n",
    "\n",
    "beir_data_loader = TruBEIRDataLoader(data_folder=\"./\", dataset_name=\"hotpotqa\")\n",
    "hotpotqa = beir_data_loader.load_dataset_to_df(download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "\n",
    "hotpotqa_raw_subset = hotpotqa.sample(n=200, random_state=42)\n",
    "\n",
    "all_responses = [\n",
    "    (row[\"query\"], row[\"expected_response\"])\n",
    "    for idx, row in hotpotqa_raw_subset.iterrows()\n",
    "]\n",
    "\n",
    "all_contexts = [\n",
    "    (row[\"query\"], context[\"text\"])\n",
    "    for idx, row in hotpotqa_raw_subset.iterrows()\n",
    "    for context in row[\"expected_chunks\"]\n",
    "]\n",
    "\n",
    "hotpotqa_subset_for_answer_relevance, hotpotqa_subset_for_context_relevance = (\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "\n",
    "for idx, row in hotpotqa_raw_subset.iterrows():\n",
    "    # Positive examples for answer relevance\n",
    "    hotpotqa_subset_for_answer_relevance.append({\n",
    "        \"query\": row[\"query\"],\n",
    "        \"expected_response\": row[\"expected_response\"],  # Positive response\n",
    "        \"expected_score\": 1,  # Positive example, score = 1\n",
    "    })\n",
    "\n",
    "    # Negative examples for answer relevance (random unrelated response)\n",
    "    negative_response = random.choice([\n",
    "        r\n",
    "        for q, r in all_responses\n",
    "        if q != row[\"query\"]  # Pick response from another query\n",
    "    ])\n",
    "\n",
    "    hotpotqa_subset_for_answer_relevance.append({\n",
    "        \"query\": row[\"query\"],\n",
    "        \"expected_response\": negative_response,  # Negative response\n",
    "        \"expected_score\": 0,  # Negative example, score = 0\n",
    "    })\n",
    "\n",
    "\n",
    "for idx, row in hotpotqa_raw_subset.iterrows():\n",
    "    positive_examples = []\n",
    "    negative_examples = []\n",
    "\n",
    "    # Generate positive examples for context relevance\n",
    "    for context in row[\"expected_chunks\"]:\n",
    "        positive_examples.append({\n",
    "            \"query\": row[\"query\"],\n",
    "            \"expected_response\": context[\"text\"],  # Positive context\n",
    "            \"expected_score\": context[\"expected_score\"],  # Should be 1\n",
    "        })\n",
    "\n",
    "    # Generate negative examples for context relevance\n",
    "    for _ in positive_examples:\n",
    "        negative_context = random.choice([\n",
    "            c\n",
    "            for q, c in all_contexts\n",
    "            if q != row[\"query\"]  # Pick context from another query\n",
    "        ])\n",
    "        negative_examples.append({\n",
    "            \"query\": row[\"query\"],\n",
    "            \"expected_response\": negative_context,  # Negative context\n",
    "            \"expected_score\": 0,  # Negative example, score = 0\n",
    "        })\n",
    "\n",
    "    # Add positive and negative examples to the result set\n",
    "    hotpotqa_subset_for_context_relevance.extend(positive_examples)\n",
    "    hotpotqa_subset_for_context_relevance.extend(negative_examples)\n",
    "\n",
    "\n",
    "hotpotqa_subset_for_context_relevance_true_labels = [\n",
    "    entry[\"expected_score\"] for entry in hotpotqa_subset_for_context_relevance\n",
    "]\n",
    "hotpotqa_subset_for_answer_relevance_true_labels = [\n",
    "    entry[\"expected_score\"] for entry in hotpotqa_subset_for_answer_relevance\n",
    "]\n",
    "\n",
    "hotpotqa_subset_for_context_relevance = pd.DataFrame(\n",
    "    hotpotqa_subset_for_context_relevance\n",
    ")\n",
    "\n",
    "hotpotqa_subset_for_answer_relevance = pd.DataFrame(\n",
    "    hotpotqa_subset_for_answer_relevance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotpotqa_subset_for_context_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summeval_subset = summeval.sample(n=200, random_state=42)\n",
    "summeval_subset_true_labels = [\n",
    "    row[\"expected_score\"] for _, row in summeval_subset.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up feedback LLM providers \n",
    "\n",
    "We will experiment with 2 current OpenAI models and a mix of commercial and open source models avaiable in Cortex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import snowflake.connector\n",
    "from trulens.providers.cortex import Cortex\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "snowflake_connection_parameters = {\n",
    "    \"account\": os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "    \"user\": os.environ[\"SNOWFLAKE_USER\"],\n",
    "    \"password\": os.environ[\"SNOWFLAKE_USER_PASSWORD\"],\n",
    "}\n",
    "snowflake_connection = snowflake.connector.connect(\n",
    "    **snowflake_connection_parameters\n",
    ")\n",
    "\n",
    "\n",
    "gpt_4o = OpenAI(model_engine=\"gpt-4o\")\n",
    "gpt_4o_mini = OpenAI(model_engine=\"gpt-4o-mini\")\n",
    "\n",
    "snowflake_arctic = Cortex(snowflake_connection, model_engine=\"snowflake-arctic\")\n",
    "mistral_large = Cortex(snowflake_connection, model_engine=\"mistral-large\")\n",
    "llama3_1_8b = Cortex(snowflake_connection, model_engine=\"llama3.1-8b\")\n",
    "\n",
    "CORTEX_PROVIDERS = [snowflake_arctic, llama3_1_8b, mistral_large]\n",
    "OPENAI_PROVIDERS = [gpt_4o, gpt_4o_mini]\n",
    "ALL_PROVIDERS = CORTEX_PROVIDERS + OPENAI_PROVIDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import TruSession\n",
    "\n",
    "session = TruSession()\n",
    "session.reset_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snowflake IT dataset experiment runs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "\n",
    "from trulens.apps.basic import TruBasicApp\n",
    "\n",
    "\n",
    "def write_results(\n",
    "    feedback_scores: List[float],\n",
    "    labels: List[float | int],\n",
    "    latencies: List[float],\n",
    "    file_name: str,\n",
    "):\n",
    "    assert len(feedback_scores) == len(labels)\n",
    "\n",
    "    with open(file_name, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(feedback_scores)\n",
    "        writer.writerow(labels)\n",
    "        writer.writerow(latencies)\n",
    "\n",
    "\n",
    "def read_results(\n",
    "    file_name: str,\n",
    ") -> Tuple[List[float | int], List[float | int], List[float]]:\n",
    "    with open(file_name, \"r\") as file:\n",
    "        reader = csv.reader(file)\n",
    "        for index, row in enumerate(reader):\n",
    "            if index == 0:\n",
    "                # First row contains scores\n",
    "                scores = list(map(float, row))  # Convert strings to floats\n",
    "            elif index == 1:\n",
    "                # Second row contains labels\n",
    "                labels = list(map(float, row))  # Convert strings to floats\n",
    "            elif index == 2:\n",
    "                # Third row contains latencies\n",
    "                latencies = list(map(float, row))\n",
    "    return scores, labels, latencies\n",
    "\n",
    "\n",
    "def run_feedback_experiment(\n",
    "    feedback_func_wrapper, app_name, app_version, dataset_df, true_labels\n",
    "):\n",
    "    if len(dataset_df) != len(true_labels):\n",
    "        raise ValueError(\"dataset df must have the same length as labels\")\n",
    "    tru_wrapped_basic_app = TruBasicApp(\n",
    "        feedback_func_wrapper, app_name=app_name, app_version=app_version\n",
    "    )\n",
    "\n",
    "    generated_scores, labels, latencies = [], [], []\n",
    "    for i in range(len(dataset_df)):\n",
    "        arg_1 = dataset_df.iloc[i][\"query\"]\n",
    "        arg_2 = dataset_df.iloc[i][\"expected_response\"]\n",
    "        try:\n",
    "            with tru_wrapped_basic_app as _:\n",
    "                start_time = time.time()\n",
    "                score = tru_wrapped_basic_app.app(arg_1, arg_2)\n",
    "\n",
    "                end_time = time.time()\n",
    "                true_score = true_labels[i]\n",
    "\n",
    "                if not math.isnan(score):\n",
    "                    generated_scores.append(score)\n",
    "                    labels.append(true_score)\n",
    "                    latencies.append(end_time - start_time)\n",
    "\n",
    "                    # print(f\"Generated score: {score} | true_score: {true_score} \\n\")\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error {e} in run_feedback_experiment row {i} with first arg {arg_1} and second arg {arg_2}\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "    write_results(\n",
    "        generated_scores,\n",
    "        labels,\n",
    "        latencies,\n",
    "        f\"results/{app_name}_{app_version}_results.csv\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = gpt_4o_mini.groundedness_measure_with_cot_reasons(\n",
    "    source=\"This is a test. Earth is round\",\n",
    "    statement=\"Earth is not not round\",\n",
    "    criteria=\"A grounded response to the query should get a score of 1, and an ungrounded response should get a score of 0. The score can only be either 0 or 1 (binary).\",\n",
    "    max_score_val=1,\n",
    ")\n",
    "print(score)\n",
    "score = gpt_4o_mini.groundedness_measure_with_cot_reasons(\n",
    "    \"This is a test. Earth is round\",\n",
    "    \"Earth is not not round\",\n",
    "    criteria=\"\"\" You should score the groundedness of the statement based on the following criteria:\n",
    "    - Statements that are directly supported by the source should be considered grounded and should get a high score.\n",
    "    - Statements that are not directly supported by the source should be considered not grounded and should get a low score.\n",
    "    - Statements of doubt, that admissions of uncertainty or not knowing the answer are considered abstention, and should be counted as the most overlap and therefore get a max score.\"\"\",\n",
    "    max_score_val=3,\n",
    ")\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.feedback.v2.feedback import Groundedness\n",
    "\n",
    "default_groundedness_criteria = Groundedness.criteria\n",
    "default_groundedness_system_prompt = Groundedness.system_prompt\n",
    "\n",
    "likert4_groundedness_criteria = \"\"\"You should score the groundedness of the statement based on the following criteria:\n",
    "    - Statements that are directly supported by the source should be considered grounded and should get a high score.\n",
    "    - Statements that are not directly supported by the source should be considered not grounded and should get a low score.\n",
    "    - Statements of doubt, that admissions of uncertainty or not knowing the answer are considered abstention, and should be counted as the most overlap and therefore get a max score.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import traceback\n",
    "\n",
    "from trulens.feedback.v2.feedback import Groundedness\n",
    "\n",
    "\n",
    "def runn_all_experiments_for_provider(provider):\n",
    "    \"\"\"\n",
    "    Runs all experiments for a given provider.\n",
    "    \"\"\"\n",
    "\n",
    "    def context_relevance_binary(input, output) -> float:\n",
    "        return provider.context_relevance_with_cot_reasons(\n",
    "            question=input,\n",
    "            context=output,\n",
    "            criteria=\"A relevant context to the question should get a score of 1, and an irrelevant context should get a score of 0. The score can only be either 0 or 1 (binary).\",\n",
    "            min_score_val=0,\n",
    "            max_score_val=1,\n",
    "        )[0]\n",
    "\n",
    "    # Run context relevance binary experiment\n",
    "    # context_relevance_binary_run = create_benchmark_experiment_app(\n",
    "    #     app_name=f\"Snowflake IT  - {provider.model_engine}\",\n",
    "    #     app_version='context_relevance_binary',\n",
    "    #     benchmark_experiment=TruBenchmarkExperiment(\n",
    "    #         feedback_fn=context_relevance_binary,\n",
    "    #         agg_funcs=snowflake_it_metrics,\n",
    "    #         benchmark_params=benchmark_params\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    # with context_relevance_binary_run as recording:\n",
    "    #     feedback_res = context_relevance_binary_run.app(snowflake_it_for_context_relevance)\n",
    "    #     print(f'feedback results: {feedback_res}')\n",
    "    #     write_results(feedback_scores=feedback_res, file_name=f\"results/{provider.model_engine}-context_relevance_binary_feedback_scores.csv\")\n",
    "\n",
    "    # # Similar pattern for answer relevance binary experiment\n",
    "    def answer_relevance_binary(input, output) -> float:\n",
    "        return provider.relevance(\n",
    "            prompt=input,\n",
    "            response=output,\n",
    "            criteria=\"A relevant response to the prompt should get a score of 1, and an irrelevant response should get a score of 0. The score can only be either 0 or 1 (binary).\",\n",
    "            min_score_val=0,\n",
    "            max_score_val=1,\n",
    "        )\n",
    "\n",
    "    # answer_relevance_binary_run = create_benchmark_experiment_app(\n",
    "    #     app_name=f\"Snowflake IT - {provider.model_engine}\",\n",
    "    #     app_version='answer_relevance_binary',\n",
    "    #     benchmark_experiment=TruBenchmarkExperiment(\n",
    "    #         feedback_fn=answer_relevance_binary,\n",
    "    #         agg_funcs=snowflake_it_metrics,\n",
    "    #         benchmark_params=benchmark_params\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    # with answer_relevance_binary_run as recording:\n",
    "    #     feedback_res = answer_relevance_binary_run.app(snowflake_it_for_answer_relevance)\n",
    "    #     write_results(feedback_scores=feedback_res, file_name=f\"results/{provider.model_engine}-answer_relevance_binary_feedback_scores.csv\")\n",
    "\n",
    "    def groundedness_binary(input, output) -> float:\n",
    "        return provider.groundedness_measure_with_cot_reasons(\n",
    "            source=input,\n",
    "            statement=output,\n",
    "            criteria=\"A grounded response to the query should get a score of 1, and an ungrounded response should get a score of 0. The score can only be either 0 or 1 (binary).\",\n",
    "            min_score_val=0,\n",
    "            max_score_val=1,\n",
    "            use_sent_tokenize=True,\n",
    "        )[0]\n",
    "\n",
    "    def groundedness_likert_4(input, output) -> float:\n",
    "        return provider.groundedness_measure_with_cot_reasons(\n",
    "            source=input,\n",
    "            statement=output,\n",
    "            use_sent_tokenize=True,\n",
    "            min_score_val=0,\n",
    "            max_score_val=3,\n",
    "            criteria=likert4_groundedness_criteria,\n",
    "        )[0]\n",
    "\n",
    "    # Define a function to wrap the run_feedback_experiment call\n",
    "    def run_experiment(\n",
    "        feedback_fn, app_name, app_version, dataset_df, true_labels\n",
    "    ):\n",
    "        run_feedback_experiment(\n",
    "            feedback_func_wrapper=feedback_fn,\n",
    "            app_name=app_name,\n",
    "            app_version=app_version,\n",
    "            dataset_df=dataset_df,\n",
    "            true_labels=true_labels,\n",
    "        )\n",
    "\n",
    "    context_relevance_experiments = [\n",
    "        {\n",
    "            \"feedback_fn\": context_relevance_binary,\n",
    "            \"app_name\": f\"Snowflake IT balanced - {provider.model_engine}\",\n",
    "            \"app_version\": \"context_relevance_binary\",\n",
    "            \"dataset_df\": snowflake_it_for_context_relevance,\n",
    "            \"true_labels\": snowflake_it_for_context_relevance_true_labels,\n",
    "        },\n",
    "        {\n",
    "            \"feedback_fn\": context_relevance_binary,\n",
    "            \"app_name\": f\"Hotpot QA (800 samples) - {provider.model_engine}\",\n",
    "            \"app_version\": \"context_relevance_binary\",\n",
    "            \"dataset_df\": hotpotqa_subset_for_context_relevance,\n",
    "            \"true_labels\": hotpotqa_subset_for_context_relevance_true_labels,\n",
    "        },\n",
    "        {\n",
    "            \"feedback_fn\": context_relevance_binary,\n",
    "            \"app_name\": f\"MS MARCO hard negatives (first 400 samples) - {provider.model_engine}\",\n",
    "            \"app_version\": \"context_relevance_binary\",\n",
    "            \"dataset_df\": ms_marco_hard_neg_balanced,\n",
    "            \"true_labels\": [\n",
    "                row[\"expected_score\"]\n",
    "                for _, row in ms_marco_hard_neg_balanced.iterrows()\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            # MS MARCO V2 for context relevance\n",
    "            \"feedback_fn\": context_relevance_binary,\n",
    "            \"app_name\": f\"MS MARCO V2 balanced (300 samples) - {provider.model_engine}\",\n",
    "            \"app_version\": \"context_relevance_binary\",\n",
    "            \"dataset_df\": ms_marco_balanced_sample_300,\n",
    "            \"true_labels\": [\n",
    "                row[\"expected_score\"]\n",
    "                for _, row in ms_marco_balanced_sample_300.iterrows()\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    groundedness_experiments = [\n",
    "        {\n",
    "            \"feedback_fn\": groundedness_binary,\n",
    "            \"app_name\": f\"Snowflake IT balanced - {provider.model_engine}\",\n",
    "            \"app_version\": \"groundedness_binary\",\n",
    "            \"dataset_df\": snowflake_it_for_groundness,\n",
    "            \"true_labels\": snowflake_it_for_groundness_true_labels,\n",
    "        },\n",
    "        {\n",
    "            \"feedback_fn\": groundedness_likert_4,\n",
    "            \"app_name\": f\"SummEval (200 samples) - {provider.model_engine}\",\n",
    "            \"app_version\": \"groundedness_likert4\",\n",
    "            \"dataset_df\": summeval_subset,\n",
    "            \"true_labels\": [\n",
    "                row[\"expected_score\"] for _, row in summeval_subset.iterrows()\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"feedback_fn\": groundedness_likert_4,\n",
    "            \"app_name\": f\"QAGS CNN_DM - {provider.model_engine}\",\n",
    "            \"app_version\": \"groundedness_likert4\",\n",
    "            \"dataset_df\": qags_cnn_dm,\n",
    "            \"true_labels\": qags_cnn_dm_true_labels,\n",
    "        },\n",
    "        {\n",
    "            \"feedback_fn\": groundedness_likert_4,\n",
    "            \"app_name\": f\"QAGS XSum - {provider.model_engine}\",\n",
    "            \"app_version\": \"groundedness_likert4\",\n",
    "            \"dataset_df\": qags_xsum,\n",
    "            \"true_labels\": qqags_xsum_true_labels,\n",
    "        },\n",
    "    ]\n",
    "    answer_relevance_experiments = [\n",
    "        {\n",
    "            \"feedback_fn\": answer_relevance_binary,\n",
    "            \"app_name\": f\"Snowflake IT balanced - {provider.model_engine}\",\n",
    "            \"app_version\": \"answer_relevance_binary\",\n",
    "            \"dataset_df\": snowflake_it_for_answer_relevance,\n",
    "            \"true_labels\": snowflake_it_answer_relevance_true_labels,\n",
    "        },\n",
    "        {\n",
    "            \"feedback_fn\": answer_relevance_binary,\n",
    "            \"app_name\": f\"Hotpot QA (400 samples) - {provider.model_engine}\",\n",
    "            \"app_version\": \"answer_relevance_binary\",\n",
    "            \"dataset_df\": hotpotqa_subset_for_answer_relevance,\n",
    "            \"true_labels\": hotpotqa_subset_for_answer_relevance_true_labels,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for exp in (\n",
    "        answer_relevance_experiments\n",
    "        + context_relevance_experiments\n",
    "        + groundedness_experiments\n",
    "    ):\n",
    "        print(f\"Running experiment: {exp['app_name']} - {exp['app_version']}\")\n",
    "        if \"groundedness\" in exp[\"app_version\"]:\n",
    "            print(f\"Groundedness system prompt: {Groundedness.system_prompt}\")\n",
    "\n",
    "        run_experiment(\n",
    "            exp[\"feedback_fn\"],\n",
    "            exp[\"app_name\"],\n",
    "            exp[\"app_version\"],\n",
    "            exp[\"dataset_df\"],\n",
    "            exp[\"true_labels\"],\n",
    "        )\n",
    "\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Submit the run_experiment_for_provider function for each provider\n",
    "    futures = [\n",
    "        executor.submit(runn_all_experiments_for_provider, provider)\n",
    "        for provider in ALL_PROVIDERS\n",
    "    ]\n",
    "\n",
    "    # Optionally, gather results or exceptions\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            result = (\n",
    "                future.result()\n",
    "            )  # This will re-raise any exceptions caught during execution\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(\n",
    "    true_labels, predicted_scores, threshold=0.5, title=\"Confusion Matrix\"\n",
    "):\n",
    "    # Binarize the predicted scores based on the threshold\n",
    "    predicted_labels = [\n",
    "        1 if score >= threshold else 0 for score in predicted_scores\n",
    "    ]\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        cbar=False,\n",
    "        xticklabels=[\"Predicted 0\", \"Predicted 1\"],\n",
    "        yticklabels=[\"True 0\", \"True 1\"],\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.feedback import GroundTruthAggregator\n",
    "\n",
    "for provider_name in [\n",
    "    \"gpt-4o-mini\",\n",
    "    \"gpt-4o\",\n",
    "    \"snowflake-arctic\",\n",
    "    \"llama3.1-8b\",\n",
    "    \"mistral-large\",\n",
    "]:\n",
    "    file_path = f\"results/Hotpot QA (400 samples) - {provider_name}_answer_relevance_binary_results.csv\"\n",
    "    scores, labels, latencies = read_results(file_path)\n",
    "    f_recall = GroundTruthAggregator(labels).recall\n",
    "    f_precision = GroundTruthAggregator(labels).precision\n",
    "    f_f1_score = GroundTruthAggregator(labels).f1_score\n",
    "\n",
    "    binary_labels = []\n",
    "    for label in labels:\n",
    "        if label >= 0.5:\n",
    "            binary_labels.append(1)\n",
    "        else:\n",
    "            binary_labels.append(0)\n",
    "\n",
    "    binary_scores = []\n",
    "    for score in scores:\n",
    "        if score >= 0.5:\n",
    "            binary_scores.append(1)\n",
    "        else:\n",
    "            binary_scores.append(0)\n",
    "\n",
    "    f_cohens_kappa = GroundTruthAggregator(binary_labels).cohens_kappa\n",
    "    f_auc = GroundTruthAggregator(labels).auc\n",
    "\n",
    "    f_mae = GroundTruthAggregator(labels).mae\n",
    "    f_pearson = GroundTruthAggregator(labels).pearson_correlation\n",
    "    f_spearman = GroundTruthAggregator(labels).spearman_correlation\n",
    "    f_matthews = GroundTruthAggregator(binary_labels).matthews_correlation\n",
    "\n",
    "    recall = f_recall(scores)\n",
    "    precision = f_precision(scores)\n",
    "    f1_score = f_f1_score(scores)\n",
    "    mae = f_mae(scores)\n",
    "    pearson = f_pearson(scores)\n",
    "    spearman = f_spearman(scores)\n",
    "    cohens_kappa = f_cohens_kappa(scores)\n",
    "    auc = f_auc(scores)\n",
    "    matthews = f_matthews(binary_scores)\n",
    "\n",
    "    for latency in latencies:\n",
    "        if latency > 20:\n",
    "            # print(f\"Warning: latency is greater than 10 seconds: {latency}\")\n",
    "            latencies.remove(latency)\n",
    "    avg_latency = sum(latencies) / len(latencies)\n",
    "\n",
    "    # print(f\"{provider_name}: mae: {mae:.4f}, pearson: {pearson:.4f}, spearman: {spearman:.4f}, Cohen's Kappa: {cohens_kappa:.4f}\")\n",
    "    print(\n",
    "        f\"{provider_name}: recall: {recall:.4f}, precision: {precision:.4f}, f1: {f1_score:.4f}, Cohen's Kappa: {cohens_kappa:.4f}, Matthews: {matthews:.4f}, AUC: {auc:.4f}, avg_latency: {avg_latency:.4f}\"\n",
    "    )\n",
    "    print(\"\\n\")\n",
    "\n",
    "    plot_confusion_matrix(\n",
    "        labels, scores, title=f\"Confusion Matrix - {provider_name}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading results of internal evaluation runs scraped from Cortex Chat orchestrator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cortex_eval_df = pd.read_csv(\"eval_scrape_mistral-large_output_1727118011.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cortex_eval_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "gpt_4o = OpenAI(model_engine=\"gpt-4o-mini\")\n",
    "\n",
    "context_relevant_likert_4_criteria = \"\"\"\n",
    "        - CONTEXT that is IRRELEVANT to the QUESTION should score 0.\n",
    "        - CONTEXT that is RELEVANT to some of the QUESTION should score of 1.\n",
    "        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 2.\n",
    "        - CONTEXT that is RELEVANT to the entirety of the QUESTION should get a score of 3, which is the full mark.\n",
    "        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 3.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "def trulens_context_relevance(query, context) -> float:\n",
    "    try:\n",
    "        return gpt_4o.context_relevance_with_cot_reasons(\n",
    "            question=query,\n",
    "            context=context,\n",
    "            max_score_val=3,\n",
    "            min_score_val=0,\n",
    "            criteria=context_relevant_likert_4_criteria,\n",
    "        )[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in trulens_context_relevance: {e}\")\n",
    "        return -1\n",
    "\n",
    "\n",
    "answer_relevant_likert_4_criteria = \"\"\"\n",
    "        - RESPONSE must be relevant to the entire PROMPT to get a score of 4.\n",
    "        - RELEVANCE score should increase as the RESPONSE provides RELEVANT context to more parts of the PROMPT.\n",
    "        - RESPONSE that is RELEVANT to none of the PROMPT should get a score of 0.\n",
    "        - RESPONSE that is RELEVANT to some of the PROMPT should get as score of 1 or 2. Higher score indicates more RELEVANCE.\n",
    "        - RESPONSE that is RELEVANT to the entire PROMPT should get a score of 3.\n",
    "        - RESPONSE that is RELEVANT and answers the entire PROMPT completely should get a score of 3.\n",
    "        - RESPONSE that confidently FALSE should get a score of 0.\n",
    "        - RESPONSE that is only seemingly RELEVANT should get a score of 0.\n",
    "        - Answers that intentionally do not answer the question, such as 'I don't know' and model refusals, should also be counted as the least RELEVANT and get a score of 0.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def trulens_answer_relevance(query, response) -> float:\n",
    "    try:\n",
    "        return gpt_4o.relevance(\n",
    "            prompt=query,\n",
    "            response=response,\n",
    "            min_score_val=0,\n",
    "            max_score_val=3,\n",
    "            criteria=answer_relevant_likert_4_criteria,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error in trulens_answer_relevance: {e}\")\n",
    "        return -1\n",
    "\n",
    "\n",
    "def trulens_answer_relevance_cot(query, response) -> float:\n",
    "    try:\n",
    "        return gpt_4o.relevance_with_cot_reasons(\n",
    "            prompt=query,\n",
    "            response=response,\n",
    "            min_score_val=0,\n",
    "            max_score_val=3,\n",
    "            criteria=answer_relevant_likert_4_criteria,\n",
    "        )[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in trulens_answer_relevance: {e}\")\n",
    "        return -1\n",
    "\n",
    "\n",
    "likert4_groundedness_criteria = \"\"\"You should score the groundedness of the statement based on the following criteria:\n",
    "    - Statements that are directly supported by the source should be considered grounded and should get a high score.\n",
    "    - Statements that are not directly supported by the source should be considered not grounded and should get a low score.\n",
    "    - Statements of doubt, that admissions of uncertainty or not knowing the answer are considered abstention, and should be counted as the most overlap and therefore get a max score.\"\"\"\n",
    "\n",
    "\n",
    "def trulens_groundedness(context, response) -> float:\n",
    "    try:\n",
    "        return gpt_4o.groundedness_measure_with_cot_reasons(\n",
    "            source=context,\n",
    "            statement=response,\n",
    "            use_sent_tokenize=True,\n",
    "            min_score_val=0,\n",
    "            max_score_val=3,\n",
    "            criteria=likert4_groundedness_criteria,\n",
    "        )[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in trulens_groundedness: {e}\")\n",
    "        return -1\n",
    "\n",
    "\n",
    "(\n",
    "    context_relevance_scores,\n",
    "    answer_relevance_scores,\n",
    "    groundedness_scores,\n",
    "    answer_relevance_cot_scores,\n",
    ") = [], [], [], []\n",
    "\n",
    "for i, row in cortex_eval_df.iterrows():\n",
    "    query = row[\"query\"]\n",
    "    context_chunks = [chunk for chunk in ast.literal_eval(row[\"golden\"])]\n",
    "    llm_response = row[\"llm_answer\"]\n",
    "\n",
    "    assert query and llm_response, \"query and llm_response should not be empty\"\n",
    "\n",
    "    print(\n",
    "        f\"ROW {i}: query: {query}\\n , llm_response: {llm_response}\\ng, context_chunks: {context_chunks} \\n\\n\"\n",
    "    )\n",
    "    answer_relevance_score = trulens_answer_relevance(query, llm_response)\n",
    "    answer_relevance_cot_score = trulens_answer_relevance_cot(\n",
    "        query, llm_response\n",
    "    )\n",
    "    answer_relevance_scores.append(answer_relevance_score)\n",
    "    answer_relevance_cot_scores.append(answer_relevance_cot_score)\n",
    "    if len(context_chunks) > 0:\n",
    "        _context_relevance_scores_per_query = []\n",
    "        _groundedness_scores_per_query = []\n",
    "        for context in context_chunks:\n",
    "            if (\n",
    "                context\n",
    "                == \"Country Work-from-home budget (USD) Welcome period (mo)\\nPoland $350 12 \"\n",
    "            ):\n",
    "                contex = \"Country Work-from-home budget (USD) Welcome period in Poland $350 12\"\n",
    "            _context_relevance_scores_per_query.append(\n",
    "                trulens_context_relevance(query, context)\n",
    "            )\n",
    "            _groundedness_scores_per_query.append(\n",
    "                trulens_groundedness(context, llm_response)\n",
    "            )\n",
    "\n",
    "        context_relevance_scores.append(\n",
    "            sum(_context_relevance_scores_per_query)\n",
    "            / len(_context_relevance_scores_per_query)\n",
    "        )\n",
    "        groundedness_scores.append(\n",
    "            sum(_groundedness_scores_per_query)\n",
    "            / len(_groundedness_scores_per_query)\n",
    "        )\n",
    "    else:\n",
    "        context_relevance_scores.append(0)\n",
    "        groundedness_scores.append(0)\n",
    "assert (\n",
    "    len(context_relevance_scores)\n",
    "    == len(answer_relevance_scores)\n",
    "    == len(groundedness_scores)\n",
    "    == len(cortex_eval_df)\n",
    "    == len(answer_relevance_cot_scores)\n",
    ")\n",
    "\n",
    "# save scores to csv\n",
    "cortex_eval_df[\"context_relevance_scores\"] = context_relevance_scores\n",
    "cortex_eval_df[\"answer_relevance_scores\"] = answer_relevance_scores\n",
    "cortex_eval_df[\"groundedness_scores\"] = groundedness_scores\n",
    "cortex_eval_df[\"answer_relevance_scores_cot\"] = answer_relevance_cot_scores\n",
    "cortex_eval_df.to_csv(\n",
    "    \"cortex_eval_df_with_trulens_scores_relevance_cot.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    cortex_eval_df[\"answer_relevance_scores\"]\n",
    "    - cortex_eval_df[\"answer_relevance_scores_cot\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_relevance_mean = (\n",
    "    cortex_eval_df[\"answer_relevance_scores_cot\"].mean() * 100\n",
    ")\n",
    "context_relevance_mean = cortex_eval_df[\"context_relevance_scores\"].mean() * 100\n",
    "groundedness_mean = cortex_eval_df[\"groundedness_scores\"].mean() * 100\n",
    "\n",
    "print(f\"Answer Relevance Mean Score: {answer_relevance_mean:.2f}%\")\n",
    "print(f\"Context Relevance Mean Score: {context_relevance_mean:.2f}%\")\n",
    "print(f\"Groundedness Mean Score: {groundedness_mean:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation test with Cortex's GT based metrics\n",
    "#### \n",
    "Cortex GT-based metrics:\n",
    "\n",
    "accuracy_llm: {-1, 0, 1, 2}, llm_citation_f1 [-1, 1.0], gt_citation_f1 [-1, 1.0], gris_llm_answer [0.0, 1.0] <-> answer relevance, context relevance, and groundedness (Likert 4)\n",
    "\n",
    "anls, gris_anls [0.0, 1.0] \n",
    "retrieval_ndcg_at_1 [0.0, 1.0], retrieval_hit_rate_at_1 BINARY, retrieval_ndcg_at_3 [0.0, 1.0], retrieval_hit_rate_at_3 BINARY <-> context relevance \n",
    "\n",
    "adjusted_llm_answer vs llm_answer?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cortex_eval_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cortex_eval_df[\"context_relevance_scores_binary\"] = cortex_eval_df[\n",
    "    \"context_relevance_scores\"\n",
    "].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "cortex_eval_df[\"answer_relevance_scores_binary\"] = cortex_eval_df[\n",
    "    \"answer_relevance_scores_cot\"\n",
    "].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "cortex_eval_df[\"groundedness_scores_binary\"] = cortex_eval_df[\n",
    "    \"groundedness_scores\"\n",
    "].apply(lambda x: 1 if x >= 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "# Calculate Matthews correlation coefficient for retrieval_hit_at_1 and context_relevance_scores_binary\n",
    "mcc_hit_at_1_context_relevance = matthews_corrcoef(\n",
    "    cortex_eval_df[\"retrieval_hit_at_1\"],\n",
    "    cortex_eval_df[\"context_relevance_scores_binary\"],\n",
    ")\n",
    "cohens_kappa_context_relevance = cohen_kappa_score(\n",
    "    cortex_eval_df[\"retrieval_hit_at_1\"],\n",
    "    cortex_eval_df[\"context_relevance_scores_binary\"],\n",
    ")\n",
    "print(\n",
    "    f\"Matthews Correlation Coefficient (retrieval_hit_at_1 vs context_relevance_scores_binary): {mcc_hit_at_1_context_relevance:.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Cohen's Kappa (retrieval_hit_at_1 vs context_relevance_scores_binary): {cohens_kappa_context_relevance:.4f}\"\n",
    ")\n",
    "\n",
    "# Calculate Matthews correlation coefficient for retrieval_hit_at_3 and context_relevance_scores_binary\n",
    "mcc_hit_at_3_context_relevance = matthews_corrcoef(\n",
    "    cortex_eval_df[\"retrieval_hit_at_3\"],\n",
    "    cortex_eval_df[\"context_relevance_scores_binary\"],\n",
    ")\n",
    "cohens_kappa_context_relevance = cohen_kappa_score(\n",
    "    cortex_eval_df[\"retrieval_hit_at_3\"],\n",
    "    cortex_eval_df[\"context_relevance_scores_binary\"],\n",
    ")\n",
    "print(\n",
    "    f\"Matthews Correlation Coefficient (retrieval_hit_at_3 vs context_relevance_scores_binary): {mcc_hit_at_3_context_relevance:.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"cohen's Kappa (retrieval_hit_at_3 vs context_relevance_scores_binary): {cohens_kappa_context_relevance:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cortex_eval_df[\"accuracy_llm_normalized\"] = cortex_eval_df[\n",
    "    \"accuracy_llm\"\n",
    "].apply(lambda x: (x - 0) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REAL_VALUED_COLS = [\n",
    "    \"accuracy_llm_normalized\",\n",
    "    \"llm_citation_f1\",\n",
    "    \"gt_citation_f1\",\n",
    "    \"gris_llm_answer\",\n",
    "    \"anls\",\n",
    "    \"gris_anls\",\n",
    "    \"retrieval_ndcg_at_3\",\n",
    "    \"retrieval_ndcg_at_1\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the results\n",
    "results = {\n",
    "    \"Cortex GT-based metrics\": [],\n",
    "    \"Spearman correlation with context_relevance_scores\": [],\n",
    "    \"Spearman correlation with answer_relevance_scores\": [],\n",
    "    \"Spearman correlation with groundedness_scores\": [],\n",
    "}\n",
    "\n",
    "# Calculate Spearman correlations and store them in the dictionary\n",
    "for col_name in REAL_VALUED_COLS:\n",
    "    spearman_corr_context, _ = spearmanr(\n",
    "        cortex_eval_df[col_name], cortex_eval_df[\"context_relevance_scores\"]\n",
    "    )\n",
    "\n",
    "    spearman_corr_answer, _ = spearmanr(\n",
    "        cortex_eval_df[col_name], cortex_eval_df[\"answer_relevance_scores_cot\"]\n",
    "    )\n",
    "\n",
    "    spearman_corr_groundedness, _ = spearmanr(\n",
    "        cortex_eval_df[col_name], cortex_eval_df[\"groundedness_scores\"]\n",
    "    )\n",
    "\n",
    "    results[\"Cortex GT-based metrics\"].append(col_name)\n",
    "    results[\"Spearman correlation with context_relevance_scores\"].append(\n",
    "        spearman_corr_context\n",
    "    )\n",
    "    results[\"Spearman correlation with answer_relevance_scores\"].append(\n",
    "        spearman_corr_answer\n",
    "    )\n",
    "    results[\"Spearman correlation with groundedness_scores\"].append(\n",
    "        spearman_corr_groundedness\n",
    "    )\n",
    "\n",
    "# Convert the dictionary to a pandas DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\n",
    "    cortex_eval_df[\"answer_relevance_scores_binary\"],\n",
    "    cortex_eval_df[\"anls\"].apply(lambda x: 1 if x >= 0.5 else 0),\n",
    "    title=\"Confusion Matrix - Answer Relevance vs ANLS\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\n",
    "    cortex_eval_df[\"answer_relevance_scores_binary\"],\n",
    "    cortex_eval_df[\"accuracy_llm_normalized\"].apply(\n",
    "        lambda x: 1 if x >= 0.5 else 0\n",
    "    ),\n",
    "    title=\"Confusion Matrix - Answer Relevance vs Accuracy LLM Normalized\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with Phoenix / Arize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.feedback import GroundTruthAggregator\n",
    "\n",
    "for provider_name in [\n",
    "    \"gpt-4o-mini\",\n",
    "    \"gpt-4o\",\n",
    "    \"snowflake-arctic\",\n",
    "    \"llama3.1-8b\",\n",
    "    \"mistral-large\",\n",
    "]:\n",
    "    file_path = f\"results/Snowflake IT balanced - {provider_name}_context_relevance_binary_results.csv\"\n",
    "    scores, labels, latencies = read_results(file_path)\n",
    "\n",
    "    scores = [1 if score >= 0.5 else 0 for score in scores]\n",
    "\n",
    "    binary_labels = []\n",
    "    for label in labels:\n",
    "        if label >= 0.5:\n",
    "            binary_labels.append(1)\n",
    "        else:\n",
    "            binary_labels.append(0)\n",
    "    f_recall = GroundTruthAggregator(binary_labels).recall\n",
    "    f_precision = GroundTruthAggregator(binary_labels).precision\n",
    "    f_f1_score = GroundTruthAggregator(binary_labels).f1_score\n",
    "\n",
    "    f_cohens_kappa = GroundTruthAggregator(binary_labels).cohens_kappa\n",
    "    f_auc = GroundTruthAggregator(labels).auc\n",
    "\n",
    "    f_mae = GroundTruthAggregator(labels).mae\n",
    "    f_pearson = GroundTruthAggregator(labels).pearson_correlation\n",
    "    f_spearman = GroundTruthAggregator(labels).spearman_correlation\n",
    "\n",
    "    recall = f_recall(scores)\n",
    "    precision = f_precision(scores)\n",
    "    f1_score = f_f1_score(scores)\n",
    "    mae = f_mae(scores)\n",
    "    pearson = f_pearson(scores)\n",
    "    spearman = f_spearman(scores)\n",
    "    cohens_kappa = f_cohens_kappa(scores)\n",
    "    auc = f_auc(scores)\n",
    "\n",
    "    for latency in latencies:\n",
    "        if latency > 20:\n",
    "            # print(f\"Warning: latency is greater than 10 seconds: {latency}\")\n",
    "            latencies.remove(latency)\n",
    "    avg_latency = sum(latencies) / len(latencies)\n",
    "\n",
    "    print(\n",
    "        f\"{provider_name}: mae: {mae:.4f}, pearson: {pearson:.4f}, spearman: {spearman:.4f}, Cohen's Kappa: {cohens_kappa:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"{provider_name}: recall: {recall:.4f}, precision: {precision:.4f}, f1: {f1_score:.4f}, Cohen's Kappa: {cohens_kappa:.4f}, avg_latency: {avg_latency:.4f}\"\n",
    "    )\n",
    "    print(\"\\n\")\n",
    "\n",
    "    plot_confusion_matrix(\n",
    "        binary_labels, scores, title=f\"Confusion Matrix - {provider_name}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
