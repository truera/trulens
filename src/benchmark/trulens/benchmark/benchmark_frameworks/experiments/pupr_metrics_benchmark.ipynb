{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.benchmark.benchmark_frameworks.dataset.beir_loader import (\n",
    "    TruBEIRDataLoader,\n",
    ")\n",
    "\n",
    "beir_data_loader = TruBEIRDataLoader(data_folder=\"./\", dataset_name=\"hotpotqa\")\n",
    "hotpotqa = beir_data_loader.load_dataset_to_df(download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(42)\n",
    "hotpotqa_raw_subset = hotpotqa.sample(n=500, random_state=42)\n",
    "\n",
    "all_responses = [\n",
    "    (row[\"query\"], row[\"expected_response\"])\n",
    "    for idx, row in hotpotqa_raw_subset.iterrows()\n",
    "]\n",
    "\n",
    "all_contexts = [\n",
    "    (row[\"query\"], context[\"text\"])\n",
    "    for idx, row in hotpotqa_raw_subset.iterrows()\n",
    "    for context in row[\"expected_chunks\"]\n",
    "]\n",
    "\n",
    "answer_relevance_dataset = []\n",
    "\n",
    "\n",
    "for idx, row in hotpotqa_raw_subset.iterrows():\n",
    "    # Positive examples for answer relevance\n",
    "    answer_relevance_dataset.append({\n",
    "        \"query\": row[\"query\"],\n",
    "        \"response\": row[\"expected_response\"],  # Positive response\n",
    "        \"expected_score\": 1,  # Positive example, score = 1\n",
    "    })\n",
    "\n",
    "    # Negative examples for answer relevance (random unrelated response)\n",
    "    negative_response = random.choice([\n",
    "        r\n",
    "        for q, r in all_responses\n",
    "        if q != row[\"query\"]  # Pick response from another query\n",
    "    ])\n",
    "\n",
    "    answer_relevance_dataset.append({\n",
    "        \"query\": row[\"query\"],\n",
    "        \"response\": negative_response,  # Negative response\n",
    "        \"expected_score\": 0,  # Negative example, score = 0\n",
    "    })\n",
    "answer_relevance_df = pd.DataFrame(answer_relevance_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_relevance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    generate_trec_dl_passage_benchmark,\n",
    ")\n",
    "\n",
    "trec_2021_samples = list(\n",
    "    generate_trec_dl_passage_benchmark(\n",
    "        max_samples_per_query_per_score=4,\n",
    "        dataset_path=\"msmarco-passage-v2/trec-dl-2021/judged\",\n",
    "    )\n",
    ")\n",
    "trec_2022_samples = list(\n",
    "    generate_trec_dl_passage_benchmark(\n",
    "        max_samples_per_query_per_score=4,\n",
    "        dataset_path=\"msmarco-passage-v2/trec-dl-2022/judged\",\n",
    "    )\n",
    ")\n",
    "trec_combined = trec_2021_samples + trec_2022_samples\n",
    "\n",
    "\n",
    "context_relevance_df = pd.DataFrame(trec_combined)\n",
    "context_relevance_df.to_csv(\n",
    "    \"trec_dl_2021_2022_combined_scoreddocs_intervals.csv\", index=False\n",
    ")\n",
    "\n",
    "context_relevance_df = pd.read_csv(\n",
    "    \"trec_dl_2021_2022_combined_scoreddocs_intervals.csv\"\n",
    ")\n",
    "print(f\"Totoal number of samples: {len(context_relevance_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_relevance_df[\"context\"] = context_relevance_df[\"expected_response\"]\n",
    "context_relevance_df[\"expected_score\"] = context_relevance_df[\n",
    "    \"expected_score\"\n",
    "].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "context_relevance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    generate_balanced_llm_aggrefact_benchmark,\n",
    ")\n",
    "\n",
    "llm_aggrefact_df = generate_balanced_llm_aggrefact_benchmark(split=\"test\")\n",
    "# Get 500 samples from each class\n",
    "groundedness_df = (\n",
    "    llm_aggrefact_df.groupby(\"label\")\n",
    "    .apply(lambda x: x.sample(n=500, random_state=42))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Verify the balance\n",
    "print(groundedness_df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundedness_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundedness_df[\"context\"] = groundedness_df[\"doc\"]\n",
    "groundedness_df[\"response\"] = groundedness_df[\"claim\"]\n",
    "groundedness_df[\"expected_score\"] = groundedness_df[\"label\"]\n",
    "groundedness_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    visualize_expected_score_distribution,\n",
    ")\n",
    "\n",
    "visualize_expected_score_distribution(answer_relevance_df[\"expected_score\"])\n",
    "visualize_expected_score_distribution(context_relevance_df[\"expected_score\"])\n",
    "visualize_expected_score_distribution(groundedness_df[\"expected_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_relevance_df[\n",
    "    \"expected_score\"\n",
    "].value_counts()  # best effort to balance the classes, given the data annotation quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.feedback.groundtruth import GroundTruthAggregator\n",
    "\n",
    "f_recall = GroundTruthAggregator(\n",
    "    answer_relevance_df[\"expected_score\"].to_list()\n",
    ").recall\n",
    "f_precision = GroundTruthAggregator(\n",
    "    answer_relevance_df[\"expected_score\"].to_list()\n",
    ").precision\n",
    "f_f1_score = GroundTruthAggregator(\n",
    "    answer_relevance_df[\"expected_score\"].to_list()\n",
    ").f1_score\n",
    "f_cohens_kappa = GroundTruthAggregator(\n",
    "    answer_relevance_df[\"expected_score\"].to_list()\n",
    ").cohens_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install krippendorff\n",
    "import krippendorff\n",
    "import numpy as np\n",
    "\n",
    "ratings_with_nan = np.array([\n",
    "    [1, 2, np.nan, 4, 5],  # llm judge scores\n",
    "    [1, 2, 3, 4, np.nan],\n",
    "    [1, 2, 3, 4, 5],\n",
    "])\n",
    "\n",
    "alpha = krippendorff.alpha(ratings_with_nan, level_of_measurement=\"ordinal\")\n",
    "print(f\"Krippendorff's Alpha (with missing data): {alpha}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
