{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TruLens vs RAGAS comparison\n",
    "\n",
    "RAGAS vs TruLens' equivalents\n",
    "\n",
    "faithfulness <-> groundedness\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.cost import get_token_usage_for_openai\n",
    "from ragas.llms import llm_factory\n",
    "from ragas.metrics import faithfulness\n",
    "from trulens.providers.openai import OpenAI\n",
    "import numpy as np\n",
    "\n",
    "langchain_llm = llm_factory(model=\"gpt-4o-mini\")\n",
    "\n",
    "faithfulness.llm = langchain_llm\n",
    "\n",
    "gpt_4o_mini = OpenAI(model_engine=\"gpt-4o-mini\")\n",
    "likert4_groundedness_criteria = \"\"\"You should score the groundedness of the statement based on the following criteria:\n",
    "    - Statements that are directly supported by the source should be considered grounded and should get a high score.\n",
    "    - Statements that are not directly supported by the source should be considered not grounded and should get a low score.\n",
    "    - Statements of doubt, that admissions of uncertainty or not knowing the answer are considered abstention, and should be counted as the most overlap and therefore get a max score.\"\"\"\n",
    "\n",
    "# data_samples = {\n",
    "#     'question': ['When was the first super bowl?', 'Who won the most super bowls?'],\n",
    "#     'answer': ['The first superbowl was held on Jan 15, 1967', 'The most super bowls have been won by The New England Patriots'],\n",
    "#     'contexts' : [['The First AFLâ€“NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles,'],\n",
    "#     ['The Green Bay Packers...Green Bay, Wisconsin.','The Packers compete...Football Conference']],\n",
    "# }\n",
    "\n",
    "# dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "# score = evaluate(dataset,metrics=[faithfulness], llm=langchain_llm,  token_usage_parser=get_token_usage_for_openai,\n",
    "# )\n",
    "\n",
    "def trulens_groundedness(input, output) -> float:\n",
    "    return gpt_4o_mini.groundedness_measure_with_cot_reasons(\n",
    "        source=input,\n",
    "        statement=output,\n",
    "        use_sent_tokenize=True,\n",
    "        min_score_val=0,\n",
    "        max_score_val=3,\n",
    "        criteria=likert4_groundedness_criteria,\n",
    "    )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ragas_experiment(\n",
    "    dataset_df,\n",
    "):\n",
    "    data_samples = {\"question\": [], \"answer\": [], \"contexts\": []}\n",
    "    for i, row in dataset_df.iterrows():\n",
    "        data_samples[\"question\"].append(str(i))\n",
    "        data_samples[\"answer\"].append(row[\"expected_response\"])\n",
    "        data_samples[\"contexts\"].append([row[\"query\"]])\n",
    "\n",
    "    ragas_dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "    score = evaluate(\n",
    "        ragas_dataset,\n",
    "        metrics=[faithfulness],\n",
    "        llm=langchain_llm,\n",
    "        token_usage_parser=get_token_usage_for_openai,\n",
    "    )\n",
    "    avg_cost = (\n",
    "        score.total_cost(\n",
    "            cost_per_input_token=0.15 / 1e6, cost_per_output_token=0.6 / 1e6\n",
    "        )\n",
    "        / 200\n",
    "    )\n",
    "    print(f\"Average cost per sample: {avg_cost}\")\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def trulens_experiment(\n",
    "    dataset_df,\n",
    "):\n",
    "    data_samples = {\"question\": [], \"answer\": [], \"contexts\": []}\n",
    "    for i, row in dataset_df.iterrows():\n",
    "        data_samples[\"question\"].append(str(i))\n",
    "        data_samples[\"answer\"].append(row[\"expected_response\"])\n",
    "        data_samples[\"contexts\"].append([row[\"query\"]])\n",
    "\n",
    "    ff_scores = []\n",
    "    for i in range(len(data_samples[\"contexts\"])):\n",
    "        ff_scores.append(\n",
    "            trulens_groundedness(\n",
    "                data_samples[\"contexts\"][i][0], data_samples[\"answer\"][i]\n",
    "            )\n",
    "        )\n",
    "    \n",
    "\n",
    "    ff_scores = np.array(ff_scores)\n",
    "    return ff_scores\n",
    "\n",
    "\n",
    "ragas_cnn_score = ragas_experiment(qags_cnn_dm)\n",
    "ragas_xsum_score = ragas_experiment(qags_xsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_cnn_score.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trulens_cnn_scores, cnn_labels, latencies = read_results(\n",
    "    \"/Users/dhuang/Documents/git/trulens/src/benchmark/trulens/benchmark/benchmark_frameworks/experiments/results/QAGS CNN_DM - gpt-4o-mini_groundedness_likert4_results.csv\"\n",
    ")\n",
    "trulens_xsum_scores, xsum_labels, latencies = read_results(\n",
    "    \"/Users/dhuang/Documents/git/trulens/src/benchmark/trulens/benchmark/benchmark_frameworks/experiments/results/QAGS XSum - gpt-4o-mini_groundedness_likert4_results.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "true_scores = np.array(cnn_labels)\n",
    "mae_trulens = np.mean(np.abs(trulens_cnn_scores - true_scores))\n",
    "mae_ragas = np.mean(\n",
    "    np.abs(\n",
    "        ragas_cnn_score.to_pandas()[\"faithfulness\"] - qags_cnn_dm_true_labels\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Trulens MAE: {mae_trulens:.4f}, Ragas MAE: {mae_ragas:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summeval_ragas_data_samples = {\"question\": [], \"answer\": [], \"contexts\": []}\n",
    "for i, row in summeval_subset.iterrows():\n",
    "    summeval_ragas_data_samples[\"question\"].append(str(i))\n",
    "    summeval_ragas_data_samples[\"answer\"].append(row[\"expected_response\"])\n",
    "    summeval_ragas_data_samples[\"contexts\"].append([row[\"query\"]])\n",
    "\n",
    "summeval_ragas_dataset = Dataset.from_dict(summeval_ragas_data_samples)\n",
    "\n",
    "score = evaluate(\n",
    "    summeval_ragas_dataset,\n",
    "    metrics=[faithfulness],\n",
    "    llm=langchain_llm,\n",
    "    token_usage_parser=get_token_usage_for_openai,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_cost = (\n",
    "    score.total_cost(\n",
    "        cost_per_input_token=0.15 / 1e6, cost_per_output_token=0.6 / 1e6\n",
    "    )\n",
    "    / 200\n",
    ")\n",
    "avg_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_scores = []\n",
    "for i in range(len(summeval_ragas_data_samples[\"contexts\"])):\n",
    "    ff_scores.append(\n",
    "        trulens_groundedness(\n",
    "            summeval_ragas_data_samples[\"contexts\"][i][0],\n",
    "            summeval_ragas_data_samples[\"answer\"][i],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ff_scores = np.array(ff_scores)\n",
    "ragas_scores = np.array(score.to_pandas()[\"faithfulness\"])\n",
    "\n",
    "true_scores = np.array(summeval_subset_true_labels)\n",
    "mae_trulens = np.mean(np.abs(ff_scores - true_scores))\n",
    "mae_ragas = np.mean(np.abs(ragas_scores - true_scores))\n",
    "\n",
    "print(f\"Trulens MAE: {mae_trulens:.4f}, Ragas MAE: {mae_ragas:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
