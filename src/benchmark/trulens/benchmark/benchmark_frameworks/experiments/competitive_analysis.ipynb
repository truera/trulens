{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TruLens vs RAGAS vs MLFlow performance comparison for groundedness\n",
    "\n",
    "In this notebook, we analyze the performance of TruLens current groundedness feedback function and its comparable or equivalent implementations from other evaluation frameworks using the same model for LLM-as-judges. \n",
    "\n",
    "\n",
    "### Definitions\n",
    "1. TruLens `groundedness`: evaluates whether a response is fully supported by the source or retrieved contexts.\n",
    "\n",
    "2. RAGAS `faithfulness`: measures the factual consistency of the generated answer against the given context [source](https://docs.ragas.io/en/stable/concepts/metrics/faithfulness.html)\n",
    "\n",
    "3. MLflow `faithfulness`: Faithfulness will be assessed based on how factually consistent the output is to the context\n",
    "[source](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.faithfulness)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install trulens-core trulens-providers-openai ragas mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from trulens.core.session import TruSession\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "connection_params = {\n",
    "    \"account\": os.environ.get(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    \"user\": os.environ.get(\"SNOWFLAKE_USER\"),\n",
    "    \"password\": os.environ.get(\"SNOWFLAKE_USER_PASSWORD\"),\n",
    "    \"database\": os.environ.get(\"SNOWFLAKE_DATABASE\"),\n",
    "    \"schema\": os.environ.get(\"SNOWFLAKE_SCHEMA\"),\n",
    "    \"warehouse\": os.environ.get(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "    \"role\": os.environ.get(\"SNOWFLAKE_ROLE\"),\n",
    "    \"init_server_side\": False,  # Set to True to enable server side feedback functions\n",
    "}\n",
    "\n",
    "# connector = SnowflakeConnector(**connection_params)\n",
    "# session = TruSession(connector=connector)\n",
    "\n",
    "session = TruSession()\n",
    "session.reset_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare 3 public benchmark datasets: QAGS CNN/Daily Mail, QAGS XSum, and SummEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    generate_qags_golden_set_groundedness,\n",
    ")\n",
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    generate_summeval_groundedness_golden_set,\n",
    ")\n",
    "\n",
    "qags_cnn_dm = pd.DataFrame(\n",
    "    list(generate_qags_golden_set_groundedness(\"data/qags_mturk_cnndm.jsonl\"))\n",
    ")\n",
    "\n",
    "qags_xsum = pd.DataFrame(\n",
    "    list(generate_qags_golden_set_groundedness(\"data/qags_mturk_xsum.jsonl\"))\n",
    ")\n",
    "\n",
    "summeval = pd.DataFrame(\n",
    "    list(\n",
    "        generate_summeval_groundedness_golden_set(\n",
    "            \"data/summeval_test.json\", max_samples_per_bucket=200\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "summeval_true_labels = [row[\"expected_score\"] for _, row in summeval.iterrows()]\n",
    "\n",
    "summeval_true_labels_binary = [\n",
    "    1 if label >= 0.5 else 0 for label in summeval_true_labels\n",
    "]\n",
    "\n",
    "qags_cnn_dm_true_labels = [\n",
    "    row[\"expected_score\"] for _, row in qags_cnn_dm.iterrows()\n",
    "]\n",
    "\n",
    "qags_cnn_dm_true_labels_binary = [\n",
    "    1 if label >= 0.5 else 0 for label in qags_cnn_dm_true_labels\n",
    "]\n",
    "\n",
    "qags_xsum_true_labels = [\n",
    "    row[\"expected_score\"] for _, row in qags_xsum.iterrows()\n",
    "]\n",
    "\n",
    "qags_xsum_true_labels_binary = [\n",
    "    1 if label >= 0.5 else 0 for label in qags_xsum_true_labels\n",
    "]\n",
    "combined_dataset = pd.concat(\n",
    "    [qags_cnn_dm, qags_xsum, summeval], ignore_index=False\n",
    ")\n",
    "combined_true_labels = (\n",
    "    qags_cnn_dm_true_labels + qags_xsum_true_labels + summeval_true_labels\n",
    ")\n",
    "\n",
    "assert len(combined_dataset) == len(combined_true_labels)\n",
    "print(f\"Total number of samples: {len(combined_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    visualize_expected_score_distribution,\n",
    ")\n",
    "\n",
    "# making sure the distribution of the expected scores is as balanced as possible for the datasets\n",
    "visualize_expected_score_distribution(combined_true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup experiments with TruLens `TruBasicApp` recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import mlflow\n",
    "from mlflow.metrics.genai import answer_relevance as answer_relevance_mlflow\n",
    "from mlflow.metrics.genai import faithfulness as faithfulness_mlflow\n",
    "from mlflow.metrics.genai import relevance as context_relevance_mlflow\n",
    "from ragas import evaluate\n",
    "from ragas.cost import get_token_usage_for_openai\n",
    "from ragas.llms import llm_factory\n",
    "from ragas.metrics import faithfulness as faithfulness_ragas\n",
    "from trulens.apps.basic import TruBasicApp\n",
    "from trulens.core import Feedback\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "OPENAI_LLM_NAME = \"gpt-4o-mini\"\n",
    "gpt_4o_mini = OpenAI(model_engine=OPENAI_LLM_NAME)\n",
    "\n",
    "\n",
    "def trulens_groundedness(context: str, response: str, gt_score: float) -> str:\n",
    "    trulens_groundedness_res = (\n",
    "        gpt_4o_mini.groundedness_measure_with_cot_reasons(\n",
    "            source=context, statement=response, use_sent_tokenize=True\n",
    "        )\n",
    "    )\n",
    "    return f\"{trulens_groundedness_res[0]};{gt_score};{trulens_groundedness_res[1]}\"\n",
    "\n",
    "\n",
    "langchain_llm = llm_factory(model=OPENAI_LLM_NAME)\n",
    "faithfulness_ragas.llm = langchain_llm\n",
    "\n",
    "\n",
    "def ragas_faithfulness(context: str, response: str, gt_score: float) -> str:\n",
    "    data_samples = {\"question\": [], \"answer\": [], \"contexts\": []}\n",
    "    data_samples[\"question\"].append(\"dummy text\")\n",
    "    data_samples[\"answer\"].append(response)\n",
    "    data_samples[\"contexts\"].append([context])\n",
    "    ragas_dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "    score_dict = evaluate(\n",
    "        ragas_dataset,\n",
    "        metrics=[faithfulness_ragas],\n",
    "        llm=langchain_llm,\n",
    "        token_usage_parser=get_token_usage_for_openai,\n",
    "    )\n",
    "\n",
    "    return f\"{score_dict['faithfulness'][0]};{gt_score}\"\n",
    "\n",
    "\n",
    "# not supplying any example as other metrics do zero-shot evaluation as well\n",
    "faithfulness_metric = faithfulness_mlflow(model=f\"openai:/{OPENAI_LLM_NAME}\")\n",
    "context_relevance_metric = context_relevance_mlflow(\n",
    "    model=f\"openai:/{OPENAI_LLM_NAME}\"\n",
    ")\n",
    "\n",
    "answer_relevance_metric = answer_relevance_mlflow(\n",
    "    model=f\"openai:/{OPENAI_LLM_NAME}\"\n",
    ")\n",
    "\n",
    "\n",
    "def mlflow_context_relevance(\n",
    "    query: str, context: str, response: str, gt_score: float\n",
    ") -> str:\n",
    "    eval_data = pd.DataFrame({\n",
    "        \"inputs\": [query],\n",
    "        \"predictions\": [\n",
    "            response\n",
    "        ],  # note that we are using the response as the prediction\n",
    "        \"context\": [context],\n",
    "    })\n",
    "\n",
    "    with mlflow.start_run() as _:\n",
    "        results = mlflow.evaluate(\n",
    "            data=eval_data,\n",
    "            predictions=\"predictions\",\n",
    "            extra_metrics=[context_relevance_metric],\n",
    "            evaluators=\"default\",\n",
    "        )\n",
    "\n",
    "    mlflow_context_relevance_score = results.metrics[\"relevance/v1/mean\"]\n",
    "\n",
    "    mlflow_context_relevance_score_norm = (\n",
    "        mlflow_context_relevance_score - 1\n",
    "    ) / 4.0  # normalizing the score to be between 0 and 1\n",
    "\n",
    "    return f\"{mlflow_context_relevance_score_norm};{gt_score}\"\n",
    "\n",
    "\n",
    "def mlflow_answer_relevance(query: str, response: str, gt_score: float) -> str:\n",
    "    eval_data = pd.DataFrame({\n",
    "        \"inputs\": [query],\n",
    "        \"predictions\": [response],\n",
    "        \"context\": [\n",
    "            \"dummy text\"\n",
    "        ],  # we are not using the context for answer relevance evaluation\n",
    "    })\n",
    "\n",
    "    with mlflow.start_run() as _:\n",
    "        results = mlflow.evaluate(\n",
    "            data=eval_data,\n",
    "            predictions=\"predictions\",\n",
    "            extra_metrics=[answer_relevance_metric],\n",
    "            evaluators=\"default\",\n",
    "        )\n",
    "\n",
    "    mlflow_answer_relevance_score = results.metrics[\"answer_relevance/v1/mean\"]\n",
    "\n",
    "    mlflow_answer_relevance_score_norm = (\n",
    "        mlflow_answer_relevance_score - 1\n",
    "    ) / 4.0  # normalizing the score to be between 0 and 1\n",
    "\n",
    "    return f\"{mlflow_answer_relevance_score_norm};{gt_score}\"\n",
    "\n",
    "\n",
    "def mlflow_faithfulness(context: str, response: str, gt_score: float) -> str:\n",
    "    eval_data = pd.DataFrame({\n",
    "        \"inputs\": [\n",
    "            \"dummy text\"  # we are not using the inputs (user's queries) for faithfulness evaluation\n",
    "        ],\n",
    "        \"predictions\": [response],\n",
    "        \"context\": [context],\n",
    "    })\n",
    "\n",
    "    with mlflow.start_run() as _:\n",
    "        results = mlflow.evaluate(\n",
    "            data=eval_data,\n",
    "            predictions=\"predictions\",\n",
    "            extra_metrics=[faithfulness_metric],\n",
    "            evaluators=\"default\",\n",
    "        )\n",
    "\n",
    "    mlflow_faithfulness_score = results.metrics[\"faithfulness/v1/mean\"]\n",
    "\n",
    "    mlflow_faithfulness_score_norm = (\n",
    "        mlflow_faithfulness_score - 1\n",
    "    ) / 4.0  # normalizing the score to be between 0 and 1\n",
    "\n",
    "    return f\"{mlflow_faithfulness_score_norm};{gt_score}\"\n",
    "\n",
    "\n",
    "def run_experiment_and_record(\n",
    "    evaluate_func_wrapper, app_name, app_version, dataset_df, true_labels\n",
    "):\n",
    "    if len(dataset_df) != len(true_labels):\n",
    "        raise ValueError(\"dataset df must have the same length as labels\")\n",
    "\n",
    "    tru_wrapped_basic_app = TruBasicApp(\n",
    "        evaluate_func_wrapper,\n",
    "        app_name=app_name,\n",
    "        app_version=app_version,\n",
    "        feedbacks=CUSTOM_FEEDBACK_FUNCS,\n",
    "    )\n",
    "\n",
    "    for i in range(len(dataset_df)):\n",
    "        arg_1 = dataset_df.iloc[i][\"query\"]\n",
    "        arg_2 = dataset_df.iloc[i][\"expected_response\"]\n",
    "        arg_3 = true_labels[i]\n",
    "\n",
    "        try:\n",
    "            with tru_wrapped_basic_app as _:\n",
    "                tru_wrapped_basic_app.app(arg_1, arg_2, arg_3)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error {e} in run_feedback_experiment row {i} with first arg {arg_1} and second arg {arg_2}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXP_NAME = \"competitive-analysis-10312024\"\n",
    "# run_experiment_and_record(\n",
    "#     evaluate_func_wrapper=trulens_groundedness,\n",
    "#     app_version=\"trulens-groundedness\",\n",
    "#     app_name=EXP_NAME,\n",
    "#     dataset_df=combined_dataset,\n",
    "#     true_labels=combined_true_labels,\n",
    "# )\n",
    "\n",
    "# run_experiment_and_record(\n",
    "#     evaluate_func_wrapper=ragas_faithfulness,\n",
    "#     app_version=\"ragas-faithfulness\",\n",
    "#     app_name=EXP_NAME,\n",
    "#     dataset_df=combined_dataset,\n",
    "#     true_labels=combined_true_labels,\n",
    "# )\n",
    "\n",
    "# run_experiment_and_record(\n",
    "#     evaluate_func_wrapper=mlflow_faithfulness,\n",
    "#     app_version=\"mlflow-faithfulness\",\n",
    "#     app_name=EXP_NAME,\n",
    "#     dataset_df=combined_dataset,\n",
    "#     true_labels=combined_true_labels,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.get_leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.run_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note about column name mapping: in all our dataframes (CNN/DM, XSUM, and SummEval), the \"expected_score\" column is the ground truth (true) label for the groundedness score, query corresponds to the context, and expected_response corresponds to the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLflow Answer Relevance and Contex Relevance experiments \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from trulens.benchmark.benchmark_frameworks.dataset.beir_loader import (\n",
    "    TruBEIRDataLoader,\n",
    ")\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "def generate_ms_marco_context_relevance_benchmark_for_mlflow(\n",
    "    file_path: str = \"data/ms_marco_v2_1_val.parquet\",\n",
    "):\n",
    "    df = pd.read_parquet(file_path, engine=\"pyarrow\")  # or engine='fastparquet'\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        assert len(row[\"passages\"][\"is_selected\"]) == len(\n",
    "            row[\"passages\"][\"passage_text\"]\n",
    "        )\n",
    "\n",
    "        if sum(row[\"passages\"][\"is_selected\"]) < 1:\n",
    "            # currently we only consider sample with one passage marked as relevant (there are samples where zero passage_text is selected)\n",
    "            continue\n",
    "        for i, passage_text in enumerate(row[\"passages\"][\"passage_text\"]):\n",
    "            yield {\n",
    "                \"query_id\": row[\"query_id\"],\n",
    "                \"query\": row[\"query\"],\n",
    "                \"expected_response\": row[\"answers\"][0],\n",
    "                \"expected_context\": passage_text,\n",
    "                \"expected_score\": row[\"passages\"][\"is_selected\"][\n",
    "                    i\n",
    "                ],  # Binary relevance\n",
    "            }\n",
    "\n",
    "\n",
    "ms_marco = list(generate_ms_marco_context_relevance_benchmark_for_mlflow())\n",
    "\n",
    "\n",
    "score_1_entries = [entry for entry in ms_marco if entry[\"expected_score\"] == 1]\n",
    "score_0_entries = [entry for entry in ms_marco if entry[\"expected_score\"] == 0]\n",
    "\n",
    "# Calculate the number of samples needed from each group\n",
    "num_samples_per_group = min(\n",
    "    len(score_1_entries), len(score_0_entries), 150\n",
    ")  # Sample 150 from each\n",
    "\n",
    "\n",
    "sampled_score_1 = random.sample(score_1_entries, num_samples_per_group)\n",
    "sampled_score_0 = random.sample(score_0_entries, num_samples_per_group)\n",
    "\n",
    "# Combine and shuffle the samples to get a balanced dataset\n",
    "balanced_sample = sampled_score_1 + sampled_score_0\n",
    "random.shuffle(balanced_sample)\n",
    "\n",
    "# Ensure the combined length is 300\n",
    "assert len(balanced_sample) == 300\n",
    "\n",
    "# Now you can use `balanced_sample` as your final dataset\n",
    "print(\n",
    "    f\"Number of entries with expected_score = 1: {len([e for e in balanced_sample if e['expected_score'] == 1])}\"\n",
    ")\n",
    "print(\n",
    "    f\"Number of entries with expected_score = 0: {len([e for e in balanced_sample if e['expected_score'] == 0])}\"\n",
    ")\n",
    "\n",
    "ms_marco_balanced_sample_300 = pd.DataFrame(balanced_sample)\n",
    "\n",
    "\n",
    "beir_data_loader = TruBEIRDataLoader(data_folder=\"./\", dataset_name=\"hotpotqa\")\n",
    "hotpotqa = beir_data_loader.load_dataset_to_df(download=True)\n",
    "\n",
    "\n",
    "hotpotqa_raw_subset = hotpotqa.sample(n=200, random_state=42)\n",
    "\n",
    "all_responses = [\n",
    "    (row[\"query\"], row[\"expected_response\"])\n",
    "    for idx, row in hotpotqa_raw_subset.iterrows()\n",
    "]\n",
    "\n",
    "\n",
    "hotpotqa_subset_for_answer_relevance = []\n",
    "\n",
    "for idx, row in hotpotqa_raw_subset.iterrows():\n",
    "    # Positive examples for answer relevance\n",
    "    hotpotqa_subset_for_answer_relevance.append({\n",
    "        \"query\": row[\"query\"],\n",
    "        \"expected_response\": row[\"expected_response\"],  # Positive response\n",
    "        \"expected_score\": 1,  # Positive example, score = 1\n",
    "    })\n",
    "\n",
    "    # Negative examples for answer relevance (random unrelated response)\n",
    "    negative_response = random.choice([\n",
    "        r\n",
    "        for q, r in all_responses\n",
    "        if q != row[\"query\"]  # Pick response from another query\n",
    "    ])\n",
    "\n",
    "    hotpotqa_subset_for_answer_relevance.append({\n",
    "        \"query\": row[\"query\"],\n",
    "        \"expected_response\": negative_response,  # Negative response\n",
    "        \"expected_score\": 0,  # Negative example, score = 0\n",
    "    })\n",
    "\n",
    "\n",
    "hotpotqa_subset_for_answer_relevance_true_labels = [\n",
    "    entry[\"expected_score\"] for entry in hotpotqa_subset_for_answer_relevance\n",
    "]\n",
    "\n",
    "hotpotqa_subset_for_answer_relevance = pd.DataFrame(\n",
    "    hotpotqa_subset_for_answer_relevance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_marco_balanced_sample_300_true_labels = ms_marco_balanced_sample_300[\n",
    "    \"expected_score\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotpotqa_subset_for_answer_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_marco_balanced_sample_300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from trulens.core.session import TruSession\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "\n",
    "session = TruSession()\n",
    "session.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import Provider\n",
    "\n",
    "THRESHOLD = 0.5  # for passage retrieval annotation, we consider a score of 0.5 or above as relevant\n",
    "\n",
    "\n",
    "class CustomTermFeedback(Provider):\n",
    "    def true_positive(self, output: str) -> float:\n",
    "        feedback_score, gt_score = (\n",
    "            float(output.split(\";\")[0]),\n",
    "            float(output.split(\";\")[1]),\n",
    "        )\n",
    "        binary_score = 1 if feedback_score >= 0.5 else 0\n",
    "        binary_gt_score = 1 if gt_score >= THRESHOLD else 0\n",
    "        return 1.0 if binary_score == 1 and binary_gt_score == 1 else 0.0\n",
    "\n",
    "    def true_negative(self, output: str) -> float:\n",
    "        feedback_score, gt_score = (\n",
    "            float(output.split(\";\")[0]),\n",
    "            float(output.split(\";\")[1]),\n",
    "        )\n",
    "        binary_score = 1 if feedback_score >= 0.5 else 0\n",
    "        binary_gt_score = 1 if gt_score >= THRESHOLD else 0\n",
    "        return 1.0 if binary_score == 0 and binary_gt_score == 0 else 0.0\n",
    "\n",
    "    def false_positive(self, output: str) -> float:\n",
    "        feedback_score, gt_score = (\n",
    "            float(output.split(\";\")[0]),\n",
    "            float(output.split(\";\")[1]),\n",
    "        )\n",
    "        binary_score = 1 if feedback_score >= 0.5 else 0\n",
    "        binary_gt_score = 1 if gt_score >= THRESHOLD else 0\n",
    "        return 1.0 if binary_score == 1 and binary_gt_score == 0 else 0.0\n",
    "\n",
    "    def false_negative(self, output: str) -> float:\n",
    "        feedback_score, gt_score = (\n",
    "            float(output.split(\";\")[0]),\n",
    "            float(output.split(\";\")[1]),\n",
    "        )\n",
    "        binary_score = 1 if feedback_score >= 0.5 else 0\n",
    "        binary_gt_score = 1 if gt_score >= THRESHOLD else 0\n",
    "        return 1.0 if binary_score == 0 and binary_gt_score == 1 else 0.0\n",
    "\n",
    "    def term_absolute_error(self, output: str) -> float:\n",
    "        feedback_score, gt_score = (\n",
    "            float(output.split(\";\")[0]),\n",
    "            float(output.split(\";\")[1]),\n",
    "        )\n",
    "        return abs(feedback_score - gt_score)\n",
    "\n",
    "    def raw_gt_score(self, output: str) -> float:\n",
    "        return float(output.split(\";\")[1]) * 3\n",
    "\n",
    "    def raw_feedback_score(self, output: str) -> float:\n",
    "        return float(output.split(\";\")[0]) * 3\n",
    "\n",
    "\n",
    "custom_term_feedback = CustomTermFeedback()\n",
    "\n",
    "f_tp = Feedback(\n",
    "    custom_term_feedback.true_positive,\n",
    "    name=\"True Positive\",\n",
    "    higher_is_better=True,\n",
    ").on_output()\n",
    "f_tn = Feedback(\n",
    "    custom_term_feedback.true_negative,\n",
    "    name=\"True Negative\",\n",
    "    higher_is_better=True,\n",
    ").on_output()\n",
    "f_fp = Feedback(\n",
    "    custom_term_feedback.false_positive,\n",
    "    name=\"False Positive\",\n",
    "    higher_is_better=False,\n",
    ").on_output()\n",
    "f_fn = Feedback(\n",
    "    custom_term_feedback.false_negative,\n",
    "    name=\"False Negative\",\n",
    "    higher_is_better=False,\n",
    ").on_output()\n",
    "f_abs_err = Feedback(\n",
    "    custom_term_feedback.term_absolute_error,\n",
    "    name=\"Absolute Error\",\n",
    "    higher_is_better=False,\n",
    ").on_output()\n",
    "f_raw_gt_score = Feedback(\n",
    "    custom_term_feedback.raw_gt_score,\n",
    "    name=\"Raw GT Score\",\n",
    "    higher_is_better=True,\n",
    ").on_output()\n",
    "f_raw_feedback_score = Feedback(\n",
    "    custom_term_feedback.raw_feedback_score,\n",
    "    name=\"Raw Feedback Score\",\n",
    "    higher_is_better=True,\n",
    ").on_output()\n",
    "\n",
    "CUSTOM_FEEDBACK_FUNCS = [\n",
    "    f_tp,\n",
    "    f_tn,\n",
    "    f_fp,\n",
    "    f_fn,\n",
    "    f_abs_err,\n",
    "    f_raw_gt_score,\n",
    "    f_raw_feedback_score,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tru_wrapped_answer_relevance_app = TruBasicApp(\n",
    "#     mlflow_answer_relevance,\n",
    "#     app_name=\"competitive-analysis-mlflow-11022024\",\n",
    "#     app_version=\"mlflow-answer-relevance\",\n",
    "#     feedbacks=CUSTOM_FEEDBACK_FUNCS,\n",
    "# )\n",
    "\n",
    "# for i in range(len(hotpotqa_subset_for_answer_relevance)):\n",
    "#     arg_1 = hotpotqa_subset_for_answer_relevance.iloc[i][\"query\"]\n",
    "#     arg_2 = hotpotqa_subset_for_answer_relevance.iloc[i][\"expected_response\"]\n",
    "#     arg_3 = hotpotqa_subset_for_answer_relevance_true_labels[i]\n",
    "\n",
    "#     try:\n",
    "#         with tru_wrapped_answer_relevance_app as _:\n",
    "#             tru_wrapped_answer_relevance_app.app(arg_1, arg_2, arg_3)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(\n",
    "#             f\"Error {e} in run_feedback_experiment row {i} with first arg {arg_1} and second arg {arg_2}\"\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_marco_balanced_sample_300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from trulens.apps.basic import TruBasicApp\n",
    "\n",
    "# tru_wrapped_answer_relevance_app = TruBasicApp(\n",
    "#     mlflow_context_relevance,\n",
    "#     app_name=\"competitive-analysis-mlflow-11022024\",\n",
    "#     app_version=\"mlflow-context-relevance\",\n",
    "#     feedbacks=CUSTOM_FEEDBACK_FUNCS,\n",
    "# )\n",
    "\n",
    "# for i in range(len(ms_marco_balanced_sample_300)):\n",
    "#     arg_1 = ms_marco_balanced_sample_300.iloc[i][\"query\"]\n",
    "#     arg_2 = ms_marco_balanced_sample_300.iloc[i][\"expected_context\"]\n",
    "#     arg_3 = ms_marco_balanced_sample_300.iloc[i][\"expected_response\"]\n",
    "#     arg_4 = ms_marco_balanced_sample_300_true_labels[i]\n",
    "\n",
    "#     try:\n",
    "#         with tru_wrapped_answer_relevance_app as _:\n",
    "#             tru_wrapped_answer_relevance_app.app(arg_1, arg_2, arg_3, arg_4)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(\n",
    "#             f\"Error {e} in run_feedback_experiment row {i} with first arg {arg_1} and second arg {arg_2} and third arg {arg_3}\"\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.get_leaderboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
