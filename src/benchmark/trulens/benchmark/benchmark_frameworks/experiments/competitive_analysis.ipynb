{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TruLens vs RAGAS performance analysis for groundedness\n",
    "\n",
    "In this notebook, we analyze the performance of TruLens current groundedness feedback function and its comparable or equivalent implementations from other evaluation frameworks using the same model for LLM-as-judges. \n",
    "\n",
    "\n",
    "### Definitions\n",
    "1. TruLens `groundedness`: evaluates whether a response is fully supported by the source or retrieved contexts.\n",
    "\n",
    "2. RAGAS `faithfulness`: measures the factual consistency of the generated answer against the given context [source](https://docs.ragas.io/en/stable/concepts/metrics/faithfulness.html)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install trulens-core trulens-providers-openai ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare public benchmark datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    generate_qags_golden_set_groundedness,\n",
    ")\n",
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    generate_summeval_groundedness_golden_set,\n",
    ")\n",
    "\n",
    "qags_cnn_dm = pd.DataFrame(\n",
    "    list(generate_qags_golden_set_groundedness(\"data/qags_mturk_cnndm.jsonl\"))\n",
    ")\n",
    "\n",
    "qags_xsum = pd.DataFrame(\n",
    "    list(generate_qags_golden_set_groundedness(\"data/qags_mturk_xsum.jsonl\"))\n",
    ")\n",
    "\n",
    "summeval = pd.DataFrame(\n",
    "    list(\n",
    "        generate_summeval_groundedness_golden_set(\n",
    "            \"data/summeval_test.json\", max_samples_per_bucket=200\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# summeval_subset = summeval.sample(n=200, random_state=42)\n",
    "summeval_subset = summeval\n",
    "summeval_subset_true_labels = [\n",
    "    row[\"expected_score\"] for _, row in summeval_subset.iterrows()\n",
    "]\n",
    "\n",
    "summeval_subset_true_labels_binary = [\n",
    "    1 if label >= 0.5 else 0 for label in summeval_subset_true_labels\n",
    "]\n",
    "\n",
    "qags_cnn_dm_true_labels = [\n",
    "    row[\"expected_score\"] for _, row in qags_cnn_dm.iterrows()\n",
    "]\n",
    "\n",
    "qags_cnn_dm_true_labels_binary = [\n",
    "    1 if label >= 0.5 else 0 for label in qags_cnn_dm_true_labels\n",
    "]\n",
    "\n",
    "qags_xsum_true_labels = [\n",
    "    row[\"expected_score\"] for _, row in qags_xsum.iterrows()\n",
    "]\n",
    "\n",
    "qags_xsum_true_labels_binary = [\n",
    "    1 if label >= 0.5 else 0 for label in qags_xsum_true_labels\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    visualize_expected_score_distribution,\n",
    ")\n",
    "\n",
    "# making sure the distribution of the expected scores is balanced for the datasets\n",
    "visualize_expected_score_distribution(qags_xsum_true_labels)\n",
    "\n",
    "len(qags_xsum_true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup TruLens groundedness experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from trulens.apps.basic import TruBasicApp\n",
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    write_results,\n",
    ")\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "gpt_4o_mini = OpenAI(model_engine=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "def trulens_groundedness(input, output) -> float:\n",
    "    return gpt_4o_mini.groundedness_measure_with_cot_reasons(\n",
    "        source=input,\n",
    "        statement=output,\n",
    "        use_sent_tokenize=True,\n",
    "    )[0]\n",
    "\n",
    "\n",
    "def trulens_groundedness_binary(input, output) -> float:\n",
    "    return gpt_4o_mini.groundedness_measure_with_cot_reasons(\n",
    "        source=input,\n",
    "        statement=output,\n",
    "        use_sent_tokenize=True,\n",
    "        min_score_val=0,\n",
    "        max_score_val=1,\n",
    "        criteria=\"A grounded response to the query should get a score of 1, and an ungrounded response should get a score of 0. The score can only be either 0 or 1 (binary).\",\n",
    "    )[0]\n",
    "\n",
    "\n",
    "def run_trulens_experiment(\n",
    "    feedback_func_wrapper, app_name, app_version, dataset_df, true_labels\n",
    "):\n",
    "    if len(dataset_df) != len(true_labels):\n",
    "        raise ValueError(\"dataset df must have the same length as labels\")\n",
    "    tru_wrapped_basic_app = TruBasicApp(\n",
    "        feedback_func_wrapper, app_name=app_name, app_version=app_version\n",
    "    )\n",
    "\n",
    "    generated_scores, labels, latencies = [], [], []\n",
    "    for i in range(len(dataset_df)):\n",
    "        arg_1 = dataset_df.iloc[i][\"query\"]\n",
    "        arg_2 = dataset_df.iloc[i][\"expected_response\"]\n",
    "        true_score = true_labels[i]\n",
    "        try:\n",
    "            with tru_wrapped_basic_app as _:\n",
    "                start_time = time.time()\n",
    "                score = tru_wrapped_basic_app.app(arg_1, arg_2)\n",
    "\n",
    "                if math.isnan(score):\n",
    "                    score = 0  # if there is an NAN, we assume the score is 0\n",
    "\n",
    "                    # print(f\"Generated score: {score} | true_score: {true_score} \\n\")\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error {e} in run_feedback_experiment row {i} with first arg {arg_1} and second arg {arg_2}\"\n",
    "            )\n",
    "            score = 0  # if there is an error, we assume the score is 0\n",
    "\n",
    "        end_time = time.time()\n",
    "        generated_scores.append(score)\n",
    "        labels.append(true_score)\n",
    "        latencies.append(end_time - start_time)\n",
    "\n",
    "    write_results(\n",
    "        generated_scores,\n",
    "        labels,\n",
    "        latencies,\n",
    "        f\"results/{app_name}_{app_version}_results.csv\",\n",
    "    )\n",
    "\n",
    "\n",
    "run_trulens_experiment(\n",
    "    feedback_func_wrapper=trulens_groundedness_binary,\n",
    "    app_name=\"groundedness-binary-10102024\",\n",
    "    app_version=\"summeval-subset\",\n",
    "    dataset_df=summeval_subset,\n",
    "    true_labels=summeval_subset_true_labels,\n",
    ")\n",
    "run_trulens_experiment(\n",
    "    feedback_func_wrapper=trulens_groundedness_binary,\n",
    "    app_name=\"groundedness-binary-10102024\",\n",
    "    app_version=\"qags-cnn-dm\",\n",
    "    dataset_df=qags_cnn_dm,\n",
    "    true_labels=qags_cnn_dm_true_labels,\n",
    ")\n",
    "run_trulens_experiment(\n",
    "    feedback_func_wrapper=trulens_groundedness_binary,\n",
    "    app_name=\"groundedness-binary-10102024\",\n",
    "    app_version=\"qags-xsum\",\n",
    "    dataset_df=qags_xsum,\n",
    "    true_labels=qags_xsum_true_labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in all our dataframes (CNN/DM, XSUM, and SummEval), the \"expected_score\" column is the true label for the groundedness score, query corresponds to the context, and expected_response corresponds to the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qags_cnn_dm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup RAGAS faithfulness experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.cost import get_token_usage_for_openai\n",
    "from ragas.llms import llm_factory\n",
    "from ragas.metrics import faithfulness\n",
    "\n",
    "langchain_llm = llm_factory(model=\"gpt-4o-mini\")\n",
    "\n",
    "faithfulness.llm = langchain_llm\n",
    "\n",
    "\n",
    "def ragas_experiment(\n",
    "    dataset_df,\n",
    "):\n",
    "    data_samples = {\"question\": [], \"answer\": [], \"contexts\": []}\n",
    "    for i, row in dataset_df.iterrows():\n",
    "        data_samples[\"question\"].append(str(i))\n",
    "        data_samples[\"answer\"].append(row[\"expected_response\"])\n",
    "        data_samples[\"contexts\"].append([row[\"query\"]])\n",
    "\n",
    "    ragas_dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "    score = evaluate(\n",
    "        ragas_dataset,\n",
    "        metrics=[faithfulness],\n",
    "        llm=langchain_llm,\n",
    "        token_usage_parser=get_token_usage_for_openai,\n",
    "    )\n",
    "    avg_cost = (\n",
    "        score.total_cost(\n",
    "            cost_per_input_token=0.15 / 1e6, cost_per_output_token=0.6 / 1e6\n",
    "        )\n",
    "        / 200\n",
    "    )\n",
    "    print(f\"Average cost per sample: {avg_cost}\")\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "ragas_cnn_score = ragas_experiment(qags_cnn_dm)\n",
    "qags_cnn_dm_true_labels = [\n",
    "    row[\"expected_score\"] for _, row in qags_cnn_dm.iterrows()\n",
    "]\n",
    "\n",
    "qags_cnn_dm_true_labels_binary = [\n",
    "    1 if label > 0.5 else 0 for label in qags_cnn_dm_true_labels\n",
    "]\n",
    "ragas_xsum_score = ragas_experiment(qags_xsum)\n",
    "qags_xsum_true_labels = [\n",
    "    row[\"expected_score\"] for _, row in qags_xsum.iterrows()\n",
    "]\n",
    "\n",
    "qags_xsum_true_labels_binary = [\n",
    "    1 if label > 0.5 else 0 for label in qags_xsum_true_labels\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_cnn_score.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking with real-valued output scores (both TruLens' feedback scores and RAGAS scores are normalized to 0.0 to 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    read_results,\n",
    ")\n",
    "\n",
    "trulens_cnn_scores, cnn_labels, latencies = read_results(\n",
    "    \"results/QAGS CNN_DM - gpt-4o-mini_groundedness_likert4_results.csv\"\n",
    ")\n",
    "trulens_xsum_scores, xsum_labels, latencies = read_results(\n",
    "    \"results/QAGS XSum - gpt-4o-mini_groundedness_likert4_results.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_true_scores = np.array(cnn_labels)\n",
    "mae_trulens_cnn = np.mean(np.abs(trulens_cnn_scores - cnn_true_scores))\n",
    "mae_ragas_cnn = np.mean(\n",
    "    np.abs(\n",
    "        ragas_cnn_score.to_pandas()[\"faithfulness\"] - qags_cnn_dm_true_labels\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"QAGS CNN/DM: Trulens MAE: {mae_trulens_cnn:.4f}, Ragas MAE: {mae_ragas_cnn:.4f}\"\n",
    ")\n",
    "\n",
    "\n",
    "xsum_true_scores = np.array(xsum_labels)\n",
    "mae_trulens_xsum = np.mean(np.abs(trulens_xsum_scores - xsum_true_scores))\n",
    "mae_ragas_xsum = np.mean(\n",
    "    np.abs(ragas_xsum_score.to_pandas()[\"faithfulness\"] - qags_xsum_true_labels)\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"QAGS XSum: Trulens MAE: {mae_trulens_xsum:.4f}, Ragas MAE: {mae_ragas_xsum:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summeval_ragas_data_samples = {\"question\": [], \"answer\": [], \"contexts\": []}\n",
    "for i, row in summeval_subset.iterrows():\n",
    "    summeval_ragas_data_samples[\"question\"].append(str(i))\n",
    "    summeval_ragas_data_samples[\"answer\"].append(row[\"expected_response\"])\n",
    "    summeval_ragas_data_samples[\"contexts\"].append([row[\"query\"]])\n",
    "\n",
    "summeval_ragas_dataset = Dataset.from_dict(summeval_ragas_data_samples)\n",
    "\n",
    "ragas_summeval_scores = evaluate(\n",
    "    summeval_ragas_dataset,\n",
    "    metrics=[faithfulness],\n",
    "    llm=langchain_llm,\n",
    "    token_usage_parser=get_token_usage_for_openai,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_cost = (\n",
    "    ragas_summeval_scores.total_cost(\n",
    "        # hard-coded cost per token values for OpenAI gpt-4o-mini\n",
    "        cost_per_input_token=0.15 / 1e6,\n",
    "        cost_per_output_token=0.6 / 1e6,\n",
    "    )\n",
    "    / 200\n",
    ")\n",
    "avg_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking with binary output scores (both TruLens' feedback scores and RAGAS scores are cast to 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    compute_binary_classification_metrics,\n",
    ")\n",
    "from trulens.feedback.groundtruth import GroundTruthAggregator\n",
    "\n",
    "trulens_cnn_dm_scores, trulens_cnn_dm_labels, trulens_cnn_dm_latencies = (\n",
    "    read_results(\"results/groundedness-binary-10102024_qags-cnn-dm_results.csv\")\n",
    ")\n",
    "trulens_cnn_dm_scores_binary = [\n",
    "    1 if score >= 0.5 else 0 for score in trulens_cnn_dm_scores\n",
    "]\n",
    "trulens_cnn_dm_labels_binary = [\n",
    "    1 if label >= 0.5 else 0 for label in trulens_cnn_dm_labels\n",
    "]\n",
    "print(len(trulens_cnn_dm_scores_binary), len(trulens_cnn_dm_labels_binary))\n",
    "\n",
    "spearman_cor = GroundTruthAggregator(\n",
    "    trulens_cnn_dm_labels\n",
    ").spearman_correlation(trulens_cnn_dm_scores)\n",
    "\n",
    "compute_binary_classification_metrics(\n",
    "    \"TruLens QAGS CNN/Daily Mail\",\n",
    "    trulens_cnn_dm_labels_binary,\n",
    "    trulens_cnn_dm_scores_binary,\n",
    "    trulens_cnn_dm_latencies,\n",
    ")\n",
    "print(f\"TruLens QAGS CNN/Daily Mail: {spearman_cor}\")\n",
    "\n",
    "\n",
    "trulens_xsum_scores, xsum_labels, trulens_xsum_latencies = read_results(\n",
    "    \"results/groundedness-binary-10102024_qags-xsum_results.csv\"\n",
    ")\n",
    "trulens_xsum_scores_binary = [\n",
    "    1 if score >= 0.5 else 0 for score in trulens_xsum_scores\n",
    "]\n",
    "trulens_xsum_labels_binary = [1 if label >= 0.5 else 0 for label in xsum_labels]\n",
    "spearman_cor = GroundTruthAggregator(xsum_labels).spearman_correlation(\n",
    "    trulens_xsum_scores\n",
    ")\n",
    "\n",
    "print(len(trulens_xsum_scores_binary), len(trulens_xsum_labels_binary))\n",
    "\n",
    "compute_binary_classification_metrics(\n",
    "    \"TruLens QAGS XSum\",\n",
    "    trulens_xsum_labels_binary,\n",
    "    trulens_xsum_scores_binary,\n",
    "    trulens_xsum_latencies,\n",
    ")\n",
    "print(f\"TruLens QAGS XSum: {spearman_cor}\")\n",
    "\n",
    "trulens_summeval_scores, summeval_labels, trulens_summeval_latencies = (\n",
    "    read_results(\n",
    "        \"results/groundedness-binary-10102024_summeval-subset_results.csv\"\n",
    "    )\n",
    ")\n",
    "trulens_summeval_binary = [\n",
    "    1 if score >= 0.5 else 0 for score in trulens_summeval_scores\n",
    "]\n",
    "trulens_summeval_labels_binary = [\n",
    "    1 if label >= 0.5 else 0 for label in summeval_labels\n",
    "]\n",
    "print(len(trulens_summeval_binary), len(trulens_summeval_labels_binary))\n",
    "spearman_cor = GroundTruthAggregator(summeval_labels).spearman_correlation\n",
    "\n",
    "compute_binary_classification_metrics(\n",
    "    \"TruLens SummEval subset\",\n",
    "    trulens_summeval_labels_binary,\n",
    "    trulens_summeval_binary,\n",
    "    trulens_summeval_latencies,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_cnn_dm_scores_binary = [\n",
    "    1 if score > 0.5 else 0\n",
    "    for score in ragas_cnn_score.to_pandas()[\"faithfulness\"]\n",
    "]\n",
    "ragas_cnn_dm_labels_binary = [\n",
    "    1 if label >= 0.5 else 0 for label in qags_cnn_dm_true_labels\n",
    "]\n",
    "print(len(ragas_cnn_dm_scores_binary), len(ragas_cnn_dm_labels_binary))\n",
    "compute_binary_classification_metrics(\n",
    "    \"Ragas QAGS CNN/DM\",\n",
    "    ragas_cnn_dm_labels_binary,\n",
    "    ragas_cnn_dm_scores_binary,\n",
    "    [],\n",
    ")\n",
    "\n",
    "ragas_xsum_scores_binary = [\n",
    "    1 if score > 0.5 else 0\n",
    "    for score in ragas_xsum_score.to_pandas()[\"faithfulness\"]\n",
    "]\n",
    "ragas_xsum_labels_binary = [\n",
    "    1 if label >= 0.5 else 0 for label in qags_xsum_true_labels\n",
    "]\n",
    "print(len(ragas_xsum_scores_binary), len(ragas_xsum_labels_binary))\n",
    "compute_binary_classification_metrics(\n",
    "    \"Ragas QAGS XSum\", ragas_xsum_labels_binary, ragas_xsum_scores_binary, []\n",
    ")\n",
    "\n",
    "ragas_summeval_scores_binary = [\n",
    "    1 if score > 0.5 else 0\n",
    "    for score in ragas_summeval_scores.to_pandas()[\"faithfulness\"]\n",
    "]\n",
    "ragas_summeval_labels_binary = [\n",
    "    1 if label >= 0.5 else 0 for label in summeval_subset_true_labels\n",
    "]\n",
    "print(len(ragas_summeval_scores_binary), len(ragas_summeval_labels_binary))\n",
    "compute_binary_classification_metrics(\n",
    "    \"Ragas SummEval subset\",\n",
    "    ragas_summeval_labels_binary,\n",
    "    ragas_summeval_scores_binary,\n",
    "    [],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
