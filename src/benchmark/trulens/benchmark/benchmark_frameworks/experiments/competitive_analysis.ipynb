{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TruLens vs RAGAS comparison\n",
    "\n",
    "RAGAS vs TruLens' equivalents\n",
    "\n",
    "faithfulness <-> groundedness\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install trulens-core trulens-providers-openai ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from ragas import evaluate\n",
    "from ragas.cost import get_token_usage_for_openai\n",
    "from ragas.llms import llm_factory\n",
    "from ragas.metrics import faithfulness\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "langchain_llm = llm_factory(model=\"gpt-4o-mini\")\n",
    "\n",
    "faithfulness.llm = langchain_llm\n",
    "\n",
    "gpt_4o_mini = OpenAI(model_engine=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "def trulens_groundedness(input, output) -> float:\n",
    "    return gpt_4o_mini.groundedness_measure_with_cot_reasons(\n",
    "        source=input,\n",
    "        statement=output,\n",
    "        use_sent_tokenize=True,\n",
    "    )[0]\n",
    "\n",
    "\n",
    "def trulens_answer_relevance(input, output) -> float:\n",
    "    return gpt_4o_mini.relevance_with_cot_reasons(\n",
    "        prompt=input,\n",
    "        response=output,\n",
    "    )[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare public benchmark datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    generate_qags_golden_set_groundedness,\n",
    ")\n",
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    generate_summeval_groundedness_golden_set,\n",
    ")\n",
    "\n",
    "qags_cnn_dm = pd.DataFrame(\n",
    "    list(generate_qags_golden_set_groundedness(\"data/qags_mturk_cnndm.jsonl\"))\n",
    ")\n",
    "\n",
    "qags_xsum = pd.DataFrame(\n",
    "    list(generate_qags_golden_set_groundedness(\"data/qags_mturk_xsum.jsonl\"))\n",
    ")\n",
    "\n",
    "summeval = pd.DataFrame(\n",
    "    list(generate_summeval_groundedness_golden_set(\"data/summeval_test.json\"))\n",
    ")\n",
    "\n",
    "summeval_subset = summeval.sample(n=200, random_state=42)\n",
    "summeval_subset_true_labels = [\n",
    "    row[\"expected_score\"] for _, row in summeval_subset.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ragas_experiment(\n",
    "    dataset_df,\n",
    "):\n",
    "    data_samples = {\"question\": [], \"answer\": [], \"contexts\": []}\n",
    "    for i, row in dataset_df.iterrows():\n",
    "        data_samples[\"question\"].append(str(i))\n",
    "        data_samples[\"answer\"].append(row[\"expected_response\"])\n",
    "        data_samples[\"contexts\"].append([row[\"query\"]])\n",
    "\n",
    "    ragas_dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "    score = evaluate(\n",
    "        ragas_dataset,\n",
    "        metrics=[faithfulness],\n",
    "        llm=langchain_llm,\n",
    "        token_usage_parser=get_token_usage_for_openai,\n",
    "    )\n",
    "    avg_cost = (\n",
    "        score.total_cost(\n",
    "            cost_per_input_token=0.15 / 1e6, cost_per_output_token=0.6 / 1e6\n",
    "        )\n",
    "        / 200\n",
    "    )\n",
    "    print(f\"Average cost per sample: {avg_cost}\")\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def trulens_experiment(\n",
    "    dataset_df,\n",
    "):\n",
    "    data_samples = {\"question\": [], \"answer\": [], \"contexts\": []}\n",
    "    for i, row in dataset_df.iterrows():\n",
    "        data_samples[\"question\"].append(str(i))\n",
    "        data_samples[\"answer\"].append(row[\"expected_response\"])\n",
    "        data_samples[\"contexts\"].append([row[\"query\"]])\n",
    "\n",
    "    ff_scores = []\n",
    "    for i in range(len(data_samples[\"contexts\"])):\n",
    "        ff_scores.append(\n",
    "            trulens_groundedness(\n",
    "                data_samples[\"contexts\"][i][0], data_samples[\"answer\"][i]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    ff_scores = np.array(ff_scores)\n",
    "    return ff_scores\n",
    "\n",
    "\n",
    "ragas_cnn_score = ragas_experiment(qags_cnn_dm)\n",
    "ragas_xsum_score = ragas_experiment(qags_xsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_cnn_score.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    read_results,\n",
    ")\n",
    "\n",
    "trulens_cnn_scores, cnn_labels, latencies = read_results(\n",
    "    \"results/QAGS CNN_DM - gpt-4o-mini_groundedness_likert4_results.csv\"\n",
    ")\n",
    "trulens_xsum_scores, xsum_labels, latencies = read_results(\n",
    "    \"results/QAGS XSum - gpt-4o-mini_groundedness_likert4_results.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_scores = np.array(cnn_labels)\n",
    "mae_trulens = np.mean(np.abs(trulens_cnn_scores - true_scores))\n",
    "mae_ragas = np.mean(\n",
    "    np.abs(ragas_cnn_score.to_pandas()[\"faithfulness\"] - true_scores)\n",
    ")\n",
    "\n",
    "print(f\"Trulens MAE: {mae_trulens:.4f}, Ragas MAE: {mae_ragas:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summeval_ragas_data_samples = {\"question\": [], \"answer\": [], \"contexts\": []}\n",
    "for i, row in summeval_subset.iterrows():\n",
    "    summeval_ragas_data_samples[\"question\"].append(str(i))\n",
    "    summeval_ragas_data_samples[\"answer\"].append(row[\"expected_response\"])\n",
    "    summeval_ragas_data_samples[\"contexts\"].append([row[\"query\"]])\n",
    "\n",
    "summeval_ragas_dataset = Dataset.from_dict(summeval_ragas_data_samples)\n",
    "\n",
    "score = evaluate(\n",
    "    summeval_ragas_dataset,\n",
    "    metrics=[faithfulness],\n",
    "    llm=langchain_llm,\n",
    "    token_usage_parser=get_token_usage_for_openai,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_cost = (\n",
    "    score.total_cost(\n",
    "        cost_per_input_token=0.15 / 1e6, cost_per_output_token=0.6 / 1e6\n",
    "    )\n",
    "    / 200\n",
    ")\n",
    "avg_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_scores = []\n",
    "for i in range(len(summeval_ragas_data_samples[\"contexts\"])):\n",
    "    ff_scores.append(\n",
    "        trulens_groundedness(\n",
    "            summeval_ragas_data_samples[\"contexts\"][i][0],\n",
    "            summeval_ragas_data_samples[\"answer\"][i],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_scores = np.array(ff_scores)\n",
    "ragas_scores = np.array(score.to_pandas()[\"faithfulness\"])\n",
    "\n",
    "true_scores = np.array(summeval_subset_true_labels)\n",
    "mae_trulens = np.mean(np.abs(ff_scores - true_scores))\n",
    "mae_ragas = np.mean(np.abs(ragas_scores - true_scores))\n",
    "\n",
    "print(f\"Trulens MAE: {mae_trulens:.4f}, Ragas MAE: {mae_ragas:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
