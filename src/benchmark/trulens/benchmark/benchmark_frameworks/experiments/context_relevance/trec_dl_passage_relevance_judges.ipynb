{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating context relevance on TREC DL track (2021, 2022, ...) with NIST human annotations on passage retrieval rankings.\n",
    "\n",
    "Note: passage level `qrels` annotations are not available at the time of writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "\n",
    "trec_dl_2021 = ir_datasets.load(\n",
    "    \"msmarco-passage-v2/trec-dl-2021/judged\"\n",
    ")  # 53 queries\n",
    "qrels_2021 = trec_dl_2021.qrels_dict()\n",
    "\n",
    "trec_dl_2022 = ir_datasets.load(\n",
    "    \"msmarco-passage-v2/trec-dl-2022/judged\"\n",
    ")  # 76 queries\n",
    "qrels_2022 = trec_dl_2022.qrels_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.feedback.v2.feedback import ContextRelevance\n",
    "\n",
    "current_criteria = \"\"\"\n",
    "- CONTEXT that is IRRELEVANT to the QUESTION should score 0.\n",
    "- CONTEXT that is RELEVANT to some of the QUESTION should get an intermediate score.\n",
    "- CONTEXT that is RELEVANT to most of the QUESTION should get a score closer to 3.\n",
    "- CONTEXT that is RELEVANT to the entirety of the QUESTION should get a score of 3, which is the full mark.\n",
    "- CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 3.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human annotation quality ananlysis and check the agreement with generated scores from `scoreddocs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def scoreddocs_qrels_confusion_matrix(\n",
    "    dataset_path=\"msmarco-passage-v2/trec-dl-2022/judged\",\n",
    "    aggregate=False,\n",
    "    normalize=False,\n",
    "):\n",
    "    # Load the dataset\n",
    "    dataset = ir_datasets.load(dataset_path)\n",
    "    qrels = dataset.qrels_dict()\n",
    "    scoreddocs = list(dataset.scoreddocs_iter())\n",
    "\n",
    "    # Prepare a DataFrame for analysis\n",
    "    data = []\n",
    "    for scored_doc in scoreddocs:\n",
    "        query_id = scored_doc.query_id\n",
    "        doc_id = scored_doc.doc_id\n",
    "        score = scored_doc.score\n",
    "        qrel_score = qrels.get(query_id, {}).get(doc_id, None)\n",
    "        if qrel_score is not None:\n",
    "            data.append({\n",
    "                \"query_id\": query_id,\n",
    "                \"doc_id\": doc_id,\n",
    "                \"score\": score,\n",
    "                \"qrel_score\": qrel_score,\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Analyze per query or aggregate\n",
    "    all_data = []\n",
    "    confusion_matrices = {}\n",
    "    for query_id, group in df.groupby(\"query_id\"):\n",
    "        min_score = group[\"score\"].min()\n",
    "        max_score = group[\"score\"].max()\n",
    "        interval_size = (max_score - min_score) / 4\n",
    "        intervals = [\n",
    "            min_score + i * interval_size for i in range(5)\n",
    "        ]  # 4 intervals\n",
    "\n",
    "        # Assign each passage to an interval\n",
    "        group[\"interval\"] = pd.cut(\n",
    "            group[\"score\"],\n",
    "            bins=intervals,\n",
    "            include_lowest=True,\n",
    "            labels=[0, 1, 2, 3],\n",
    "        )\n",
    "\n",
    "        if aggregate:\n",
    "            # Append all data for aggregation\n",
    "            all_data.append(group)\n",
    "        else:\n",
    "            # Create confusion matrix for each query\n",
    "            confusion_matrix = pd.crosstab(\n",
    "                group[\"qrel_score\"], group[\"interval\"]\n",
    "            )\n",
    "\n",
    "            # Normalize across rows (qrels scores)\n",
    "            if normalize:\n",
    "                confusion_matrix = confusion_matrix.div(\n",
    "                    confusion_matrix.sum(axis=1), axis=0\n",
    "                )\n",
    "\n",
    "            confusion_matrices[query_id] = confusion_matrix\n",
    "\n",
    "            # Visualize the confusion matrix\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(\n",
    "                confusion_matrix, annot=True, fmt=\".2f\", cmap=\"Blues\", cbar=True\n",
    "            )\n",
    "            plt.title(f\"Confusion Matrix for Query {query_id}\")\n",
    "            plt.xlabel(\"Scoreddocs Intervals\")\n",
    "            plt.ylabel(\"Qrels Scores\")\n",
    "            plt.show()\n",
    "\n",
    "    if aggregate:\n",
    "        # Combine all groups into a single DataFrame\n",
    "        aggregated_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "        # Create an aggregate confusion matrix\n",
    "        aggregate_confusion_matrix = pd.crosstab(\n",
    "            aggregated_df[\"qrel_score\"], aggregated_df[\"interval\"]\n",
    "        )\n",
    "\n",
    "        # Normalize across rows (qrels scores)\n",
    "        if normalize:\n",
    "            aggregate_confusion_matrix = aggregate_confusion_matrix.div(\n",
    "                aggregate_confusion_matrix.sum(axis=1), axis=0\n",
    "            )\n",
    "\n",
    "        # Visualize the aggregate confusion matrix\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(\n",
    "            aggregate_confusion_matrix,\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            cmap=\"Blues\",\n",
    "            cbar=True,\n",
    "        )\n",
    "        plt.title(\"Aggregate Confusion Matrix Across All Queries\")\n",
    "        plt.xlabel(\"Scoreddocs Intervals\")\n",
    "        plt.ylabel(\"Qrels Scores\")\n",
    "        plt.show()\n",
    "\n",
    "        return aggregate_confusion_matrix\n",
    "\n",
    "    return confusion_matrices\n",
    "\n",
    "\n",
    "# Run the analysis for individual queries\n",
    "# confusion_matrices = scoreddocs_qrels_confusion_matrix(aggregate=False)\n",
    "\n",
    "# Run the analysis for aggregate across all queries\n",
    "aggregate_confusion_matrix = scoreddocs_qrels_confusion_matrix(aggregate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: scoreddocs scores vs qrels annotations\n",
    "\n",
    "The motivation is that we oberseve dubious annotations in `qrels` in both TREC DL 2021 and 2022, and we are curious if the scoreddocs submitted by participants of  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    generate_trec_dl_passage_benchmark,\n",
    ")\n",
    "\n",
    "trec_2021_samples = list(\n",
    "    generate_trec_dl_passage_benchmark(\n",
    "        max_samples_per_query_per_score=4,\n",
    "        dataset_path=\"msmarco-passage-v2/trec-dl-2021/judged\",\n",
    "    )\n",
    ")\n",
    "trec_2022_samples = list(\n",
    "    generate_trec_dl_passage_benchmark(\n",
    "        max_samples_per_query_per_score=4,\n",
    "        dataset_path=\"msmarco-passage-v2/trec-dl-2022/judged\",\n",
    "    )\n",
    ")\n",
    "trec_combined = trec_2021_samples + trec_2022_samples\n",
    "\n",
    "\n",
    "trec_combined_df = pd.DataFrame(trec_combined)\n",
    "\n",
    "print(f\"Totoal number of samples: {len(trec_combined_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(qrels_2021), len(qrels_2022))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    visualize_expected_score_distribution,\n",
    ")\n",
    "\n",
    "trec_combined_relevance_scores = [\n",
    "    entry[\"expected_score\"] * 3 for _, entry in trec_combined_df.iterrows()\n",
    "]\n",
    "visualize_expected_score_distribution(trec_combined_relevance_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from trulens.core.session import TruSession\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "connection_params = {\n",
    "    \"account\": os.environ.get(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    \"user\": os.environ.get(\"SNOWFLAKE_USER\"),\n",
    "    \"password\": os.environ.get(\"SNOWFLAKE_USER_PASSWORD\"),\n",
    "    \"database\": os.environ.get(\"SNOWFLAKE_DATABASE\"),\n",
    "    \"schema\": os.environ.get(\"SNOWFLAKE_SCHEMA\"),\n",
    "    \"warehouse\": os.environ.get(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "    \"role\": os.environ.get(\"SNOWFLAKE_ROLE\"),\n",
    "    \"init_server_side\": False,  # Set to True to enable server side feedback functions\n",
    "}\n",
    "\n",
    "# connector = SnowflakeConnector(**connection_params)\n",
    "# session = TruSession(connector=connector)\n",
    "\n",
    "session = TruSession()\n",
    "session.reset_database()\n",
    "\n",
    "\n",
    "# snowpark_session = Session.builder.configs(connection_params).create()\n",
    "\n",
    "gpt_4o = OpenAI(model_engine=\"gpt-4o\")\n",
    "gpt_4o_mini = OpenAI(model_engine=\"gpt-4o-mini\")\n",
    "# llama3_405b = Cortex(snowflake.connector.connect(**connection_params), model_engine=\"llama3.1-405b\")\n",
    "# mistral_large = Cortex(snowflake.connector.connect(**connection_params), model_engine=\"mistral-large\")\n",
    "# llama3_1_8b = Cortex(snowflake.connector.connect(**connection_params), model_engine=\"llama3.1-8b\")\n",
    "\n",
    "\n",
    "PROVIDERS = [\n",
    "    gpt_4o,\n",
    "    gpt_4o_mini,\n",
    "]\n",
    "\n",
    "\n",
    "# criteria without explicit rubrics\n",
    "current_criteria = \"\"\"\n",
    "- CONTEXT that is IRRELEVANT to the QUESTION should score 0.\n",
    "- CONTEXT that is RELEVANT to some of the QUESTION should get an intermediate score.\n",
    "- CONTEXT that is RELEVANT to most of the QUESTION should get a score closer to 3.\n",
    "- CONTEXT that is RELEVANT to the entirety of the QUESTION should get a score of 3, which is the full mark.\n",
    "- CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 3.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def trulens_context_relevance(\n",
    "    provider, query: str, context: str, gt_score: float\n",
    ") -> str:\n",
    "    trulens_context_relevance_res = provider.context_relevance_with_cot_reasons(\n",
    "        question=query, context=context\n",
    "    )\n",
    "    return f\"{trulens_context_relevance_res[0]};{gt_score};{trulens_context_relevance_res[1]}\"\n",
    "\n",
    "\n",
    "def trulens_context_relevance_no_rubric(\n",
    "    provider, query: str, context: str, gt_score: float\n",
    ") -> str:\n",
    "    trulens_context_relevance_res = provider.context_relevance_with_cot_reasons(\n",
    "        question=query, context=context, criteria=current_criteria\n",
    "    )\n",
    "    return f\"{trulens_context_relevance_res[0]};{gt_score};{trulens_context_relevance_res[1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ContextRelevance.criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.apps.basic import TruBasicApp\n",
    "from trulens.core import Feedback\n",
    "from trulens.core import Provider\n",
    "\n",
    "THRESHOLD = 0.5  # for passage retrieval annotation, we consider a score of 0.5 or above as relevant\n",
    "\n",
    "\n",
    "class CustomTermFeedback(Provider):\n",
    "    def true_positive(self, output: str) -> float:\n",
    "        feedback_score, gt_score = (\n",
    "            float(output.split(\";\")[0]),\n",
    "            float(output.split(\";\")[1]),\n",
    "        )\n",
    "        binary_score = 1 if feedback_score >= 0.5 else 0\n",
    "        binary_gt_score = 1 if gt_score >= THRESHOLD else 0\n",
    "        return 1.0 if binary_score == 1 and binary_gt_score == 1 else 0.0\n",
    "\n",
    "    def true_negative(self, output: str) -> float:\n",
    "        feedback_score, gt_score = (\n",
    "            float(output.split(\";\")[0]),\n",
    "            float(output.split(\";\")[1]),\n",
    "        )\n",
    "        binary_score = 1 if feedback_score >= 0.5 else 0\n",
    "        binary_gt_score = 1 if gt_score >= THRESHOLD else 0\n",
    "        return 1.0 if binary_score == 0 and binary_gt_score == 0 else 0.0\n",
    "\n",
    "    def false_positive(self, output: str) -> float:\n",
    "        feedback_score, gt_score = (\n",
    "            float(output.split(\";\")[0]),\n",
    "            float(output.split(\";\")[1]),\n",
    "        )\n",
    "        binary_score = 1 if feedback_score >= 0.5 else 0\n",
    "        binary_gt_score = 1 if gt_score >= THRESHOLD else 0\n",
    "        return 1.0 if binary_score == 1 and binary_gt_score == 0 else 0.0\n",
    "\n",
    "    def false_negative(self, output: str) -> float:\n",
    "        feedback_score, gt_score = (\n",
    "            float(output.split(\";\")[0]),\n",
    "            float(output.split(\";\")[1]),\n",
    "        )\n",
    "        binary_score = 1 if feedback_score >= 0.5 else 0\n",
    "        binary_gt_score = 1 if gt_score >= THRESHOLD else 0\n",
    "        return 1.0 if binary_score == 0 and binary_gt_score == 1 else 0.0\n",
    "\n",
    "    def term_absolute_error(self, output: str) -> float:\n",
    "        feedback_score, gt_score = (\n",
    "            float(output.split(\";\")[0]),\n",
    "            float(output.split(\";\")[1]),\n",
    "        )\n",
    "        return abs(feedback_score - gt_score)\n",
    "\n",
    "    def raw_gt_score(self, output: str) -> float:\n",
    "        return float(output.split(\";\")[1]) * 3\n",
    "\n",
    "    def raw_feedback_score(self, output: str) -> float:\n",
    "        return float(output.split(\";\")[0]) * 3\n",
    "\n",
    "\n",
    "custom_term_feedback = CustomTermFeedback()\n",
    "\n",
    "f_tp = Feedback(\n",
    "    custom_term_feedback.true_positive,\n",
    "    name=\"True Positive\",\n",
    "    higher_is_better=True,\n",
    ").on_output()\n",
    "f_tn = Feedback(\n",
    "    custom_term_feedback.true_negative,\n",
    "    name=\"True Negative\",\n",
    "    higher_is_better=True,\n",
    ").on_output()\n",
    "f_fp = Feedback(\n",
    "    custom_term_feedback.false_positive,\n",
    "    name=\"False Positive\",\n",
    "    higher_is_better=False,\n",
    ").on_output()\n",
    "f_fn = Feedback(\n",
    "    custom_term_feedback.false_negative,\n",
    "    name=\"False Negative\",\n",
    "    higher_is_better=False,\n",
    ").on_output()\n",
    "f_abs_err = Feedback(\n",
    "    custom_term_feedback.term_absolute_error,\n",
    "    name=\"Absolute Error\",\n",
    "    higher_is_better=False,\n",
    ").on_output()\n",
    "f_raw_gt_score = Feedback(\n",
    "    custom_term_feedback.raw_gt_score,\n",
    "    name=\"Raw GT Score\",\n",
    "    higher_is_better=True,\n",
    ").on_output()\n",
    "f_raw_feedback_score = Feedback(\n",
    "    custom_term_feedback.raw_feedback_score,\n",
    "    name=\"Raw Feedback Score\",\n",
    "    higher_is_better=True,\n",
    ").on_output()\n",
    "\n",
    "CUSTOM_FEEDBACK_FUNCS = [\n",
    "    f_tp,\n",
    "    f_tn,\n",
    "    f_fp,\n",
    "    f_fn,\n",
    "    f_abs_err,\n",
    "    f_raw_gt_score,\n",
    "    f_raw_feedback_score,\n",
    "]\n",
    "\n",
    "\n",
    "def run_experiment_for_provider(provider, func_wrapper, dataset_df, app_name):\n",
    "    tru_wrapped_app = TruBasicApp(\n",
    "        func_wrapper,\n",
    "        app_name=app_name,\n",
    "        app_version=f\"{provider.model_engine}-context-relevance\",\n",
    "        feedbacks=CUSTOM_FEEDBACK_FUNCS,\n",
    "    )\n",
    "\n",
    "    for i, row in dataset_df.iterrows():\n",
    "        arg_1 = row[\"query\"]\n",
    "        arg_2 = row[\"expected_response\"]\n",
    "        arg_3 = row[\"expected_score\"]\n",
    "\n",
    "        try:\n",
    "            with tru_wrapped_app as _:\n",
    "                tru_wrapped_app.app(provider, arg_1, arg_2, arg_3)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error {e} in run_feedback_experiment row {i} with first arg {arg_1} and second arg {arg_2}\"\n",
    "            )\n",
    "\n",
    "\n",
    "# with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#     futures = [executor.submit(run_experiment_for_provider, provider, trec_doc_2022) for provider in PROVIDERS]\n",
    "#     concurrent.futures.wait(futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for provider in PROVIDERS:\n",
    "    print(f\"Running provider: {provider.model_engine}\")\n",
    "    run_experiment_for_provider(\n",
    "        provider,\n",
    "        trulens_context_relevance,\n",
    "        trec_combined_df,\n",
    "        \"trec_dl_2021_2022_combined\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Gaurav's prompt, UMBRELA prompt, and zero-shot categorical prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_prompt = \"\"\"\n",
    "You are an expert search result rater. You are given a user query and a search result. Your task is to rate the search result based on its relevance to the user query. You should rate the search result on a scale of 0 to 3, where:\n",
    "    0: The search result has no relevance to the user query.\n",
    "    1: The search result has low relevance to the user query. In this case the search result may contain some information which seems very slightly related to the user query but not enough information to answer the user query. The search result contains some references or very limited information about some entities present in the user query. In case the query is a statement on a topic, the search result should be tangentially related to it.\n",
    "    2: The search result has medium relevance to the user query. If the user query is a question, the search result may contain some information that is relevant to the user query but not enough information to answer the user query. If the user query is a search phrase/sentence, either the search result is centered around about most but not all entities present in the user query, or if all the entities are present in the result, the search result while not being centered around it has medium level of relevance. In case the query is a statement on a topic, the search result should be related to the topic.\n",
    "    3: The search result has high relevance to the user query. If the user query is a question, the search result contains information that can answer the user query. Otherwise if the search query is a search phrase/sentence, it provides relevant information about all entities that are present in the user query and the search result is centered around the entities mentioned in the query. In case the query is a statement on a topic, the search result should be either be directly addressing it or be on the same topic.\n",
    "    \n",
    "    You should think step by step about the user query and the search result and rate the search result. You should also provide a reasoning for your rating.\n",
    "    \n",
    "    Use the following format:\n",
    "    Rating: Example Rating\n",
    "    Reasoning: Example Reasoning\n",
    "    \n",
    "    ### Examples\n",
    "    Example:\n",
    "    Example 1:\n",
    "    INPUT:\n",
    "    User Query: What is the definition of an accordion?\n",
    "    Search Result: Accordion definition, Also called piano accordion. a portable wind instrument having a large bellows for forcing air through small metal reeds, a keyboard for the right hand, and buttons for sounding single bass notes or chords for the left hand. a similar instrument having single-note buttons instead of a keyboard.\n",
    "    OUTPUT:\n",
    "    Rating: 3\n",
    "    Reasoning: In this case the search query is a question. The search result directly answers the user question for the definition of an accordion, hence it has high relevance to the user query.\n",
    "    \n",
    "    Example 2:\n",
    "    INPUT:\n",
    "    User Query: dark horse\n",
    "    Search Result: Darkhorse is a person who everyone expects to be last in a race. Think of it this way. The person who looks like he can never get laid defies the odds and gets any girl he can by being sly,shy and cunning. Although he\\'s not a player, he can really charm the ladies.\n",
    "    OUTPUT:\n",
    "    Rating: 3\n",
    "    Reasoning: In this case the search query is a search phrase mentioning \\'dark horse\\'. The search result contains information about the term \\'dark horse\\' and provides a definition for it and is centered around it. Hence it has high relevance to the user query.\n",
    "    \n",
    "    Example 3:\n",
    "    INPUT:\n",
    "    User Query: Global warming and polar bears\n",
    "    Search Result: Polar bear The polar bear is a carnivorous bear whose native range lies largely within the Arctic Circle, encompassing the Arctic Ocean, its surrounding seas and surrounding land masses. It is a large bear, approximately the same size as the omnivorous Kodiak bear (Ursus arctos middendorffi).\n",
    "    OUTPUT:\n",
    "    Rating: 2\n",
    "    Reasoning: In this case the search query is a search phrase mentioning two entities \\'Global warming\\' and \\'polar bears\\'. The search result contains is centered around the polar bear which is one of the two entities in the search query. Therefore it addresses most of the entities present and hence has medium relevance. \n",
    "    \n",
    "    Example 4:\n",
    "    INPUT:\n",
    "    User Query: Snowflake synapse private link\n",
    "    Search Result: \"This site can\\'t be reached\" error when connecting to Snowflake via Private Connectivity\\nThis KB article addresses an issue that prevents connections to Snowflake failing with: \"This site can\\'t be reached\" ISSUE: Attempting to reach Snowflake via Private Connectivity fails with the \"This site can\\'t be reached\" error\n",
    "    OUTPUT:\n",
    "    Rating: 1\n",
    "    Reasoning: In this case the search result is a search query mentioning \\'Snowflake synapse private link\\'. However the search result doesn\\'t contain information about it. However it shows an error message for a generic private link which is tangentially related to the query, since snowflake synapse private link is a type of private link. Hence it has low relevance to the user query.\n",
    "    \n",
    "    Example 5:\n",
    "    INPUT:\n",
    "    User Query: The Punisher is American.\n",
    "    Search Result: The Rev(Samuel Smith) is a fictional character, a supervillain appearing in American comic books published by Marvel Comics. Created by Mike Baron and Klaus Janson, the character made his first appearance in The Punisher Vol. 2, #4 (November 1987). He is an enemy of the Punisher.\n",
    "    OUTPUT:\n",
    "    Rating: 1\n",
    "    Reasoning: In this case the search query is a statement concerning the Punisher. However the search result is about a character called Rev, who is an enemy of the Punisher. The search result is tangentially related to the user query but does not address topic about Punisher being an American. Hence it has low relevance to the user query.\n",
    "\n",
    "    Example 6:\n",
    "    INPUT:\n",
    "    User Query: query_history\n",
    "    Search Result: The function task_history() is not enough for the purposes when the required result set is more than 10k.If we perform UNION between information_schema and account_usage , then we will get more than 10k records along with recent records as from information_schema.query_history to snowflake.account_usage.query_history is 45 mins behind.\n",
    "    OUTPUT:\n",
    "    Rating: 1\n",
    "    Reasoning: In this case the search query mentioning one entity \\'query_history\\'. The search result is neither centered around it and neither has medium relevance, it only contains an unimportant reference to it. Hence it has low relevance to the user query.\n",
    "    \n",
    "    Example 7:\n",
    "    INPUT:\n",
    "    User Query: Who directed pulp fiction?\n",
    "    Search Result: Life on Earth first appeared as early as 4.28 billion years ago, soon after ocean formation 4.41 billion years ago, and not long after the formation of the Earth 4.54 billion years ago.\n",
    "    OUTPUT:\n",
    "    Rating: 0\n",
    "    Reasoning: In the case the search query is a question. However the search result does is completely unrelated to it. Hence the search result is completely irrelevant to the movie pulp fiction. \n",
    "    ###\n",
    "    \n",
    "    Now given the user query and search result below, rate the search result based on its relevance to the user query and provide a reasoning for your rating.\n",
    "   \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "umbrela_prompt = \"\"\"\n",
    "Given a query and a passage, you must provide a score on an\n",
    "integer scale of 0 to 3 with the following meanings:\n",
    "0 = represent that the passage has nothing to do with the query,\n",
    "1 = represents that the passage seems related to the query but\n",
    "does not answer it,\n",
    "2 = represents that the passage has some answer for the query,\n",
    "but the answer may be a bit unclear, or hidden amongst extraneous\n",
    "information and\n",
    "3 = represents that the passage is dedicated to the query and\n",
    "contains the exact answer.\n",
    "Important Instruction: Assign category 1 if the passage is\n",
    "somewhat related to the topic but not completely, category 2 if\n",
    "passage presents something very important related to the entire\n",
    "topic but also has some extra information and category 3 if the\n",
    "passage only and entirely refers to the topic. If none of the\n",
    "above satisfies give it category 0.\n",
    "Query: {query}\n",
    "Passage: {passage}\n",
    "Split this problem into steps:\n",
    "Consider the underlying intent of the search.\n",
    "Measure how well the content matches a likely intent of the query\n",
    "(M).\n",
    "Measure how trustworthy the passage is (T).\n",
    "Consider the aspects above and the relative importance of each,\n",
    "and decide on a final score (O). Final score must be an integer\n",
    "value only.\n",
    "Do not provide any code in result. Provide each score in the\n",
    "format of: ##final score: score without providing any reasoning.\n",
    "Always provide an output of the final category score described above (the final score\n",
    "on a scale of 0 to 3).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "zero_shot_categorical_prompt = \"\"\"\n",
    "Given a query and a passage, you just categorize the passage based on how well it answers the query. The categories that can be assigned are the following:\n",
    "\n",
    "IRRELEVANT -- if a passage is categorized as irrelevant to a query, this means that the passage has nothing to do with the query.\n",
    "FAIR -- if a passage is categorized as fair to a query, this means that the passage has low relevance to the query. The passage contains some information which seems very slightly related to the query but not enough information to answer the query. The passage contains some references or very limited information about some entities present in the query. In case the query is a statement on a topic, the passage should be tangentially related to it.\n",
    "GOOD -- if a passage is categorized as good to a query, this means that the passage has medium relevance to the query. If the query is a question, the passage may contain some information that is relevant to the query but not enough information to answer the query. If the query is a phrase/sentence, either the result is centered around most but not all entities present in the query, or if all the entities are present in the passage, the passage, while not being centered around it, has a medium level of relevance. In case the query is a statement on a topic, the passage should be related to the topic.\n",
    "EXCELLENT -- if a passage is categorized as excellent to a query, this means that the passage has a high relevance to the query. If the query is a question, the passage should contain information that can answer the query. Otherwise if the query is a phrase/sentence, it provides relevant information about all entities that are present in the query and the passage is centered around the entities mentioned in the query. In case the query is a statement on a topic, the passage should be either directly addressing it or be on the same topic.\n",
    "\n",
    "You should think step by step about the query and the passage and provide a categorization. You should also provide a reasoning for your categorization. If you absolutely cannot figure out a categorization, assign IRRELEVANT.\n",
    "\n",
    "\n",
    "Query: {query}\n",
    "Passage: {passage}\n",
    "\n",
    "Provide the output in the format of: ##Categorization: <category chosen for the query passage pair>\n",
    "Always provide an output of the final categoriy described above.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.feedback.v2.feedback import ContextRelevance\n",
    "\n",
    "trulens_prompt = (\n",
    "    ContextRelevance.system_prompt + \"\\n\\n\" + ContextRelevance.user_prompt\n",
    ")\n",
    "print(f\"TruLens prompt: \\n\\n {trulens_prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "# Function to rate context relevance\n",
    "def internal_prompt_relevance(\n",
    "    query: str, passage: str, model_engine=\"gpt-4o\"\n",
    ") -> dict:\n",
    "    # Prepare the prompt\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_engine,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": internal_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\" INPUT:\n",
    "                            User Query: {query}\n",
    "                            Search Result: {passage}\n",
    "                            OUTPUT:\\n\"\"\",\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Parse the response\n",
    "    output = response.choices[0].message.content.strip()\n",
    "\n",
    "    # Extract the rating and reasoning from the output\n",
    "    rating = None\n",
    "    reasoning = None\n",
    "    try:\n",
    "        for line in output.split(\"\\n\"):\n",
    "            if line.startswith(\"Rating:\"):\n",
    "                rating = int(line.split(\":\")[1].strip())\n",
    "            elif line.startswith(\"Reasoning:\"):\n",
    "                reasoning = line.split(\":\")[1].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing response: {e}\")\n",
    "\n",
    "    return {\"rating\": rating, \"reasoning\": reasoning, \"raw_response\": output}\n",
    "\n",
    "\n",
    "def umbrela_prompt_relevance(\n",
    "    query: str, passage: str, model_engine=\"gpt-4o\"\n",
    ") -> dict:\n",
    "    # Prepare the prompt\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_engine,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": umbrela_prompt.format(query=query, passage=passage),\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Parse the response\n",
    "    output = response.choices[0].message.content.strip()\n",
    "\n",
    "    # Extract the rating and reasoning from the output\n",
    "    rating = None\n",
    "    reasoning = None\n",
    "    try:\n",
    "        for line in output.split(\"\\n\"):\n",
    "            if line.startswith(\"##final score:\"):\n",
    "                rating = int(line.split(\":\")[1].strip())\n",
    "            elif line.startswith(\"Final score:\"):\n",
    "                rating = int(line.split(\":\")[1].strip())\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing response: {e}\")\n",
    "\n",
    "    return {\"rating\": rating, \"reasoning\": reasoning, \"raw_response\": output}\n",
    "\n",
    "\n",
    "def categorical_prompt_relevance(\n",
    "    query: str, passage: str, model_engine=\"gpt-4o\"\n",
    ") -> dict:\n",
    "    # Prepare the prompt\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_engine,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": zero_shot_categorical_prompt.format(\n",
    "                    query=query, passage=passage\n",
    "                ),\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Parse the response\n",
    "    output = response.choices[0].message.content.strip()\n",
    "\n",
    "    # Extract the rating and reasoning from the output\n",
    "    category = None\n",
    "    reasoning = None\n",
    "    try:\n",
    "        for line in output.split(\"\\n\"):\n",
    "            if line.startswith(\"##Categorization:\"):\n",
    "                category = line.split(\":\")[1].strip()\n",
    "            elif line.startswith(\"Reasoning:\"):\n",
    "                reasoning = line.split(\":\")[1].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing response: {e}\")\n",
    "\n",
    "    CATEGORY_TO_RATING = {\n",
    "        \"IRRELEVANT\": 0,\n",
    "        \"FAIR\": 1,\n",
    "        \"GOOD\": 2,\n",
    "        \"EXCELLENT\": 3,\n",
    "    }\n",
    "    if category in CATEGORY_TO_RATING:\n",
    "        rating = CATEGORY_TO_RATING[category]\n",
    "    else:\n",
    "        rating = None\n",
    "\n",
    "    return {\"rating\": rating, \"reasoning\": reasoning, \"raw_response\": output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "for model in [\"gpt-4o\", \"gpt-4o-mini\"]:\n",
    "    results = []\n",
    "\n",
    "    # Iterate over the DataFrame rows\n",
    "    for i, row in trec_combined_df.iterrows():\n",
    "        query = row[\"query\"]\n",
    "        passage = row[\"expected_response\"]\n",
    "        ground_truth = (\n",
    "            row[\"expected_score\"] * 3\n",
    "        )  # recover raw score {0, 1, 2, 3}\n",
    "\n",
    "        # print(f\"Query: {query}\")\n",
    "        # print(f\"Passage: {passage}\")\n",
    "\n",
    "        # Snowflake internal prompt\n",
    "        internal_result = internal_prompt_relevance(\n",
    "            query, passage, model_engine=model\n",
    "        )\n",
    "        internal_rating = internal_result[\"rating\"]\n",
    "\n",
    "        # Umbrela Prompt\n",
    "        umbrela_result = umbrela_prompt_relevance(\n",
    "            query, passage, model_engine=model\n",
    "        )\n",
    "        umbrela_rating = umbrela_result[\"rating\"]\n",
    "\n",
    "        # Categorical Prompt\n",
    "        categorical_result = categorical_prompt_relevance(\n",
    "            query, passage, model_engine=model\n",
    "        )\n",
    "        categorical_rating = categorical_result[\"rating\"]\n",
    "\n",
    "        # Append results to the list\n",
    "        results.append({\n",
    "            \"query_id\": row[\n",
    "                \"query_id\"\n",
    "            ],  # Assuming 'query_id' column exists in trec_combined_df\n",
    "            \"query\": query,\n",
    "            \"passage\": passage,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"internal_rating\": internal_rating,\n",
    "            \"umbrela_rating\": umbrela_rating,\n",
    "            \"categorical_rating\": categorical_rating,\n",
    "        })\n",
    "\n",
    "    # Convert results into a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Save results to CSV for further analysis\n",
    "    results_df.to_csv(f\"{model}_3_prompts_results.csv\", index=False)\n",
    "\n",
    "    # Inspect the DataFrame\n",
    "    print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "\n",
    "def plot_confusion_matrix_with_metrics(\n",
    "    csv_path, rating_column, title_prefix=\"\"\n",
    "):\n",
    "    data = pd.read_csv(csv_path)\n",
    "\n",
    "    # Define the full range of possible ground truth and predicted scores\n",
    "    all_scores = [0, 1, 2, 3]\n",
    "\n",
    "    # Create a confusion matrix with all scores explicitly defined\n",
    "    confusion_matrix = pd.crosstab(\n",
    "        data[\"ground_truth\"], data[rating_column], dropna=True\n",
    "    ).reindex(index=all_scores, columns=all_scores, fill_value=0)\n",
    "\n",
    "    # Skip plotting if all rows for rating_column are empty\n",
    "    if confusion_matrix.sum().sum() == 0:\n",
    "        print(f\"Skipping {rating_column}: no data available.\")\n",
    "        return\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        confusion_matrix, annot=True, fmt=\".0f\", cmap=\"Blues\", cbar=True\n",
    "    )\n",
    "    plt.title(f\"{title_prefix}Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Score\")\n",
    "    plt.ylabel(\"Ground Truth\")\n",
    "    plt.show()\n",
    "\n",
    "    # Flatten the confusion matrix for metrics calculation\n",
    "    ground_truth = data[\"ground_truth\"].apply(lambda x: 1 if x >= 2 else 0)\n",
    "    predictions = data[rating_column].apply(lambda x: 1 if x >= 2 else 0)\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision = precision_score(ground_truth, predictions, zero_division=0)\n",
    "    recall = recall_score(ground_truth, predictions, zero_division=0)\n",
    "    f1 = f1_score(ground_truth, predictions, zero_division=0)\n",
    "    # Calculate off-by-1 accuracy\n",
    "    off_by_1_correct = data.apply(\n",
    "        lambda row: abs(row[\"ground_truth\"] - row[rating_column]) <= 1, axis=1\n",
    "    ).sum()\n",
    "    off_by_1_accuracy = off_by_1_correct / len(data)\n",
    "\n",
    "    # Print the metrics\n",
    "    print(f\"{title_prefix}Metrics:\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Off-by-1 Accuracy: {off_by_1_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "csv_file = \"gpt-4o_3_prompts_results.csv\"\n",
    "plot_confusion_matrix_with_metrics(\n",
    "    csv_file, \"internal_rating\", \"Internal Ratings: \"\n",
    ")\n",
    "plot_confusion_matrix_with_metrics(\n",
    "    csv_file, \"umbrela_rating\", \"Umbrela Ratings: \"\n",
    ")\n",
    "plot_confusion_matrix_with_metrics(\n",
    "    csv_file, \"categorical_rating\", \"Categorical Ratings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the CSV file\n",
    "csv_file = \"/Users/dhuang/Documents/git/trulens/src/benchmark/trulens/benchmark/benchmark_frameworks/experiments/data/TREC_no_rubric.csv\"\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Step 2: Inspect the data\n",
    "print(data.head())\n",
    "\n",
    "# Ensure your CSV has columns: 'APP_VERSION', 'RAW_GT_SCORE', 'RAW_FEEDBACK_SCORE', and 'COUNT'\n",
    "\n",
    "# Step 3: Group data by 'APP_VERSION' and create a confusion matrix for each version\n",
    "app_versions = data[\"APP_VERSION\"].unique()  # Get unique app versions\n",
    "\n",
    "for app_version in app_versions:\n",
    "    # Filter data for the current app version\n",
    "    app_data = data[data[\"APP_VERSION\"] == app_version]\n",
    "\n",
    "    # Pivot the data to create a confusion matrix\n",
    "    confusion_matrix = app_data.pivot(\n",
    "        index=\"RAW_GT_SCORE\", columns=\"RAW_FEEDBACK_SCORE\", values=\"COUNT\"\n",
    "    ).fillna(0)\n",
    "\n",
    "    # Normalize the confusion matrix (optional)\n",
    "    confusion_matrix_normalized = confusion_matrix.div(\n",
    "        confusion_matrix.sum(axis=1), axis=0\n",
    "    )\n",
    "\n",
    "    # Step 4: Plot the confusion matrix for the current app version\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(confusion_matrix, annot=True, fmt=\".0f\", cmap=\"Blues\")\n",
    "    plt.title(f\"Confusion Matrix {app_version}\")\n",
    "    plt.xlabel(\"Feedback Score\")\n",
    "    plt.ylabel(\"Ground Truth\")\n",
    "    plt.show()\n",
    "\n",
    "    # Step 5: Plot the normalized confusion matrix for the current app version\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        confusion_matrix_normalized, annot=True, fmt=\".2f\", cmap=\"Blues\"\n",
    "    )\n",
    "    plt.title(f\"Normalized Confusion Matrix {app_version}\")\n",
    "    plt.xlabel(\"Feedback Score\")\n",
    "    plt.ylabel(\"Ground Truth\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
