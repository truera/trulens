{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q scikit-learn litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import groundedness feedback function\n",
    "from benchmark_frameworks.eval_as_recommendation import (\n",
    "    run_benchmark_with_temp_scaling,\n",
    ")\n",
    "from test_cases import generate_ms_marco_context_relevance_benchmark\n",
    "from trulens.core import TruSession\n",
    "\n",
    "TruSession().reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "os.environ[\"SNOWFLAKE_ACCOUNT\"] = \"xxx-xxx\"  # xxx-xxx.snowflakecomputing.com\n",
    "os.environ[\"SNOWFLAKE_USER\"] = \"xxx\"\n",
    "os.environ[\"SNOWFLAKE_USER_PASSWORD\"] = \"xxx\"\n",
    "os.environ[\"SNOWFLAKE_DATABASE\"] = \"xxx\"\n",
    "os.environ[\"SNOWFLAKE_SCHEMA\"] = \"xxx\"\n",
    "os.environ[\"SNOWFLAKE_WAREHOUSE\"] = \"xxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Session\n",
    "from trulens.core.utils.keys import check_keys\n",
    "\n",
    "check_keys(\"SNOWFLAKE_ACCOUNT\", \"SNOWFLAKE_USER\", \"SNOWFLAKE_USER_PASSWORD\")\n",
    "\n",
    "connection_params = {\n",
    "    \"account\": os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "    \"user\": os.environ[\"SNOWFLAKE_USER\"],\n",
    "    \"password\": os.environ[\"SNOWFLAKE_USER_PASSWORD\"],\n",
    "}\n",
    "\n",
    "\n",
    "# Create a Snowflake session\n",
    "snowpark_session = Session.builder.configs(connection_params).create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up initial model providers as evaluators for meta evaluation\n",
    "\n",
    "We will start with GPT-4o as the benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.providers.cortex import Cortex\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "gpt4o = OpenAI(model_engine=\"gpt-4o\")\n",
    "mistral = Cortex(snowpark_session, model_engine=\"mistral-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4o.context_relevance_with_cot_reasons(\n",
    "    \"who is the guy calling?\", \"some guy calling saying his name is Danny\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, confidence = gpt4o.context_relevance_verb_confidence(\n",
    "    \"who is steve jobs\", \"apple founder is steve jobs\"\n",
    ")\n",
    "print(f\"score: {score}, confidence: {confidence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, confidence = mistral.context_relevance_verb_confidence(\n",
    "    \"who is the guy calling?\",\n",
    "    \"some guy calling saying his name is Danny\",\n",
    "    temperature=0.5,\n",
    ")\n",
    "print(f\"score: {score}, confidence: {confidence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_data = []\n",
    "for i in range(1, 6):\n",
    "    dataset_path = f\"./datasets/ms_marco/ms_marco_train_v2.1_{i}.json\"\n",
    "    benchmark_data.extend(\n",
    "        list(generate_ms_marco_context_relevance_benchmark(dataset_path))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(benchmark_data)\n",
    "\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "temperatures = [0, 0.3, 0.7, 1]\n",
    "\n",
    "\n",
    "def wrapped_relevance_gpt4o(input, output, temperature):\n",
    "    return gpt4o.context_relevance_verb_confidence(\n",
    "        question=input, context=output, temperature=temperature\n",
    "    )\n",
    "\n",
    "\n",
    "def wrapped_relevance_mistral(input, output, temperature):\n",
    "    return mistral.context_relevance_verb_confidence(\n",
    "        question=input, context=output, temperature=temperature\n",
    "    )\n",
    "\n",
    "\n",
    "feedback_functions = {\n",
    "    \"gpt-4o\": wrapped_relevance_gpt4o,\n",
    "    \"mistral-large\": wrapped_relevance_mistral,\n",
    "}\n",
    "\n",
    "backoffs_by_functions = {\n",
    "    \"gpt-4o\": 0,\n",
    "    \"mistral-large\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "# Parallelizing temperature scaling\n",
    "k = 1  #  MS MARCO specific\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [\n",
    "        executor.submit(\n",
    "            run_benchmark_with_temp_scaling,\n",
    "            df,\n",
    "            feedback_functions,\n",
    "            temp,\n",
    "            k,\n",
    "            backoffs_by_functions,\n",
    "        )\n",
    "        for temp in temperatures\n",
    "    ]\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of calibration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "\n",
    "def plot_reliability_diagram(csv_file, temperature, ece_value, brier_score):\n",
    "    data = pd.read_csv(\n",
    "        csv_file,\n",
    "        header=None,\n",
    "        names=[\"query_id\", \"relevance_score\", \"confidence_score\", \"true_label\"],\n",
    "    )\n",
    "\n",
    "    # Compute calibration curve\n",
    "    true_pred = (\n",
    "        (data[\"relevance_score\"] >= 0.5).astype(int) == data[\"true_label\"]\n",
    "    ).astype(int)\n",
    "\n",
    "    prob_true, prob_pred = calibration_curve(\n",
    "        true_pred, data[\"confidence_score\"], n_bins=5\n",
    "    )\n",
    "\n",
    "    # Plot reliability diagram\n",
    "    plt.plot(\n",
    "        prob_pred,\n",
    "        prob_true,\n",
    "        marker=\"o\",\n",
    "        linewidth=1,\n",
    "        label=f\"Temperature {temperature}\",\n",
    "    )\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", label=\"Perfectly calibrated\")\n",
    "\n",
    "    # Display ECE value\n",
    "    plt.text(\n",
    "        0.6,\n",
    "        0.2,\n",
    "        f\"ECE: {ece_value:.4f}\",\n",
    "        bbox=dict(facecolor=\"white\", alpha=0.5),\n",
    "    )\n",
    "    plt.text(\n",
    "        0.6,\n",
    "        0.1,\n",
    "        f\"Brier score: {brier_score:.4f}\",\n",
    "        bbox=dict(facecolor=\"white\", alpha=0.5),\n",
    "    )\n",
    "    # Labels and title\n",
    "    plt.xlabel(\"Confidence bins\")\n",
    "    plt.ylabel(\"Accuracy bins\")\n",
    "    plt.title(f\"Reliability Diagram for GPT-4o with t={temperature}\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"results/gpt-4o-t_0-benchmark_eval_results.csv\"\n",
    "ece = 0.25978426229508195\n",
    "brier_score = 0.23403157255616272"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reliability_diagram(csv_file, 0, ece, brier_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of temperatures and corresponding CSV files\n",
    "temperatures = [0, 0.3, 0.7, 1]\n",
    "csv_files = [\n",
    "    \"consolidated_results_verbalized_ece_t_0.csv\",\n",
    "    \"consolidated_results_verbalized_ece_t_0.3.csv\",\n",
    "    \"consolidated_results_verbalized_ece_t_0.7.csv\",\n",
    "    \"consolidated_results_verbalized_ece_t_1.csv\",\n",
    "]\n",
    "\n",
    "# Load and combine data\n",
    "data = []\n",
    "for temp, csv_file in zip(temperatures, csv_files):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df[\"Temperature\"] = temp\n",
    "    data.append(df)\n",
    "\n",
    "combined_data = pd.concat(data)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 8))\n",
    "bar_width = 0.1\n",
    "\n",
    "# Plot Precision@1\n",
    "plt.subplot(3, 1, 1)\n",
    "for i, function_name in enumerate(combined_data[\"Function Name\"].unique()):\n",
    "    subset = combined_data[combined_data[\"Function Name\"] == function_name]\n",
    "    plt.bar(\n",
    "        [t + i * bar_width for t in temperatures],\n",
    "        subset[\"Precision@1\"],\n",
    "        width=bar_width,\n",
    "        label=function_name,\n",
    "    )\n",
    "plt.title(\"Precision@1 (higher the better)\")\n",
    "plt.xlabel(\"Temperature\")\n",
    "plt.ylabel(\"Precision@1\")\n",
    "plt.xticks(\n",
    "    [\n",
    "        t + bar_width * (len(combined_data[\"Function Name\"].unique()) - 1) / 2\n",
    "        for t in temperatures\n",
    "    ],\n",
    "    temperatures,\n",
    ")\n",
    "plt.legend()\n",
    "\n",
    "# Plot ECE\n",
    "plt.subplot(3, 1, 2)\n",
    "for i, function_name in enumerate(combined_data[\"Function Name\"].unique()):\n",
    "    subset = combined_data[combined_data[\"Function Name\"] == function_name]\n",
    "    plt.bar(\n",
    "        [t + i * bar_width for t in temperatures],\n",
    "        subset[\"ECE\"],\n",
    "        width=bar_width,\n",
    "        label=function_name,\n",
    "    )\n",
    "plt.title(\"ECE (lower the better)\")\n",
    "plt.xlabel(\"Temperature\")\n",
    "plt.ylabel(\"ECE\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot Brier Score\n",
    "plt.subplot(3, 1, 3)\n",
    "for i, function_name in enumerate(combined_data[\"Function Name\"].unique()):\n",
    "    subset = combined_data[combined_data[\"Function Name\"] == function_name]\n",
    "    plt.bar(\n",
    "        [t + i * bar_width for t in temperatures],\n",
    "        subset[\"Brier Score\"],\n",
    "        width=bar_width,\n",
    "        label=function_name,\n",
    "    )\n",
    "plt.title(\"Brier Score (lower the better)\")\n",
    "plt.xlabel(\"Temperature\")\n",
    "plt.ylabel(\"Brier Score\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = [0, 0.3, 0.7, 1]\n",
    "csv_files = [\n",
    "    \"consolidated_results_verbalized_ece_t_0.csv\",\n",
    "    \"consolidated_results_verbalized_ece_t_0.3.csv\",\n",
    "    \"consolidated_results_verbalized_ece_t_0.7.csv\",\n",
    "    \"consolidated_results_verbalized_ece_t_1.csv\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and combine data\n",
    "data = []\n",
    "for temp, csv_file in zip(temperatures, csv_files):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df[\"Temperature\"] = temp\n",
    "    data.append(df)\n",
    "\n",
    "combined_data = pd.concat(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.groupby([\"Function Name\", \"Temperature\"]).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
