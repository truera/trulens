{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ““ Groundedness Evaluations\n",
    "\n",
    "In many ways, feedbacks can be thought of as LLM apps themselves. Given text, they return some result. Thinking in this way, we can use TruLens to evaluate and track our feedback quality. We can even do this for different models (e.g. gpt-3.5 and gpt-4) or prompting schemes (such as chain-of-thought reasoning).\n",
    "\n",
    "This notebook follows an evaluation of a set of test cases generated from human annotated datasets. In particular, we generate test cases from [SummEval](https://arxiv.org/abs/2007.12626).\n",
    "\n",
    "SummEval is one of the datasets dedicated to automated evaluations on summarization tasks, which are closely related to the groundedness evaluation in RAG with the retrieved context (i.e. the source) and response (i.e. the summary). It contains human annotation of numerical score (**1** to **5**) comprised of scoring from 3 human expert annotators and 5 crowd-sourced annotators. There are 16 models being used for generation in total for 100 paragraphs in the test set, so there are a total of 16,000 machine-generated summaries. Each paragraph also has several human-written summaries for comparative analysis. \n",
    "\n",
    "\n",
    "For evaluating groundedness feedback functions, we compute the annotated \"consistency\" scores, a measure of whether the summarized response is factually consistent with the source texts and hence can be used as a proxy to evaluate groundedness in our RAG triad, and normalized to **0** to **1** score as our **expected_score** and to match the output of feedback functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import groundedness feedback function\n",
    "from test_cases import generate_summeval_groundedness_golden_set\n",
    "from trulens.apps.basic import TruBasicApp\n",
    "from trulens.core import Feedback\n",
    "from trulens.core import Select\n",
    "from trulens.core import TruSession\n",
    "from trulens.feedback import GroundTruthAgreement\n",
    "\n",
    "TruSession().reset_database()\n",
    "\n",
    "# generator for groundedness golden set\n",
    "test_cases_gen = generate_summeval_groundedness_golden_set(\n",
    "    \"./datasets/summeval/summeval_test_100.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the number of test cases we want to run the smoke test on\n",
    "groundedness_golden_set = []\n",
    "for i in range(5):\n",
    "    groundedness_golden_set.append(next(test_cases_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundedness_golden_set[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking various Groundedness feedback function providers (OpenAI GPT-3.5-turbo vs GPT-4 vs Huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from trulens.feedback.v2.feedback import Groundedness\n",
    "from trulens.providers.huggingface import Huggingface\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "openai_provider = OpenAI()\n",
    "openai_gpt4_provider = OpenAI(model_engine=\"gpt-4\")\n",
    "huggingface_provider = Huggingface()\n",
    "\n",
    "\n",
    "groundedness_hug = Groundedness(groundedness_provider=huggingface_provider)\n",
    "groundedness_openai = Groundedness(groundedness_provider=openai_provider)\n",
    "groundedness_openai_gpt4 = Groundedness(\n",
    "    groundedness_provider=openai_gpt4_provider\n",
    ")\n",
    "\n",
    "f_groundedness_hug = (\n",
    "    Feedback(\n",
    "        huggingface_provider.groundedness_measure,\n",
    "        name=\"Groundedness Huggingface\",\n",
    "    )\n",
    "    .on_input()\n",
    "    .on_output()\n",
    "    .aggregate(groundedness_hug.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "\n",
    "def wrapped_groundedness_hug(input, output):\n",
    "    return np.mean(list(f_groundedness_hug(input, output)[0].values()))\n",
    "\n",
    "\n",
    "f_groundedness_openai = (\n",
    "    Feedback(\n",
    "        OpenAI(model_engine=\"gpt-3.5-turbo\").groundedness_measure,\n",
    "        name=\"Groundedness OpenAI GPT-3.5\",\n",
    "    )\n",
    "    .on_input()\n",
    "    .on_output()\n",
    "    .aggregate(groundedness_openai.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "\n",
    "def wrapped_groundedness_openai(input, output):\n",
    "    return f_groundedness_openai(input, output)[0][\"full_doc_score\"]\n",
    "\n",
    "\n",
    "f_groundedness_openai_gpt4 = (\n",
    "    Feedback(\n",
    "        OpenAI(model_engine=\"gpt-3.5-turbo\").groundedness_measure,\n",
    "        name=\"Groundedness OpenAI GPT-4\",\n",
    "    )\n",
    "    .on_input()\n",
    "    .on_output()\n",
    "    .aggregate(groundedness_openai_gpt4.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "\n",
    "def wrapped_groundedness_openai_gpt4(input, output):\n",
    "    return f_groundedness_openai_gpt4(input, output)[0][\"full_doc_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Feedback object using the numeric_difference method of the ground_truth object\n",
    "ground_truth = GroundTruthAgreement(groundedness_golden_set, provider=OpenAI())\n",
    "# Call the numeric_difference method with app and record and aggregate to get the mean absolute error\n",
    "f_absolute_error = (\n",
    "    Feedback(ground_truth.absolute_error, name=\"Mean Absolute Error\")\n",
    "    .on(Select.Record.calls[0].args.args[0])\n",
    "    .on(Select.Record.calls[0].args.args[1])\n",
    "    .on_output()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_wrapped_groundedness_hug = TruBasicApp(\n",
    "    wrapped_groundedness_hug,\n",
    "    app_name=\"groundedness\",\n",
    "    app_version=\"huggingface\",\n",
    "    feedbacks=[f_absolute_error],\n",
    ")\n",
    "tru_wrapped_groundedness_openai = TruBasicApp(\n",
    "    wrapped_groundedness_openai,\n",
    "    app_name=\"groundedness\",\n",
    "    app_version=\"openai gpt-3.5\",\n",
    "    feedbacks=[f_absolute_error],\n",
    ")\n",
    "tru_wrapped_groundedness_openai_gpt4 = TruBasicApp(\n",
    "    wrapped_groundedness_openai_gpt4,\n",
    "    app_name=\"groundedness\",\n",
    "    app_version=\"openai gpt-4\",\n",
    "    feedbacks=[f_absolute_error],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(groundedness_golden_set)):\n",
    "    source = groundedness_golden_set[i][\"query\"]\n",
    "    response = groundedness_golden_set[i][\"response\"]\n",
    "    with tru_wrapped_groundedness_hug as recording:\n",
    "        tru_wrapped_groundedness_hug.app(source, response)\n",
    "    with tru_wrapped_groundedness_openai as recording:\n",
    "        tru_wrapped_groundedness_openai.app(source, response)\n",
    "    with tru_wrapped_groundedness_openai_gpt4 as recording:\n",
    "        tru_wrapped_groundedness_openai_gpt4.app(source, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TruSession().get_leaderboard().sort_values(by=\"Mean Absolute Error\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
