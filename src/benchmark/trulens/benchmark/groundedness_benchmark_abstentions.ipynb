{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ““ Groundedness Evaluations for Abstention Handling\n",
    "\n",
    "In many ways, feedbacks can be thought of as LLM apps themselves. Given text, they return some result. Thinking in this way, we can use TruLens to evaluate and track our feedback quality. We can even do this for different models or prompting schemes (such as chain-of-thought reasoning).\n",
    "\n",
    "This notebook follows an evaluation of a set of test cases generated from human annotated datasets. In particular, we generate test cases from [SummEval](https://arxiv.org/abs/2007.12626).\n",
    "\n",
    "SummEval is one of the datasets dedicated to automated evaluations on summarization tasks, which are closely related to the groundedness evaluation in RAG with the retrieved context (i.e. the source) and response (i.e. the summary). It contains human annotation of numerical score (**1** to **5**) comprised of scoring from 3 human expert annotators and 5 croweded-sourced annotators. There are 16 models being used for generation in total for 100 paragraphs in the test set, so there are a total of 16,000 machine-generated summaries. Each paragraph also has several human-written summaries for comparative analysis. \n",
    "\n",
    "For evaluating groundedness feedback functions, we compute the annotated \"consistency\" scores, a measure of whether the summarized response is factually consistent with the source texts and hence can be used as a proxy to evaluate groundedness in our RAG triad, and normalized to **0** to **1** score as our **expected_score** and to match the output of feedback functions.\n",
    "\n",
    "## Abstention Background\n",
    "\n",
    "In this particular set of evaluations, we are focused on the handling of abstentions. Uncertainty-based abstention in LLMs has been shown to improve safety and reduce hallucination ([Tomani](https://arxiv.org/abs/2404.10960)). For groundedness evaluations, we want to ensure these are handled in a manner that is consistent with human preferences; in other words, calibrated.\n",
    "\n",
    "Abstentions can be broken down into two distinct groups, distinguished by whether the question is answerable given the context. In other words, whether the abstention is **justified**. We take an opinionated stance that abstentions for unanswerable questions are justified and therefore, **grounded**. Alternatively, abstentions for questions answerable by the context are therefore not grounded.\n",
    "\n",
    "## Experimental Setup\n",
    "\n",
    "For this set of experiments, we take the same test cases used for groundedness evaluations with a few key changes:\n",
    "1. We randomly sample approximately 50% of the test cases and replace the response with an abstention. We'll refer to this as the abstention set. The rest will be head as control.\n",
    "2. In the abstention set, randomly sample approximately 50% of the test cases to generate a related question using GPT4o-mini, and generate an unrelated question for the rest. This splits the absention set into **answerable** and **unanswerable** abstentions.\n",
    "\n",
    "From here, we have two tests sets to test against.\n",
    "1. The expected score for all abstentions is 1.\n",
    "2. The expected score for answerable abstentions will be set to 0, and then the expected score for unanswerable abstentions will be set to 1.\n",
    "\n",
    "We will then compute the MAE of our groundedness evaluator against the expected score for each test set. Results will be displayed for the following subgroups:\n",
    "- Abstentions v. Control\n",
    "- Answerable Abstentions v. Unanswerable Abstentions\n",
    "\n",
    "We will also show results for the test cases with high and low human-annotated consistency to ensure that this treatment is consistent across expected scores.\n",
    "\n",
    "\n",
    "## Improving Groundedness\n",
    "\n",
    "To improve the groundedness feedback function against these tests, we consider the following changes:\n",
    "- Abstentions are excluded from statements to evaluate; empty strings are considered grounded.\n",
    "- The answerability in abstention cases is assessed; unanswerable abstentions score high, answerable abstentions score low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import groundedness feedback function\n",
    "import copy\n",
    "\n",
    "from test_cases import generate_summeval_groundedness_golden_set\n",
    "from trulens.apps.basic import TruBasicApp\n",
    "from trulens.core import Feedback\n",
    "from trulens.core import Select\n",
    "from trulens.core import TruSession\n",
    "from trulens.feedback import GroundTruthAgreement\n",
    "\n",
    "tru = TruSession()\n",
    "\n",
    "# generator for groundedness golden set\n",
    "test_cases_gen = generate_summeval_groundedness_golden_set(\n",
    "    \"./datasets/summeval/summeval_test_100.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the number of test cases we want to run the smoke test on\n",
    "groundedness_golden_set_abstentions = []\n",
    "for i in range(500):\n",
    "    groundedness_golden_set_abstentions.append(next(test_cases_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Specify the number of test cases we want to run the smoke test on\n",
    "groundedness_golden_set_abstentions = []\n",
    "for i in range(500):\n",
    "    groundedness_golden_set_abstentions.append(next(test_cases_gen))\n",
    "\n",
    "# Randomly sample 50% of the test cases and replace the response with a random abstention\n",
    "# Split into answerable and unanswerable set\n",
    "abstention_set = []\n",
    "control_set = []\n",
    "for item in groundedness_golden_set_abstentions:\n",
    "    if random.random() < 0.5:\n",
    "        abstention_set.append(item)\n",
    "    else:\n",
    "        control_set.append(item)\n",
    "\n",
    "for item in abstention_set:\n",
    "    item[\"response\"] = random.choice([\n",
    "        \"I don't know\",\n",
    "        \"I don't have the information available to answer this question\",\n",
    "        \"I'm unsure of the answer given the provided context\",\n",
    "        \"I'm not certain\",\n",
    "        \"I can't say for sure\",\n",
    "        \"I'm not familiar with that\",\n",
    "        \"I'm not knowledgeable about this topic\",\n",
    "        \"I'm not the right person to ask\",\n",
    "        \"I'm unable to provide a definitive answer\",\n",
    "        \"I'm sorry, I can't help with that\",\n",
    "        \"This is beyond my current understanding\",\n",
    "        \"That's outside my area of expertise\",\n",
    "        \"I need more information to give a proper response\",\n",
    "        \"This topic is a bit too complex for a quick answer\",\n",
    "        \"I'm not equipped to answer this question\",\n",
    "        \"My knowledge on this subject is limited\",\n",
    "        \"I don't have a clear answer for you\",\n",
    "        \"This requires more research than I can provide at the moment\",\n",
    "        \"I'm not confident enough in my knowledge to give an answer\",\n",
    "        \"I'm afraid I don't have an answer to that\",\n",
    "        \"This question stumps me, sorry\",\n",
    "        \"I'm drawing a blank on this one\",\n",
    "        \"I'm not sure I'm the best source for this information\",\n",
    "        \"I don't have the expertise to answer this\",\n",
    "        \"I'm at a loss for words on this topic\",\n",
    "        \"I'm not sure how to respond to that\",\n",
    "        \"I don't have a definitive answer\",\n",
    "        \"I'm not certain how to answer\",\n",
    "        \"I'm not sure I understand the question fully\",\n",
    "        \"I'm not able to provide a satisfactory answer\",\n",
    "        \"I'm not sure I can provide the best answer\",\n",
    "        \"I'm not qualified to answer this question\",\n",
    "        \"I'm not sure I have the information you're looking for\",\n",
    "        \"I'm not able to answer that question\",\n",
    "        \"I'm not sure I can be of help with that question\",\n",
    "        \"I'm not the right one to answer that\",\n",
    "        \"I'm not able to give a precise answer\",\n",
    "        \"I'm not sure I have a good answer for you\",\n",
    "        \"I'm not able to provide an accurate answer\",\n",
    "        \"I'm not sure I can answer that accurately\",\n",
    "        \"I'm not able to give you a clear answer\",\n",
    "        \"I'm not sure I can be of assistance with that question\",\n",
    "        \"I'm not able to provide a clear response\",\n",
    "        \"I'm not sure I can give you the answer you're looking for\",\n",
    "        \"I'm not able to provide the information you need\",\n",
    "        \"I'm not sure I can help with that\",\n",
    "        \"I'm not able to give a definitive response\",\n",
    "        \"I'm not sure I can provide a clear answer\",\n",
    "        \"I'm not able to answer that with certainty\",\n",
    "        \"I'm not sure I have the answer to that\",\n",
    "        \"I'm not able to provide a definite answer\",\n",
    "        \"I'm not sure I can give a precise answer\",\n",
    "        \"I'm not able to give an exact answer\",\n",
    "        \"I'm not sure I can provide an exact response\",\n",
    "        \"I'm not able to provide a specific answer\",\n",
    "        \"I'm not sure I can offer a specific response\",\n",
    "        \"I'm not able to give a detailed answer\",\n",
    "        \"I'm not sure I can provide a detailed response\",\n",
    "        \"I'm not able to offer a comprehensive answer\",\n",
    "        \"I'm not sure I can give a comprehensive response\",\n",
    "        \"I'm not able to provide a thorough answer\",\n",
    "        \"I'm not sure I can offer a thorough response\",\n",
    "        \"I'm not able to give an in-depth answer\",\n",
    "        \"I'm not sure I can provide an in-depth response\",\n",
    "        \"I'm not able to offer a nuanced answer\",\n",
    "        \"I'm not sure I can give a nuanced response\",\n",
    "        \"I'm not able to provide a nuanced explanation\",\n",
    "        \"I'm not sure I can offer a nuanced explanation\",\n",
    "        \"I'm not able to give a complete answer\",\n",
    "        \"I'm not sure I can provide a complete response\",\n",
    "        \"I'm not able to offer a complete explanation\",\n",
    "        \"I'm not sure I can give a complete explanation\",\n",
    "        \"I'm not able to provide a full answer\",\n",
    "        \"I'm not sure I can offer a full response\",\n",
    "        \"I'm not able to give a full explanation\",\n",
    "        \"I'm not sure I can provide a full explanation\",\n",
    "        \"I'm not able to offer a detailed explanation\",\n",
    "        \"I'm not sure I can give a detailed explanation\",\n",
    "        \"I'm not able to provide an exhaustive answer\",\n",
    "        \"I'm not sure I can offer an exhaustive response\",\n",
    "        \"I'm not able to give an exhaustive explanation\",\n",
    "        \"I'm not sure I can provide an exhaustive explanation\",\n",
    "        \"I'm not able to offer a precise explanation\",\n",
    "        \"I'm not sure I can give a precise explanation\",\n",
    "        \"I'm not able to provide a specific explanation\",\n",
    "        \"I'm not sure I can offer a specific explanation\",\n",
    "        \"I'm not able to give a clear explanation\",\n",
    "        \"I'm not sure I can provide a clear explanation\",\n",
    "        \"I'm sorry, but I can't provide the information requested\",\n",
    "        \"Unfortunately, I'm not able to answer that question\",\n",
    "        \"I'm unable to assist with this query\",\n",
    "        \"My capabilities do not extend to providing an answer for this\",\n",
    "        \"I'm sorry, but that's beyond what I can offer assistance with\",\n",
    "        \"I don't have the capability to answer that question\",\n",
    "        \"That's outside the scope of my assistance\",\n",
    "        \"I'm not equipped to provide an answer to that question\",\n",
    "        \"I'm sorry, but I can't help with that specific request\",\n",
    "        \"Unfortunately, I don't have the resources to answer that\",\n",
    "        \"I'm unable to provide a response to that question\",\n",
    "    ])\n",
    "    item[\"group\"] = \"Abstention\"\n",
    "    split_result = item[\"query\"].split(\".\", 1)\n",
    "    item[\"user_question\"] = split_result[\n",
    "        0\n",
    "    ]  # keep the original first part of the query as the user question no matter what\n",
    "\n",
    "# Set the group for the remaining cases as 'Control'\n",
    "for item in groundedness_golden_set_abstentions:\n",
    "    if \"group\" not in item:\n",
    "        item[\"group\"] = \"Control\"\n",
    "        item[\"user_question\"] = (\n",
    "            client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a helpful assistant.\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"Write a question that can be answered from the following information: {item['query']}\",\n",
    "                    },\n",
    "                ],\n",
    "            )\n",
    "            .choices[0]\n",
    "            .message.content\n",
    "        )\n",
    "\n",
    "# Split into answerable and unanswerable set\n",
    "unanswerable_abstentions = []\n",
    "answerable_abstentions = []\n",
    "\n",
    "abstention_set_copy = copy.deepcopy(abstention_set)\n",
    "for item in abstention_set_copy:\n",
    "    item[\"expected_score\"] = 1.0\n",
    "    if random.random() < 0.5:\n",
    "        item[\"group\"] = \"Unanswerable Abstention\"\n",
    "        item[\"user_question\"] = (\n",
    "            client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a helpful assistant.\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": \"Write a random, fact-based question. It can be about anything.\",\n",
    "                    },\n",
    "                ],\n",
    "            )\n",
    "            .choices[0]\n",
    "            .message.content\n",
    "        )\n",
    "        unanswerable_abstentions.append(item)\n",
    "    else:\n",
    "        item[\"group\"] = \"Answerable Abstention\"\n",
    "        item[\"user_question\"] = random.choice([\n",
    "            f\"What is the answer to this question: {split_result[-1]}?\",\n",
    "            f\"Can you answer this question: {split_result[-1]}?\",\n",
    "        ])\n",
    "        item[\"user_question\"] = (\n",
    "            client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a helpful assistant.\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"Write a question that can be answered from the following information: {item['query']}\",\n",
    "                    },\n",
    "                ],\n",
    "            )\n",
    "            .choices[0]\n",
    "            .message.content\n",
    "        )\n",
    "        answerable_abstentions.append(item)\n",
    "\n",
    "# create a set that includes all abstentions and the control set\n",
    "groundedness_golden_set_abstensions_score_high = (\n",
    "    control_set + answerable_abstentions + unanswerable_abstentions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Write out the data\n",
    "with open(\"groundedness_golden_set_abstensions.pkl\", \"wb\") as file:\n",
    "    pickle.dump(groundedness_golden_set_abstensions_score_high, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data back in\n",
    "with open(\"groundedness_golden_set_abstensions.pkl\", \"rb\") as file:\n",
    "    groundedness_golden_set_abstensions_score_high = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answerable_abstentions_ca = copy.deepcopy(answerable_abstentions)\n",
    "for item in answerable_abstentions_ca:\n",
    "    item[\"expected_score\"] = 0.0\n",
    "\n",
    "groundedness_golden_set_abstensions_consider_answerability = (\n",
    "    control_set + answerable_abstentions_ca + unanswerable_abstentions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundedness_golden_set_abstensions_consider_answerability_copy = copy.deepcopy(\n",
    "    groundedness_golden_set_abstensions_consider_answerability\n",
    ")\n",
    "\n",
    "test_set = [\n",
    "    {\n",
    "        \"query\": item[\"query\"],\n",
    "        \"response\": item[\"response\"],\n",
    "        \"user_question\": item[\"user_question\"],\n",
    "        \"group\": item[\"group\"],\n",
    "    }\n",
    "    for item in groundedness_golden_set_abstensions_consider_answerability_copy\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Consider answerability of abstentions\n",
    "print(\"\\n Consider answerability of abstentions\")\n",
    "\n",
    "## Count the data by group (Control, Answerable Abstention, Unanswerable Abstention)\n",
    "ca_df = pd.DataFrame(groundedness_golden_set_abstensions_consider_answerability)\n",
    "ca_group_counts = ca_df[\"group\"].value_counts()\n",
    "print(ca_group_counts)\n",
    "\n",
    "## Calculate average values for expected_score and human_score by group\n",
    "ca_group_avg = ca_df.groupby(\"group\").agg({\n",
    "    \"expected_score\": \"mean\",\n",
    "    \"human_score\": \"mean\",\n",
    "})\n",
    "ca_group_avg = ca_group_avg.reindex([\n",
    "    \"Control\",\n",
    "    \"Answerable Abstention\",\n",
    "    \"Unanswerable Abstention\",\n",
    "])\n",
    "print(ca_group_avg)\n",
    "\n",
    "# Reward all abstentions equally\n",
    "print(\"\\n Reward all abstentions equally\")\n",
    "\n",
    "sh_df = pd.DataFrame(groundedness_golden_set_abstensions_score_high)\n",
    "sh_group_counts = sh_df[\"group\"].value_counts()\n",
    "print(sh_group_counts)\n",
    "\n",
    "## Calculate average values for expected_score and human_score by group\n",
    "sh_group_avg = sh_df.groupby(\"group\").agg({\n",
    "    \"expected_score\": \"mean\",\n",
    "    \"human_score\": \"mean\",\n",
    "})\n",
    "sh_group_avg = sh_group_avg.reindex([\n",
    "    \"Control\",\n",
    "    \"Answerable Abstention\",\n",
    "    \"Unanswerable Abstention\",\n",
    "])\n",
    "print(sh_group_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking GPT4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "openai_provider = OpenAI(model_engine=\"gpt-4o\")\n",
    "f_groundedness_openai_gpt4o = Feedback(\n",
    "    openai_provider.groundedness_measure_with_cot_reasons,\n",
    "    name=\"Groundedness OpenAI GPT-4o - Reward Abstention\",\n",
    ")\n",
    "\n",
    "\n",
    "def wrapped_groundedness_openai_gpt4o(input: str, output: str) -> float:\n",
    "    return f_groundedness_openai_gpt4o(input, output)[0]\n",
    "\n",
    "\n",
    "f_groundedness_openai_gpt4o_consider_answerability = Feedback(\n",
    "    openai_provider.groundedness_measure_with_cot_reasons_consider_answerability\n",
    ")\n",
    "\n",
    "\n",
    "def wrapped_groundedness_openai_gpt4o_consider_answerability(\n",
    "    input: str, output: str, question: str\n",
    ") -> float:\n",
    "    return f_groundedness_openai_gpt4o_consider_answerability(\n",
    "        input, output, question\n",
    "    )[0]\n",
    "\n",
    "\n",
    "# Create a Feedback object using the numeric_difference method of the ground_truth object\n",
    "ground_truth_consider_answerability = GroundTruthAgreement(\n",
    "    groundedness_golden_set_abstensions_consider_answerability,\n",
    "    provider=openai_provider,\n",
    ")\n",
    "\n",
    "# Call the numeric_difference method with app and record and aggregate to get the mean absolute error\n",
    "f_absolute_error_consider_answerability = (\n",
    "    Feedback(\n",
    "        ground_truth_consider_answerability.absolute_error,\n",
    "        name=\"Mean Absolute Error (consider answerability)\",\n",
    "        higher_is_better=False,\n",
    "    )\n",
    "    .on(Select.Record.app._call.args.args[0])\n",
    "    .on(Select.Record.app._call.args.args[1])\n",
    "    .on(Select.RecordOutput)\n",
    ")\n",
    "\n",
    "# Create a Feedback object using the numeric_difference method of the ground_truth object\n",
    "ground_truth_abstensions_score_high = GroundTruthAgreement(\n",
    "    groundedness_golden_set_abstensions_score_high, provider=openai_provider\n",
    ")\n",
    "# Call the numeric_difference method with app and record and aggregate to get the mean absolute error\n",
    "f_absolute_error_abstensions_score_high = (\n",
    "    Feedback(\n",
    "        ground_truth_abstensions_score_high.absolute_error,\n",
    "        name=\"Mean Absolute Error (all abstensions score high)\",\n",
    "        higher_is_better=False,\n",
    "    )\n",
    "    .on(Select.Record.app._call.args.args[0])\n",
    "    .on(Select.Record.app._call.args.args[1])\n",
    "    .on(Select.RecordOutput)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_wrapped_groundedness_gpt4o = TruBasicApp(\n",
    "    wrapped_groundedness_openai_gpt4o_consider_answerability,\n",
    "    app_name=\"groundedness\",\n",
    "    app_version=\"GPT-4o-instruct - Consider Answerability\",\n",
    "    feedbacks=[\n",
    "        f_absolute_error_consider_answerability,\n",
    "        f_absolute_error_abstensions_score_high,\n",
    "    ],\n",
    ")\n",
    "\n",
    "for i in range(len(groundedness_golden_set_abstensions_score_high)):\n",
    "    source = groundedness_golden_set_abstensions_consider_answerability[i][\n",
    "        \"query\"\n",
    "    ]\n",
    "    response = groundedness_golden_set_abstensions_consider_answerability[i][\n",
    "        \"response\"\n",
    "    ]\n",
    "    question = groundedness_golden_set_abstensions_consider_answerability[i][\n",
    "        \"user_question\"\n",
    "    ]\n",
    "    group = groundedness_golden_set_abstensions_consider_answerability[i][\n",
    "        \"group\"\n",
    "    ]\n",
    "\n",
    "    with tru_wrapped_groundedness_gpt4o as recording:\n",
    "        try:\n",
    "            recording.record_metadata = dict(group=group)\n",
    "            tru_wrapped_groundedness_gpt4o.app(source, response, question)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "openai_provider = OpenAI(model_engine=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_provider.groundedness_measure_with_cot_reasons_consider_answerability(\n",
    "    \"328.2 million people live in the United States\",\n",
    "    \"I don't know\",\n",
    "    \"How many people live in the United States?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_provider.groundedness_measure_with_cot_reasons_consider_answerability(\n",
    "    \"10 million people live in Kenya\",\n",
    "    \"I don't know\",\n",
    "    \"How many people live in the United States?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_wrapped_groundedness_gpt4o = TruBasicApp(\n",
    "    wrapped_groundedness_openai_gpt4o_consider_answerability,\n",
    "    app_name=\"groundedness\",\n",
    "    app_version=\"GPT-4o-instruct - Consider Answerability - test 2\",\n",
    "    feedbacks=[\n",
    "        f_absolute_error_consider_answerability,\n",
    "        f_absolute_error_abstensions_score_high,\n",
    "    ],\n",
    ")\n",
    "\n",
    "for i in range(len(groundedness_golden_set_abstensions_score_high)):\n",
    "    source = groundedness_golden_set_abstensions_consider_answerability[i][\n",
    "        \"query\"\n",
    "    ]\n",
    "    response = groundedness_golden_set_abstensions_consider_answerability[i][\n",
    "        \"response\"\n",
    "    ]\n",
    "    question = groundedness_golden_set_abstensions_consider_answerability[i][\n",
    "        \"user_question\"\n",
    "    ]\n",
    "    group = groundedness_golden_set_abstensions_consider_answerability[i][\n",
    "        \"group\"\n",
    "    ]\n",
    "\n",
    "    with tru_wrapped_groundedness_gpt4o as recording:\n",
    "        try:\n",
    "            recording.record_metadata = dict(group=group)\n",
    "            tru_wrapped_groundedness_gpt4o.app(source, response, question)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import TruSession\n",
    "\n",
    "tru = TruSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_leaderboard(\n",
    "    app_ids=[\n",
    "        \"groundedness GPT-4o-instruct\",\n",
    "        \"groundedness GPT-4o-instruct - Consider Answerability - test 2\",\n",
    "        \"groundedness GPT-4o-instruct - Reward Abstention\",\n",
    "    ]\n",
    ").sort_values(by=[\"app_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_leaderboard(\n",
    "    group_by_metadata_key=\"group\",\n",
    "    app_ids=[\n",
    "        \"groundedness GPT-4o-instruct\",\n",
    "        \"groundedness GPT-4o-instruct - Consider Answerability - test 2\",\n",
    "        \"groundedness GPT-4o-instruct - Reward Abstention\",\n",
    "    ],\n",
    ").sort_values(by=[\"group\", \"app_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "run_dashboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
