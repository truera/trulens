{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ““ Groundedness Evaluations\n",
    "\n",
    "In many ways, feedbacks can be thought of as LLM apps themselves. Given text, they return some result. Thinking in this way, we can use TruLens to evaluate and track our feedback quality. We can even do this for different models or prompting schemes (such as chain-of-thought reasoning). In particular, here we will evaluate the latest open source LLM published by Snowflake, [`arctic-instruct`](https://huggingface.co/Snowflake/snowflake-arctic-instruct), a 480B dense-MOE hybrid transformer-based model.\n",
    "\n",
    "This notebook follows an evaluation of a set of test cases generated from human annotated datasets. In particular, we generate test cases from [SummEval](https://arxiv.org/abs/2007.12626).\n",
    "\n",
    "SummEval is one of the datasets dedicated to automated evaluations on summarization tasks, which are closely related to the groundedness evaluation in RAG with the retrieved context (i.e. the source) and response (i.e. the summary). It contains human annotation of numerical score (**1** to **5**) comprised of scoring from 3 human expert annotators and 5 crowd-sourced annotators. There are 16 models being used for generation in total for 100 paragraphs in the test set, so there are a total of 16,000 machine-generated summaries. Each paragraph also has several human-written summaries for comparative analysis. \n",
    "\n",
    "\n",
    "For evaluating groundedness feedback functions, we compute the annotated \"consistency\" scores, a measure of whether the summarized response is factually consistent with the source texts and hence can be used as a proxy to evaluate groundedness in our RAG triad, and normalized to **0** to **1** score as our **expected_score** and to match the output of feedback functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import groundedness feedback function\n",
    "from test_cases import generate_summeval_groundedness_golden_set\n",
    "from trulens.apps.basic import TruBasicApp\n",
    "from trulens.core import Feedback\n",
    "from trulens.core import Select\n",
    "from trulens.core import TruSession\n",
    "from trulens.feedback import GroundTruthAgreement\n",
    "\n",
    "tru = TruSession()\n",
    "tru.reset_database()\n",
    "\n",
    "# generator for groundedness golden set\n",
    "test_cases_gen = generate_summeval_groundedness_golden_set(\n",
    "    \"./datasets/summeval/summeval_test_100.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the number of test cases we want to run the smoke test on\n",
    "groundedness_golden_set = []\n",
    "for i in range(500):\n",
    "    groundedness_golden_set.append(next(test_cases_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(groundedness_golden_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundedness_golden_set[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = \"r8_...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking various Groundedness feedback function providers (OpenAI GPT-4o vs Snowflake's Arctic Instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.providers.litellm import LiteLLM\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "replicate_provider_arctic = LiteLLM(\n",
    "    model_engine=\"replicate/snowflake/snowflake-arctic-instruct\"\n",
    ")\n",
    "\n",
    "f_groundedness_arctic = Feedback(\n",
    "    replicate_provider_arctic.groundedness_measure_with_cot_reasons,\n",
    "    name=\"Groundedness arctic-instruct\",\n",
    ").on_input_output()\n",
    "\n",
    "\n",
    "def wrapped_groundedness_arctic(input, output) -> float:\n",
    "    score = f_groundedness_arctic(input, output)[0]\n",
    "    print(score)\n",
    "    return score\n",
    "\n",
    "\n",
    "replicate_provider_llama3 = LiteLLM(\n",
    "    model_engine=\"replicate/meta/meta-llama-3-70b-instruct\"\n",
    ")\n",
    "\n",
    "f_groundedness_llama3 = Feedback(\n",
    "    replicate_provider_llama3.groundedness_measure_with_cot_reasons,\n",
    "    name=\"Groundedness Llama-3-70b-instruct\",\n",
    ").on_input_output()\n",
    "\n",
    "\n",
    "def wrapped_groundedness_llama3(input, output) -> float:\n",
    "    score = f_groundedness_llama3(input, output)[0]\n",
    "    print(score)\n",
    "    return score\n",
    "\n",
    "\n",
    "replicate_provider_mixtral = LiteLLM(\n",
    "    model_engine=\"replicate/mistralai/mixtral-8x7b-instruct-v0.1\"\n",
    ")\n",
    "f_groundedness_mixtral = Feedback(\n",
    "    replicate_provider_mixtral.groundedness_measure_with_cot_reasons,\n",
    "    name=\"Groundedness Mixtral-8x7b-instruct-v0.1\",\n",
    ").on_input_output()\n",
    "\n",
    "\n",
    "def wrapped_groundedness_mixtral(input, output) -> float:\n",
    "    score = f_groundedness_mixtral(input, output)[0]\n",
    "    print(score)\n",
    "    return score\n",
    "\n",
    "\n",
    "openai_provider_turbo = OpenAI(model_engine=\"gpt-4-turbo\")\n",
    "f_groundedness_openai_gpt4_turbo = Feedback(\n",
    "    openai_provider_turbo.groundedness_measure_with_cot_reasons,\n",
    "    name=\"Groundedness OpenAI GPT-4-turbo\",\n",
    ").on_input_output()\n",
    "\n",
    "\n",
    "def wrapped_groundedness_openai_gpt4_turbo(input, output) -> float:\n",
    "    return f_groundedness_openai_gpt4_turbo(input, output)[0]\n",
    "\n",
    "\n",
    "openai_provider = OpenAI(model_engine=\"gpt-4o\")\n",
    "f_groundedness_openai_gpt4 = Feedback(\n",
    "    openai_provider.groundedness_measure_with_cot_reasons,\n",
    "    name=\"Groundedness OpenAI GPT-4o\",\n",
    ").on_input_output()\n",
    "\n",
    "\n",
    "def wrapped_groundedness_openai_gpt4(input, output) -> float:\n",
    "    return f_groundedness_openai_gpt4(input, output)[0]\n",
    "\n",
    "\n",
    "# Create a Feedback object using the numeric_difference method of the ground_truth object\n",
    "ground_truth = GroundTruthAgreement(\n",
    "    groundedness_golden_set, provider=openai_provider\n",
    ")\n",
    "# Call the numeric_difference method with app and record and aggregate to get the mean absolute error\n",
    "f_absolute_error = (\n",
    "    Feedback(ground_truth.absolute_error, name=\"Mean Absolute Error\")\n",
    "    .on(Select.Record.calls[0].args.args[0])\n",
    "    .on(Select.Record.calls[0].args.args[1])\n",
    "    .on_output()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_wrapped_groundedness_llama3 = TruBasicApp(\n",
    "    wrapped_groundedness_llama3,\n",
    "    app_name=\"groundedness\",\n",
    "    app_version=\"Llama3-70b-instruct\",\n",
    "    feedbacks=[f_absolute_error],\n",
    ")\n",
    "for i in range(len(groundedness_golden_set)):\n",
    "    source = groundedness_golden_set[i][\"query\"]\n",
    "    response = groundedness_golden_set[i][\"response\"]\n",
    "\n",
    "    with tru_wrapped_groundedness_llama3 as recording:\n",
    "        try:\n",
    "            tru_wrapped_groundedness_llama3.app(source, response)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_wrapped_groundedness_mixtral = TruBasicApp(\n",
    "    wrapped_groundedness_mixtral,\n",
    "    app_name=\"groundedness\",\n",
    "    app_version=\"mixtral-8x7b-instruct-v0.1\",\n",
    "    feedbacks=[f_absolute_error],\n",
    ")\n",
    "for i in range(len(groundedness_golden_set)):\n",
    "    source = groundedness_golden_set[i][\"query\"]\n",
    "    response = groundedness_golden_set[i][\"response\"]\n",
    "\n",
    "    with tru_wrapped_groundedness_mixtral as recording:\n",
    "        try:\n",
    "            tru_wrapped_groundedness_mixtral.app(source, response)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_leaderboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
