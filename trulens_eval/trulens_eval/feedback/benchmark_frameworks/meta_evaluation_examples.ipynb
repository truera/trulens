{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevance feedback function\n",
    "from trulens_eval.feedback import GroundTruthAgreement, BenchmarkAggregator\n",
    "from trulens_eval import Tru\n",
    "import numpy as np\n",
    "\n",
    "tru = Tru()\n",
    "golden_set = [\n",
    "    {\n",
    "        \"query\": \"who are the Apple's competitors?\",\n",
    "        \"response\": \"Apple competitors include Samsung, Google, and Microsoft.\",\n",
    "        \"expected_score\": 1.0,\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"what is the capital of France?\",\n",
    "        \"response\": \"Paris is the capital of France.\",\n",
    "        \"expected_score\": 1.0,\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"what is the capital of Spain?\",\n",
    "        \"response\": \"I love going to Spain.\",\n",
    "        \"expected_score\": 0,\n",
    "    },\n",
    "]\n",
    "# Create a Feedback object using the numeric_difference method of the ground_truth object\n",
    "ground_truth = GroundTruthAgreement(golden_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.feedback import Cortex\n",
    "\n",
    "provider = Cortex(model_engine=\"snowflake-arctic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def context_relevance_ff_to_score(input, output, temperature):\n",
    "    return provider.context_relevance(\n",
    "        question=input, context=output, temperature=temperature\n",
    "    )\n",
    "\n",
    "\n",
    "def context_relevance_ff_to_score_with_confidence(\n",
    "    input, output, temperature\n",
    ") -> Tuple[float, float]:\n",
    "    return provider.context_relevance_verb_confidence(\n",
    "        question=input, context=output, temperature=temperature\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect all prompt and expected responses from the golden set and pass to BenchmarkAggregator as ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "responses = []\n",
    "for i in range(len(golden_set)):\n",
    "    prompt = golden_set[i][\"query\"]\n",
    "    response = golden_set[i][\"response\"]\n",
    "\n",
    "    prompts.append(prompt)\n",
    "    responses.append(response)\n",
    "\n",
    "true_labels = [entry[\"expected_score\"] for entry in golden_set]\n",
    "\n",
    "mae_agg_func = BenchmarkAggregator(true_labels=true_labels).mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.feedback.benchmark_frameworks.tru_benchmark_experiment import (\n",
    "    BenchmarkParams,\n",
    ")\n",
    "\n",
    "tru_benchmark_arctic = tru.BenchmarkExperiment(\n",
    "    app_id=\"benchmark_arctic\",\n",
    "    ground_truth=golden_set,\n",
    "    feedback_to_score_fn=context_relevance_ff_to_score,\n",
    "    agg_funcs=[mae_agg_func],\n",
    "    benchmark_params=BenchmarkParams(temperature=0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_benchmark_arctic as recording:\n",
    "    feedback_res = tru_benchmark_arctic.app.collect_feedback_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_res  # generate feedback scores from our context relevance feedback function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_leaderboard(app_ids=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ece_agg_func = BenchmarkAggregator(true_labels=true_labels).ece\n",
    "tru_benchmark_arctic_calibration = tru.BenchmarkExperiment(\n",
    "    app_id=\"benchmark_arctic with calibration\",\n",
    "    ground_truth=golden_set,\n",
    "    feedback_to_score_fn=context_relevance_ff_to_score_with_confidence,\n",
    "    agg_funcs=[ece_agg_func],\n",
    "    benchmark_params=BenchmarkParams(temperature=0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_benchmark_arctic_calibration as recording:\n",
    "    feedback_results = (\n",
    "        tru_benchmark_arctic_calibration.app.collect_feedback_scores()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_leaderboard(app_ids=[])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
