{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(Path().cwd().parent.parent.resolve()))\n",
    "\n",
    "from trulens_eval.keys import *\n",
    "\n",
    "\"ignore me\"\n",
    "\n",
    "import datetime\n",
    "import io\n",
    "from multiprocessing import Event\n",
    "from pathlib import Path\n",
    "from queue import Queue\n",
    "import sqlite3\n",
    "import tempfile\n",
    "from threading import Thread\n",
    "from time import sleep\n",
    "from typing import Callable, Iterable\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import humanize\n",
    "from langchain.document_loaders import PagedPDFSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter, NLTKTextSplitter\n",
    "from langchain.vectorstores import Pinecone\n",
    "import numpy as np\n",
    "import pdfreader\n",
    "import pinecone\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "from url_normalize import url_normalize\n",
    "\n",
    "from trulens_eval.util import first\n",
    "from trulens_eval.util import UNICODE_CHECK\n",
    "\n",
    "TRUERA_BASE_URL = 'https://truera.com/'\n",
    "TRUERA_DOC_URL = 'https://docs.truera.com/1.34/public/'\n",
    "TRUERA_SUPPORT_URL = \"https://support.truera.com/hc/en-us/\"\n",
    "TRUERA_BLOG_URL = \"https://truera.com/ai-quality-blog/\"\n",
    "TRULENS_URL = \"https://trulens.org/\"\n",
    "TRUERA_URLS = [TRUERA_BASE_URL, TRUERA_DOC_URL, TRUERA_SUPPORT_URL, TRUERA_BLOG_URL, TRULENS_URL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scrape():\n",
    "    TABLE_PAGES = \"page\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        filename: Path = Path(\"scrape.sqlite\"),\n",
    "        n_threads: int = 8,\n",
    "        filters: Callable[[str], bool] = None\n",
    "    ):\n",
    "        self.filename = filename\n",
    "        # self.db = sqlite3.connect(self.filename)\n",
    "        self.n_threads = n_threads\n",
    "        self._create_tables()\n",
    "\n",
    "        # self.executor = ThreadPoolExecutor(max_workers=n_threads)\n",
    "\n",
    "        if isinstance(filters, str):\n",
    "            filter_func = lambda url: filters in url\n",
    "        elif isinstance(filters, Iterable):\n",
    "            filter_func = lambda url: any(map(lambda f: f in url, filters))\n",
    "        elif isinstance(filter, Callable):\n",
    "            filter_func = filters\n",
    "        else:\n",
    "            raise TypeError(f\"Unhandled filters type {type(filters)}\")\n",
    "\n",
    "        self.filter_func = filter_func\n",
    "\n",
    "    @staticmethod\n",
    "    def custom_normalize(url, base_url=None):\n",
    "        if url.startswith(\"tel:\"):\n",
    "            return url\n",
    "\n",
    "        if base_url is not None:\n",
    "            base_url = url_normalize(\n",
    "                urlparse(base_url)._replace(fragment=None, query=None).geturl()\n",
    "            )\n",
    "            if not base_url.endswith(\"/\"):\n",
    "                base_url += \"/\"\n",
    "            url = urljoin(base_url, url)\n",
    "        else:\n",
    "            url = urlparse(url)._replace(fragment=None, query=None).geturl()\n",
    "\n",
    "        url = url_normalize(url)\n",
    "\n",
    "        return url\n",
    "\n",
    "    def cursor(self):\n",
    "        connection = sqlite3.connect(self.filename)\n",
    "        cursor = connection.cursor()\n",
    "        return cursor, connection\n",
    "\n",
    "    def _create_tables(self):\n",
    "        c, conn = self.cursor()\n",
    "        c.execute(\n",
    "            f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {Scrape.TABLE_PAGES} (\n",
    "                url VARCHAR(128),\n",
    "                type VARCHAR(64),\n",
    "                retrieved INTEGER,\n",
    "                content BYTES,\n",
    "                PRIMARY KEY (url)\n",
    "            )\n",
    "        \"\"\"\n",
    "        )\n",
    "        conn.commit()\n",
    "        c.close()\n",
    "\n",
    "    def get_urls(self) -> Iterable[sqlite3.Row]:\n",
    "        c, conn = self.cursor()\n",
    "\n",
    "        c.execute(\n",
    "            f\"\"\"\n",
    "            SELECT url\n",
    "            FROM {Scrape.TABLE_PAGES}\n",
    "            \"\"\"\n",
    "        )\n",
    "        rows = c.fetchall()\n",
    "\n",
    "        c.close()\n",
    "\n",
    "        return map(first, rows)\n",
    "\n",
    "    def get_page(self, url: str) -> sqlite3.Row:\n",
    "        c, conn = self.cursor()\n",
    "\n",
    "        c.execute(\n",
    "            f\"\"\"\n",
    "            SELECT * \n",
    "            FROM {Scrape.TABLE_PAGES} \n",
    "            WHERE url=?\"\"\", (url,)\n",
    "        )\n",
    "        row = c.fetchone()\n",
    "\n",
    "        c.close()\n",
    "        return row\n",
    "\n",
    "    def request(self, url: str):\n",
    "        return requests.get(url, stream=True)\n",
    "\n",
    "    def delete_page(self, url: str):\n",
    "        c, conn = self.cursor()\n",
    "\n",
    "        c.execute(\n",
    "            f\"\"\"\n",
    "            DELETE FROM {Scrape.TABLE_PAGES}\n",
    "            WHERE url=?\n",
    "            \"\"\", (url,)\n",
    "        )\n",
    "        conn.commit()\n",
    "        c.close()\n",
    "\n",
    "        print(f\"page {url} deleted\")\n",
    "\n",
    "    def insert_page(self, url: str, type: str, content: bytes):\n",
    "        retrieved = datetime.datetime.now().timestamp()\n",
    "\n",
    "        c, conn = self.cursor()\n",
    "\n",
    "        size = len(content)\n",
    "\n",
    "        c.execute(\n",
    "            f\"\"\"\n",
    "            INSERT OR REPLACE \n",
    "            INTO {Scrape.TABLE_PAGES} \n",
    "            VALUES (?, ?, ?, ?)\"\"\", (url, type, retrieved, content)\n",
    "        )\n",
    "        conn.commit()\n",
    "        c.close()\n",
    "\n",
    "        print(\n",
    "            f\"{UNICODE_CHECK} page {type} {humanize.naturalsize(size)} {url} -> {self.filename}\"\n",
    "        )\n",
    "\n",
    "    def scrape(self, url: str, redownload: bool = False):\n",
    "        q = Queue(maxsize=1024 * 1024)\n",
    "        q.put((url, None))\n",
    "\n",
    "        stopped = Event()\n",
    "        stopped.clear()\n",
    "\n",
    "        scraped = set()\n",
    "        threads = []\n",
    "\n",
    "        for _ in range(self.n_threads):\n",
    "            thread = Thread(\n",
    "                target=self._scrape,\n",
    "                kwargs=dict(\n",
    "                    queue=q,\n",
    "                    redownload=redownload,\n",
    "                    scraped=scraped,\n",
    "                    stopped=stopped\n",
    "                )\n",
    "            )\n",
    "            thread.start()\n",
    "            threads.append(thread)\n",
    "\n",
    "        sleep(1)\n",
    "\n",
    "        while not q.empty():\n",
    "            print(\"queue size:\", q.qsize())\n",
    "            sleep(1)\n",
    "\n",
    "        print(\"queue empty\")\n",
    "        stopped.set()\n",
    "\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "\n",
    "    def _scrape(\n",
    "        self, queue: Queue, stopped: Event, redownload: bool, scraped: set\n",
    "    ):\n",
    "        while not stopped.is_set():\n",
    "            if not queue.empty():\n",
    "                (url, from_url) = queue.get()\n",
    "            else:\n",
    "                sleep(1)\n",
    "                continue\n",
    "\n",
    "            url = Scrape.custom_normalize(url)\n",
    "\n",
    "            if url in scraped:\n",
    "                continue\n",
    "\n",
    "            scraped.add(url)\n",
    "\n",
    "            page = self.get_page(url)\n",
    "            if page is not None:\n",
    "                ctype = page[1]\n",
    "                content = page[3]\n",
    "\n",
    "            if page is None or redownload:\n",
    "                try:\n",
    "                    res = self.request(url)\n",
    "                except Exception as e:\n",
    "                    print(f\"WARNING: {url} from {from_url}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                if not res.ok:\n",
    "                    print(f\"WARNING: {url} from {from_url}: {res.status_code}\")\n",
    "                    continue\n",
    "\n",
    "                if \"content-type\" not in res.headers:\n",
    "                    print(\n",
    "                        f\"WARNING: {url} from {from_url} lacks needed headers:\\n{list(res.headers.keys())}\"\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                ctype = res.headers['content-type']\n",
    "\n",
    "                if \"content-length\" in res.headers:\n",
    "                    size = int(res.headers['content-length'])\n",
    "                    if size > 100 * (1024**2):\n",
    "                        print(\n",
    "                            f\"WARNING: {url} from {from_url} is large {humanize.naturalsize(size)}\"\n",
    "                        )\n",
    "                        continue  # skipping\n",
    "\n",
    "                if ctype.startswith(\"image/\"):\n",
    "                    continue  # skipping\n",
    "\n",
    "                content = res.content\n",
    "                self.insert_page(url=url, type=ctype, content=res.content)\n",
    "\n",
    "            size = len(content)\n",
    "            if size > 100 * (1024**2):\n",
    "                print(\n",
    "                    f\"WARNING: {url} from {from_url}: is large: {humanize.naturalsize(size)}\"\n",
    "                )\n",
    "                pass\n",
    "\n",
    "            if ctype.startswith(\"text/html\"):\n",
    "                soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "                anchors = soup.findAll(\"a\")\n",
    "                sub_urls = [a.get(\"href\") for a in anchors]\n",
    "\n",
    "            elif ctype.startswith(\"application/pdf\"):\n",
    "\n",
    "                with io.BytesIO() as fh:\n",
    "                    fh.write(content)\n",
    "                    fh.seek(0)\n",
    "\n",
    "                    pdf = pdfreader.SimplePDFViewer(fh)\n",
    "\n",
    "                    sub_urls = []\n",
    "\n",
    "                    if pdf.annotations is not None:\n",
    "                        for annot in pdf.annotations:\n",
    "                            if annot.Subtype == \"Link\":\n",
    "                                sub_url = annot.A.URI\n",
    "                                if sub_url is not None:\n",
    "                                    sub_url = sub_url.decode('ascii')\n",
    "                                    if sub_url.startswith(\"http\"):\n",
    "                                        sub_urls.append(sub_url)\n",
    "\n",
    "            else:\n",
    "                print(\n",
    "                    f\"WARNING: {url} from {from_url}: unknown content type {ctype}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            for sub_url in sub_urls:\n",
    "                if sub_url in scraped:\n",
    "                    continue\n",
    "\n",
    "                if sub_url is None:\n",
    "                    continue\n",
    "\n",
    "                if sub_url.startswith(\"tel:\"):\n",
    "                    # print(f\"skip: {sub_url} from {url}: is tel\")\n",
    "                    scraped.add(sub_url)\n",
    "                    continue\n",
    "\n",
    "                sub_url = Scrape.custom_normalize(sub_url, base_url=url)\n",
    "\n",
    "                if sub_url in scraped:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    parts = urlparse(sub_url)\n",
    "                    if parts.scheme is None:\n",
    "                        print(f\"WARNING: {sub_url} from {url}: no scheme\")\n",
    "                        scraped.add(sub_url)\n",
    "                        continue\n",
    "\n",
    "                    if parts.scheme not in [\"http\", \"https\"]:\n",
    "                        # print(f\"skip: {sub_url} from {url}: skip scheme {parts.scheme}\")\n",
    "                        scraped.add(sub_url)\n",
    "                        continue\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"WARNING: {sub_url} from {url}: {e}\")\n",
    "                    scraped.add(sub_url)\n",
    "                    continue\n",
    "\n",
    "                if self.filter_func(sub_url):\n",
    "                    # print(\"adding\", sub_url)\n",
    "                    queue.put((sub_url, url))\n",
    "                else:\n",
    "                    scraped.add(sub_url)\n",
    "                    pass\n",
    "\n",
    "    def get_documents(self):\n",
    "        docs = []\n",
    "\n",
    "        seen_texts = dict()\n",
    "\n",
    "        for url in tqdm(list(self.get_urls())):\n",
    "            canon_url = Scrape.custom_normalize(url)\n",
    "            if url != canon_url:\n",
    "                s.delete_page(url=url)\n",
    "                continue\n",
    "\n",
    "            if url in {\n",
    "                    'https://truera.com/resources/',\n",
    "                    'https://truera.com/ai-quality-blog/',\n",
    "                    'https://truera.com/event/live-events/',\n",
    "                    'https://truera.com/ai-quality-research/ai-quality-education/',\n",
    "                    'https://medium.com/trulens/archive'\n",
    "            }:\n",
    "                print(\"skipping\", url)\n",
    "                continue\n",
    "            elif \"/page/\" in url:\n",
    "                print(\"skipping\", url)\n",
    "                continue\n",
    "            elif \"/category/\" in url:\n",
    "                print(\"skipping\", url)\n",
    "                continue\n",
    "            elif \"Datasheet\" in url and url.endswith(\".pdf\"):\n",
    "                print(\"skipping\", url)\n",
    "                continue\n",
    "            elif url.startswith(\"https://pypi.org/project/\"):\n",
    "                print(\"skipping\", url)\n",
    "                continue\n",
    "\n",
    "            row = s.get_page(url=url)\n",
    "            type = row[1]\n",
    "\n",
    "            if type.startswith(\"text/html\"):\n",
    "                loader = UnstructuredHTMLLoader\n",
    "            elif type.startswith(\"application/pdf\"):\n",
    "                loader = PagedPDFSplitter\n",
    "                # UnstructuredPDFLoader\n",
    "            elif type.startswith(\"image\"):\n",
    "                # s.delete_page(url=url)\n",
    "                continue\n",
    "            else:\n",
    "                # markdown: UnstructuredMarkdownLoader\n",
    "                # jupyter?\n",
    "                # github?\n",
    "                # print(url, type)\n",
    "                continue\n",
    "\n",
    "            content = row[3]\n",
    "            size = len(content)\n",
    "            if content is None:\n",
    "                raise ValueError(url)\n",
    "            if size == 0:\n",
    "                raise ValueError(f\"empty: {url}\")\n",
    "            if size >= 100 * (1024**2):\n",
    "                print(\n",
    "                    f\"WARNING: big content {url} {humanize.naturalsize(size)}\"\n",
    "                )\n",
    "\n",
    "            file = tempfile.NamedTemporaryFile(mode='bw')\n",
    "            file.write(row[3])\n",
    "            file.flush()\n",
    "\n",
    "            try:\n",
    "                new_docs = loader(file.name).load()\n",
    "                for new_doc in new_docs:\n",
    "                    new_doc.metadata['source'] = url\n",
    "\n",
    "                cont = new_doc.page_content\n",
    "\n",
    "                if cont in seen_texts:\n",
    "                    #print(\n",
    "                    #    f\"WARNING: {url} Already seen text in {seen_texts[cont]}. Skipping.\"\n",
    "                    #)\n",
    "                    continue\n",
    "                seen_texts[cont] = url\n",
    "\n",
    "                docs.append(new_doc)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"WARNING: {url} {type} {e}\")\n",
    "\n",
    "            file.close()\n",
    "\n",
    "        return docs\n",
    "\n",
    "\n",
    "s = Scrape(\n",
    "    filters=lambda url: (\"truera\" in url or \"trulens\" in url) and \"github.com\"\n",
    "    not in url and \"support.truera.com\" not in url and \"cbinsights.com\" not in\n",
    "    url and \"files.pythonhosted.org\" not in url and \"libraries.io\" not in url\n",
    ")\n",
    "# r = s.scrape(TRUERA_BASE_URL)\n",
    "s.scrape(TRUERA_DOC_URL)\n",
    "# r = s.scrape(TRUERA_BLOG_URL)\n",
    "# s.scrape(TRULENS_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = s.get_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = NLTKTextSplitter(chunk_size=1024, chunk_overlap=0)\n",
    "chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_chunks = [c for c in chunks if len(c.page_content) >= 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_content = dict()\n",
    "unique_chunks = []\n",
    "for chunk in big_chunks:\n",
    "    content = chunk.page_content\n",
    "    if content in seen_content:\n",
    "        print(f\"{chunk.metadata} already seen in {seen_content[content]}\")\n",
    "        continue\n",
    "    seen_content[content] = chunk.metadata\n",
    "    unique_chunks.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smallest chunk:\n",
    "print(unique_chunks[np.array([len(c.page_content) for c in unique_chunks]).argmin()].page_content)\n",
    "\n",
    "# number of chunks:\n",
    "print(len(unique_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings(model='text-embedding-ada-002')  # 1536 dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import DocArrayHnswSearch, DocArrayInMemorySearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DocArrayHnswSearch.from_documents(unique_chunks, embedding, work_dir='hnswlib_truera', n_dim=1536, max_elements=len(unique_chunks)*2)\n",
    "db = DocArrayHnswSearch.from_params(\n",
    "    embedding=embedding,\n",
    "    work_dir='hnswlib_truera',\n",
    "    n_dim=1536,\n",
    "    max_elements=len(unique_chunks) * 2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db = DocArrayInMemorySearch.from_documents(docs, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in db.similarity_search(\"Python SDK?\"):\n",
    "    print(\"====\")\n",
    "    print(doc.metadata)\n",
    "    print(doc.page_content)\n",
    "    print(\"====\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To pinecone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keys import *\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "    environment=PINECONE_ENV  # next to api key in console\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create / upload an index of the docs to pinecone\n",
    "\n",
    "index_name = \"llmdemo\"\n",
    "pinecone.delete_index(index_name)\n",
    "pinecone.create_index(index_name, dimension=1536)\n",
    "Pinecone.from_documents(bigdocs, embedding, index_name=index_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
