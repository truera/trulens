{
 "cells": [
  {
<<<<<<< HEAD
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape websites to create document retrieval stores."
   ]
  },
  {
=======
>>>>>>> public_to_merge_in
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
<<<<<<< HEAD
    "import sys\n",
    "\n",
    "sys.path.append(str(Path().cwd().parent.parent.resolve()))\n",
    "\n",
    "from trulens_eval.keys import *\n",
    "\n",
    "\"ignore me\"\n",
    "\n",
    "import datetime\n",
    "import io\n",
    "from multiprocessing import Event\n",
    "from pathlib import Path\n",
    "from queue import Queue\n",
    "import sqlite3\n",
    "import tempfile\n",
    "from threading import Thread\n",
    "from time import sleep\n",
    "from typing import Callable, Iterable\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import humanize\n",
    "from langchain.document_loaders import PagedPDFSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter, NLTKTextSplitter\n",
    "from langchain.vectorstores import Pinecone\n",
    "import numpy as np\n",
    "import pdfreader\n",
    "import pinecone\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "from url_normalize import url_normalize\n",
    "\n",
    "from trulens_eval.util import first\n",
    "from trulens_eval.util import UNICODE_CHECK\n",
    "\n",
    "TRUERA_BASE_URL = 'https://truera.com/'\n",
    "TRUERA_DOC_URL = 'https://docs.truera.com/1.34/public/'\n",
    "TRUERA_SUPPORT_URL = \"https://support.truera.com/hc/en-us/\"\n",
    "TRUERA_BLOG_URL = \"https://truera.com/ai-quality-blog/\"\n",
    "TRULENS_URL = \"https://trulens.org/\"\n",
    "TRUERA_URLS = [TRUERA_BASE_URL, TRUERA_DOC_URL, TRUERA_SUPPORT_URL, TRUERA_BLOG_URL, TRULENS_URL]"
=======
    "\n",
    "import pinecone\n",
    "import requests\n",
    "from langchain.document_loaders import (PagedPDFSplitter, TextLoader,\n",
    "                                        UnstructuredHTMLLoader,\n",
    "                                        UnstructuredMarkdownLoader,\n",
    "                                        UnstructuredPDFLoader)\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "# from urllib.parse import urlparse\n",
    "\n",
    "TRUERA_BASE_URL = 'https://truera.com'\n",
    "TRUREA_DOC_URL = 'https://docs.truera.com/1.34/public'"
>>>>>>> public_to_merge_in
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "class WebScrape():\n",
    "    TABLE_PAGES = \"page\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        filename: Path = Path(\"scrape.sqlite\"),\n",
    "        n_threads: int = 8,\n",
    "        filters: Callable[[str], bool] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Web document downloader. Walks over links, collecting documents.\n",
    "\n",
    "        NOTE: This is not a serious scraper for large crawls.\n",
    "        \"\"\"\n",
    "\n",
    "        self.filename = filename\n",
    "        self.n_threads = n_threads\n",
    "        self._create_tables()\n",
    "\n",
    "        if isinstance(filters, str):\n",
    "            filter_func = lambda url: filters in url\n",
    "        elif isinstance(filters, Iterable):\n",
    "            filter_func = lambda url: any(map(lambda f: f in url, filters))\n",
    "        elif isinstance(filter, Callable):\n",
    "            filter_func = filters\n",
    "        else:\n",
    "            raise TypeError(f\"Unhandled filters type {type(filters)}\")\n",
    "\n",
    "        self.filter_func = filter_func\n",
    "\n",
    "    @staticmethod\n",
    "    def custom_normalize(url, base_url=None):\n",
    "        if url.startswith(\"tel:\"):\n",
    "            return url\n",
    "\n",
    "        if base_url is not None:\n",
    "            base_url = url_normalize(\n",
    "                urlparse(base_url)._replace(fragment=None, query=None).geturl()\n",
    "            )\n",
    "            if not base_url.endswith(\"/\"):\n",
    "                base_url += \"/\"\n",
    "            url = urljoin(base_url, url)\n",
    "        else:\n",
    "            url = urlparse(url)._replace(fragment=None, query=None).geturl()\n",
    "\n",
    "        url = url_normalize(url)\n",
    "\n",
    "        return url\n",
    "\n",
    "    def cursor(self):\n",
    "        connection = sqlite3.connect(self.filename)\n",
    "        cursor = connection.cursor()\n",
    "        return cursor, connection\n",
    "\n",
    "    def _create_tables(self):\n",
    "        c, conn = self.cursor()\n",
    "        c.execute(\n",
    "            f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {WebScrape.TABLE_PAGES} (\n",
    "                url VARCHAR(128),\n",
    "                type VARCHAR(64),\n",
    "                retrieved INTEGER,\n",
    "                content BYTES,\n",
    "                PRIMARY KEY (url)\n",
    "            )\n",
    "        \"\"\"\n",
    "        )\n",
    "        conn.commit()\n",
    "        c.close()\n",
    "\n",
    "    def get_urls(self) -> Iterable[sqlite3.Row]:\n",
    "        c, conn = self.cursor()\n",
    "\n",
    "        c.execute(\n",
    "            f\"\"\"\n",
    "            SELECT url\n",
    "            FROM {WebScrape.TABLE_PAGES}\n",
    "            \"\"\"\n",
    "        )\n",
    "        rows = c.fetchall()\n",
    "\n",
    "        c.close()\n",
    "\n",
    "        return map(first, rows)\n",
    "\n",
    "    def get_page(self, url: str) -> sqlite3.Row:\n",
    "        c, conn = self.cursor()\n",
    "\n",
    "        c.execute(\n",
    "            f\"\"\"\n",
    "            SELECT * \n",
    "            FROM {WebScrape.TABLE_PAGES} \n",
    "            WHERE url=?\"\"\", (url,)\n",
    "        )\n",
    "        row = c.fetchone()\n",
    "\n",
    "        c.close()\n",
    "        return row\n",
    "\n",
    "    def request(self, url: str):\n",
    "        return requests.get(url, stream=True)\n",
    "\n",
    "    def delete_page(self, url: str):\n",
    "        c, conn = self.cursor()\n",
    "\n",
    "        c.execute(\n",
    "            f\"\"\"\n",
    "            DELETE FROM {WebScrape.TABLE_PAGES}\n",
    "            WHERE url=?\n",
    "            \"\"\", (url,)\n",
    "        )\n",
    "        conn.commit()\n",
    "        c.close()\n",
    "\n",
    "        print(f\"page {url} deleted\")\n",
    "\n",
    "    def insert_page(self, url: str, type: str, content: bytes):\n",
    "        retrieved = datetime.datetime.now().timestamp()\n",
    "\n",
    "        c, conn = self.cursor()\n",
    "\n",
    "        size = len(content)\n",
    "\n",
    "        c.execute(\n",
    "            f\"\"\"\n",
    "            INSERT OR REPLACE \n",
    "            INTO {WebScrape.TABLE_PAGES} \n",
    "            VALUES (?, ?, ?, ?)\"\"\", (url, type, retrieved, content)\n",
    "        )\n",
    "        conn.commit()\n",
    "        c.close()\n",
    "\n",
    "        print(\n",
    "            f\"{UNICODE_CHECK} page {type} {humanize.naturalsize(size)} {url} -> {self.filename}\"\n",
    "        )\n",
    "\n",
    "    def scrape(self, url: str, redownload: bool = False):\n",
    "        q = Queue(maxsize=1024 * 1024)\n",
    "        q.put((url, None))\n",
    "\n",
    "        stopped = Event()\n",
    "        stopped.clear()\n",
    "\n",
    "        scraped = set()\n",
    "        threads = []\n",
    "\n",
    "        for _ in range(self.n_threads):\n",
    "            thread = Thread(\n",
    "                target=self._scrape,\n",
    "                kwargs=dict(\n",
    "                    queue=q,\n",
    "                    redownload=redownload,\n",
    "                    scraped=scraped,\n",
    "                    stopped=stopped\n",
    "                )\n",
    "            )\n",
    "            thread.start()\n",
    "            threads.append(thread)\n",
    "\n",
    "        sleep(1)\n",
    "\n",
    "        while not q.empty():\n",
    "            print(\"queue size:\", q.qsize())\n",
    "            sleep(1)\n",
    "\n",
    "        print(\"queue empty\")\n",
    "        stopped.set()\n",
    "\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "\n",
    "    def _scrape(\n",
    "        self, queue: Queue, stopped: Event, redownload: bool, scraped: set\n",
    "    ):\n",
    "        while not stopped.is_set():\n",
    "            if not queue.empty():\n",
    "                (url, from_url) = queue.get()\n",
    "            else:\n",
    "                sleep(1)\n",
    "                continue\n",
    "\n",
    "            url = WebScrape.custom_normalize(url)\n",
    "\n",
    "            if url in scraped:\n",
    "                continue\n",
    "\n",
    "            scraped.add(url)\n",
    "\n",
    "            page = self.get_page(url)\n",
    "            if page is not None:\n",
    "                ctype = page[1]\n",
    "                content = page[3]\n",
    "\n",
    "            if page is None or redownload:\n",
    "                try:\n",
    "                    res = self.request(url)\n",
    "                except Exception as e:\n",
    "                    print(f\"WARNING: {url} from {from_url}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                if not res.ok:\n",
    "                    print(f\"WARNING: {url} from {from_url}: {res.status_code}\")\n",
    "                    continue\n",
    "\n",
    "                if \"content-type\" not in res.headers:\n",
    "                    print(\n",
    "                        f\"WARNING: {url} from {from_url} lacks needed headers:\\n{list(res.headers.keys())}\"\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                ctype = res.headers['content-type']\n",
    "\n",
    "                if \"content-length\" in res.headers:\n",
    "                    size = int(res.headers['content-length'])\n",
    "                    if size > 100 * (1024**2):\n",
    "                        print(\n",
    "                            f\"WARNING: {url} from {from_url} is large {humanize.naturalsize(size)}\"\n",
    "                        )\n",
    "                        continue  # skipping\n",
    "\n",
    "                if ctype.startswith(\"image/\"):\n",
    "                    continue  # skipping\n",
    "\n",
    "                content = res.content\n",
    "                self.insert_page(url=url, type=ctype, content=res.content)\n",
    "\n",
    "            size = len(content)\n",
    "            if size > 100 * (1024**2):\n",
    "                print(\n",
    "                    f\"WARNING: {url} from {from_url}: is large: {humanize.naturalsize(size)}\"\n",
    "                )\n",
    "                pass\n",
    "\n",
    "            if ctype.startswith(\"text/html\"):\n",
    "                soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "                anchors = soup.findAll(\"a\")\n",
    "                sub_urls = [a.get(\"href\") for a in anchors]\n",
    "\n",
    "            elif ctype.startswith(\"application/pdf\"):\n",
    "\n",
    "                with io.BytesIO() as fh:\n",
    "                    fh.write(content)\n",
    "                    fh.seek(0)\n",
    "\n",
    "                    pdf = pdfreader.SimplePDFViewer(fh)\n",
    "\n",
    "                    sub_urls = []\n",
    "\n",
    "                    if pdf.annotations is not None:\n",
    "                        for annot in pdf.annotations:\n",
    "                            if annot.Subtype == \"Link\":\n",
    "                                sub_url = annot.A.URI\n",
    "                                if sub_url is not None:\n",
    "                                    sub_url = sub_url.decode('ascii')\n",
    "                                    if sub_url.startswith(\"http\"):\n",
    "                                        sub_urls.append(sub_url)\n",
    "\n",
    "            else:\n",
    "                print(\n",
    "                    f\"WARNING: {url} from {from_url}: unknown content type {ctype}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            for sub_url in sub_urls:\n",
    "                if sub_url in scraped:\n",
    "                    continue\n",
    "\n",
    "                if sub_url is None:\n",
    "                    continue\n",
    "\n",
    "                if sub_url.startswith(\"tel:\"):\n",
    "                    # print(f\"skip: {sub_url} from {url}: is tel\")\n",
    "                    scraped.add(sub_url)\n",
    "                    continue\n",
    "\n",
    "                sub_url = WebScrape.custom_normalize(sub_url, base_url=url)\n",
    "\n",
    "                if sub_url in scraped:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    parts = urlparse(sub_url)\n",
    "                    if parts.scheme is None:\n",
    "                        print(f\"WARNING: {sub_url} from {url}: no scheme\")\n",
    "                        scraped.add(sub_url)\n",
    "                        continue\n",
    "\n",
    "                    if parts.scheme not in [\"http\", \"https\"]:\n",
    "                        # print(f\"skip: {sub_url} from {url}: skip scheme {parts.scheme}\")\n",
    "                        scraped.add(sub_url)\n",
    "                        continue\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"WARNING: {sub_url} from {url}: {e}\")\n",
    "                    scraped.add(sub_url)\n",
    "                    continue\n",
    "\n",
    "                if self.filter_func(sub_url):\n",
    "                    # print(\"adding\", sub_url)\n",
    "                    queue.put((sub_url, url))\n",
    "                else:\n",
    "                    scraped.add(sub_url)\n",
    "                    pass\n",
    "\n",
    "    def get_documents(self):\n",
    "        docs = []\n",
    "\n",
    "        seen_texts = dict()\n",
    "\n",
    "        for url in tqdm(list(self.get_urls())):\n",
    "            canon_url = WebScrape.custom_normalize(url)\n",
    "            if url != canon_url:\n",
    "                s.delete_page(url=url)\n",
    "                continue\n",
    "\n",
    "            if url in {\n",
    "                    'https://truera.com/resources/',\n",
    "                    'https://truera.com/ai-quality-blog/',\n",
    "                    'https://truera.com/event/live-events/',\n",
    "                    'https://truera.com/ai-quality-research/ai-quality-education/',\n",
    "                    'https://medium.com/trulens/archive'\n",
    "            }:\n",
    "                print(\"skipping\", url)\n",
    "                continue\n",
    "            elif \"/page/\" in url:\n",
    "                print(\"skipping\", url)\n",
    "                continue\n",
    "            elif \"/category/\" in url:\n",
    "                print(\"skipping\", url)\n",
    "                continue\n",
    "            elif \"Datasheet\" in url and url.endswith(\".pdf\"):\n",
    "                print(\"skipping\", url)\n",
    "                continue\n",
    "            elif url.startswith(\"https://pypi.org/project/\"):\n",
    "                print(\"skipping\", url)\n",
    "                continue\n",
    "            elif \"trulens\" in url: # temporarily skipping anything with trulens\n",
    "                print(\"skipping\", url)\n",
    "                continue\n",
    "\n",
    "            row = s.get_page(url=url)\n",
    "            type = row[1]\n",
    "\n",
    "            if type.startswith(\"text/html\"):\n",
    "                loader = UnstructuredHTMLLoader\n",
    "            elif type.startswith(\"application/pdf\"):\n",
    "                loader = PagedPDFSplitter\n",
    "                # UnstructuredPDFLoader\n",
    "            elif type.startswith(\"image\"):\n",
    "                # s.delete_page(url=url)\n",
    "                continue\n",
    "            else:\n",
    "                # markdown: UnstructuredMarkdownLoader\n",
    "                # jupyter?\n",
    "                # github?\n",
    "                # print(url, type)\n",
    "                continue\n",
    "\n",
    "            content = row[3]\n",
    "            size = len(content)\n",
    "            if content is None:\n",
    "                raise ValueError(url)\n",
    "            if size == 0:\n",
    "                raise ValueError(f\"empty: {url}\")\n",
    "            if size >= 100 * (1024**2):\n",
    "                print(\n",
    "                    f\"WARNING: big content {url} {humanize.naturalsize(size)}\"\n",
    "                )\n",
    "\n",
    "            file = tempfile.NamedTemporaryFile(mode='bw')\n",
    "            file.write(row[3])\n",
    "            file.flush()\n",
    "\n",
    "            try:\n",
    "                new_docs = loader(file.name).load()\n",
    "                for new_doc in new_docs:\n",
    "                    new_doc.metadata['source'] = url\n",
    "\n",
    "                cont = new_doc.page_content\n",
    "\n",
    "                if cont in seen_texts:\n",
    "                    #print(\n",
    "                    #    f\"WARNING: {url} Already seen text in {seen_texts[cont]}. Skipping.\"\n",
    "                    #)\n",
    "                    continue\n",
    "                seen_texts[cont] = url\n",
    "\n",
    "                docs.append(new_doc)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"WARNING: {url} {type} {e}\")\n",
    "\n",
    "            file.close()\n",
    "\n",
    "        return docs\n",
    "\n",
    "\n",
    "s = WebScrape(\n",
    "    filters=lambda url: (\"truera\" in url or \"trulens\" in url) and \"github.com\"\n",
    "    not in url and \"support.truera.com\" not in url and \"cbinsights.com\" not in\n",
    "    url and \"files.pythonhosted.org\" not in url and \"libraries.io\" not in url\n",
    ")\n",
    "# r = s.scrape(TRUERA_BASE_URL)\n",
    "# s.scrape(TRUERA_DOC_URL)\n",
    "# r = s.scrape(TRUERA_BLOG_URL)\n",
    "# s.scrape(TRULENS_URL)"
=======
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Create a pinecone vector db from a few blogs and docs.\n",
    "# TODO: langchain includes html loaders which may produce better chunks.\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "pdf_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "\n",
    "scrape_path = Path(\"webscrape\")\n",
    "\n",
    "collected = dict()\n",
    "documents = []\n",
    "\n",
    "\n",
    "def url_to_path(url):\n",
    "    url_esc = url.replace(\"https://\", \"\").replace(\"http://\",\n",
    "                                                  \"\").replace(\"/\", \":\")\n",
    "\n",
    "    ext = \".html\"\n",
    "\n",
    "    if url_esc.endswith(\".png\"):\n",
    "        ext = \"\"\n",
    "    elif url_esc.endswith(\".pdf\"):\n",
    "        ext = \"\"\n",
    "    elif url_esc.endswith(\".jpg\"):\n",
    "        ext = \"\"\n",
    "    elif url_esc.endswith(\".md\"):\n",
    "        ext = \"\"\n",
    "\n",
    "    return scrape_path / (url_esc + ext)\n",
    "\n",
    "\n",
    "def scrape(url):\n",
    "    if url in collected:\n",
    "        return\n",
    "\n",
    "    collected[url] = True\n",
    "\n",
    "    print(url)\n",
    "\n",
    "    scrape_file = url_to_path(url)\n",
    "\n",
    "    if str(url).endswith(\".pdf\"):\n",
    "        # skipping for now since issues with the content extractors noted below\n",
    "        # return\n",
    "        pass\n",
    "\n",
    "    if scrape_file.exists():\n",
    "        print(\"cached\", end=\" \")\n",
    "        content = bytes()\n",
    "        with scrape_file.open(\"rb\") as fh:\n",
    "            for line in fh.readlines():\n",
    "                content += line\n",
    "    else:\n",
    "        print(\"downloading\", end=\" \")\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.encoding is None:\n",
    "            content = response.content\n",
    "\n",
    "            with scrape_file.open(\"wb\") as fh:\n",
    "                fh.write(content)\n",
    "\n",
    "        else:\n",
    "            content = response.text\n",
    "\n",
    "            with scrape_file.open(\"w\") as fh:\n",
    "                fh.write(content)\n",
    "\n",
    "    loader = UnstructuredHTMLLoader\n",
    "    if url.endswith(\".pdf\"):\n",
    "        #return\n",
    "        loader = PagedPDFSplitter # freezes for some pdfs\n",
    "        # loader = UnstructuredPDFLoader # cannot get requirement installation figured out\n",
    "\n",
    "    elif url.endswith(\".png\"):\n",
    "        return\n",
    "    \n",
    "    elif url.endswith(\".jpg\"):\n",
    "        return\n",
    "    \n",
    "    elif url.endswith(\".md\"):\n",
    "        loader = UnstructuredMarkdownLoader\n",
    "\n",
    "    elif (not url.endswith(\"truera.com\")) and (\n",
    "            not url.endswith(\"truera.net\")) and \".\" in url[-5:]:\n",
    "        \n",
    "        raise RuntimeError(f\"Unhandled source type {url}\")\n",
    "\n",
    "    docs = loader(str(scrape_file)).load()\n",
    "    print(f\"got {len(docs)} document(s)\")\n",
    "    for doc in docs:\n",
    "        doc.metadata['source'] = url\n",
    "        documents.append(doc)\n",
    "\n",
    "    try:\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "    for surl in soup.findAll(\"a\"):\n",
    "        # print(url)\n",
    "        sub = surl.get('href')\n",
    "        if sub is not None:\n",
    "            sub = str(sub)\n",
    "            # print(\"\\t\", sub)\n",
    "\n",
    "            if sub.startswith(\"mailto\") or sub.startswith(\"tel\"):\n",
    "                continue\n",
    "\n",
    "            if not (sub.startswith(\"http\") or sub.startswith(\"//\")):\n",
    "                sub = url + \"/\" + sub\n",
    "\n",
    "            # print(\"sub=\", sub)\n",
    "\n",
    "            if not (sub.startswith(\"https://truera.com\")\n",
    "                    or sub.startswith(\"https://support.truera.com\")\n",
    "                    or sub.startswith(\"https://marketing.truera.com\")\n",
    "                    or sub.startswith(\"https://go.truera.com\")\n",
    "                    or sub.startswith(\"https://app.truera.net\")\n",
    "                    or sub.startswith(\"https://docs.truera.com\")):\n",
    "                continue\n",
    "\n",
    "            if \"?\" in sub:\n",
    "                continue\n",
    "\n",
    "            if \"#\" in sub:\n",
    "                sub = sub.split(\"#\")[0]\n",
    "\n",
    "            while \"/\" == sub[-1]:\n",
    "                sub = sub[0:-1]\n",
    "\n",
    "            if sub.endswith(\"/.\"):\n",
    "                continue\n",
    "\n",
    "            if sub.endswith(\"/..\"):\n",
    "                continue\n",
    "\n",
    "            if \"..\" in sub:\n",
    "                continue\n",
    "\n",
    "            if sub.endswith(\"//\"):\n",
    "                continue\n",
    "\n",
    "            scrape(sub)"
>>>>>>> public_to_merge_in
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "docs = s.get_documents()"
=======
    "scrape(TRUERA_BASE_URL)"
>>>>>>> public_to_merge_in
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# text_splitter = NLTKTextSplitter(chunk_size=1024, chunk_overlap=0)\n",
    "text_splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=0)\n",
    "chunks = text_splitter.split_documents(docs)"
=======
    "#collected = dict()\n",
    "scrape(TRUREA_DOC_URL)"
>>>>>>> public_to_merge_in
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "big_chunks = [c for c in chunks if len(c.page_content) >= 256]"
=======
    "scrape(\"https://truera.com/ai-quality-blog/\")"
>>>>>>> public_to_merge_in
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "seen_content = dict()\n",
    "unique_chunks = []\n",
    "for chunk in big_chunks:\n",
    "    content = chunk.page_content\n",
    "    #if content in seen_content:\n",
    "        #print(f\"{chunk.metadata} already seen in {seen_content[content]}\")\n",
    "        #continue\n",
    "    seen_content[content] = chunk.metadata\n",
    "    unique_chunks.append(chunk)"
=======
    "len(documents)"
>>>>>>> public_to_merge_in
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# smallest chunk:\n",
    "print(unique_chunks[np.array([len(c.page_content) for c in unique_chunks]).argmin()])\n",
    "\n",
    "# number of chunks:\n",
    "print(len(unique_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings(model='text-embedding-ada-002')  # 1536 dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import DocArrayHnswSearch, DocArrayInMemorySearch"
=======
    "# split scraped documents into chunks\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)"
>>>>>>> public_to_merge_in
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "len(unique_chunks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To DocArrayHnswSearch\n",
    "\n",
    "This is a local document store and retriever that requires no additional api keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DocArrayHnswSearch.from_documents(unique_chunks, embedding, work_dir='hnswlib_trubot', n_dim=1536, max_elements=int(len(unique_chunks)*1.1))\n",
    "db = DocArrayHnswSearch.from_params(\n",
    "    embedding=embedding,\n",
    "    work_dir='hnswlib_trubot',\n",
    "    n_dim=1536,\n",
    "    max_elements=int(len(unique_chunks) * 1.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in db.similarity_search(\"Who is Shayak?\"):\n",
    "    print(\"====\")\n",
    "    print(doc.metadata)\n",
    "    print(doc.page_content)\n",
    "    print(\"====\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Pinecone:"
=======
    "# keep only big ones\n",
    "\n",
    "print(len(docs))\n",
    "bigdocs = [doc for doc in docs if len(doc.page_content) > 256]\n",
    "print(len(bigdocs))"
>>>>>>> public_to_merge_in
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keys import *\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "    environment=PINECONE_ENV  # next to api key in console\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create / upload an index of the docs to pinecone\n",
    "\n",
    "index_name = \"llmdemo\"\n",
<<<<<<< HEAD
=======
    "embedding = OpenAIEmbeddings(model='text-embedding-ada-002')  # 1536 dims\n",
>>>>>>> public_to_merge_in
    "pinecone.delete_index(index_name)\n",
    "pinecone.create_index(index_name, dimension=1536)\n",
    "Pinecone.from_documents(bigdocs, embedding, index_name=index_name)"
   ]
<<<<<<< HEAD
=======
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
>>>>>>> public_to_merge_in
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.8.16"
=======
   "version": "3.10.6"
>>>>>>> public_to_merge_in
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
