{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Local LLM (Alpaca7B) with TruLens\n",
        "\n",
        "In this example, we'll load Alpaca7B from huggingface and run inferences locally, and use langchain as our framework to hold the different parts of our application (conversation memory, the llm, prompt templates, etc.). We'll use prompt templates to prime the model to be a gardening expert and ask questions about gardening that rely on past prompts.\n",
        "\n",
        "We will also track the quality of this model using TruLens. As we get further in the conversation, we may run into issues which we can identify and debug."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "!pip3 install torch\n",
        "!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
        "!pip install -q datasets loralib sentencepiece \n",
        "!pip -q install bitsandbytes accelerate\n",
        "!pip -q install langchain\n",
        "!pip install xformers\n",
        "!pip install trulens-eval\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "import openai\n",
        "import torch\n",
        "from trulens_eval.tru_db import Query\n",
        "from trulens_eval.tru import Tru\n",
        "from trulens_eval import tru_chain\n",
        "from trulens_eval.tru_feedback import Feedback\n",
        "from trulens_eval.tru_feedback import OpenAI as Feedback_OpenAI\n",
        "tru = Tru()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Feedback Function\n",
        "\n",
        "The first thing we should do is define the qualities of our model we care about. In this case, we primarily care if the statement returned by the LLM is relevant to the user's query. We'll use OpenAI to set up a feedback function for query-statement relevance. Make sure to add your own openai API key!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
        "feedback_openai = Feedback_OpenAI()\n",
        "qs_relevance = Feedback(feedback_openai.qs_relevance).on(question=Query.RecordInput, statement = Query.RecordOutput)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhauDrynY0cj"
      },
      "source": [
        "## Loading Alpaca7B\n",
        "\n",
        "Here we're loading a Alpaca7B using HuggingFacePipeline's from_model_id. Alpaca7B has similar performance to OpenAI's text-davinci-003, but can be run locally on your own machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "jllBMgfD-IpL",
        "outputId": "5e55a354-ef8d-42e8-f814-c15a04b8582f"
      },
      "outputs": [],
      "source": [
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "local_llm = HuggingFacePipeline.from_model_id(model_id=\"chavinlo/alpaca-native\",\n",
        "    task=\"text-generation\",\n",
        "    model_kwargs={\"temperature\":0.6, \"top_p\":0.95, \"max_length\":256})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb5iT0OMqISl"
      },
      "source": [
        "## Setting up a Chat with memory\n",
        "\n",
        "It's also important for our AI assistant to have memory of the things we tell it. That way it can give information that is most relevant to our location, conditions, etc. and feels more like we are talking to a human.\n",
        "\n",
        "First we'll set up our AI assistant to remember up to 4 turns in our conversation using ConversationBufferWindowMemory.\n",
        "\n",
        "Then we'll update our prompt template to prime it as a gardening expert.\n",
        "\n",
        "Last, we'll wrap it with truchain. You'll notice that this results in our first logs of the chain itself along with the feedback definition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seS9A42Em8Hf"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "\n",
        "# set the window memory to go back 4 turns\n",
        "window_memory = ConversationBufferWindowMemory(k=4)\n",
        "\n",
        "# create the conversation chain with the given window memory\n",
        "conversation = ConversationChain(\n",
        "    llm=local_llm, \n",
        "    verbose=True, \n",
        "    memory=window_memory\n",
        ")\n",
        "\n",
        "# update the conversation prompt template to prime it as a gardening expert\n",
        "conversation.prompt.template = '''The following is a friendly conversation between a human and an AI gardening expert. The AI is an expert on gardening and gives recommendations specific to location and conditions. If the AI does not know the answer to a question, it truthfully says it does not know. \n",
        "\n",
        "Current conversation:\n",
        "{history}\n",
        "Human: {input}\n",
        "AI:'''\n",
        "\n",
        "# wrap with truchain to instrument it\n",
        "tc_conversation = tru.Chain(conversation, chain_id='GardeningAIwithMemory_v1', feedbacks=[qs_relevance])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we've set up our chain, we can make the first call and ask our AI gardening assistant a question!\n",
        "\n",
        "While this takes a bit of time to run on our local machine, it's nonetheless pretty impressive that we can run such a high quality LLM locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBcJQ6_Vn97h"
      },
      "outputs": [],
      "source": [
        "# make the first call to our AI gardening assistant!\n",
        "response, record = tc_conversation.call_with_record(\"I live in the pacific northwest, what can I plant in my outside garden?\")\n",
        "display(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Konke2xn-Av"
      },
      "outputs": [],
      "source": [
        "# continue the conversation!\n",
        "response, record = tc_conversation.call_with_record(\"What kind of birds am I most likely to see?\")\n",
        "display(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# keep it up!\n",
        "response, record = tc_conversation.call_with_record(\"Thanks! Blue Jays would be awesome, what kind of bird feeder should I get to attract them?\")\n",
        "display(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Oh, looks like something is going wrong and our LLM stopped responding usefully. Let's run the trulens dashboard to explore what the issue might be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tru.run_dashboard(force=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Exploring the dashboard, we found that quality degraded on the third call to the LLM. We've also hypothesized that there may be a conflict between our max token limit of the LLM and the 4 turn window memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "local_llm = HuggingFacePipeline.from_model_id(model_id=\"chavinlo/alpaca-native\",\n",
        "    task=\"text-generation\",\n",
        "    model_kwargs={\"temperature\":0.6, \"top_p\":0.95, \"max_length\":400})\n",
        "\n",
        "from langchain.memory import ConversationTokenBufferMemory\n",
        "\n",
        "# Instead of window memory, let's use token memory to match the model token limit\n",
        "token_memory = ConversationTokenBufferMemory(llm = local_llm, max_token_limit=400)\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=local_llm, \n",
        "    verbose=True, \n",
        "    memory=token_memory\n",
        ")\n",
        "\n",
        "# wrap with truchain to instrument your chain\n",
        "tc_conversation = tru.Chain(conversation, chain_id='GardeningAIwithMemory_v2', feedbacks=[qs_relevance])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response, record = tc_conversation.call_with_record(\"What kind of pests I should worry about?\")\n",
        "display(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response, record = tc_conversation.call_with_record(\"What kind of flowers will grow best in the northeast US?\")\n",
        "display(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response, record = tc_conversation.call_with_record(\"What is the typical soil make-up in gardens in my area?\")\n",
        "display(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response, record = tc_conversation.call_with_record(\"I'd like to grow a large tree in my backyard. Any recommendations that work well with the soil?\")\n",
        "display(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response, record = tc_conversation.call_with_record(\"What other garden improvements should I make to complement these tree recommendations?\")\n",
        "display(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our AI assistant now no longer runs out of tokens in memory. Wahoo!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3.11.3 ('torch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "d5737f6101ac92451320b0e41890107145710b89f85909f3780d702e7818f973"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
