{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(Path().cwd().parent.parent.resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEY SET: OPENAI_API_KEY\n",
      "KEY SET: PINECONE_API_KEY\n",
      "KEY SET: PINECONE_ENV\n",
      "KEY SET: HUGGINGFACE_API_KEY\n",
      "KEY SET: SLACK_TOKEN\n",
      "KEY SET: SLACK_SIGNING_SECRET\n",
      "KEY SET: COHERE_API_KEY\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval import Query\n",
    "from trulens_eval.util import Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import JSON\n",
    "\n",
    "# Imports main tools:\n",
    "from trulens_eval import Feedback\n",
    "from trulens_eval import Huggingface\n",
    "from trulens_eval import Tru\n",
    "from trulens_eval import TruChain\n",
    "from trulens_eval import Query\n",
    "\n",
    "tru = Tru()\n",
    "\n",
    "# imports from langchain to build app\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.prompts.chat import HumanMessagePromptTemplate\n",
    "from langchain.prompts.chat import PromptTemplate\n",
    "import numpy as np\n",
    "\n",
    "full_prompt = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        template=\n",
    "        \"Provide a helpful response with relevant background information for the following: {prompt}\",\n",
    "        input_variables=[\"prompt\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([full_prompt])\n",
    "\n",
    "llm = OpenAI(temperature=0.9, max_tokens=128)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=chat_prompt_template, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic.json import pydantic_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ chain hello -> default.sqlite\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "inputs= {'prompt': 'Who is Piotr?'}\n",
      "creating record with\n",
      "{'main_input': 'Who is Piotr?', 'main_output': '\\n\\nPiotr is a Polish given name derived from the Greek name \"Petros,\" meaning \"rock.\" The name is widely used throughout the Slavic world, and is a popular name in Poland, where it ranked 22nd in terms of popularity in 2019. Piotr is also used in other countries, such as Russia and Ukraine. It is often anglicized to Peter or Peter.', 'main_error': 'None', 'calls': [RecordChainCall(chain_stack=(RecordChainCallMethod(path=JSONPath()._chain, method=MethodIdent(module_name='langchain.chains.llm', class_name='LLMChain', method_name='_call')),), args={'inputs': {'prompt': 'Who is Piotr?'}}, rets={'text': '\\n\\nPiotr is a Polish given name derived from the Greek name \"Petros,\" meaning \"rock.\" The name is widely used throughout the Slavic world, and is a popular name in Poland, where it ranked 22nd in terms of popularity in 2019. Piotr is also used in other countries, such as Russia and Ukraine. It is often anglicized to Peter or Peter.'}, error=None, start_time=datetime.datetime(2023, 6, 1, 16, 49, 19, 224350), end_time=datetime.datetime(2023, 6, 1, 16, 49, 24, 933754), pid=1812552, tid=1812552)], 'cost': RecordCost(n_tokens=101, cost=0.00202), 'chain_id': 'hello'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'record_id': 'record_hash_3efd199e247822b4798a005f7f1b6555',\n",
       " 'chain_id': 'hello',\n",
       " 'cost': {'n_tokens': 101, 'cost': 0.00202},\n",
       " 'main_input': 'Who is Piotr?',\n",
       " 'main_output': '\\n\\nPiotr is a Polish given name derived from the Greek name \"Petros,\" meaning \"rock.\" The name is widely used throughout the Slavic world, and is a popular name in Poland, where it ranked 22nd in terms of popularity in 2019. Piotr is also used in other countries, such as Russia and Ukraine. It is often anglicized to Peter or Peter.',\n",
       " 'main_error': 'None',\n",
       " 'calls': [{'chain_stack': ({'path': {'path': ({'item_or_attribute': '_chain'},)},\n",
       "     'method': {'module_name': 'langchain.chains.llm',\n",
       "      'class_name': 'LLMChain',\n",
       "      'method_name': '_call'}},),\n",
       "   'args': {'inputs': {'prompt': 'Who is Piotr?'}},\n",
       "   'rets': {'text': '\\n\\nPiotr is a Polish given name derived from the Greek name \"Petros,\" meaning \"rock.\" The name is widely used throughout the Slavic world, and is a popular name in Poland, where it ranked 22nd in terms of popularity in 2019. Piotr is also used in other countries, such as Russia and Ukraine. It is often anglicized to Peter or Peter.'},\n",
       "   'error': None,\n",
       "   'start_time': datetime.datetime(2023, 6, 1, 16, 49, 19, 224350),\n",
       "   'end_time': datetime.datetime(2023, 6, 1, 16, 49, 24, 933754),\n",
       "   'pid': 1812552,\n",
       "   'tid': 1812552}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ record record_hash_3efd199e247822b4798a005f7f1b6555 from hello -> default.sqlite\n"
     ]
    }
   ],
   "source": [
    "tc = Tru().Chain(chain=chain, chain_id=\"hello\")\n",
    "res, record = tc.call_with_record(\"Who is Piotr?\")\n",
    "# tc.dict()\n",
    "record.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"record_id\": \"record_hash_3efd199e247822b4798a005f7f1b6555\", \"chain_id\": \"hello\", \"cost\": {\"n_tokens\": 101, \"cost\": 0.00202}, \"main_input\": \"Who is Piotr?\", \"main_output\": \"\\\\n\\\\nPiotr is a Polish given name derived from the Greek name \\\\\"Petros,\\\\\" meaning \\\\\"rock.\\\\\" The name is widely used throughout the Slavic world, and is a popular name in Poland, where it ranked 22nd in terms of popularity in 2019. Piotr is also used in other countries, such as Russia and Ukraine. It is often anglicized to Peter or Peter.\", \"main_error\": \"None\", \"calls\": [{\"chain_stack\": [{\"path\": {\"path\": [{\"item_or_attribute\": \"_chain\"}]}, \"method\": {\"module_name\": \"langchain.chains.llm\", \"class_name\": \"LLMChain\", \"method_name\": \"_call\"}}], \"args\": {\"inputs\": {\"prompt\": \"Who is Piotr?\"}}, \"rets\": {\"text\": \"\\\\n\\\\nPiotr is a Polish given name derived from the Greek name \\\\\"Petros,\\\\\" meaning \\\\\"rock.\\\\\" The name is widely used throughout the Slavic world, and is a popular name in Poland, where it ranked 22nd in terms of popularity in 2019. Piotr is also used in other countries, such as Russia and Ukraine. It is often anglicized to Peter or Peter.\"}, \"error\": null, \"start_time\": \"2023-06-01T16:49:19.224350\", \"end_time\": \"2023-06-01T16:49:24.933754\", \"pid\": 1812552, \"tid\": 1812552}]}'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trulens_eval.util import json_str_of_obj\n",
    "json_str_of_obj(tc)\n",
    "json_str_of_obj(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import PrettyPrinter\n",
    "from typing import Sequence\n",
    "\n",
    "from IPython.display import JSON\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.vectorstores import DocArrayHnswSearch\n",
    "import numpy as np\n",
    "\n",
    "from trulens_eval import Tru\n",
    "from trulens_eval import tru_feedback\n",
    "from trulens_eval.keys import *\n",
    "from trulens_eval.tru_db import Query\n",
    "from trulens_eval.tru_feedback import Feedback\n",
    "from trulens_eval.tru_feedback import Huggingface\n",
    "from trulens_eval.utils.langchain import WithFilterDocuments\n",
    "\n",
    "# if using Pinecone vectordb:\n",
    "# from langchain.vectorstores import Pinecone\n",
    "# import pinecone\n",
    "\n",
    "pp = PrettyPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-3.5-turbo\"\n",
    "chain_id = \"TruBot\"\n",
    "\n",
    "# Pinecone configuration if using pinecone.\n",
    "# pinecone.init(\n",
    "#    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "#    environment=PINECONE_ENV  # next to api key in console\n",
    "#)\n",
    "#docsearch = Pinecone.from_existing_index(\n",
    "#    index_name=\"llmdemo\", embedding=embedding\n",
    "#)\n",
    "\n",
    "# LLM for completing prompts, and other tasks.\n",
    "llm = OpenAI(temperature=0, max_tokens=256)\n",
    "\n",
    "openai = tru_feedback.OpenAI()\n",
    "\n",
    "def new_conversation(\n",
    "    lang_prompt_fix: bool = False,\n",
    "    context_prompt_fix: bool = False,\n",
    "    context_filter_fix: bool = False,\n",
    "    feedbacks: Sequence[Feedback] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a chain for a new conversation (blank memory). Set flags to enable\n",
    "    adjustments to prompts or add context filtering.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert not(lang_prompt_fix and context_prompt_fix), \"Cannot use both prompt fixes at the same time.\"\n",
    "\n",
    "    # Embedding needed for Pinecone vector db.\n",
    "    embedding = OpenAIEmbeddings(model='text-embedding-ada-002')  # 1536 dims\n",
    "\n",
    "    # Conversation memory.\n",
    "    memory = ConversationSummaryBufferMemory(\n",
    "        max_token_limit=650,\n",
    "        llm=llm,\n",
    "        memory_key=\"chat_history\",\n",
    "        output_key='answer'\n",
    "    )\n",
    "\n",
    "    # Pinecone alternative. Requires precomputed 'hnswlib_truera' folder.\n",
    "    docsearch = DocArrayHnswSearch.from_params(\n",
    "        embedding=embedding,\n",
    "        work_dir='hnswlib_trubot',\n",
    "        n_dim=1536,\n",
    "        max_elements=1024\n",
    "    )\n",
    "    retriever = docsearch.as_retriever()\n",
    "\n",
    "    # Better contexts fix, filter contexts with relevance:\n",
    "    if context_filter_fix: \n",
    "        def filter_by_relevance(query, doc):\n",
    "            return openai.qs_relevance(\n",
    "                question=query, statement=doc.page_content\n",
    "            ) > 0.5\n",
    "\n",
    "        retriever = WithFilterDocuments.of_retriever(\n",
    "            retriever=retriever, filter_func=filter_by_relevance\n",
    "        )\n",
    "\n",
    "    # Conversational chain puts it all together.\n",
    "    chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        memory=memory,\n",
    "        get_chat_history=lambda a: a,\n",
    "        max_tokens_limit=4096\n",
    "    )\n",
    "\n",
    "    # Need to copy these otherwise various chains will feature templates that\n",
    "    # point to the same objects.\n",
    "    chain.combine_docs_chain.llm_chain.prompt = \\\n",
    "        chain.combine_docs_chain.llm_chain.prompt.copy()\n",
    "    chain.combine_docs_chain.document_prompt = \\\n",
    "        chain.combine_docs_chain.document_prompt.copy()\n",
    "\n",
    "    # Language mismatch fix:\n",
    "    if lang_prompt_fix:\n",
    "        chain.combine_docs_chain.llm_chain.prompt.template = \\\n",
    "            \"Use the following pieces of context to answer the question at the end \" \\\n",
    "            \"in the same language as the question. If you don't know the answer, \" \\\n",
    "            \"just say that you don't know, don't try to make up an answer.\\n\\n\" \\\n",
    "            \"{context}\\n\\n\" \\\n",
    "            \"Question: {question}\\n\" \\\n",
    "            \"Helpful Answer: \"\n",
    "\n",
    "    # Poor contexts fix using prompts:\n",
    "    elif context_prompt_fix:\n",
    "        chain.combine_docs_chain.llm_chain.prompt.template = \\\n",
    "            \"Use only the relevant contexts to answer the question at the end \" \\\n",
    "            \". Some pieces of context may not be relevant. If you don't know the answer, \" \\\n",
    "            \"just say that you don't know, don't try to make up an answer.\\n\\n\" \\\n",
    "            \"Contexts: \\n{context}\\n\\n\" \\\n",
    "            \"Question: {question}\\n\" \\\n",
    "            \"Helpful Answer: \"\n",
    "        chain.combine_docs_chain.document_prompt.template = \"\\tContext: {page_content}\"\n",
    "\n",
    "    # Trulens instrumentation.\n",
    "    tc = Tru().Chain(chain=chain, feedbacks=feedbacks, verbose=True)\n",
    "\n",
    "    return tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedbacks=[\n",
    "#    f_toxic, \n",
    "#    f_lang_match, \n",
    "    # f_relevance, \n",
    "#    f_qs_relevance\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = new_conversation(\n",
    "    feedbacks=feedbacks,\n",
    "    context_filter_fix=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, record = tc.call_with_record(\"Who is Shayak?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
