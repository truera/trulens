{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(Path().cwd().parent.parent.resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEY SET: OPENAI_API_KEY\n",
      "KEY SET: PINECONE_API_KEY\n",
      "KEY SET: PINECONE_ENV\n",
      "KEY SET: HUGGINGFACE_API_KEY\n",
      "KEY SET: SLACK_TOKEN\n",
      "KEY SET: SLACK_SIGNING_SECRET\n",
      "KEY SET: COHERE_API_KEY\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval.schema import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import PrettyPrinter\n",
    "from typing import Sequence\n",
    "\n",
    "from IPython.display import JSON\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.vectorstores import DocArrayHnswSearch\n",
    "import numpy as np\n",
    "\n",
    "from trulens_eval import Tru\n",
    "from trulens_eval import tru_feedback\n",
    "from trulens_eval.keys import *\n",
    "from trulens_eval.tru_db import Query\n",
    "from trulens_eval.tru_db import Record\n",
    "from trulens_eval.tru_feedback import Feedback\n",
    "from trulens_eval.tru_feedback import Huggingface\n",
    "from trulens_eval.utils.langchain import WithFilterDocuments\n",
    "\n",
    "# if using Pinecone vectordb:\n",
    "# from langchain.vectorstores import Pinecone\n",
    "# import pinecone\n",
    "\n",
    "pp = PrettyPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-3.5-turbo\"\n",
    "chain_id = \"TruBot\"\n",
    "\n",
    "# Pinecone configuration if using pinecone.\n",
    "# pinecone.init(\n",
    "#    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "#    environment=PINECONE_ENV  # next to api key in console\n",
    "#)\n",
    "#docsearch = Pinecone.from_existing_index(\n",
    "#    index_name=\"llmdemo\", embedding=embedding\n",
    "#)\n",
    "\n",
    "# LLM for completing prompts, and other tasks.\n",
    "llm = OpenAI(temperature=0, max_tokens=256)\n",
    "\n",
    "def new_conversation(\n",
    "    lang_prompt_fix: bool = False,\n",
    "    context_prompt_fix: bool = False,\n",
    "    context_filter_fix: bool = False,\n",
    "    feedbacks: Sequence[Feedback] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a chain for a new conversation (blank memory). Set flags to enable\n",
    "    adjustments to prompts or add context filtering.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert not(lang_prompt_fix and context_prompt_fix), \"Cannot use both prompt fixes at the same time.\"\n",
    "\n",
    "    # Embedding needed for Pinecone vector db.\n",
    "    embedding = OpenAIEmbeddings(model='text-embedding-ada-002')  # 1536 dims\n",
    "\n",
    "    # Conversation memory.\n",
    "    memory = ConversationSummaryBufferMemory(\n",
    "        max_token_limit=650,\n",
    "        llm=llm,\n",
    "        memory_key=\"chat_history\",\n",
    "        output_key='answer'\n",
    "    )\n",
    "\n",
    "    # Pinecone alternative. Requires precomputed 'hnswlib_truera' folder.\n",
    "    docsearch = DocArrayHnswSearch.from_params(\n",
    "        embedding=embedding,\n",
    "        work_dir='hnswlib_trubot',\n",
    "        n_dim=1536,\n",
    "        max_elements=1024\n",
    "    )\n",
    "    retriever = docsearch.as_retriever()\n",
    "\n",
    "    # Better contexts fix, filter contexts with relevance:\n",
    "    if context_filter_fix: \n",
    "        def filter_by_relevance(query, doc):\n",
    "            return openai.qs_relevance(\n",
    "                question=query, statement=doc.page_content\n",
    "            ) > 0.5\n",
    "\n",
    "        retriever = WithFilterDocuments.of_retriever(\n",
    "            retriever=retriever, filter_func=filter_by_relevance\n",
    "        )\n",
    "\n",
    "    # Conversational chain puts it all together.\n",
    "    chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        memory=memory,\n",
    "        get_chat_history=lambda a: a,\n",
    "        max_tokens_limit=4096\n",
    "    )\n",
    "\n",
    "    # Need to copy these otherwise various chains will feature templates that\n",
    "    # point to the same objects.\n",
    "    chain.combine_docs_chain.llm_chain.prompt = \\\n",
    "        chain.combine_docs_chain.llm_chain.prompt.copy()\n",
    "    chain.combine_docs_chain.document_prompt = \\\n",
    "        chain.combine_docs_chain.document_prompt.copy()\n",
    "\n",
    "    # Language mismatch fix:\n",
    "    if lang_prompt_fix:\n",
    "        chain.combine_docs_chain.llm_chain.prompt.template = \\\n",
    "            \"Use the following pieces of context to answer the question at the end \" \\\n",
    "            \"in the same language as the question. If you don't know the answer, \" \\\n",
    "            \"just say that you don't know, don't try to make up an answer.\\n\\n\" \\\n",
    "            \"{context}\\n\\n\" \\\n",
    "            \"Question: {question}\\n\" \\\n",
    "            \"Helpful Answer: \"\n",
    "\n",
    "    # Poor contexts fix using prompts:\n",
    "    elif context_prompt_fix:\n",
    "        chain.combine_docs_chain.llm_chain.prompt.template = \\\n",
    "            \"Use only the relevant contexts to answer the question at the end \" \\\n",
    "            \". Some pieces of context may not be relevant. If you don't know the answer, \" \\\n",
    "            \"just say that you don't know, don't try to make up an answer.\\n\\n\" \\\n",
    "            \"Contexts: \\n{context}\\n\\n\" \\\n",
    "            \"Question: {question}\\n\" \\\n",
    "            \"Helpful Answer: \"\n",
    "        chain.combine_docs_chain.document_prompt.template = \"\\tContext: {page_content}\"\n",
    "\n",
    "    # Trulens instrumentation.\n",
    "    tc = Tru().Chain(chain, feedbacks=feedbacks, verbose=True)\n",
    "\n",
    "    return tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91b80f8c5d1425b96540d513ec97109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "huggingface api: 0requests [00:00, ?requests/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e58945eb6540129eaad1ee4413a297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openai api: 0requests [00:00, ?requests/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'Feedback' object has no attribute 'to_json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/piotrm/repos/trulens/trulens_eval/trulens_eval/examples/trubot.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhome/home/piotrm/repos/trulens/trulens_eval/trulens_eval/examples/trubot.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m openai \u001b[39m=\u001b[39m tru_feedback\u001b[39m.\u001b[39mOpenAI()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhome/home/piotrm/repos/trulens/trulens_eval/trulens_eval/examples/trubot.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Toxicity (of the response):\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bhome/home/piotrm/repos/trulens/trulens_eval/trulens_eval/examples/trubot.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m f_toxic \u001b[39m=\u001b[39m tru_feedback\u001b[39m.\u001b[39;49mFeedback(hugs\u001b[39m.\u001b[39;49mnot_toxic)\u001b[39m.\u001b[39;49mon_response()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhome/home/piotrm/repos/trulens/trulens_eval/trulens_eval/examples/trubot.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Language match (between prompt and response):\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhome/home/piotrm/repos/trulens/trulens_eval/trulens_eval/examples/trubot.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m f_lang_match \u001b[39m=\u001b[39m tru_feedback\u001b[39m.\u001b[39mFeedback(hugs\u001b[39m.\u001b[39mlanguage_match)\u001b[39m.\u001b[39mon(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhome/home/piotrm/repos/trulens/trulens_eval/trulens_eval/examples/trubot.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     text1\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m, text2\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhome/home/piotrm/repos/trulens/trulens_eval/trulens_eval/examples/trubot.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m )\n",
      "File \u001b[0;32m~/repos/trulens/trulens_eval/trulens_eval/tru_feedback.py:337\u001b[0m, in \u001b[0;36mFeedback.on_response\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_response\u001b[39m(\u001b[39mself\u001b[39m, arg: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    332\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[39m    Create a variant of `self` that will take in the main chain output or\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m    \"response\" as input, sending it as an argument `arg` to implementation.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     \u001b[39mreturn\u001b[39;00m Feedback(imp\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimp, selectors\u001b[39m=\u001b[39;49m{arg: \u001b[39m\"\u001b[39;49m\u001b[39mresponse\u001b[39;49m\u001b[39m\"\u001b[39;49m})\n",
      "File \u001b[0;32m~/repos/trulens/trulens_eval/trulens_eval/tru_feedback.py:156\u001b[0m, in \u001b[0;36mFeedback.__init__\u001b[0;34m(self, imp, selectors, feedback_id)\u001b[0m\n\u001b[1;32m    152\u001b[0m impj[\u001b[39m'\u001b[39m\u001b[39mmethod_name\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m imp\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimp_json \u001b[39m=\u001b[39m impj\n\u001b[0;32m--> 156\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeedback_json \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mto_json()\n\u001b[1;32m    157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeedback_id \u001b[39m=\u001b[39m feedback_id \u001b[39mor\u001b[39;00m obj_id_of_obj(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeedback_json, prefix\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfeedback\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeedback_json[\u001b[39m'\u001b[39m\u001b[39mfeedback_id\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeedback_id\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Feedback' object has no attribute 'to_json'"
     ]
    }
   ],
   "source": [
    "# Setup some feedback functions.\n",
    "\n",
    "hugs = tru_feedback.Huggingface()\n",
    "openai = tru_feedback.OpenAI()\n",
    "\n",
    "# Toxicity (of the response):\n",
    "f_toxic = tru_feedback.Feedback(hugs.not_toxic).on_response()\n",
    "\n",
    "# Language match (between prompt and response):\n",
    "f_lang_match = tru_feedback.Feedback(hugs.language_match).on(\n",
    "    text1=\"prompt\", text2=\"response\"\n",
    ")\n",
    "\n",
    "# Question to answer relevance:\n",
    "f_relevance = tru_feedback.Feedback(openai.relevance).on(\n",
    "    prompt=\"input\", response=\"output\"\n",
    ")\n",
    "\n",
    "# Question to context piece relevance:\n",
    "f_qs_relevance = tru_feedback.Feedback(openai.qs_relevance).on(\n",
    "    question=\"input\",\n",
    "    statement=Record.chain.combine_docs_chain._call.args.inputs.input_documents\n",
    ").on_multiple(\n",
    "    multiarg=\"statement\", each_query=Record.page_content, agg=np.min\n",
    ")\n",
    "\n",
    "feedbacks=[\n",
    "    # f_toxic, \n",
    "    f_lang_match, \n",
    "    # f_relevance, \n",
    "    f_qs_relevance\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = new_conversation(feedbacks=feedbacks, context_filter_fix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = [dict(), dict()\n",
    "\n",
    "]\n",
    "messages = [\"Who is Shayak?\", \"Wer ist Shayak?\", \"Kim jest Shayak?\", \"¿Quién es Shayak?\", \"Was ist QII?\", \"Co jest QII?\"]\n",
    "\n",
    "# selectors = selectors[0:2]\n",
    "# messages = messages[0:2]\n",
    "\n",
    "def test_bot(selector, question):\n",
    "    print(selector, question)\n",
    "    chain = get_or_make_chain(cid=question + str(selector), selector=selector)\n",
    "    answer = get_answer(chain=chain, question=question)\n",
    "    return answer\n",
    "\n",
    "results = []\n",
    "\n",
    "for s in selectors:\n",
    "    for m in messages:\n",
    "        results.append(TP().promise(test_bot, selector=s, question=m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal langchain usage:\n",
    "\n",
    "res = tc(\"Who is Shayak?\")\n",
    "pp.pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also retrieve trulens records if needed for inspection or manual feedback\n",
    "# evaluation:\n",
    "\n",
    "res, record = tc.call_with_record(\"Who is Shayak?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the feedback functions manually:\n",
    "\n",
    "feedback = Tru().run_feedback_functions(\n",
    "    record_json=record,\n",
    "    feedback_functions=feedbacks\n",
    ")\n",
    "\n",
    "feedback"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from streamlit_jupyter import StreamlitPatcher, tqdm\n",
    "StreamlitPatcher().jupyter()\n",
    "\n",
    "from trulens_eval.tru_db import TruDB\n",
    "from trulens_eval.ux.components import render_calls\n",
    "render_calls(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.write(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.write(tc.json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
