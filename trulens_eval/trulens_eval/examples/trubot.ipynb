{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(Path().cwd().parent.parent.resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEY SET: OPENAI_API_KEY\n",
      "KEY SET: PINECONE_API_KEY\n",
      "KEY SET: PINECONE_ENV\n",
      "KEY SET: HUGGINGFACE_API_KEY\n",
      "KEY SET: SLACK_TOKEN\n",
      "KEY SET: SLACK_SIGNING_SECRET\n",
      "KEY SET: COHERE_API_KEY\n"
     ]
    }
   ],
   "source": [
    "from pprint import PrettyPrinter\n",
    "from typing import Sequence\n",
    "\n",
    "from IPython.display import JSON\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.vectorstores import DocArrayHnswSearch\n",
    "import numpy as np\n",
    "\n",
    "from trulens_eval import Tru\n",
    "from trulens_eval import tru_feedback\n",
    "from trulens_eval.keys import *\n",
    "from trulens_eval.tru_db import Query\n",
    "from trulens_eval.tru_db import Record\n",
    "from trulens_eval.tru_feedback import Feedback\n",
    "from trulens_eval.tru_feedback import Huggingface\n",
    "from trulens_eval.utils.langchain import WithFilterDocuments\n",
    "\n",
    "# if using Pinecone vectordb:\n",
    "# from langchain.vectorstores import Pinecone\n",
    "# import pinecone\n",
    "\n",
    "pp = PrettyPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-3.5-turbo\"\n",
    "chain_id = \"TruBot\"\n",
    "\n",
    "# Pinecone configuration if using pinecone.\n",
    "# pinecone.init(\n",
    "#    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "#    environment=PINECONE_ENV  # next to api key in console\n",
    "#)\n",
    "#docsearch = Pinecone.from_existing_index(\n",
    "#    index_name=\"llmdemo\", embedding=embedding\n",
    "#)\n",
    "\n",
    "# LLM for completing prompts, and other tasks.\n",
    "llm = OpenAI(temperature=0, max_tokens=256)\n",
    "\n",
    "def new_conversation(\n",
    "    lang_prompt_fix: bool = False,\n",
    "    context_prompt_fix: bool = False,\n",
    "    context_filter_fix: bool = False,\n",
    "    feedbacks: Sequence[Feedback] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a chain for a new conversation (blank memory). Set flags to enable\n",
    "    adjustments to prompts or add context filtering.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert not(lang_prompt_fix and context_prompt_fix), \"Cannot use both prompt fixes at the same time.\"\n",
    "\n",
    "    # Embedding needed for Pinecone vector db.\n",
    "    embedding = OpenAIEmbeddings(model='text-embedding-ada-002')  # 1536 dims\n",
    "\n",
    "    # Conversation memory.\n",
    "    memory = ConversationSummaryBufferMemory(\n",
    "        max_token_limit=650,\n",
    "        llm=llm,\n",
    "        memory_key=\"chat_history\",\n",
    "        output_key='answer'\n",
    "    )\n",
    "\n",
    "    # Pinecone alternative. Requires precomputed 'hnswlib_truera' folder.\n",
    "    docsearch = DocArrayHnswSearch.from_params(\n",
    "        embedding=embedding,\n",
    "        work_dir='hnswlib_trubot',\n",
    "        n_dim=1536,\n",
    "        max_elements=1024\n",
    "    )\n",
    "    retriever = docsearch.as_retriever()\n",
    "\n",
    "    # Better contexts fix, filter contexts with relevance:\n",
    "    if context_filter_fix: \n",
    "        def filter_by_relevance(query, doc):\n",
    "            return openai.qs_relevance(\n",
    "                question=query, statement=doc.page_content\n",
    "            ) > 0.5\n",
    "\n",
    "        retriever = WithFilterDocuments.of_retriever(\n",
    "            retriever=retriever, filter_func=filter_by_relevance\n",
    "        )\n",
    "\n",
    "    # Conversational chain puts it all together.\n",
    "    chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        memory=memory,\n",
    "        get_chat_history=lambda a: a,\n",
    "        max_tokens_limit=4096\n",
    "    )\n",
    "\n",
    "    # Need to copy these otherwise various chains will feature templates that\n",
    "    # point to the same objects.\n",
    "    chain.combine_docs_chain.llm_chain.prompt = \\\n",
    "        chain.combine_docs_chain.llm_chain.prompt.copy()\n",
    "    chain.combine_docs_chain.document_prompt = \\\n",
    "        chain.combine_docs_chain.document_prompt.copy()\n",
    "\n",
    "    # Language mismatch fix:\n",
    "    if lang_prompt_fix:\n",
    "        chain.combine_docs_chain.llm_chain.prompt.template = \\\n",
    "            \"Use the following pieces of context to answer the question at the end \" \\\n",
    "            \"in the same language as the question. If you don't know the answer, \" \\\n",
    "            \"just say that you don't know, don't try to make up an answer.\\n\\n\" \\\n",
    "            \"{context}\\n\\n\" \\\n",
    "            \"Question: {question}\\n\" \\\n",
    "            \"Helpful Answer: \"\n",
    "\n",
    "    # Poor contexts fix using prompts:\n",
    "    elif context_prompt_fix:\n",
    "        chain.combine_docs_chain.llm_chain.prompt.template = \\\n",
    "            \"Use only the relevant contexts to answer the question at the end \" \\\n",
    "            \". Some pieces of context may not be relevant. If you don't know the answer, \" \\\n",
    "            \"just say that you don't know, don't try to make up an answer.\\n\\n\" \\\n",
    "            \"Contexts: \\n{context}\\n\\n\" \\\n",
    "            \"Question: {question}\\n\" \\\n",
    "            \"Helpful Answer: \"\n",
    "        chain.combine_docs_chain.document_prompt.template = \"\\tContext: {page_content}\"\n",
    "\n",
    "    # Trulens instrumentation.\n",
    "    tc = Tru().Chain(chain, feedbacks=feedbacks)\n",
    "\n",
    "    return tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e2785ca2084325ad849784304b597b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "huggingface api: 0requests [00:00, ?requests/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b9ff3b851794625946190d650250175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openai api: 0requests [00:00, ?requests/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup some feedback functions.\n",
    "\n",
    "hugs = tru_feedback.Huggingface()\n",
    "openai = tru_feedback.OpenAI()\n",
    "\n",
    "# Toxicity (of the response):\n",
    "f_toxic = tru_feedback.Feedback(hugs.not_toxic).on_response()\n",
    "\n",
    "# Language match (between prompt and response):\n",
    "f_lang_match = tru_feedback.Feedback(hugs.language_match).on(\n",
    "    text1=\"prompt\", text2=\"response\"\n",
    ")\n",
    "\n",
    "# Question to answer relevance:\n",
    "f_relevance = tru_feedback.Feedback(openai.relevance).on(\n",
    "    prompt=\"input\", response=\"output\"\n",
    ")\n",
    "\n",
    "# Question to context piece relevance:\n",
    "f_qs_relevance = tru_feedback.Feedback(openai.qs_relevance).on(\n",
    "    question=\"input\",\n",
    "    statement=Record.chain.combine_docs_chain._call.args.inputs.input_documents\n",
    ").on_multiple(\n",
    "    multiarg=\"statement\", each_query=Record.page_content, agg=np.min\n",
    ")\n",
    "\n",
    "feedbacks=[\n",
    "    # f_toxic, \n",
    "    f_lang_match, \n",
    "    # f_relevance, \n",
    "    f_qs_relevance\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ chain chain_hash_a3bbf04ab4f88cf3ab470c786ee3f2e7 -> default.sqlite\n",
      "✅ feedback def. feedback_hash_26c84c76d907d808c36028e024af63c0 -> default.sqlite\n",
      "✅ feedback def. feedback_hash_b9475c84f84aee5e117ba7ec7aab7e25 -> default.sqlite\n"
     ]
    }
   ],
   "source": [
    "tc = new_conversation(feedbacks=feedbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': ' Shayak Sen is a researcher who has been building systems and '\n",
      "           'leading research to make machine learning and big data systems '\n",
      "           'more explainable, privacy compliant, and fair. He obtained his PhD '\n",
      "           'in Computer Science from Carnegie Mellon University and BTech in '\n",
      "           'Computer Science from the Indian Institute of Technology, Delhi. '\n",
      "           'Most recently, he was Group Chief Data Officer at Standard '\n",
      "           'Chartered Bank.',\n",
      " 'chat_history': '',\n",
      " 'question': 'Who is Shayak?',\n",
      " 'source_documents': [Document(page_content='When Shayak started building production grade machine learning models for algorithmic trading 10 years ago, he realized the need for putting the ‘science’ back in ‘data science’. Since then, he has been building systems and leading research to make machine learning and big data systems more explainable, privacy compliant, and fair. Shayak’s research at Carnegie Mellon University introduced a number of pioneering breakthroughs to the field of explainable AI. Shayak obtained his PhD in Computer Science from Carnegie Mellon University and BTech in Computer Science from the Indian Institute of Technology, Delhi.', metadata={'source': 'https://truera.com/ai-quality-leader/'}),\n",
      "                      Document(page_content='When Shayak started building production grade machine learning models for algorithmic trading 10 years ago, he realized the need for putting the ‘science’ back in ‘data science’. Since then, he has been building systems and leading research to make machine learning and big data systems more explainable, privacy compliant, and fair. Shayak’s research at Carnegie Mellon University introduced a number of pioneering breakthroughs to the field of explainable AI. Shayak obtained his PhD in Computer Science from Carnegie Mellon University and BTech in Computer Science from the Indian Institute of Technology, Delhi.', metadata={'source': 'https://truera.com/ai-quality-research/'}),\n",
      "                      Document(page_content='Anupam is passionate about enabling responsible adoption of artificial intelligence. As a Professor of Electrical & Computer Engineering and Computer Science at Carnegie Mellon University for over a decade, he has led groundbreaking research in the areas of AI explainability and governance as well as privacy and data protection. Anupam obtained PhD and MS degrees from Stanford University and a BTech from the Indian Institute of Technology, Kharagpur, all in Computer Science.\\n\\nLinkedin\\n\\nShayak Sen', metadata={'source': 'https://truera.com/ai-quality-leader/'}),\n",
      "                      Document(page_content='Most recently, Shameek was Group Chief Data Officer at Standard Chartered Bank, where he helped the bank explore and adopt AI in multiple areas (e.g., credit, financial crime compliance, customer analytics, surveillance), and shaped the bank’s internal approach to responsible AI', metadata={'source': 'https://marketing.truera.com/webinar-eu-ai-law'})]}\n"
     ]
    }
   ],
   "source": [
    "# Normal langchain usage:\n",
    "\n",
    "res = tc(\"Who is Shayak?\")\n",
    "pp.pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ record record_hash_6a05079940559fd6e399a59ba6a945ff from chain_hash_a3bbf04ab4f88cf3ab470c786ee3f2e7 -> default.sqlite\n",
      "✅ feedback feedback_hash_26c84c76d907d808c36028e024af63c0 on record_hash_6a05079940559fd6e399a59ba6a945ff -> default.sqlite\n",
      "✅ feedback feedback_hash_b9475c84f84aee5e117ba7ec7aab7e25 on record_hash_6a05079940559fd6e399a59ba6a945ff -> default.sqlite\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ record record_hash_97e02355b8c0eab238a09b2cfe140330 from chain_hash_a3bbf04ab4f88cf3ab470c786ee3f2e7 -> default.sqlite\n",
      "✅ feedback feedback_hash_26c84c76d907d808c36028e024af63c0 on record_hash_97e02355b8c0eab238a09b2cfe140330 -> default.sqlite\n",
      "✅ feedback feedback_hash_b9475c84f84aee5e117ba7ec7aab7e25 on record_hash_97e02355b8c0eab238a09b2cfe140330 -> default.sqlite\n"
     ]
    }
   ],
   "source": [
    "# Also retrieve trulens records if needed for inspection or manual feedback\n",
    "# evaluation:\n",
    "\n",
    "res, record = tc.call_with_record(\"Who is Shayak?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_success': True,\n",
       "  'feedback_id': 'feedback_hash_26c84c76d907d808c36028e024af63c0',\n",
       "  'record_id': 'record_hash_97e02355b8c0eab238a09b2cfe140330',\n",
       "  'language_match': 0.7016496420837939},\n",
       " {'_success': True,\n",
       "  'feedback_id': 'feedback_hash_b9475c84f84aee5e117ba7ec7aab7e25',\n",
       "  'record_id': 'record_hash_97e02355b8c0eab238a09b2cfe140330',\n",
       "  'qs_relevance': 0.1}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:openai request failed <class 'openai.error.RateLimitError'>=That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 9cd5fb5453980ff66cd66d72e39dc29e in your message.). Retries=3.\n"
     ]
    }
   ],
   "source": [
    "# Run the feedback functions manually:\n",
    "\n",
    "feedback = Tru().run_feedback_functions(\n",
    "    record_json=record,\n",
    "    feedback_functions=feedbacks\n",
    ")\n",
    "\n",
    "feedback"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
