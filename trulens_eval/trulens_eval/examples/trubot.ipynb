{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(Path().cwd().parent.parent.resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEY SET: OPENAI_API_KEY\n",
      "KEY SET: PINECONE_API_KEY\n",
      "KEY SET: PINECONE_ENV\n",
      "KEY SET: HUGGINGFACE_API_KEY\n",
      "KEY SET: SLACK_TOKEN\n",
      "KEY SET: SLACK_SIGNING_SECRET\n",
      "KEY SET: COHERE_API_KEY\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ChainQuery' from 'trulens_eval.tru_db' (/home/piotrm/repos/trulens/trulens_eval/trulens_eval/tru_db.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/piotrm/repos/trulens/trulens_eval/trulens_eval/examples/trubot.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhome/home/piotrm/repos/trulens/trulens_eval/trulens_eval/examples/trubot.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m \u001b[39mimport\u001b[39;00m JSON\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhome/home/piotrm/repos/trulens/trulens_eval/trulens_eval/examples/trubot.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Imports main tools:\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bhome/home/piotrm/repos/trulens/trulens_eval/trulens_eval/examples/trubot.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrulens_eval\u001b[39;00m \u001b[39mimport\u001b[39;00m Feedback\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhome/home/piotrm/repos/trulens/trulens_eval/trulens_eval/examples/trubot.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrulens_eval\u001b[39;00m \u001b[39mimport\u001b[39;00m Huggingface\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhome/home/piotrm/repos/trulens/trulens_eval/trulens_eval/examples/trubot.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrulens_eval\u001b[39;00m \u001b[39mimport\u001b[39;00m Tru\n",
      "File \u001b[0;32m~/repos/trulens/trulens_eval/trulens_eval/__init__.py:29\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# Imports of most common parts of the library.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m    - util.py keys.py\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m0.1.1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrulens_eval\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtru_chain\u001b[39;00m \u001b[39mimport\u001b[39;00m TruChain\n\u001b[1;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrulens_eval\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtru_feedback\u001b[39;00m \u001b[39mimport\u001b[39;00m Feedback\n\u001b[1;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrulens_eval\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtru_feedback\u001b[39;00m \u001b[39mimport\u001b[39;00m OpenAI\n",
      "File \u001b[0;32m~/repos/trulens/trulens_eval/trulens_eval/tru_chain.py:96\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrulens_eval\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtru_db\u001b[39;00m \u001b[39mimport\u001b[39;00m Query\n\u001b[1;32m     95\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrulens_eval\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtru_db\u001b[39;00m \u001b[39mimport\u001b[39;00m TruDB\n\u001b[0;32m---> 96\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrulens_eval\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtru_feedback\u001b[39;00m \u001b[39mimport\u001b[39;00m Feedback\n\u001b[1;32m     97\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrulens_eval\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtru\u001b[39;00m \u001b[39mimport\u001b[39;00m Tru\n\u001b[1;32m     98\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrulens_eval\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m TP, JSONPath, jsonify\n",
      "File \u001b[0;32m~/repos/trulens/trulens_eval/trulens_eval/tru_feedback.py:55\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrulens_eval\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeys\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     54\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrulens_eval\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprovider_apis\u001b[39;00m \u001b[39mimport\u001b[39;00m Endpoint\n\u001b[0;32m---> 55\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrulens_eval\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtru_db\u001b[39;00m \u001b[39mimport\u001b[39;00m JSON, ChainQuery, RecordInput, RecordOutput, RecordQuery\n\u001b[1;32m     56\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrulens_eval\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtru_db\u001b[39;00m \u001b[39mimport\u001b[39;00m obj_id_of_obj\n\u001b[1;32m     57\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrulens_eval\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtru_db\u001b[39;00m \u001b[39mimport\u001b[39;00m Query\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ChainQuery' from 'trulens_eval.tru_db' (/home/piotrm/repos/trulens/trulens_eval/trulens_eval/tru_db.py)"
     ]
    }
   ],
   "source": [
    "from IPython.display import JSON\n",
    "\n",
    "# Imports main tools:\n",
    "from trulens_eval import Feedback\n",
    "from trulens_eval import Huggingface\n",
    "from trulens_eval import Tru\n",
    "from trulens_eval import TruChain\n",
    "from trulens_eval import Query\n",
    "\n",
    "tru = Tru()\n",
    "\n",
    "# imports from langchain to build app\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.prompts.chat import HumanMessagePromptTemplate\n",
    "from langchain.prompts.chat import PromptTemplate\n",
    "import numpy as np\n",
    "\n",
    "full_prompt = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        template=\n",
    "        \"Provide a helpful response with relevant background information for the following: {prompt}\",\n",
    "        input_variables=[\"prompt\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([full_prompt])\n",
    "\n",
    "llm = OpenAI(temperature=0.9, max_tokens=128)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=chat_prompt_template, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tc = Tru().Chain(chain=chain, chain_id=\"hello\")\n",
    "# res, record = tc.call_with_record(\"Who is Piotr?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import PrettyPrinter\n",
    "from typing import Sequence\n",
    "\n",
    "from IPython.display import JSON\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.vectorstores import DocArrayHnswSearch\n",
    "import numpy as np\n",
    "\n",
    "from trulens_eval import Tru\n",
    "from trulens_eval import tru_feedback\n",
    "from trulens_eval.keys import *\n",
    "from trulens_eval.tru_db import Query\n",
    "from trulens_eval.tru_db import RecordQuery\n",
    "from trulens_eval.tru_feedback import Feedback\n",
    "from trulens_eval.tru_feedback import Huggingface\n",
    "from trulens_eval.utils.langchain import WithFilterDocuments\n",
    "\n",
    "# if using Pinecone vectordb:\n",
    "# from langchain.vectorstores import Pinecone\n",
    "# import pinecone\n",
    "\n",
    "pp = PrettyPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-3.5-turbo\"\n",
    "chain_id = \"TruBot\"\n",
    "\n",
    "# Pinecone configuration if using pinecone.\n",
    "# pinecone.init(\n",
    "#    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "#    environment=PINECONE_ENV  # next to api key in console\n",
    "#)\n",
    "#docsearch = Pinecone.from_existing_index(\n",
    "#    index_name=\"llmdemo\", embedding=embedding\n",
    "#)\n",
    "\n",
    "# LLM for completing prompts, and other tasks.\n",
    "llm = OpenAI(temperature=0, max_tokens=256)\n",
    "\n",
    "openai = tru_feedback.OpenAI()\n",
    "\n",
    "def new_conversation(\n",
    "    lang_prompt_fix: bool = False,\n",
    "    context_prompt_fix: bool = False,\n",
    "    context_filter_fix: bool = False,\n",
    "    feedbacks: Sequence[Feedback] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a chain for a new conversation (blank memory). Set flags to enable\n",
    "    adjustments to prompts or add context filtering.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert not(lang_prompt_fix and context_prompt_fix), \"Cannot use both prompt fixes at the same time.\"\n",
    "\n",
    "    # Embedding needed for Pinecone vector db.\n",
    "    embedding = OpenAIEmbeddings(model='text-embedding-ada-002')  # 1536 dims\n",
    "\n",
    "    # Conversation memory.\n",
    "    memory = ConversationSummaryBufferMemory(\n",
    "        max_token_limit=650,\n",
    "        llm=llm,\n",
    "        memory_key=\"chat_history\",\n",
    "        output_key='answer'\n",
    "    )\n",
    "\n",
    "    # Pinecone alternative. Requires precomputed 'hnswlib_truera' folder.\n",
    "    docsearch = DocArrayHnswSearch.from_params(\n",
    "        embedding=embedding,\n",
    "        work_dir='hnswlib_trubot',\n",
    "        n_dim=1536,\n",
    "        max_elements=1024\n",
    "    )\n",
    "    retriever = docsearch.as_retriever()\n",
    "\n",
    "    # Better contexts fix, filter contexts with relevance:\n",
    "    if context_filter_fix: \n",
    "        def filter_by_relevance(query, doc):\n",
    "            return openai.qs_relevance(\n",
    "                question=query, statement=doc.page_content\n",
    "            ) > 0.5\n",
    "\n",
    "        retriever = WithFilterDocuments.of_retriever(\n",
    "            retriever=retriever, filter_func=filter_by_relevance\n",
    "        )\n",
    "\n",
    "    # Conversational chain puts it all together.\n",
    "    chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        memory=memory,\n",
    "        get_chat_history=lambda a: a,\n",
    "        max_tokens_limit=4096\n",
    "    )\n",
    "\n",
    "    # Need to copy these otherwise various chains will feature templates that\n",
    "    # point to the same objects.\n",
    "    chain.combine_docs_chain.llm_chain.prompt = \\\n",
    "        chain.combine_docs_chain.llm_chain.prompt.copy()\n",
    "    chain.combine_docs_chain.document_prompt = \\\n",
    "        chain.combine_docs_chain.document_prompt.copy()\n",
    "\n",
    "    # Language mismatch fix:\n",
    "    if lang_prompt_fix:\n",
    "        chain.combine_docs_chain.llm_chain.prompt.template = \\\n",
    "            \"Use the following pieces of context to answer the question at the end \" \\\n",
    "            \"in the same language as the question. If you don't know the answer, \" \\\n",
    "            \"just say that you don't know, don't try to make up an answer.\\n\\n\" \\\n",
    "            \"{context}\\n\\n\" \\\n",
    "            \"Question: {question}\\n\" \\\n",
    "            \"Helpful Answer: \"\n",
    "\n",
    "    # Poor contexts fix using prompts:\n",
    "    elif context_prompt_fix:\n",
    "        chain.combine_docs_chain.llm_chain.prompt.template = \\\n",
    "            \"Use only the relevant contexts to answer the question at the end \" \\\n",
    "            \". Some pieces of context may not be relevant. If you don't know the answer, \" \\\n",
    "            \"just say that you don't know, don't try to make up an answer.\\n\\n\" \\\n",
    "            \"Contexts: \\n{context}\\n\\n\" \\\n",
    "            \"Question: {question}\\n\" \\\n",
    "            \"Helpful Answer: \"\n",
    "        chain.combine_docs_chain.document_prompt.template = \"\\tContext: {page_content}\"\n",
    "\n",
    "    # Trulens instrumentation.\n",
    "    tc = Tru().Chain(chain=chain, feedbacks=feedbacks, verbose=True)\n",
    "\n",
    "    return tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedbacks=[\n",
    "    f_toxic, \n",
    "    f_lang_match, \n",
    "    # f_relevance, \n",
    "    f_qs_relevance\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = new_conversation(\n",
    "    feedbacks=feedbacks,\n",
    "    #context_filter_fix=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, record = tc.call_with_record(\"Who is Shayak?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
