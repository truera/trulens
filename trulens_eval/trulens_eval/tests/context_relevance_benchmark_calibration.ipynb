{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q scikit-learn litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import groundedness feedback function\n",
    "from benchmark_frameworks.eval_as_recommendation import compute_ece\n",
    "from benchmark_frameworks.eval_as_recommendation import score_passages\n",
    "from test_cases import generate_ms_marco_context_relevance_benchmark\n",
    "\n",
    "from trulens_eval import Tru\n",
    "\n",
    "Tru().reset_database()\n",
    "\n",
    "benchmark_data = []\n",
    "for i in range(1, 6):\n",
    "    dataset_path = f\"./datasets/ms_marco/ms_marco_train_v2.1_{i}.json\"\n",
    "    benchmark_data.extend(\n",
    "        list(generate_ms_marco_context_relevance_benchmark(dataset_path))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = \"...\"\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"...\"\n",
    "os.environ[\"TOGETHERAI_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(benchmark_data)\n",
    "\n",
    "print(len(df.groupby(\"query_id\").count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"query_id\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define feedback functions for contexnt relevance to be evaluated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.feedback import LiteLLM\n",
    "from trulens_eval.feedback import OpenAI\n",
    "\n",
    "temperatures = [0, 0.3, 0.7, 1]\n",
    "# GPT 3.5\n",
    "turbo = OpenAI(model_engine=\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "def wrapped_relevance_turbo_t(input, output, temperature):\n",
    "    return turbo.context_relevance_confidence_verb_2s_top1(\n",
    "        input, output, temperature\n",
    "    )\n",
    "\n",
    "\n",
    "# # GPT 4 turbo\n",
    "gpt4 = OpenAI(model_engine=\"gpt-4-1106-preview\")\n",
    "\n",
    "\n",
    "def wrapped_relevance_gpt4_t(input, output, temperature):\n",
    "    return gpt4.context_relevance_confidence_verb_2s_top1(\n",
    "        input, output, temperature\n",
    "    )\n",
    "\n",
    "\n",
    "claude_1 = LiteLLM(model_engine=\"claude-instant-1\")\n",
    "\n",
    "\n",
    "def wrapped_relevance_claude1_t(input, output, temperature):\n",
    "    claude_1.context_relevance_confidence_verb_2s_top1(\n",
    "        input, output, temperature\n",
    "    )\n",
    "\n",
    "\n",
    "claude_2 = LiteLLM(model_engine=\"claude-2\")\n",
    "\n",
    "\n",
    "def wrapped_relevance_claude2_t(input, output, temperature):\n",
    "    claude_2.context_relevance_confidence_verb_2s_top1(\n",
    "        input, output, temperature\n",
    "    )\n",
    "\n",
    "\n",
    "feedback_functions = {\n",
    "    \"GPT-3.5-Turbo\": wrapped_relevance_turbo_t,\n",
    "    \"GPT-4-Turbo\": wrapped_relevance_gpt4_t,\n",
    "    # 'Claude-1': wrapped_relevance_claude1_t,\n",
    "    # 'Claude-2': wrapped_relevance_claude2_t,\n",
    "}\n",
    "\n",
    "backoffs_by_functions = {\n",
    "    \"GPT-3.5-Turbo\": 0,\n",
    "    \"GPT-4-Turbo\": 0.5,\n",
    "    # 'Claude-1': 1.5,\n",
    "    # 'Claude-2': 1.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp in temperatures:\n",
    "    # Running the benchmark\n",
    "    results = []\n",
    "\n",
    "    intermediate_results = []\n",
    "    for name, func in feedback_functions.items():\n",
    "        try:\n",
    "            scores, true_relevance = score_passages(\n",
    "                df,\n",
    "                name,\n",
    "                func,\n",
    "                backoffs_by_functions[name]\n",
    "                if name in backoffs_by_functions\n",
    "                else 0.5,\n",
    "                n=1,\n",
    "                temperature=temp,\n",
    "            )\n",
    "            ece_value = compute_ece(scores, true_relevance)\n",
    "\n",
    "            results.append(\n",
    "                (\n",
    "                    name,\n",
    "                    ece_value,\n",
    "                )\n",
    "            )\n",
    "            print(f\"Finished running feedback function name {name}\")\n",
    "\n",
    "            print(\"Saving results...\")\n",
    "            tmp_results_df = pd.DataFrame(\n",
    "                results, columns=[f\"Model-t-{temp}\", \"ECE\"]\n",
    "            )\n",
    "\n",
    "            tmp_results_df.to_csv(f\"results_verbalized_ece_t_{temp}.csv\")\n",
    "            print(tmp_results_df)\n",
    "            intermediate_results.append(tmp_results_df)\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Failed to run benchmark for feedback function name {name} due to {e}\"\n",
    "            )\n",
    "    # Convert results to DataFrame for display\n",
    "    results_df = pd.DataFrame(\n",
    "        results,\n",
    "        columns=[\n",
    "            f\"Model-t-{temp}\",\n",
    "            \"ECE\",\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"results_verbalized_ece_temp_scaling.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_1 = pd.read_csv(\"results_temp_scaling_gpt-3.5.csv\")\n",
    "results_df_2 = pd.read_csv(\"results_temp_scaling_gpt-4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make sure results_df is defined and contains the necessary columns\n",
    "# Also, ensure that K is defined\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Graph for nDCG, Recall@K, and Precision@K\n",
    "plt.subplot(2, 1, 1)  # First subplot\n",
    "ax1 = results_df.plot(\n",
    "    x=\"Model\",\n",
    "    y=[\"nDCG\", f\"Recall@{K}\", \"Precision@1\"],\n",
    "    kind=\"bar\",\n",
    "    ax=plt.gca(),\n",
    ")\n",
    "plt.title(\"Feedback Function Performance (Higher is Better)\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "# Graph for ECE\n",
    "plt.subplot(2, 1, 2)  # Second subplot\n",
    "ax2 = results_df.plot(\n",
    "    x=\"Model\", y=[\"ECE\"], kind=\"bar\", ax=plt.gca(), color=\"orange\"\n",
    ")\n",
    "plt.title(\"Feedback Function Calibration (Lower is Better)\")\n",
    "plt.ylabel(\"ECE\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
