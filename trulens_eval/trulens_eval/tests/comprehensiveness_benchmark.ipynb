{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ““ Comprehensiveness Evaluations\n",
    "\n",
    "In many ways, feedbacks can be thought of as LLM apps themselves. Given text,\n",
    "they return some result. Thinking in this way, we can use TruLens to evaluate\n",
    "and track our feedback quality. We can even do this for different models (e.g.\n",
    "gpt-3.5 and gpt-4) or prompting schemes (such as chain-of-thought reasoning).\n",
    "\n",
    "This notebook follows an evaluation of a set of test cases generated from human\n",
    "annotated datasets. In particular, we generate test cases from\n",
    "[MeetingBank](https://arxiv.org/abs/2305.17529) to evaluate our\n",
    "comprehensiveness feedback function.\n",
    "\n",
    "MeetingBank is one of the datasets dedicated to automated evaluations on\n",
    "summarization tasks, which are closely related to the comprehensiveness\n",
    "evaluation in RAG with the retrieved context (i.e. the source) and response\n",
    "(i.e. the summary). It contains human annotation of numerical score (**1** to\n",
    "**5**). \n",
    "\n",
    "For evaluating comprehensiveness feedback functions, we compute the annotated\n",
    "\"informativeness\" scores, a measure of how well  the summaries capture all the\n",
    "main points of the meeting segment. A good summary should contain all and only\n",
    "the important information of the source., and normalized to **0** to **1** score\n",
    "as our **expected_score** and to match the output of feedback functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_cases import generate_meetingbank_comprehensiveness_benchmark\n",
    "\n",
    "test_cases_gen = generate_meetingbank_comprehensiveness_benchmark(\n",
    "    human_annotation_file_path=\"./datasets/meetingbank/human_scoring.json\",\n",
    "    meetingbank_file_path=\"/home/daniel/MeetingBank.json\"\n",
    ")\n",
    "length = sum(1 for _ in test_cases_gen)\n",
    "test_cases_gen = generate_meetingbank_comprehensiveness_benchmark(\n",
    "    human_annotation_file_path=\"./datasets/meetingbank/human_scoring.json\",\n",
    "    meetingbank_file_path=\"/home/daniel/MeetingBank.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehensiveness_golden_set = []\n",
    "for i in range(length):\n",
    "    comprehensiveness_golden_set.append(next(test_cases_gen))\n",
    "\n",
    "assert(len(comprehensiveness_golden_set) == length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehensiveness_golden_set[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\" # for groundtruth feedback function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import feedback\n",
    "from trulens_eval.feedback import GroundTruthAgreement\n",
    "from trulens_eval import Feedback, Select, Tru\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "tru = Tru()\n",
    "\n",
    "provider = feedback.OpenAI(model_engine=\"gpt-4-turbo-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comprehensiveness of summary with transcript as reference\n",
    "f_comprehensiveness_openai = (\n",
    "    Feedback(provider.comprehensiveness_with_cot_reasons)\n",
    "    .on_input_output()\n",
    "    .aggregate(np.mean)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Feedback object using the numeric_difference method of the\n",
    "# ground_truth object.\n",
    "ground_truth = GroundTruthAgreement(comprehensiveness_golden_set)\n",
    "\n",
    "# Call the numeric_difference method with app and record and aggregate to get\n",
    "# the mean absolute error.\n",
    "f_mae = Feedback(\n",
    "    ground_truth.mae,\n",
    "    name=\"Mean Absolute Error\"\n",
    ").on(Select.Record.calls[0].args.args[0])\\\n",
    " .on(Select.Record.calls[0].args.args[1])\\\n",
    "  .on_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_frameworks.eval_as_recommendation \\\n",
    "    import compute_ndcg, compute_ece, recall_at_k, precision_at_k\n",
    "\n",
    "scores = []\n",
    "true_scores = [] # human prefrences / scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "for i in range(len(comprehensiveness_golden_set)):\n",
    "    source = comprehensiveness_golden_set[i][\"query\"]\n",
    "    summary = comprehensiveness_golden_set[i][\"response\"]\n",
    "    expected_score = comprehensiveness_golden_set[i][\"expected_score\"]\n",
    "    feedback_score = f_comprehensiveness_openai(source, summary)[0]\n",
    "\n",
    "    scores.append(feedback_score)\n",
    "    true_scores.append(expected_score)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    if i % 200 == 0:\n",
    "        df_results = pd.DataFrame({'scores': scores, 'true_scores': true_scores})\n",
    "\n",
    "        # Save the DataFrame to a CSV file\n",
    "        df_results.to_csv(\n",
    "            './results/results_comprehensiveness_benchmark.csv',\n",
    "            index=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECE might not make much sense here as we have groundtruth in numeric values.\n",
    "ece = compute_ece([scores], [true_scores], n_bins=10) \n",
    "\n",
    "mae = sum(\n",
    "    abs(score - true_score) \\\n",
    "    for score, true_score in zip(scores, true_scores)\n",
    ") / len(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ECE: {ece}; MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(true_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization to help investigation in LLM alignments with (mean) absolute errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "scores = []\n",
    "true_scores = []\n",
    "\n",
    "# Open the CSV file and read its contents\n",
    "with open(\"./results/results_comprehensiveness_benchmark.csv\", 'r') as csvfile:\n",
    "    # Create a CSV reader object\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    \n",
    "    # Skip the header row\n",
    "    next(csvreader)\n",
    "    \n",
    "    # Iterate over each row in the CSV\n",
    "    for row in csvreader:\n",
    "        # Append the scores and true_scores to their respective lists\n",
    "        scores.append(float(row[0]))\n",
    "        true_scores.append(float(row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming scores and true_scores are flat lists of predicted probabilities and\n",
    "# their corresponding ground truth relevances\n",
    "\n",
    "# Calculate the absolute errors\n",
    "errors = np.abs(np.array(scores) - np.array(true_scores))\n",
    "\n",
    "# Scatter plot of scores vs true_scores\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# First subplot: scatter plot with color-coded errors\n",
    "plt.subplot(1, 2, 1)\n",
    "scatter = plt.scatter(scores, true_scores, c=errors, cmap='viridis')\n",
    "plt.colorbar(scatter, label='Absolute Error')\n",
    "plt.plot([0, 1], [0, 1], 'r--', label='Perfect Alignment')  # Line of perfect alignment\n",
    "plt.xlabel('Model Scores')\n",
    "plt.ylabel('True Scores')\n",
    "plt.title('Model Scores vs. True Scores')\n",
    "plt.legend()\n",
    "\n",
    "# Second subplot: Error across score ranges\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(scores, errors, color='blue')\n",
    "plt.xlabel('Model Scores')\n",
    "plt.ylabel('Absolute Error')\n",
    "plt.title('Error Across Score Ranges')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
