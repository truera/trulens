{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ““ Groundedness Evaluations for Abstention Handling\n",
    "\n",
    "In many ways, feedbacks can be thought of as LLM apps themselves. Given text, they return some result. Thinking in this way, we can use TruLens to evaluate and track our feedback quality. We can even do this for different models or prompting schemes (such as chain-of-thought reasoning).\n",
    "\n",
    "This notebook follows an evaluation of a set of test cases generated from human annotated datasets. In particular, we generate test cases from [SummEval](https://arxiv.org/abs/2007.12626).\n",
    "\n",
    "SummEval is one of the datasets dedicated to automated evaluations on summarization tasks, which are closely related to the groundedness evaluation in RAG with the retrieved context (i.e. the source) and response (i.e. the summary). It contains human annotation of numerical score (**1** to **5**) comprised of scoring from 3 human expert annotators and 5 croweded-sourced annotators. There are 16 models being used for generation in total for 100 paragraphs in the test set, so there are a total of 16,000 machine-generated summaries. Each paragraph also has several human-written summaries for comparative analysis. \n",
    "\n",
    "For evaluating groundedness feedback functions, we compute the annotated \"consistency\" scores, a measure of whether the summarized response is factually consisntent with the source texts and hence can be used as a proxy to evaluate groundedness in our RAG triad, and normalized to **0** to **1** score as our **expected_score** and to match the output of feedback functions.\n",
    "\n",
    "## Abstention Background\n",
    "\n",
    "In this particular set of evaluations, we are focused on the handling of abstentions. Uncertainty-based abstention in LLMs has been shown to improve safety and reduce hallucination ([Tomani](https://arxiv.org/abs/2404.10960)). For groundedness evaluations, we want to ensure these are handled in a manner that is consistent with human preferences; in other words, calibrated.\n",
    "\n",
    "Abstentions can be broken down into two distinct groups, distinguished by whether the question is answerable given the context. In other words, whether the abstention is **justified**. We take an opinionated stance that abstentions for unanswerable questions are justified and therefore, **grounded**. Alternatively, abstentions for questions answerable by the context are therefore not grounded.\n",
    "\n",
    "## Experimental Setup\n",
    "\n",
    "For this set of experiments, we take the same test cases used for groundedness evaluations with a few key changes:\n",
    "1. We randomly sample approximately 50% of the test cases and replace the response with an abstention. We'll refer to this as the abstention set. The rest will be head as control.\n",
    "2. In the abstention set, we will take a random sample of approximately 50% of cases and truncate to remove all but the first sentence in the query, removing a majority of the context. This splits the absention set into **answerable** and **unanswerable** abstentions.\n",
    "\n",
    "From here, we have two tests sets to test against.\n",
    "1. The expected score for all abstentions is 1.\n",
    "2. The expected score for answerable abstentions will be set to 0, and then the expected score for unanswerable abstentions will be set to 1.\n",
    "\n",
    "We will then compute the MAE of our groundedness evaluator against the expected score for each test set. Results will be displayed for the following subgroups:\n",
    "- Abstentions v. Control\n",
    "- Answerable Abstentions v. Unanswerable Abstentions\n",
    "\n",
    "We will also show results for the test cases with high and low human-annotated consistency to ensure that this treatment is consistent across expected scores.\n",
    "\n",
    "\n",
    "## Improving Groundedness\n",
    "\n",
    "To improve the groundedness feedback function against these tests, we consider the following changes:\n",
    "- Abstentions are excluded from statements to evaluate; empty strings are considered grounded.\n",
    "- The answerability in abstention cases is assessed; unanswerable abstentions score high, answerable abstentions score low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/snowday/lib/python3.11/site-packages/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n",
      "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of `Tru` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "# Import groundedness feedback function\n",
    "from trulens_eval.feedback import GroundTruthAgreement\n",
    "from trulens_eval import TruBasicApp, Feedback, Tru, Select\n",
    "from test_cases import generate_summeval_groundedness_golden_set\n",
    "import copy\n",
    "\n",
    "tru = Tru()\n",
    "tru.reset_database()\n",
    "\n",
    "# generator for groundedness golden set\n",
    "test_cases_gen = generate_summeval_groundedness_golden_set(\"./datasets/summeval/summeval_test_100.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the number of test cases we want to run the smoke test on\n",
    "groundedness_golden_set_abstentions = []\n",
    "for i in range(500):\n",
    "    groundedness_golden_set_abstentions.append(next(test_cases_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Specify the number of test cases we want to run the smoke test on\n",
    "groundedness_golden_set_abstentions = []\n",
    "for i in range(500):\n",
    "    groundedness_golden_set_abstentions.append(next(test_cases_gen))\n",
    "\n",
    "# Randomly sample 50% of the test cases and replace the response with a random abstention\n",
    "# Split into answerable and unanswerable set\n",
    "abstention_set = []\n",
    "control_set = []\n",
    "for item in groundedness_golden_set_abstentions:\n",
    "    if random.random() < 0.5:\n",
    "        abstention_set.append(item)\n",
    "    else:\n",
    "        control_set.append(item)\n",
    "\n",
    "for item in abstention_set:\n",
    "    item['response'] = random.choice([\n",
    "        \"I don't know\",\n",
    "        \"I don't have the information available to answer this question\",\n",
    "        \"I'm unsure of the answer given the provided context\",\n",
    "        \"I'm not certain\",\n",
    "        \"I can't say for sure\",\n",
    "        \"I'm not familiar with that\",\n",
    "        \"I'm not knowledgeable about this topic\",\n",
    "        \"I'm not the right person to ask\",\n",
    "        \"I'm unable to provide a definitive answer\",\n",
    "        \"I'm sorry, I can't help with that\"\n",
    "    ])\n",
    "    item['group'] = 'Abstention'\n",
    "\n",
    "# Set the group for the remaining cases as 'Control'\n",
    "for item in groundedness_golden_set_abstentions:\n",
    "    if 'group' not in item:\n",
    "        item['group'] = 'Control'\n",
    "\n",
    "# Split into answerable and unanswerable set\n",
    "unanswerable_abstentions = []\n",
    "answerable_abstentions = []\n",
    "\n",
    "abstention_set_copy = copy.deepcopy(abstention_set)\n",
    "for item in abstention_set_copy:\n",
    "    item['expected_score'] = 1.0\n",
    "    if random.random() < 0.5:\n",
    "        item['group'] = 'Unanswerable Abstention'\n",
    "        split_result = item['query'].split('.', 1)\n",
    "        if len(split_result) > 1:\n",
    "            item['query'] = split_result[0]\n",
    "            unanswerable_abstentions.append(item)\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        item['group'] = 'Answerable Abstention'\n",
    "        answerable_abstentions.append(item)\n",
    "    \n",
    "# create a set that includes all abstentions and the control set\n",
    "groundedness_golden_set_abstensions_score_high = control_set + answerable_abstentions + unanswerable_abstentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "answerable_abstentions_ca = copy.deepcopy(answerable_abstentions)\n",
    "for item in answerable_abstentions_ca :\n",
    "    item['expected_score'] = 0.0\n",
    "\n",
    "groundedness_golden_set_abstensions_consider_answerability = control_set + answerable_abstentions_ca + unanswerable_abstentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundedness_golden_set_abstensions_consider_answerability_copy = copy.deepcopy(groundedness_golden_set_abstensions_consider_answerability)\n",
    "\n",
    "test_set = [{'query': item['query'], 'response': item['response'], 'group': item['group']} for item in groundedness_golden_set_abstensions_consider_answerability_copy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Consider answerability of abstentions\n",
      "group\n",
      "Control                    256\n",
      "Unanswerable Abstention    129\n",
      "Answerable Abstention      115\n",
      "Name: count, dtype: int64\n",
      "                         expected_score  human_score\n",
      "group                                               \n",
      "Control                        0.924297     4.696615\n",
      "Answerable Abstention          0.000000     4.669565\n",
      "Unanswerable Abstention        1.000000     4.715762\n",
      "\n",
      " Reward all abstentions equally\n",
      "group\n",
      "Control                    256\n",
      "Unanswerable Abstention    129\n",
      "Answerable Abstention      115\n",
      "Name: count, dtype: int64\n",
      "                         expected_score  human_score\n",
      "group                                               \n",
      "Control                        0.924297     4.696615\n",
      "Answerable Abstention          1.000000     4.669565\n",
      "Unanswerable Abstention        1.000000     4.715762\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Consider answerability of abstentions\n",
    "print(\"\\n Consider answerability of abstentions\")\n",
    "\n",
    "## Count the data by group (Control, Answerable Abstention, Unanswerable Abstention)\n",
    "ca_df = pd.DataFrame(groundedness_golden_set_abstensions_consider_answerability)\n",
    "ca_group_counts = ca_df['group'].value_counts()\n",
    "print(ca_group_counts)\n",
    "\n",
    "## Calculate average values for expected_score and human_score by group\n",
    "ca_group_avg = ca_df.groupby('group').agg({'expected_score': 'mean', 'human_score': 'mean'})\n",
    "ca_group_avg = ca_group_avg.reindex(['Control', 'Answerable Abstention', 'Unanswerable Abstention'])\n",
    "print(ca_group_avg)\n",
    "\n",
    "# Reward all abstentions equally\n",
    "print(\"\\n Reward all abstentions equally\")\n",
    "\n",
    "sh_df = pd.DataFrame(groundedness_golden_set_abstensions_score_high)\n",
    "sh_group_counts = sh_df['group'].value_counts()\n",
    "print(sh_group_counts)\n",
    "\n",
    "## Calculate average values for expected_score and human_score by group\n",
    "sh_group_avg = sh_df.groupby('group').agg({'expected_score': 'mean', 'human_score': 'mean'})\n",
    "sh_group_avg = sh_group_avg.reindex(['Control', 'Answerable Abstention', 'Unanswerable Abstention'])\n",
    "print(sh_group_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking GPT4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In Groundedness OpenAI GPT-4o, input source will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Groundedness OpenAI GPT-4o, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "âœ… In Mean Absolute Error (consider answerability), input prompt will be set to __record__.app._call.args.args[0] .\n",
      "âœ… In Mean Absolute Error (consider answerability), input response will be set to __record__.app._call.args.args[1] .\n",
      "âœ… In Mean Absolute Error (consider answerability), input score will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "âœ… In Mean Absolute Error (all abstensions score high), input prompt will be set to __record__.app._call.args.args[0] .\n",
      "âœ… In Mean Absolute Error (all abstensions score high), input response will be set to __record__.app._call.args.args[1] .\n",
      "âœ… In Mean Absolute Error (all abstensions score high), input score will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval.feedback.provider import OpenAI\n",
    "\n",
    "openai_provider = OpenAI(model_engine=\"gpt-4o\")\n",
    "f_groundedness_openai_gpt4o = Feedback(openai_provider.groundedness_measure_with_cot_reasons, name = \"Groundedness OpenAI GPT-4o\").on_input_output()\n",
    "def wrapped_groundedness_openai_gpt4o(input: str, output: float) -> float:\n",
    "    return f_groundedness_openai_gpt4o(input, output)[0]\n",
    "\n",
    "# Create a Feedback object using the numeric_difference method of the ground_truth object\n",
    "ground_truth_consider_answerability = GroundTruthAgreement(groundedness_golden_set_abstensions_consider_answerability)\n",
    "# Call the numeric_difference method with app and record and aggregate to get the mean absolute error\n",
    "f_mae_consider_answerability = Feedback(ground_truth_consider_answerability.mae, name = \"Mean Absolute Error (consider answerability)\", higher_is_better=False).on(Select.Record.app._call.args.args[0]).on(Select.Record.app._call.args.args[1]).on(Select.RecordOutput)\n",
    "\n",
    "# Create a Feedback object using the numeric_difference method of the ground_truth object\n",
    "ground_truth_abstensions_score_high = GroundTruthAgreement(groundedness_golden_set_abstensions_score_high)\n",
    "# Call the numeric_difference method with app and record and aggregate to get the mean absolute error\n",
    "f_mae_abstensions_score_high = Feedback(ground_truth_abstensions_score_high.mae, name = \"Mean Absolute Error (all abstensions score high)\", higher_is_better=False).on(Select.Record.app._call.args.args[0]).on(Select.Record.app._call.args.args[1]).on(Select.RecordOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_wrapped_groundedness_gpt4o = TruBasicApp(wrapped_groundedness_openai_gpt4o, app_id=\"groundedness GPT-4o-instruct\",\n",
    "                                             feedbacks=[f_mae_consider_answerability, f_mae_abstensions_score_high])\n",
    "for i in range(len(groundedness_golden_set_abstensions_score_high)):\n",
    "    source = groundedness_golden_set_abstensions_consider_answerability[i][\"query\"]\n",
    "    response = groundedness_golden_set_abstensions_consider_answerability[i][\"response\"]\n",
    "    group = groundedness_golden_set_abstensions_consider_answerability[i][\"group\"]\n",
    "   \n",
    "    with tru_wrapped_groundedness_gpt4o as recording:\n",
    "        try:\n",
    "            recording.record_metadata = dict(group = group)\n",
    "            tru_wrapped_groundedness_gpt4o.app(source, response)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Mean Absolute Error (all abstensions score high)</th>\n",
       "      <th>Mean Absolute Error (consider answerability)</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_id</th>\n",
       "      <th>group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">groundedness GPT-4o-instruct</th>\n",
       "      <th>Unanswerable Abstention</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.294574</td>\n",
       "      <td>0.001298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Answerable Abstention</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.003262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Control</th>\n",
       "      <td>0.07612</td>\n",
       "      <td>0.07612</td>\n",
       "      <td>2.535156</td>\n",
       "      <td>0.027785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Mean Absolute Error (all abstensions score high)  \\\n",
       "app_id                       group                                                                       \n",
       "groundedness GPT-4o-instruct Unanswerable Abstention                                           1.00000   \n",
       "                             Answerable Abstention                                             1.00000   \n",
       "                             Control                                                           0.07612   \n",
       "\n",
       "                                                      Mean Absolute Error (consider answerability)  \\\n",
       "app_id                       group                                                                   \n",
       "groundedness GPT-4o-instruct Unanswerable Abstention                                       1.00000   \n",
       "                             Answerable Abstention                                         0.00000   \n",
       "                             Control                                                       0.07612   \n",
       "\n",
       "                                                       latency  total_cost  \n",
       "app_id                       group                                          \n",
       "groundedness GPT-4o-instruct Unanswerable Abstention  0.294574    0.001298  \n",
       "                             Answerable Abstention    0.260870    0.003262  \n",
       "                             Control                  2.535156    0.027785  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.get_leaderboard(group_by_metadata_key = \"group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n",
      "Config file already exists. Skipping writing process.\n",
      "Credentials file already exists. Skipping writing process.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72df4aaa30994d049725657e0a133a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valuâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard started at http://192.168.4.206:56106 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.run_dashboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
