{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Response Relevance Feedback Requirements\n",
    "1. Relevance requires adherence to the entire prompt.\n",
    "2. Responses that don't provide a definitive answer can still be relevant\n",
    "3. Admitting lack of knowledge and refusals are still relevant.\n",
    "4. Feedback mechanism should differentiate between seeming and actual relevance.\n",
    "5. Relevant but inconclusive statements should get increasingly high scores as they are more helpful for answering the query.\n",
    "\n",
    "Below are examples of usage. Find the [results](#test-results) of the tests tabulated at the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevance feedback function\n",
    "from trulens_eval.feedback import GroundTruthAgreement, OpenAI as fOpenAI\n",
    "from trulens_eval import TruBasicApp, Feedback, Tru, Select\n",
    "from test_cases import answer_relevance_golden_set\n",
    "\n",
    "\n",
    "import openai\n",
    "\n",
    "Tru().reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "turbo = fOpenAI(model_engine=\"gpt-3.5-turbo\")\n",
    "# Define your feedback functions\n",
    "def wrapped_relevance_turbo(input, output):\n",
    "    return turbo.relevance(input, output)\n",
    "\n",
    "# Define your feedback functions\n",
    "def wrapped_relevance_with_cot_turbo(input, output):\n",
    "    return turbo.relevance_with_cot_reasons(input, output)\n",
    "\n",
    "gpt4 = fOpenAI(model_engine=\"gpt-4\")\n",
    "# Define your feedback functions\n",
    "def wrapped_relevance_gpt4(input, output):\n",
    "    return gpt4.relevance(input, output)\n",
    "\n",
    "# Define your feedback functions\n",
    "def wrapped_relevance_with_cot_gpt4(input, output):\n",
    "    return gpt4.relevance_with_cot_reasons(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In Relevance Smoke Test, input prompt will be set to *.__record__.calls[0].args.args[0] .\n",
      "✅ In Relevance Smoke Test, input response will be set to *.__record__.calls[0].args.args[1] .\n",
      "✅ In Relevance Smoke Test, input score will be set to *.__record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "# Create a Feedback object using the numeric_difference method of the ground_truth object\n",
    "ground_truth = GroundTruthAgreement(answer_relevance_golden_set)\n",
    "# Call the numeric_difference method with app and record\n",
    "f_groundtruth = Feedback(ground_truth.numeric_difference, name = \"Relevance Smoke Test\").on(Select.Record.calls[0].args.args[0]).on(Select.Record.calls[0].args.args[1]).on_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_wrapped_relevance_turbo = TruBasicApp(wrapped_relevance_turbo, app_id = \"answer relevance gpt-3.5-turbo\", feedbacks=[f_groundtruth])\n",
    "tru_wrapped_relevance_with_cot_turbo = TruBasicApp(wrapped_relevance_with_cot_turbo, app_id = \"answer relevance with cot reasoning gpt-3.5-turbo\", feedbacks=[f_groundtruth])\n",
    "tru_wrapped_relevance_gpt4 = TruBasicApp(wrapped_relevance_gpt4, app_id = \"answer relevance gpt-4\", feedbacks=[f_groundtruth])\n",
    "tru_wrapped_relevance_with_cot_gpt4 = TruBasicApp(wrapped_relevance_with_cot_gpt4, app_id = \"answer relevance with cot reasoning gpt-4\", feedbacks=[f_groundtruth])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task queue full. Finishing existing tasks.\n",
      "Task queue full. Finishing existing tasks.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(answer_relevance_golden_set)):\n",
    "    prompt = answer_relevance_golden_set[i][\"query\"]\n",
    "    response = answer_relevance_golden_set[i][\"response\"]\n",
    "    tru_wrapped_relevance_turbo.call_with_record(prompt, response)\n",
    "    tru_wrapped_relevance_with_cot_turbo.call_with_record(prompt, response)\n",
    "    tru_wrapped_relevance_gpt4.call_with_record(prompt, response)\n",
    "    tru_wrapped_relevance_with_cot_gpt4.call_with_record(prompt, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Relevance Smoke Test</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>answer relevance gpt-3.5-turbo</th>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer relevance gpt-4</th>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.015277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer relevance with cot reasoning gpt-3.5-turbo</th>\n",
       "      <td>0.770588</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer relevance with cot reasoning gpt-4</th>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.019181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Relevance Smoke Test  \\\n",
       "app_id                                                                    \n",
       "answer relevance gpt-3.5-turbo                                 0.794118   \n",
       "answer relevance gpt-4                                         0.764706   \n",
       "answer relevance with cot reasoning gpt-3.5-turbo              0.770588   \n",
       "answer relevance with cot reasoning gpt-4                      0.764706   \n",
       "\n",
       "                                                    latency  total_cost  \n",
       "app_id                                                                   \n",
       "answer relevance gpt-3.5-turbo                     0.058824    0.000763  \n",
       "answer relevance gpt-4                             0.058824    0.015277  \n",
       "answer relevance with cot reasoning gpt-3.5-turbo  0.058824    0.000912  \n",
       "answer relevance with cot reasoning gpt-4          0.058824    0.019181  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tru().get_records_and_feedback(app_ids=[])[0].groupby('app_id')[['Relevance Smoke Test', 'latency', 'total_cost']].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d153714b979d5e6d08dd8ec90712dd93bff2c9b6c1f0c118169738af3430cd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
