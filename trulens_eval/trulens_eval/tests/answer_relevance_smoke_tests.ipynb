{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ““ Answer Relevance Feedback Evaluation\n",
    "In many ways, feedbacks can be thought of as LLM apps themselves. Given text, they return some result. Thinking in this way, we can use TruLens to evaluate and track our feedback quality. We can even do this for different models (e.g. gpt-3.5 and gpt-4) or prompting schemes (such as chain-of-thought reasoning).\n",
    "\n",
    "This notebook follows an evaluation of a set of test cases. You are encouraged to run this on your own and even expand the test cases to evaluate performance on test cases applicable to your scenario or domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n",
      "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of `Tru` to prevent this.\n",
      "Deleted 9 rows.\n"
     ]
    }
   ],
   "source": [
    "# Import relevance feedback function\n",
    "from trulens_eval.feedback import GroundTruthAgreement, OpenAI, LiteLLM\n",
    "from trulens_eval import TruBasicApp, Feedback, Tru, Select\n",
    "from test_cases import answer_relevance_golden_set\n",
    "\n",
    "Tru().reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
    "os.environ[\"COHERE_API_KEY\"] = \"...\"\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = \"...\"\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"...\"\n",
    "os.environ[\"TOGETHERAI_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT 3.5\n",
    "turbo = OpenAI(model_engine=\"gpt-3.5-turbo\")\n",
    "\n",
    "def wrapped_relevance_turbo(input, output):\n",
    "    return turbo.relevance(input, output)\n",
    "\n",
    "# GPT 4\n",
    "gpt4 = OpenAI(model_engine=\"gpt-4\")\n",
    "\n",
    "def wrapped_relevance_gpt4(input, output):\n",
    "    return gpt4.relevance(input, output)\n",
    "\n",
    "# Cohere\n",
    "command_nightly = LiteLLM(model_engine=\"cohere/command-nightly\")\n",
    "def wrapped_relevance_command_nightly(input, output):\n",
    "    return command_nightly.relevance(input, output)\n",
    "\n",
    "# Anthropic\n",
    "claude_1 = LiteLLM(model_engine=\"claude-instant-1\")\n",
    "def wrapped_relevance_claude1(input, output):\n",
    "    return claude_1.relevance(input, output)\n",
    "\n",
    "claude_2 = LiteLLM(model_engine=\"claude-2\")\n",
    "def wrapped_relevance_claude2(input, output):\n",
    "    return claude_2.relevance(input, output)\n",
    "\n",
    "# Meta\n",
    "llama_2_13b = LiteLLM(model_engine=\"together_ai/togethercomputer/Llama-2-7B-32K-Instruct\")\n",
    "def wrapped_relevance_llama2(input, output):\n",
    "    return llama_2_13b.relevance(input, output)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll set up our golden set as a set of prompts, responses and expected scores stored in `test_cases.py`. Then, our numeric_difference method will look up the expected score for each prompt/response pair by **exact match**. After looking up the expected score, we will then take the L1 difference between the actual score and expected score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In Mean Absolute Error, input prompt will be set to __record__.calls[0].args.args[0] .\n",
      "âœ… In Mean Absolute Error, input response will be set to __record__.calls[0].args.args[1] .\n",
      "âœ… In Mean Absolute Error, input score will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "# Create a Feedback object using the numeric_difference method of the ground_truth object\n",
    "ground_truth = GroundTruthAgreement(answer_relevance_golden_set)\n",
    "# Call the numeric_difference method with app and record and aggregate to get the mean absolute error\n",
    "f_mae = Feedback(ground_truth.mae, name = \"Mean Absolute Error\").on(Select.Record.calls[0].args.args[0]).on(Select.Record.calls[0].args.args[1]).on_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… added app answer relevance gpt-3.5-turbo\n",
      "âœ… added feedback definition feedback_definition_hash_d2c7c6ef797c4fdbdcb802c0c74d451a\n",
      "âœ… added app answer relevance with cot reasoning gpt-3.5-turbo\n",
      "âœ… added feedback definition feedback_definition_hash_d2c7c6ef797c4fdbdcb802c0c74d451a\n",
      "âœ… added app answer relevance gpt-4\n",
      "âœ… added feedback definition feedback_definition_hash_d2c7c6ef797c4fdbdcb802c0c74d451a\n",
      "âœ… added app answer relevance with cot reasoning gpt-4\n",
      "âœ… added feedback definition feedback_definition_hash_d2c7c6ef797c4fdbdcb802c0c74d451a\n",
      "âœ… added app answer relevance Command-Nightly\n",
      "âœ… added feedback definition feedback_definition_hash_d2c7c6ef797c4fdbdcb802c0c74d451a\n",
      "âœ… added app answer relevance Claude 1\n",
      "âœ… added feedback definition feedback_definition_hash_d2c7c6ef797c4fdbdcb802c0c74d451a\n",
      "âœ… added app answer relevance Claude 2\n",
      "âœ… added feedback definition feedback_definition_hash_d2c7c6ef797c4fdbdcb802c0c74d451a\n",
      "âœ… added app answer relevance Llama-2-13b\n",
      "âœ… added feedback definition feedback_definition_hash_d2c7c6ef797c4fdbdcb802c0c74d451a\n"
     ]
    }
   ],
   "source": [
    "tru_wrapped_relevance_turbo = TruBasicApp(wrapped_relevance_turbo, app_id = \"answer relevance gpt-3.5-turbo\", feedbacks=[f_mae])\n",
    "\n",
    "tru_wrapped_relevance_gpt4 = TruBasicApp(wrapped_relevance_gpt4, app_id = \"answer relevance gpt-4\", feedbacks=[f_mae])\n",
    "\n",
    "tru_wrapped_relevance_commandnightly = TruBasicApp(wrapped_relevance_command_nightly, app_id = \"answer relevance Command-Nightly\", feedbacks=[f_mae])\n",
    "\n",
    "tru_wrapped_relevance_claude1 = TruBasicApp(wrapped_relevance_claude1, app_id = \"answer relevance Claude 1\", feedbacks=[f_mae])\n",
    "\n",
    "tru_wrapped_relevance_claude2 = TruBasicApp(wrapped_relevance_claude2, app_id = \"answer relevance Claude 2\", feedbacks=[f_mae])\n",
    "\n",
    "tru_wrapped_relevance_llama2 = TruBasicApp(wrapped_relevance_llama2, app_id = \"answer relevance Llama-2-13b\", feedbacks=[f_mae])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(answer_relevance_golden_set)):\n",
    "    prompt = answer_relevance_golden_set[i][\"query\"]\n",
    "    response = answer_relevance_golden_set[i][\"response\"]\n",
    "    \n",
    "    with tru_wrapped_relevance_turbo as recording:\n",
    "        tru_wrapped_relevance_turbo.app(prompt, response)\n",
    "    \n",
    "    with tru_wrapped_relevance_gpt4 as recording:\n",
    "        tru_wrapped_relevance_gpt4.app(prompt, response)\n",
    "    \n",
    "    with tru_wrapped_relevance_commandnightly as recording:\n",
    "        tru_wrapped_relevance_commandnightly.app(prompt, response)\n",
    "    \n",
    "    with tru_wrapped_relevance_claude1 as recording:\n",
    "        tru_wrapped_relevance_claude1.app(prompt, response)\n",
    "\n",
    "    with tru_wrapped_relevance_claude2 as recording:\n",
    "        tru_wrapped_relevance_claude2.app(prompt, response)\n",
    "\n",
    "    with tru_wrapped_relevance_llama2 as recording:\n",
    "        tru_wrapped_relevance_llama2.app(prompt, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean Absolute Error</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>answer relevance gpt-3.5-turbo</th>\n",
       "      <td>0.172727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer relevance gpt-4</th>\n",
       "      <td>0.245455</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.014804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer relevance Claude 1</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer relevance Claude 2</th>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer relevance Command-Nightly</th>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer relevance Llama-2-13b</th>\n",
       "      <td>0.590000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Mean Absolute Error   latency  total_cost\n",
       "app_id                                                                     \n",
       "answer relevance gpt-3.5-turbo               0.172727  0.090909    0.000739\n",
       "answer relevance gpt-4                       0.245455  0.090909    0.014804\n",
       "answer relevance Claude 1                    0.250000  0.100000    0.000000\n",
       "answer relevance Claude 2                    0.300000  0.100000    0.000000\n",
       "answer relevance Command-Nightly             0.300000  0.100000    0.000000\n",
       "answer relevance Llama-2-13b                 0.590000  0.100000    0.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tru().get_leaderboard(app_ids=[]).sort_values(by='Mean Absolute Error')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d153714b979d5e6d08dd8ec90712dd93bff2c9b6c1f0c118169738af3430cd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
