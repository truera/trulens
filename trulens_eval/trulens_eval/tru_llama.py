"""
# LlamaIndex instrumentation.
"""

from inspect import BoundArguments
from inspect import Signature
import logging
from pprint import PrettyPrinter
from typing import Any, Callable, ClassVar, Dict, Optional, Union

from pkg_resources import parse_version
from pydantic import Field

from trulens_eval.app import App
from trulens_eval.instruments import ClassFilter
from trulens_eval.instruments import Instrument
from trulens_eval.utils.containers import dict_set_with_multikey
from trulens_eval.utils.imports import Dummy
from trulens_eval.utils.imports import get_package_version
from trulens_eval.utils.imports import OptionalImports
from trulens_eval.utils.imports import REQUIREMENT_LLAMA
from trulens_eval.utils.pyschema import Class
from trulens_eval.utils.pyschema import FunctionOrMethod
from trulens_eval.utils.python import EmptyType
from trulens_eval.utils.serial import Lens

logger = logging.getLogger(__name__)

pp = PrettyPrinter()

with OptionalImports(messages=REQUIREMENT_LLAMA):
    import llama_index

    version = get_package_version("llama_index")

    # If llama index is not installed, will get a dummy for llama_index. In that
    # case or if it is installed and sufficiently new version, continue with
    # this set of imports. We don't want to partially run the new imports and
    # fail midway to continue with the legacy imports.

    legacy: bool = version is None or isinstance(llama_index, Dummy)

    if not legacy:
        # Check if llama_index is new enough.

        if version < parse_version("0.10.0"):
            legacy = True

    if not legacy:
        from llama_index.core.base.base_query_engine import BaseQueryEngine
        from llama_index.core.base.base_query_engine import \
            QueryEngineComponent
        from llama_index.core.base.embeddings.base import BaseEmbedding
        from llama_index.core.base.llms.base import BaseLLM
        from llama_index.core.base.llms.types import LLMMetadata
        from llama_index.core.base.response.schema import Response
        from llama_index.core.chat_engine.types import AgentChatResponse
        from llama_index.core.chat_engine.types import BaseChatEngine
        from llama_index.core.chat_engine.types import \
            StreamingAgentChatResponse
        from llama_index.core.indices.base import BaseIndex
        from llama_index.core.indices.prompt_helper import PromptHelper
        from llama_index.core.memory.types import BaseMemory
        from llama_index.core.node_parser.interface import NodeParser
        from llama_index.core.postprocessor.types import BaseNodePostprocessor
        from llama_index.core.question_gen.types import BaseQuestionGenerator
        from llama_index.core.response.schema import StreamingResponse
        from llama_index.core.response_synthesizers import BaseSynthesizer
        from llama_index.core.response_synthesizers import Refine
        from llama_index.core.retrievers import BaseRetriever
        from llama_index.core.schema import BaseComponent
        from llama_index.core.schema import QueryBundle
        from llama_index.core.tools.types import AsyncBaseTool
        from llama_index.core.tools.types import BaseTool
        from llama_index.core.tools.types import ToolMetadata
        from llama_index.core.vector_stores.types import VectorStore
        from llama_index.legacy.llm_predictor import LLMPredictor
        from llama_index.legacy.llm_predictor.base import BaseLLMPredictor

        # These exist in the bridge but not here so define placeholders.
        RetrieverComponent = EmptyType

        from trulens_eval.utils.llama import WithFeedbackFilterNodes

    else:
        # Otherwise llama_index is installed but is old so we have to use older imports.
        # Bridge for versions < 0.10

        if version is not None:
            logger.warning(
                "Using legacy llama_index version %s. Consider upgrading to 0.10.0 or later.",
                version
            )

        from llama_index.chat_engine.types import AgentChatResponse
        from llama_index.chat_engine.types import BaseChatEngine
        from llama_index.chat_engine.types import StreamingAgentChatResponse
        from llama_index.core.base_query_engine import BaseQueryEngine
        from llama_index.core.base_query_engine import QueryEngineComponent
        from llama_index.core.base_retriever import BaseRetriever
        from llama_index.core.base_retriever import RetrieverComponent
        from llama_index.embeddings.base import BaseEmbedding
        from llama_index.indices.base import BaseIndex
        from llama_index.indices.prompt_helper import PromptHelper
        from llama_index.indices.query.schema import QueryBundle
        from llama_index.indices.service_context import ServiceContext
        from llama_index.llm_predictor import LLMPredictor
        from llama_index.llm_predictor.base import BaseLLMPredictor
        from llama_index.llm_predictor.base import LLMMetadata
        from llama_index.llms.base import BaseLLM
        from llama_index.memory import BaseMemory
        from llama_index.node_parser.interface import NodeParser
        from llama_index.postprocessor.types import BaseNodePostprocessor
        from llama_index.prompts.base import Prompt
        from llama_index.question_gen.types import BaseQuestionGenerator
        from llama_index.response.schema import Response
        from llama_index.response.schema import StreamingResponse
        from llama_index.response_synthesizers.base import BaseSynthesizer
        from llama_index.response_synthesizers.refine import Refine
        from llama_index.schema import BaseComponent
        from llama_index.tools.types import AsyncBaseTool
        from llama_index.tools.types import BaseTool
        from llama_index.tools.types import ToolMetadata
        from llama_index.vector_stores.types import VectorStore

        from trulens_eval.utils.llama import WithFeedbackFilterNodes

# Need to `from ... import ...` for the below as referring to some of these
# later in this file by full path does not work due to lack of intermediate
# modules in the path.

# Fail outside of optional imports contexts so that anything that follows gets
# to be a dummy which will cause failures if used.
OptionalImports(messages=REQUIREMENT_LLAMA).assert_installed(llama_index)

from trulens_eval.tru_chain import LangChainInstrument


class LlamaInstrument(Instrument):
    """Instrumentation for LlamaIndex apps."""

    class Default:
        """Instrumentation specification for LlamaIndex apps."""

        MODULES = {"llama_index.",
                   "llama_hub."}.union(LangChainInstrument.Default.MODULES)
        """Modules by prefix to instrument.
         
        Note that llama_index uses langchain internally for some things.
        """

        CLASSES = lambda: {
            BaseComponent, BaseLLM, BaseQueryEngine, BaseRetriever, BaseIndex,
            BaseChatEngine, BaseQuestionGenerator, BaseSynthesizer, Refine,
            LLMPredictor, LLMMetadata, BaseLLMPredictor, VectorStore,
            PromptHelper, BaseEmbedding, NodeParser, ToolMetadata, BaseTool,
            BaseMemory, WithFeedbackFilterNodes, BaseNodePostprocessor,
            QueryEngineComponent, RetrieverComponent
        }.union(LangChainInstrument.Default.CLASSES())
        """Classes to instrument."""

        METHODS: Dict[str, ClassFilter] = dict_set_with_multikey(
            dict(LangChainInstrument.Default.METHODS),
            {
                # LLM:
                (
                    "complete", "stream_complete", "acomplete", "astream_complete"
                ):
                    BaseLLM,

                # BaseTool/AsyncBaseTool:
                ("__call__", "call"):
                    BaseTool,
                ("acall"):
                    AsyncBaseTool,

                # Memory:
                ("put"):
                    BaseMemory,

                # Misc.:
                ("get_response"):
                    Refine,
                ("predict"):
                    BaseLLMPredictor,

                # BaseQueryEngine:
                ("query", "aquery"):
                    BaseQueryEngine,

                # BaseChatEngine/LLM:
                ("chat", "achat", "stream_chat", "astream_achat"):
                    (BaseLLM, BaseChatEngine),

                # BaseRetriever/BaseQueryEngine:
                ("retrieve", "_retrieve", "_aretrieve"):
                    (BaseQueryEngine, BaseRetriever, WithFeedbackFilterNodes),

                # BaseQueryEngine:
                ("synthesize"):
                    BaseQueryEngine,

                # BaseNodePostProcessor
                ("_postprocess_nodes"):
                    BaseNodePostprocessor,

                # Components
                ("_run_component"): (QueryEngineComponent, RetrieverComponent)
            }
        )
        """Methods to instrument."""

    def __init__(self, *args, **kwargs):
        super().__init__(
            include_modules=LlamaInstrument.Default.MODULES,
            include_classes=LlamaInstrument.Default.CLASSES(),
            include_methods=LlamaInstrument.Default.METHODS,
            *args,
            **kwargs
        )


class TruLlama(App):
    """
    LlamaIndex app recorder.

    Further information about LlamaIndex apps can be found on the [ðŸ¦™ LlamaIndex
    Documentation](https://docs.llamaindex.ai/en/stable/) page.

    Example:
        LLama-Index code: (see [LLama Index
        Quickstart](https://gpt-index.readthedocs.io/en/stable/getting_started/starter_example.html))
    
        ```python
        from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

        documents = SimpleDirectoryReader("data").load_data()
        index = VectorStoreIndex.from_documents(documents)

        query_engine = index.as_query_engine()
        ```

        Trulens Eval Code:
        ```python
        from trulens_eval import TruLlama
        # f_lang_match, f_qa_relevance, f_qs_relevance are feedback functions
        tru_recorder = TruLlama(query_engine,
            app_id='LlamaIndex_App1',
            feedbacks=[f_lang_match, f_qa_relevance, f_qs_relevance])

        with tru_recorder as recording:
            query_engine.query("What is llama index?")

        tru_record = recording.records[0]

        # To add record metadata 
        with tru_recorder as recording:
            recording.record_metadata="this is metadata for all records in this context that follow this line"
            query_engine.query("What is llama index?")
            recording.record_metadata="this is different metadata for all records in this context that follow this line"
            query_engine.query("Where do I download llama index?")
        
        ```

        See [Feedback Functions](https://www.trulens.org/trulens_eval/api/feedback/) for instantiating feedback functions.

    Args:
        app: A LlamaIndex application.

        **kwargs: Additional arguments to pass to [App][trulens_eval.app.App]
            and [AppDefinition][trulens_eval.app.AppDefinition]
    """

    model_config: ClassVar[dict] = dict(arbitrary_types_allowed=True)

    app: Union[BaseQueryEngine, BaseChatEngine]

    root_callable: ClassVar[FunctionOrMethod] = Field(
        default_factory=lambda: FunctionOrMethod.of_callable(TruLlama.query)
    )

    def __init__(
        self, app: Union[BaseQueryEngine, BaseChatEngine], **kwargs: dict
    ):
        # TruLlama specific:
        kwargs['app'] = app
        kwargs['root_class'] = Class.of_object(app)  # TODO: make class property
        kwargs['instrument'] = LlamaInstrument(app=self)

        super().__init__(**kwargs)

    @classmethod
    def select_source_nodes(cls) -> Lens:
        """
        Get the path to the source nodes in the query output.
        """
        return cls.select_outputs().source_nodes[:]

    @classmethod
    def select_context(
        cls,
        app: Optional[Union[BaseQueryEngine, BaseChatEngine]] = None
    ) -> Lens:
        """
        Get the path to the context in the query output.
        """
        return cls.select_outputs().source_nodes[:].node.text

    def main_input(
        self, func: Callable, sig: Signature, bindings: BoundArguments
    ) -> str:
        """
        Determine the main input string for the given function `func` with
        signature `sig` if it is to be called with the given bindings
        `bindings`.
        """

        if 'str_or_query_bundle' in bindings.arguments:
            # llama_index specific
            str_or_bundle = bindings.arguments['str_or_query_bundle']
            if isinstance(str_or_bundle, QueryBundle):
                return str_or_bundle.query_str
            else:
                return str_or_bundle

        elif 'message' in bindings.arguments:
            # llama_index specific
            return bindings.arguments['message']

        else:

            return App.main_input(self, func, sig, bindings)

    def main_output(
        self, func: Callable, sig: Signature, bindings: BoundArguments, ret: Any
    ) -> Optional[str]:
        """
        Determine the main out string for the given function `func` with
        signature `sig` after it is called with the given `bindings` and has
        returned `ret`.
        """

        try:
            attr = self._main_output_attribute(ret)

            if attr is not None:
                return getattr(ret, attr)
            else:  # attr is None
                return App.main_output(self, func, sig, bindings, ret)

        except NotImplementedError:
            return None

    def _main_output_attribute(self, ret: Any) -> Optional[str]:
        """
        Which attribute in ret contains the main output of this llama_index app.
        """

        if isinstance(ret, Response):  # query, aquery
            return "response"

        elif isinstance(ret, AgentChatResponse):  #  chat, achat
            return "response"

        elif isinstance(ret, (StreamingResponse, StreamingAgentChatResponse)):
            raise NotImplementedError(
                "App produced a streaming response. "
                "Tracking content of streams in llama_index is not yet supported. "
                "App main_output will be None."
            )

        return None

    def main_call(self, human: str):
        # If available, a single text to a single text invocation of this app.
        # Note that we are ultimately calling a different langchain method here
        # than in the `main_acall` method so we don't reuse the async version
        # here in case langchain does something special between them.

        if isinstance(self.app, BaseQueryEngine):
            ret = self.app.query(human)
        elif isinstance(self.app, BaseChatEngine):
            ret = self.app.chat(human)
        else:
            raise RuntimeError(
                f"Do not know what the main method for app of type {type(self.app).__name__} is."
            )

        try:
            attr = self._main_output_attribute(ret)
            assert attr is not None
            return getattr(ret, attr)

        except Exception:
            raise NotImplementedError(
                f"Do not know what in object of type {type(ret).__name__} is the main app output."
            )

    async def main_acall(self, human: str):
        # If available, a single text to a single text invocation of this app.

        if isinstance(self.app, BaseQueryEngine):
            ret = await self.app.aquery(human)
        elif isinstance(self.app, BaseChatEngine):
            ret = await self.app.achat(human)
        else:
            raise RuntimeError(
                f"Do not know what the main async method for app of type {type(self.app).__name__} is."
            )

        try:
            attr = self._main_output_attribute(ret)
            assert attr is not None
            return getattr(ret, attr)

        except Exception:
            raise NotImplementedError(
                f"Do not know what in object of type {type(ret).__name__} is the main app output."
            )

    # TOREMOVE
    # llama_index.chat_engine.types.BaseChatEngine
    def chat(self, *args, **kwargs) -> None:

        self._throw_dep_message(
            method="chat", is_async=False, with_record=False
        )

    # TOREMOVE
    # llama_index.chat_engine.types.BaseChatEngine
    async def achat(self, *args, **kwargs) -> None:

        self._throw_dep_message(
            method="achat", is_async=True, with_record=False
        )

    # TOREMOVE
    # llama_index.chat_engine.types.BaseChatEngine
    def stream_chat(self, *args, **kwargs) -> None:

        self._throw_dep_message(
            method="stream_chat", is_async=False, with_record=False
        )

    # TOREMOVE
    # llama_index.chat_engine.types.BaseChatEngine
    async def astream_chat(self, *args, **kwargs) -> None:

        self._throw_dep_message(
            method="astream_chat", is_async=True, with_record=False
        )

    # TOREMOVE
    # llama_index.indices.query.base.BaseQueryEngine
    def query(self, *args, **kwargs) -> None:

        self._throw_dep_message(
            method="query", is_async=False, with_record=False
        )

    # TOREMOVE
    # llama_index.indices.query.base.BaseQueryEngine
    async def aquery(self, *args, **kwargs) -> None:
        self._throw_dep_message(
            method="aquery", is_async=True, with_record=False
        )

    # TOREMOVE
    # Mirrors llama_index.indices.query.base.BaseQueryEngine.query .
    def query_with_record(self, *args, **kwargs) -> None:

        self._throw_dep_message(
            method="query", is_async=False, with_record=True
        )

    # TOREMOVE
    # Mirrors llama_index.indices.query.base.BaseQueryEngine.aquery .
    async def aquery_with_record(self, *args, **kwargs) -> None:

        self._throw_dep_message(
            method="aquery", is_async=True, with_record=True
        )

    # TOREMOVE
    # Compatible with llama_index.chat_engine.types.BaseChatEngine.chat .
    def chat_with_record(self, *args, **kwargs) -> None:

        self._throw_dep_message(method="chat", is_async=False, with_record=True)

    # TOREMOVE
    # Compatible with llama_index.chat_engine.types.BaseChatEngine.achat .
    async def achat_with_record(self, *args, **kwargs) -> None:

        self._throw_dep_message(method="achat", is_async=True, with_record=True)

    # TOREMOVE
    # Compatible with llama_index.chat_engine.types.BaseChatEngine.stream_chat .
    def stream_chat_with_record(self, *args, **kwargs) -> None:

        self._throw_dep_message(
            method="stream", is_async=False, with_record=True
        )

    # TOREMOVE
    # Compatible with llama_index.chat_engine.types.BaseChatEngine.astream_chat .
    async def astream_chat_with_record(self, *args, **kwargs) -> None:

        self._throw_dep_message(
            method="astream_chat", is_async=True, with_record=True
        )


import trulens_eval  # for App class annotations

TruLlama.model_rebuild()
