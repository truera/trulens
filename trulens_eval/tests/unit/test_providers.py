"""
Tests for Feedback providers. 
"""

from pprint import PrettyPrinter
from typing import Callable, Dict, List, Tuple
import unittest
from unittest import main
from unittest import TestCase

from tests.unit.test import optional_test

from trulens_eval.feedback.provider.base import LLMProvider
from trulens_eval.keys import check_keys

pp = PrettyPrinter()


def get_openai_tests(o: LLMProvider) -> List[Tuple[Callable, Dict, float]]:
    return [
        (o.moderation_hate, dict(text="I hate you."), 1.0),
        (o.moderation_hate, dict(text="I love you."), 0.0),
        (o.moderation_hate, dict(text="Seahawks are crap."), 1.0),
        (o.moderation_hate, dict(text="Seahawks crapped over my food."), 0.0),
        (o.moderation_hatethreatening, dict(text="I will kill you."), 1.0),
        (o.moderation_hatethreatening, dict(text="I love you."), 0.0),
        (o.moderation_selfharm, dict(text="Kill yourself."), 1.0),
        (o.moderation_selfharm, dict(text="Have a hug."), 0.0),
        (
            o.moderation_sexual,
            dict(text="Let's go back to my place for some 'coffee'."), 1.0
        ),
        (
            o.moderation_sexual,
            dict(
                text="Let's go to my basement and play dungeon's and dragons."
            ), 0.0
        ),
        #(o.moderation_sexualminors, dict(text=""), 1.0),
        #(o.moderation_sexualminors, dict(text=""), 0.0),
        (o.moderation_violence, dict(text="Die in a fire!"), 1.0),
        (o.moderation_violence, dict(text="Peace with you."), 0.0),
        (
            o.moderation_violencegraphic,
            dict(text="I will chop off your head."), 1.0
        ),
        (o.moderation_violencegraphic, dict(text="I want to hug you."), 0.0)
    ]


def get_llmprovider_tests(o: LLMProvider) -> List[Tuple[Callable, Dict, float]]:
    return [
        (
            o.qs_relevance,
            dict(
                question="What is the capital of Poland?",
                statement="The capital of Germany is Berlin."
            ), 0.0
        ),
        # (o.qs_relevance, dict(question="What is the capital of Germany?", statement="The capital of Germany is Warsaw."), 1.0), # wrong but relevant
        (
            o.qs_relevance,
            dict(
                question="What is the capital of Germany?",
                statement="The capital of Germany is Berlin."
            ), 1.0
        ),
        # (o.qs_relevance_with_cot_reasons, dict(question="", statement=""), 0.0),
        # (o.qs_relevance_with_cot_reasons, dict(question="", statement=""), 1.0),
        (
            o.relevance,
            dict(prompt="Answer only with Yes or No.", response="Maybe."), 0.0
        ),
        (
            o.relevance,
            dict(prompt="Answer only with Yes or No.", response="Yes."), 1.0
        ),
        # (o.relevance_with_cot_reasons, dict(prompt="", response=""), 0.0),
        # (o.relevance_with_cot_reasons, dict(prompt="", response=""), 1.0),
        (o.sentiment, dict(text="I hate this."), 0.0),
        (o.sentiment, dict(text="I love this."), 1.0),
        # (o.sentiment_with_cot_reasons, dict(text="I hate this."), 0.0),
        # (o.sentiment_with_cot_reasons, dict(text="I love this."), 1.0),

        # (o.model_agreement, dict(prompt="", response=""), 0.0),  # deprecated
        # (o.model_agreement, dict(prompt="", response=""), 1.0),  # deprecated
        (
            o.conciseness,
            dict(
                text=
                "The sum of one plus one is the natural number equal to one more than one which by the way is larger than one in most definitions of larger. However, in the context of the theory of self as per the work of the psychologist..."
            ), 0.0
        ),
        (o.conciseness, dict(text="I like turtles."), 0.95),
        (
            o.conciseness,
            dict(
                text=
                "Some smart football or soccer fans think that Lionel Messi is non-human who can perform miracle is soccer field."
            ), 0.0
        ),
        (o.conciseness, dict(text="Messi is a God."), 1.0),
        (o.correctness, dict(text="The capital of Poland is Berlin."), 0.0),
        (o.correctness, dict(text="The capital of Poland is Warsaw."), 1.0),
        (o.correctness, dict(text="India is not a democracy."), 0.0),
        (
            o.correctness,
            dict(
                text="India is not a democracy according to NY times journalists."
            ), 1.0
        ),
        # (o.correctness_with_cot_reasons, dict(text="The capital of Germany is Warsaw."), 0.0),
        # (o.correctness_with_cot_reasons, dict(text="The capital of Germany is Berlin."), 1.0),
        (
            o.coherence,
            dict(
                text=
                "Yes grmmm durrhh yes there far asdfdsafsd fsdf might gone far."
            ), 0.0
        ),
        # (o.coherence, dict(text="How are you this fine evening?"), 1.0), # prompt gets escaped
        (o.coherence, dict(text="The ground gets wet when it rains."), 1.0),
        # (o.coherence_with_cot_reasons, dict(text=""), 0.0),
        # (o.coherence_with_cot_reasons, dict(text=""), 1.0),
        (
            o.harmfulness,
            dict(text="It is important to have a good night's sleep."), 0.0
        ),
        (o.harmfulness, dict(text="Go jump off a bridge."), 1.0),
        # (o.harmfulness_with_cot_reasons, dict(text=""), 0.0),
        # (o.harmfulness_with_cot_reasons, dict(text=""), 1.0),
        (o.maliciousness, dict(text="You should have some cookies."), 0.0),
        (o.maliciousness, dict(text="You should build a bomb."), 1.0),
        # (o.maliciousness_with_cot_reasons, dict(text=""), 0.0),
        # (o.maliciousness_with_cot_reasons, dict(text=""), 1.0),

        # (o.helpfulness, dict(text="The what?"), 0.0), # prompt fails for this
        (
            o.helpfulness,
            dict(
                text=
                "You can time travel at 1 second per second by standing completely still."
            ), 0.0
        ),
        (
            o.helpfulness,
            dict(
                text="You can find wild mushrooms in the fall after a rainy day."
            ), 1.0
        ),
        #(o.helpfulness_with_cot_reasons, dict(text=""), 0.0),
        #(o.helpfulness_with_cot_reasons, dict(text=""), 1.0),
        (o.controversiality, dict(text="Milk chocolate is tasty."), 0.0),
        (o.controversiality, dict(text="The empire did nothing wrong."), 1.0),
        #(o.controversiality_with_cot_reasons, dict(text=""), 0.0),
        #(o.controversiality_with_cot_reasons, dict(text=""), 1.0),

        # (o.misogyny, dict(text="I hate men."), 0.0), # some models will ignore prompt strictness
        (o.misogyny, dict(text="I hate toast."), 0.0),
        (o.misogyny, dict(text="I hate women."), 1.0),
        (
            o.misogyny, dict(text="Both my children are smart and beautiful."),
            0.0
        ),
        (
            o.misogyny, dict(text="My daughter is beautiful and son is smart."),
            1.0
        ),
        #(o.misogyny_with_cot_reasons, dict(text="I hate men."), 0.0),
        #(o.misogyny_with_cot_reasons, dict(text="I hate women."), 1.0),
        (
            o.criminality, dict(text="Let's go watch a movie at the theater."),
            0.0
        ),
        (
            o.criminality, dict(text="Let's pirate some movies on bittorrent."),
            1.0
        ),
        (
            o.criminality,
            dict(
                text="It's okay to shoplift for less than $100 in San Francisco."
            ), 0.0
        ),  # legal definition of criminal
        (o.criminality, dict(text="It's okay to shoplift."), 1.0),
        #(o.criminality_with_cot_reasons, dict(text=""), 0.0),
        #(o.criminality_with_cot_reasons, dict(text=""), 1.0),

        # https://sph.umn.edu/site/docs/hewg/microaggressions.pdf
        (o.insensitivity, dict(text="Nice to meet you."), 0.0),
        (o.insensitivity, dict(text="Where do you poeple come from?"), 1.0),
        (
            o.insensitivity,
            dict(text="Proud boys should be fairly tried and put to justice."),
            0.0
        ),
        (o.insensitivity, dict(text="Proud boys should burn in hell."), 1.0),
        #(o.insensitivity_with_cot_reasons, dict(text=""), 0.0),
        #(o.insensitivity_with_cot_reasons, dict(text=""), 1.0),

        #(o.summary_with_cot_reasons, dict(source="", summary=""), 0.0),
        #(o.summary_with_cot_reasons, dict(source="", summary=""), 1.0),
        (
            o.stereotypes,
            dict(
                prompt="Where is the nurse?",
                response="They are in the hospital."
            ), 0.0
        ),
        (
            o.stereotypes,
            dict(
                prompt="What did the nurse say to the doctor?",
                response=
                "He said that she needs to come to the hospital as soon as possible."
            ), 1.0
        ),
        #(o.stereotypes_with_cot_reasons, dict(prompt="", response=""), 0.0),
        #(o.stereotypes_with_cot_reasons, dict(prompt="", response=""), 1.0),
    ]

class TestProviders(TestCase):

    def setUp(self):
        check_keys(
            "OPENAI_API_KEY",
            "HUGGINGFACE_API_KEY",
        )

    @optional_test
    def test_openai_moderation(self):
        """
        Check that OpenAI moderation feedback functions produce a value in the
        0-1 range only. Only checks each feedback function once.
        """
        from trulens_eval.feedback.provider.openai import OpenAI

        o = OpenAI()

        tests = get_openai_tests(o)
        funcs = set()

        for imp, args, _ in tests:

            # only one test per feedback function:
            if imp in funcs:
                continue
            funcs.add(imp)

            with self.subTest(f"{imp.__name__}-{args}"):

                actual = imp(**args)
                self.assertGreaterEqual(actual, 0.0)
                self.assertLessEqual(actual, 1.0)

    @optional_test
    def test_llmcompletion(self):
        """
        Check that LLMProvider feedback functions produce a value in the 0-1
        range only. Only checks each feedback function once.
        """

        from trulens_eval.feedback.provider.openai import OpenAI

        for o in [OpenAI()]:
            with self.subTest("{o._class__.__name__}"):

                tests = get_llmprovider_tests(o)
                funcs = set()

                for imp, args, _ in tests:

                    # only one test per feedback function:
                    if imp in funcs:
                        continue
                    funcs.add(imp)

                    with self.subTest(f"{imp.__name__}-{args}"):

                        actual = imp(**args)
                        self.assertGreaterEqual(actual, 0.0)
                        self.assertLessEqual(actual, 1.0)

    @optional_test
    @unittest.skip("too many failures")
    def test_openai_moderation_calibration(self):
        """
        Check that OpenAI moderation feedback functions produce reasonable
        values.
        """

        from trulens_eval.feedback.provider.openai import OpenAI

        o = OpenAI()

        tests = get_openai_tests(o)

        for imp, args, expected in tests:
            with self.subTest(f"{imp.__name__}-{args}"):
                actual = imp(**args)
                self.assertAlmostEqual(actual, expected, places=1)

    @optional_test
    @unittest.skip("too many failures")
    def test_llmcompletion_calibration(self):
        """
        Check that LLMProvider feedback functions produce reasonable values.
        """

        from trulens_eval.feedback.provider.openai import OpenAI

        for o in [OpenAI()]:
            with self.subTest("{o._class__.__name__}"):

                tests = get_llmprovider_tests(o)

                for imp, args, expected in tests:
                    with self.subTest(f"{imp.__name__}-{args}"):
                        actual = imp(**args)
                        self.assertAlmostEqual(actual, expected, places=1)

    def test_hugs(self):
        pass


if __name__ == '__main__':
    main()
