{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruLens for LLMs: Quickstart\n",
    "\n",
    "In this quickstart you will create a simple LLM Chain and learn how to log it and get feedback on an LLM response.\n",
    "\n",
    "Note: If you haven't already, make sure to set up your local .env file with your OpenAI and Huggingface Keys. Your .env file should be in the same directory that you run this notebook. If you need help, take a look at the [.env.example](https://github.com/truera/trulens_private/blob/e8b11c4689e644687d2eafe09d90d8d7774b581c/trulens_eval/trulens_eval/.env.example#L4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import from LangChain and TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from langchain to build app\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.prompts.chat import HumanMessagePromptTemplate\n",
    "# imports from trulens to log and get feedback on chain\n",
    "from trulens_eval import tru\n",
    "from trulens_eval import tru_chain\n",
    "from trulens_eval.tru_feedback import Feedback, Huggingface, OpenAI\n",
    "from trulens_eval.keys import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Simple LLM Application\n",
    "\n",
    "This example uses a LangChain framework and OpenAI LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_prompt = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        template=\"Provide a helpful response with relevant background information for the following: {prompt}\",\n",
    "            input_variables=[\"prompt\"],\n",
    "        )\n",
    "    )\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([full_prompt])\n",
    "\n",
    "chat = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.9)\n",
    "\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt_template)\n",
    "\n",
    "# wrap with truchain\n",
    "tc = tru_chain.TruChain(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send your first request to your new app, asking how to adopt a dog in spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = 'como adopto un perro?'\n",
    "gpt3_response, record = tc.call_with_record(prompt_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log data and feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_id = tru.add_data(\n",
    "        chain_id='Chain1_ChatApplication', #name your chain\n",
    "        prompt=prompt_input, # prompt input\n",
    "        response=gpt3_response['text'], # LLM response\n",
    "        record=record, # record is returned by the TruChain wrapper\n",
    "        tags='dev' #add a tag\n",
    "    )\n",
    "\n",
    "\n",
    "# initialize Huggingface class for feedback function generation\n",
    "hugs = Huggingface()\n",
    "\n",
    "# Generate a language match feedback function using HuggingFace\n",
    "f_lang_match = Feedback(hugs.language_match).on(text1=\"prompt\", text2=\"response\")\n",
    "\n",
    "# Run feedack functions\n",
    "feedback = tru.run_feedback_functions(\n",
    "        chain=chain, # the unwrapped chain\n",
    "        record=record, # record is returned by the TruChain wrapper\n",
    "        feedback_functions=[f_lang_match] # a list of feedback functions to apply\n",
    "    )\n",
    "\n",
    "tru.add_feedback(record_id, feedback) # log the feedback by providing the record id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the TruLens dashboard to explore the quality of your LLM chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.run_dashboard() # open a local streamlit app to explore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('trulens')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c633204c92f433e69d41413efde9db4a539ce972d10326abcceb024ad118839e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
