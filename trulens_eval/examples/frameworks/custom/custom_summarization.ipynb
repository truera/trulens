{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed3e8c10",
   "metadata": {},
   "source": [
    "# Evaluating Summarization with TruLens\n",
    "\n",
    "In this notebook, we will evaluate a summarization application based on [DialogSum dataset](https://github.com/cylnlp/dialogsum). Using a number of different metrics. These will break down into two main categories: \n",
    "1. Ground truth agreement: For these set of metrics, we will measure how similar the generated summary is to some human-created ground truth. We will use for different measures: BERT score, BLEU, ROUGE and a measure where an LLM is prompted to produce a similarity score.\n",
    "2. Groundedness: For this measure, we will estimate if the generated summary can be traced back to parts of the original transcript."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602ed89a",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "Let's first install the packages that this notebook depends on. Uncomment these linse to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85d254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"!pip install bert_score==0.3.13 \\\n",
    "             evaluate==0.4.0 \\\n",
    "             absl-py==1.4.0 \\\n",
    "             rouge-score==0.1.2 \\\n",
    "             pandas \\\n",
    "             tenacity \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd22c9c2",
   "metadata": {},
   "source": [
    "For the latest metrics, install TruLens from development branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc613e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"!pip install git+https://github.com/truera/trulens.git@ss/comparison_scores#subdirectory=trulens_eval\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f443aac",
   "metadata": {},
   "source": [
    "### Download and load data\n",
    "Now we will download a portion of the DialogSum dataset from github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6769a0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8133c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O dialogsum.dev.jsonl https://raw.githubusercontent.com/cylnlp/dialogsum/main/DialogSum_Data/dialogsum.dev.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0829ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_dev = 'dialogsum.dev.jsonl'\n",
    "dev_df = pd.read_json(path_or_buf=file_path_dev, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e7714b",
   "metadata": {},
   "source": [
    "Let's preview the data to make sure that the data was properly loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad85d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716d57bc",
   "metadata": {},
   "source": [
    "## Create a simple summarization app and instrument it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ffb3d7",
   "metadata": {},
   "source": [
    "We will create a simple summarization app based on the OpenAI ChatGPT model and instrument it for use with TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2472f205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.tru_custom_app import instrument\n",
    "from trulens_eval.tru_custom_app import TruCustomApp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc60cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "class DialogSummaryApp:\n",
    "    \n",
    "    @instrument\n",
    "    def summarize(self, dialog):\n",
    "        summary = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"\"\"Summarize the given dialog into 1-2 sentences based on the following criteria: \n",
    "                     1. Convey only the most salient information; \n",
    "                     2. Be brief; \n",
    "                     3. Preserve important named entities within the conversation; \n",
    "                     4. Be written from an observer perspective; \n",
    "                     5. Be written in formal language. \"\"\"},\n",
    "                    {\"role\": \"user\", \"content\": dialog}\n",
    "                ]\n",
    "            )[\"choices\"][0][\"message\"][\"content\"]\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a81c191",
   "metadata": {},
   "source": [
    "## Initialize Database and view dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba28354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Tru\n",
    "tru = Tru()\n",
    "# If you have a database you can connect to, use a URL. For example:\n",
    "# tru = Tru(database_url=\"postgresql://hostname/database?user=username&password=password\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990c0dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad02e597",
   "metadata": {},
   "source": [
    "## Write feedback functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56247d16",
   "metadata": {},
   "source": [
    "We will now create the feedback functions that will evaluate the app. Remember that the criteria we were evaluating against were:\n",
    "1. Ground truth agreement: For these set of metrics, we will measure how similar the generated summary is to some human-created ground truth. We will use for different measures: BERT score, BLEU, ROUGE and a measure where an LLM is prompted to produce a similarity score.\n",
    "2. Groundedness: For this measure, we will estimate if the generated summary can be traced back to parts of the original transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3ee39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Feedback, feedback\n",
    "from trulens_eval.feedback import GroundTruthAgreement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4db1975",
   "metadata": {},
   "source": [
    "We select the golden dataset based on dataset we downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2168ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_set = dev_df[['dialogue', 'summary']].rename(columns={'dialogue': 'query', 'summary': 'response'}).to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bc13e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_collection = GroundTruthAgreement(golden_set)\n",
    "f_groundtruth = Feedback(ground_truth_collection.agreement_measure).on_input_output()\n",
    "f_bert_score = Feedback(ground_truth_collection.bert_score).on_input_output()\n",
    "f_bleu = Feedback(ground_truth_collection.bleu).on_input_output()\n",
    "f_rouge = Feedback(ground_truth_collection.rouge).on_input_output()\n",
    "# Groundedness between each context chunk and the response.\n",
    "grounded = feedback.Groundedness()\n",
    "f_groundedness = feedback.Feedback(grounded.groundedness_measure).on_input().on_output().aggregate(grounded.grounded_statements_aggregator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13f57a9",
   "metadata": {},
   "source": [
    "## Create the app and wrap it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0c5432",
   "metadata": {},
   "source": [
    "Now we are ready to wrap our summarization app with TruLens as a `TruCustomApp`. Now each time it will be called, TruLens will log inputs, outputs and any instrumented intermediate steps and evaluate them ith the feedback functions we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf42a5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = DialogSummaryApp()\n",
    "#print(app.summarize(dev_df.dialogue[498]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a31835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ta = TruCustomApp(app, app_id='Summarize_v1', feedbacks = [f_groundtruth, f_groundedness, f_bert_score, f_bleu, f_rouge])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a63099",
   "metadata": {},
   "source": [
    "We can test a single run of the App as so. This should show up on the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8d4eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ta.with_record(app.summarize, dialog=dev_df.dialogue[498])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd0f0c5",
   "metadata": {},
   "source": [
    "We'll make a lot of queries in a short amount of time, so we need tenacity to make sure that most of our requests eventually go through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4274c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b0c8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def run_with_backoff(doc):\n",
    "    return ta.with_record(app.summarize, dialog=doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175df188",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in golden_set:\n",
    "    llm_response = run_with_backoff(pair[\"query\"])\n",
    "    print(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae8b4b3",
   "metadata": {},
   "source": [
    "And that's it! This might take a few minutes to run, at the end of it, you can explore the dashboard to see how well your app does."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
