{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Async Example\n",
    "\n",
    "This notebook demonstrates how to use the streaming capability of langchain and monitor the results using trulens. Example app by user Anunaya Joshi ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# If running from github repo, can use this:\n",
    "sys.path.append(str(Path().cwd().parent.parent.parent.resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact\n",
    "from ipywidgets import widgets\n",
    "from langchain import LLMChain\n",
    "from langchain import PromptTemplate\n",
    "from langchain.callbacks import AsyncIteratorCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "from trulens_eval import Feedback\n",
    "from trulens_eval import feedback\n",
    "from trulens_eval import Tru\n",
    "from trulens_eval.keys import check_keys\n",
    "import trulens_eval.utils.python  # makes sure asyncio gets instrumented\n",
    "\n",
    "Tru().reset_database()\n",
    "\n",
    "check_keys(\n",
    "    \"OPENAI_API_KEY\",\n",
    "    \"HUGGINGFACE_API_KEY\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a language match feedback function.\n",
    "\n",
    "tru = Tru()\n",
    "hugs = feedback.Huggingface()\n",
    "f_lang_match = Feedback(hugs.language_match).on_input_output()\n",
    "\n",
    "# Set up an async callback.\n",
    "callback = AsyncIteratorCallbackHandler()\n",
    "\n",
    "chatllm = ChatOpenAI(\n",
    "    temperature=0.0,\n",
    "    streaming=True# important\n",
    "    # callbacks=[callback]\n",
    ")\n",
    "llm = OpenAI(\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"human_input\",\n",
    "    llm=llm,\n",
    "    max_token_limit=50\n",
    ")\n",
    "\n",
    "# Setup a simple question/answer chain with streaming ChatOpenAI.\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"human_input\", \"chat_history\"],\n",
    "    template='''\n",
    "    You are having a conversation with a person. Make small talk.\n",
    "    {chat_history}\n",
    "        Human: {human_input}\n",
    "        AI:'''\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=chatllm, prompt=prompt, memory=memory)\n",
    "tc = tru.Chain(chain, feedbacks=[f_lang_match], app_id=\"chat_with_memory\")\n",
    "\n",
    "message = \"Hi. How are you?\"\n",
    "\n",
    "# Create a task with the call to the chain, but don't wait for it yet.\n",
    "f_res_record = asyncio.create_task(\n",
    "    tc.acall_with_record(\n",
    "        inputs=dict(human_input=message),\n",
    "        callbacks=[callback]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Instead wait for the callback's async generator, getting us each token as it comes in.\n",
    "async for token in callback.aiter():\n",
    "    print(token)\n",
    "\n",
    "# By now the acall_with_record results should be ready.\n",
    "res, record = await f_res_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the result:\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the record:\n",
    "\n",
    "record.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive version of the above.\n",
    "\n",
    "\n",
    "out = widgets.HTML(value=\"\", layout=dict(width=\"50%\"))\n",
    "mem_out = widgets.HTML(value=\"\", layout=dict(width=\"50%\"))\n",
    "display(widgets.HBox([out, mem_out]))\n",
    "\n",
    "chatllm = ChatOpenAI(\n",
    "    temperature=0.0,\n",
    "    streaming=True# important\n",
    "    # callbacks=[callback]\n",
    ")\n",
    "llm = OpenAI(\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"human_input\",\n",
    "    llm=llm,\n",
    "    max_token_limit=50\n",
    ")\n",
    "\n",
    "# Setup a simple question/answer chain with streaming ChatOpenAI.\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"human_input\", \"chat_history\"],\n",
    "    template='''\n",
    "    You are having a conversation with a person. Make small talk.\n",
    "    {chat_history}\n",
    "        Human: {human_input}\n",
    "        AI:'''\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=chatllm, prompt=prompt, memory=memory)\n",
    "tc = tru.Chain(chain, feedbacks=[f_lang_match], app_id=\"chat_with_memory\")\n",
    "\n",
    "async def process_question(message):\n",
    "    if message == \"\":\n",
    "        return\n",
    "\n",
    "    out.value += \"<b>Human:</b> \" + message + \"<br/><br/><b>AI:</b> \"\n",
    "    \n",
    "    # Set up an async callback.\n",
    "    callback = AsyncIteratorCallbackHandler()\n",
    "\n",
    "    f_res_record = asyncio.create_task(\n",
    "        tc.acall_with_record(\n",
    "            inputs=dict(human_input=message),\n",
    "            callbacks=[callback]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Instead wait for the callback's async generator, getting us each token as it comes in.\n",
    "    async for token in callback.aiter():\n",
    "        out.value += token\n",
    "\n",
    "    out.value += \"<br/><br/>\"\n",
    "\n",
    "    # By now the acall_with_record results should be ready.\n",
    "    res, record = await f_res_record\n",
    "\n",
    "    temp = \"<b>Memory summary:</b><br/>\" + memory.moving_summary_buffer + \"<br/><br/><b>Memory buffer</b>:</br>\"\n",
    "    for msg in memory.chat_memory.messages:\n",
    "        temp += str(msg.content) + \"<br/><br/>\"\n",
    "    mem_out.value = temp\n",
    "\n",
    "@interact(message=widgets.Text(continuous_update=False))\n",
    "def ask_question(message: str):\n",
    "    asyncio.ensure_future(process_question(message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the dashboard. `_dev` is for running from the github repo:\n",
    "\n",
    "Tru().start_dashboard(force=True, _dev=Path.cwd().parent.parent.parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
