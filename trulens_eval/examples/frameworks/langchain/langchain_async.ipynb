{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Async Example\n",
    "\n",
    "This notebook demonstrates how to use the streaming capability of langchain and monitor the results using trulens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# If running from github repo, can use this:\n",
    "sys.path.append(str(Path().cwd().parent.parent.parent.resolve()))\n",
    "\n",
    "# Uncomment for more debugging printouts.\n",
    "\"\"\"\n",
    "import logging\n",
    "root = logging.getLogger()\n",
    "root.setLevel(logging.DEBUG)\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "root.addHandler(handler)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from langchain import LLMChain\n",
    "from langchain import PromptTemplate\n",
    "from langchain.callbacks import AsyncIteratorCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "\n",
    "from trulens_eval import Tru, Feedback, feedback\n",
    "from trulens_eval.keys import check_keys\n",
    "import trulens_eval.utils.python  # makes sure asyncio gets instrumented\n",
    "\n",
    "check_keys(\n",
    "    \"OPENAI_API_KEY\",\n",
    "    \"HUGGINGFACE_API_KEY\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a language match feedback function.\n",
    "tru = Tru()\n",
    "hugs = feedback.Huggingface()\n",
    "f_lang_match = Feedback(hugs.language_match).on_input_output()\n",
    "\n",
    "# Set up an async callback.\n",
    "callback = AsyncIteratorCallbackHandler()\n",
    "\n",
    "# Setup a simple question/answer chain with streaming ChatOpenAI.\n",
    "prompt = PromptTemplate.from_template(\"Honestly answer this question: {question}.\")\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.0,\n",
    "    streaming=True, # important\n",
    "    callbacks=[callback]\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "tc = tru.Chain(chain, feedbacks=[f_lang_match])\n",
    "\n",
    "message = \"What is 1+2? Explain your answer.\"\n",
    "\n",
    "# Create a task with the call to the chain, but don't wait for it yet.\n",
    "f_res_record = asyncio.create_task(\n",
    "    tc.acall_with_record(\n",
    "        inputs=dict(question=message),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Instead wait for the callback's async generator, getting us each token as it comes in.\n",
    "async for token in callback.aiter():\n",
    "    print(token)\n",
    "\n",
    "# By now the acall_with_record results should be ready.\n",
    "res, record = await f_res_record\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive version of the above.\n",
    "\n",
    "from ipywidgets import widgets, interact\n",
    "from IPython.display import clear_output, display\n",
    "import asyncio\n",
    "\n",
    "out = widgets.HTML(value=\"\")\n",
    "display(out)\n",
    "\n",
    "async def process_question(message):\n",
    "    if message == \"\":\n",
    "        return\n",
    "\n",
    "    out.value = message + \"<br/><br/>\"\n",
    "\n",
    "    # Set up an async callback.\n",
    "    callback = AsyncIteratorCallbackHandler()\n",
    "\n",
    "    # Setup a simple question/answer chain with streaming ChatOpenAI.\n",
    "    prompt = PromptTemplate.from_template(\"Honestly answer this question: {question}.\")\n",
    "    llm = ChatOpenAI(\n",
    "        temperature=0.0,\n",
    "        streaming=True, # important\n",
    "        callbacks=[callback]\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    tc = tru.Chain(chain, feedbacks=[f_lang_match])\n",
    "\n",
    "    f_res_record = asyncio.create_task(\n",
    "        tc.acall_with_record(\n",
    "            inputs=dict(question=message),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Instead wait for the callback's async generator, getting us each token as it comes in.\n",
    "    async for token in callback.aiter():\n",
    "        out.value += token\n",
    "\n",
    "    # By now the acall_with_record results should be ready.\n",
    "    res, record = await f_res_record\n",
    "\n",
    "    print(res)\n",
    "\n",
    "@interact(message=widgets.Text(continuous_update=False))\n",
    "def ask_question(message: str):\n",
    "    asyncio.ensure_future(process_question(message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
