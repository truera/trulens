{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Flan Model Sizes\n",
    "\n",
    "Here we'll build a simple app with langchain and load large and small flan.\n",
    "\n",
    "Then we'll ask it a few football questions and compare the quality of the responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from IPython.display import JSON\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Imports main tools:\n",
    "from trulens_eval import TruChain, Feedback, Huggingface, Tru\n",
    "# Imports main tools:\n",
    "from trulens_eval import Feedback\n",
    "from trulens_eval import feedback\n",
    "from trulens_eval import FeedbackMode\n",
    "from trulens_eval import Select\n",
    "from trulens_eval import TP\n",
    "from trulens_eval import Tru\n",
    "from trulens_eval.utils.langchain import WithFeedbackFilterDocuments\n",
    "\n",
    "# Tru object manages the database of apps, records, and feedbacks; and the\n",
    "# dashboard to display these\n",
    "tru = Tru()\n",
    "\n",
    "# Imports from langchain to build app. You may need to install langchain first\n",
    "# with the following:\n",
    "# ! pip install langchain>=0.0.170\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.prompts.chat import HumanMessagePromptTemplate\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.keys import check_keys\n",
    "\n",
    "check_keys(\n",
    "    \"OPENAI_API_KEY\",\n",
    "    \"HUGGINGFACE_API_KEY\",\n",
    "    \"HUGGINGFACEHUB_API_TOKEN\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(\n",
    "        template=template,\n",
    "    input_variables=['question']\n",
    ")\n",
    "\n",
    "# user question\n",
    "question = \"Which NFL team won the Super Bowl in the 2010 season?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up feedback functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API endpoints for models used in feedback functions:\n",
    "hugs = feedback.Huggingface()\n",
    "openai = feedback.OpenAI()\n",
    "\n",
    "# Language match between question/answer.\n",
    "f_lang_match = Feedback(hugs.language_match).on_input_output()\n",
    "# By default this will evaluate feedback on main app input and main app output.\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = Feedback(openai.relevance).on_input_output()\n",
    "# By default this will evaluate feedback on main app input and main app output.\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_qs_relevance = feedback.Feedback(openai.qs_relevance).on_input().on(\n",
    "    Select.Record.app.combine_docs_chain._call.args.inputs.input_documents[:].page_content\n",
    ").aggregate(np.min)\n",
    "# First feedback argument is set to main app input, and the second is taken from\n",
    "# the context sources as passed to an internal `combine_docs_chain._call`.\n",
    "\n",
    "all_feedbacks = [f_lang_match, f_qa_relevance, f_qs_relevance]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a couple sizes of Flan and ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub, LLMChain\n",
    "\n",
    "model = 'google/flan-t5-small'\n",
    "\n",
    "# initialize Hub LLM\n",
    "hub_llm = HuggingFaceHub(\n",
    "        repo_id = model,\n",
    "    model_kwargs = {'temperature':1e-10}\n",
    ")\n",
    "\n",
    "# create prompt template > LLM chain\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=hub_llm\n",
    ")\n",
    "\n",
    "# Trulens instrumentation.\n",
    "tc = tru.Chain(\n",
    "        app_id=f\"{model}/v1\",\n",
    "        chain=llm_chain,\n",
    "        feedbacks=all_feedbacks\n",
    "    )\n",
    "\n",
    "tc('Who won the superbowl in 2010?')    \n",
    "tc('Who won the heisman in 1995')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'google/flan-t5-large'\n",
    "\n",
    "# initialize Hub LLM\n",
    "hub_llm = HuggingFaceHub(\n",
    "        repo_id = model,\n",
    "    model_kwargs = {'temperature':1e-10}\n",
    ")\n",
    "\n",
    "# create prompt template > LLM chain\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=hub_llm\n",
    ")\n",
    "\n",
    "# Trulens instrumentation.\n",
    "tc = tru.Chain(\n",
    "        app_id=f\"{model}/v1\",\n",
    "        chain=llm_chain,\n",
    "        feedbacks=all_feedbacks\n",
    "    )\n",
    "\n",
    "tc('Who won the superbowl in 2010?')    \n",
    "tc('Who won the heisman in 1995')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load OpenAI Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'text-davinci-003'\n",
    "\n",
    "davinci = OpenAI(model_name=model)\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=davinci\n",
    ")\n",
    "\n",
    "# Trulens instrumentation.\n",
    "tc = tru.Chain(\n",
    "        app_id=f\"{model}/v1\",\n",
    "        chain=llm_chain,\n",
    "        feedbacks=all_feedbacks\n",
    "    )\n",
    "\n",
    "tc('Who won the superbowl in 2010?')    \n",
    "tc('Who won the heisman in 1995')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.run_dashboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 ('trulens')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c633204c92f433e69d41413efde9db4a539ce972d10326abcceb024ad118839e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
