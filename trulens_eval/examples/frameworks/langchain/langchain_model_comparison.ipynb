{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Flan Model Sizes\n",
    "\n",
    "Here we'll build a simple app with langchain and load large and small flan.\n",
    "\n",
    "Then we'll ask it a few football questions and compare the quality of the responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No .env found in /Users/jreini/Desktop/development/trulens/trulens_eval/examples/vector-dbs/pinecone or its parents. You may need to specify secret keys in another manner.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from IPython.display import JSON\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Imports main tools:\n",
    "from trulens_eval import TruChain, Feedback, Huggingface, Tru\n",
    "# Imports main tools:\n",
    "from trulens_eval import Feedback\n",
    "from trulens_eval import feedback\n",
    "from trulens_eval import FeedbackMode\n",
    "from trulens_eval import Select\n",
    "from trulens_eval import TP\n",
    "from trulens_eval import Tru\n",
    "from trulens_eval.utils.langchain import WithFeedbackFilterDocuments\n",
    "\n",
    "# Tru object manages the database of apps, records, and feedbacks; and the\n",
    "# dashboard to display these\n",
    "tru = Tru()\n",
    "\n",
    "# Imports from langchain to build app. You may need to install langchain first\n",
    "# with the following:\n",
    "# ! pip install langchain>=0.0.170\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.prompts.chat import HumanMessagePromptTemplate\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Key OPENAI_API_KEY set explicitly.\n",
      "✅ Key HUGGINGFACE_API_KEY set explicitly.\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval.keys import setup_keys\n",
    "\n",
    "setup_keys(\n",
    "    OPENAI_API_KEY=\"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\",\n",
    "    HUGGINGFACE_API_KEY=\"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\"\n",
    ")\n",
    "\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = \"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\"\n",
    "os.environ['OPENAI_API_KEY'] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(\n",
    "        template=template,\n",
    "    input_variables=['question']\n",
    ")\n",
    "\n",
    "# user question\n",
    "question = \"Which NFL team won the Super Bowl in the 2010 season?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up feedback functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In language_match, input text1 will be set to *.__record__.main_input or `Select.RecordInput` .\n",
      "✅ In language_match, input text2 will be set to *.__record__.main_output or `Select.RecordOutput` .\n",
      "✅ In relevance, input prompt will be set to *.__record__.main_input or `Select.RecordInput` .\n",
      "✅ In relevance, input response will be set to *.__record__.main_output or `Select.RecordOutput` .\n",
      "✅ In qs_relevance, input question will be set to *.__record__.main_input or `Select.RecordInput` .\n",
      "✅ In qs_relevance, input statement will be set to *.__record__.app.combine_docs_chain._call.args.inputs.input_documents[:].page_content .\n"
     ]
    }
   ],
   "source": [
    "# API endpoints for models used in feedback functions:\n",
    "hugs = feedback.Huggingface()\n",
    "openai = feedback.OpenAI()\n",
    "\n",
    "# Language match between question/answer.\n",
    "f_lang_match = Feedback(hugs.language_match).on_input_output()\n",
    "# By default this will evaluate feedback on main app input and main app output.\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = Feedback(openai.relevance).on_input_output()\n",
    "# By default this will evaluate feedback on main app input and main app output.\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_qs_relevance = feedback.Feedback(openai.qs_relevance).on_input().on(\n",
    "    Select.Record.app.combine_docs_chain._call.args.inputs.input_documents[:].page_content\n",
    ").aggregate(np.min)\n",
    "# First feedback argument is set to main app input, and the second is taken from\n",
    "# the context sources as passed to an internal `combine_docs_chain._call`.\n",
    "\n",
    "all_feedbacks = [f_lang_match, f_qa_relevance, f_qs_relevance]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a couple sizes of Flan and ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ app google/flan-t5-small/v1 -> default.sqlite\n",
      "✅ feedback def. feedback_definition_hash_36e27643030c60771697d9b90efef699 -> default.sqlite\n",
      "✅ feedback def. feedback_definition_hash_4d0a28967bbc08fd2f0e93a95b9a94ab -> default.sqlite\n",
      "✅ feedback def. feedback_definition_hash_f6905e7a2bf42e13d252ecd56ebb5f25 -> default.sqlite\n",
      "✅ record record_hash_8dc34057bbe527fd1367079bf69d7816 from google/flan-t5-small/v1 -> default.sqlite\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Who won the heisman in 1995', 'text': 'samuel wilson'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ record record_hash_f9692dad61bba85cd437f780f28748d9 from google/flan-t5-small/v1 -> default.sqlite\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub, LLMChain\n",
    "\n",
    "model = 'google/flan-t5-small'\n",
    "\n",
    "# initialize Hub LLM\n",
    "hub_llm = HuggingFaceHub(\n",
    "        repo_id = model,\n",
    "    model_kwargs = {'temperature':1e-10}\n",
    ")\n",
    "\n",
    "# create prompt template > LLM chain\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=hub_llm\n",
    ")\n",
    "\n",
    "# Trulens instrumentation.\n",
    "tc = tru.Chain(\n",
    "        app_id=f\"{model}/v1\",\n",
    "        chain=llm_chain,\n",
    "        feedbacks=all_feedbacks\n",
    "    )\n",
    "\n",
    "tc('Who won the superbowl in 2010?')    \n",
    "tc('Who won the heisman in 1995')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ app google/flan-t5-large/v1 -> default.sqlite\n",
      "✅ feedback def. feedback_definition_hash_36e27643030c60771697d9b90efef699 -> default.sqlite\n",
      "✅ feedback def. feedback_definition_hash_4d0a28967bbc08fd2f0e93a95b9a94ab -> default.sqlite\n",
      "✅ feedback def. feedback_definition_hash_f6905e7a2bf42e13d252ecd56ebb5f25 -> default.sqlite\n",
      "✅ record record_hash_bf442db5bb9ce0eff4991649852c4685 from google/flan-t5-large/v1 -> default.sqlite\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Who won the heisman in 1995', 'text': 'joe hudson'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ record record_hash_9d0e8c8cf06a9776b9fc2d7b32c8c501 from google/flan-t5-large/v1 -> default.sqlite\n"
     ]
    }
   ],
   "source": [
    "model = 'google/flan-t5-large'\n",
    "\n",
    "# initialize Hub LLM\n",
    "hub_llm = HuggingFaceHub(\n",
    "        repo_id = model,\n",
    "    model_kwargs = {'temperature':1e-10}\n",
    ")\n",
    "\n",
    "# create prompt template > LLM chain\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=hub_llm\n",
    ")\n",
    "\n",
    "# Trulens instrumentation.\n",
    "tc = tru.Chain(\n",
    "        app_id=f\"{model}/v1\",\n",
    "        chain=llm_chain,\n",
    "        feedbacks=all_feedbacks\n",
    "    )\n",
    "\n",
    "tc('Who won the superbowl in 2010?')    \n",
    "tc('Who won the heisman in 1995')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load OpenAI Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ app text-davinci-003/v1 -> default.sqlite\n",
      "✅ feedback def. feedback_definition_hash_36e27643030c60771697d9b90efef699 -> default.sqlite\n",
      "✅ feedback def. feedback_definition_hash_4d0a28967bbc08fd2f0e93a95b9a94ab -> default.sqlite\n",
      "✅ feedback def. feedback_definition_hash_f6905e7a2bf42e13d252ecd56ebb5f25 -> default.sqlite\n",
      "✅ record record_hash_0bc9bb8a16eb76eb79cca70541cd64da from text-davinci-003/v1 -> default.sqlite\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Who won the heisman in 1995',\n",
       " 'text': ' Eddie George won the Heisman Trophy in 1995.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ record record_hash_7f40192b8d91c40ac6529aa67a6a3b7d from text-davinci-003/v1 -> default.sqlite\n"
     ]
    }
   ],
   "source": [
    "model = 'text-davinci-003'\n",
    "\n",
    "davinci = OpenAI(model_name=model)\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=davinci\n",
    ")\n",
    "\n",
    "# Trulens instrumentation.\n",
    "tc = tru.Chain(\n",
    "        app_id=f\"{model}/v1\",\n",
    "        chain=llm_chain,\n",
    "        feedbacks=all_feedbacks\n",
    "    )\n",
    "\n",
    "tc('Who won the superbowl in 2010?')    \n",
    "tc('Who won the heisman in 1995')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bcbf7c6c5e3400ea2b317dc7f525c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard started at http://192.168.4.23:8503 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.run_dashboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 ('trulens')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c633204c92f433e69d41413efde9db4a539ce972d10326abcceb024ad118839e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
