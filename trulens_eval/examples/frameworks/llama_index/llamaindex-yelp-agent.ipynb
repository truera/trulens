{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama-Index Agents + Ground Truth & Custom Evaluations\n",
    "\n",
    "In this example, we build an agent-based app with Llama Index to answer questions with the help of Yelp. We'll evaluate it using two feedback functions:\n",
    "\n",
    "1. Definitiveness - we want our app to respond with authority. We'll accomplish this with a simple, custom feedback function.\n",
    "2. Ground truth eval - we want to make sure our app responds correctly. We will create a ground truth set for this evaluation.\n",
    "\n",
    "Last, we'll compare the evaluation of this app against a standalone LLM. May the best bot win?\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/frameworks/llama_index/llamaindex-yelp-agent.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install trulens_eval==0.6.0 llama_index==0.7.11 llama_hub==0.0.13 yelpapi==2.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up our Llama-Index App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup OpenAI Agent\n",
    "import llama_index\n",
    "from llama_index.agent import OpenAIAgent\n",
    "from llama_index import question_gen\n",
    "from llama_index.question_gen import types\n",
    "import openai\n",
    "import os\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and initialize our tool spec\n",
    "from llama_hub.tools.yelp.base import YelpToolSpec\n",
    "from llama_index.tools.tool_spec.load_and_search.base import LoadAndSearchToolSpec\n",
    "\n",
    "# Add Yelp API key and client ID\n",
    "tool_spec = YelpToolSpec(api_key=\"...\",\n",
    "client_id=\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Agent with our tools\n",
    "tools = tool_spec.to_tool_list()\n",
    "agent = OpenAIAgent.from_tools(\n",
    "    [\n",
    "        *LoadAndSearchToolSpec.from_defaults(tools[0]).to_tool_list(),\n",
    "        *LoadAndSearchToolSpec.from_defaults(tools[1]).to_tool_list()\n",
    "    ],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a standalone GPT3.5 for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_standalone(prompt):\n",
    "    return openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a question and answer bot, and you answer concisely.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom yelp evaluations\n",
    "\n",
    "Here we'll set a number of custom evals specific to our problem, including:\n",
    "1. Query translation score: Check to make sure the query used by Yelp Business Search matches the user query.\n",
    "2. Check to see if Yelp ratings are included in the context returned by Yelp business search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Feedback, OpenAI, Tru, TruBasicApp, TruLlama\n",
    "from trulens_eval.feedback import GroundTruthAgreement\n",
    "\n",
    "tru = Tru()\n",
    "\n",
    "class OpenAI_custom(OpenAI):\n",
    "    def query_translation_score(self, question1: str, question2: str) -> float:\n",
    "        return float(openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Your job is to rate how similar two quesitons are on a scale of 1 to 10. Respond with the number only.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"QUESTION 1: {question1}; QUESTION 2: {question2}\"}\n",
    "        ]\n",
    "    )[\"choices\"][0][\"message\"][\"content\"]) / 10\n",
    "\n",
    "    def ratings_usage(self, last_context: str) -> float:\n",
    "        print(last_context)\n",
    "        return float(openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Your job is to respond with a '1' if the following statement mentions ratings or reviews, and a '0' if not.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"STATEMENT: {last_context}\"}\n",
    "        ]\n",
    "    )[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "custom = OpenAI_custom()\n",
    "from trulens_eval import Select\n",
    "query_translation_score = Feedback(custom.query_translation_score).on_input().on(\n",
    "    Select.Record.calls[0].args.str_or_query_bundle # check the query bundle passed to yelp api\n",
    ")\n",
    "ratings_usage = Feedback(custom.ratings_usage).on(\n",
    "    Select.App.app.chat_history[-1][\"content\"] # check the last content chunk for mentions of ratings or reviews\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground Truth Eval\n",
    "\n",
    "It's also useful in many cases to do ground truth eval with small golden sets. We'll do so here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_set = [\n",
    "    {\"query\": \"What's the vibe like at oprhan andy's in SF?\", \"response\": \"welcoming and friendly\"},\n",
    "    {\"query\": \"Is park tavern in San Fran open yet?\", \"response\": \"Yes\"},\n",
    "    {\"query\": \"I'm in san francisco for the morning, does Juniper serve pastries?\", \"response\": \"Yes\"},\n",
    "    {\"query\": \"What's the address of Gumbo Social in San Francisco?\", \"response\": \"5176 3rd St, San Francisco, CA 94124\"},\n",
    "    {\"query\": \"What are the reviews like of Gola in SF?\", \"response\": \"Excellent, 4.6/5\"},\n",
    "    {\"query\": \"Where's the best pizza in New York City\", \"response\": \"Joe's Pizza\"},\n",
    "    {\"query\": \"What's the best diner in Toronto?\", \"response\": \"The George Street Diner\"}\n",
    "]\n",
    "\n",
    "f_groundtruth = Feedback(GroundTruthAgreement(golden_set).agreement_measure).on_input_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Relevance Evals\n",
    "\n",
    "Last, we'll add in our standard relevance evals.\n",
    "\n",
    "We can use the Select function to capture deep context in our app for QS relevance, alongside our standard QA relevance on input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from trulens_eval import OpenAI as fOpenAI\n",
    "fopenai = fOpenAI()\n",
    "# Question/statement relevance between question and last context chunk (i.e. summary)\n",
    "f_qs_relevance = Feedback(fopenai.qs_relevance).on_input().on(\n",
    "    Select.App.app.chat_history[-1][\"content\"] # check the last context chunk\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = Feedback(fopenai.relevance).on_input_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the dashboard\n",
    "\n",
    "By running the dashboard before we start to make app calls, we can see them come in 1 by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrument Yelp App\n",
    "\n",
    "We can instrument our yelp app with TruLlama and utilize the full suite of evals we set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_app = TruLlama(agent,\n",
    "    app_id='YelpAgent',\n",
    "    tags = \"agent prototype\",\n",
    "    feedbacks = [f_qa_relevance, f_groundtruth, f_qs_relevance, query_translation_score, ratings_usage])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrument Standalone LLM app.\n",
    "\n",
    "Since we don't have insight into the OpenAI innerworkings, we cannot run many of the evals on intermediate steps.\n",
    "\n",
    "We can still do QA relevance on input and output, and check for similarity of the answers compared to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standalone_app = TruBasicApp(llm_standalone, app_id=\"OpenAIChatCompletion\", tags = \"comparison\", feedbacks=[f_qa_relevance, f_groundtruth])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start using our apps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_set = [\"What's the vibe like at oprhan andy's in SF?\",\n",
    "                \"What are the reviews like of Gola in SF?\",\n",
    "                \"Where's the best pizza in New York City\",\n",
    "                \"What's the address of Gumbo Social in San Francisco?\",\n",
    "                \"I'm in san francisco for the morning, does Juniper serve pastries?\",\n",
    "                \"What's the best diner in Toronto?\"\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in prompt_set:\n",
    "    standalone_app.call_with_record(prompt)\n",
    "    yelp_app.query(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "9404565ab2271c01f32a5ebf202ce74a5a811d6909f1828a5ff1298ed34c2b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
