{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PF9uQB4X2gJ"
   },
   "source": [
    "## Llama-Index Agents + Ground Truth & Custom Evaluations\n",
    "\n",
    "In this example, we build an agent-based app with Llama Index to answer questions with the help of Yelp. We'll evaluate it using a few different feedback functions (some custom, some out-of-the-box)\n",
    "\n",
    "The first set of feedback functions complete what the non-hallucination triad. However because we're dealing with agents here,  we've added a fourth leg (query translation) to cover the additional interaction between the query planner and the agent. This combination provides a foundation for eliminating hallucination in LLM applications.\n",
    "\n",
    "1. Query Translation - The first step. Here we compare the similarity of the original user query to the query sent to the agent. This ensures that we're providing the agent with the correct question.\n",
    "2. Context or QS Relevance - Next, we compare the relevance of the context provided by the agent back to the original query. This ensures that we're providing context for the right question.\n",
    "3. Groundedness - Third, we ensure that the final answer is supported by the context. This ensures that the LLM is not extending beyond the information provided by the agent.\n",
    "4. Question Answer Relevance - Last, we want to make sure that the final answer provided is relevant to the user query. This last step confirms that the answer is not only supported but also useful to the end user.\n",
    "\n",
    "In this example, we'll add two additional feedback functions.\n",
    "\n",
    "5. Ratings usage - evaluate if the summarized context uses ratings as justification. Note: this may not be relevant for all queries.\n",
    "6. Ground truth eval - we want to make sure our app responds correctly. We will create a ground truth set for this evaluation.\n",
    "\n",
    "Last, we'll compare the evaluation of this app against a standalone LLM. May the best bot win?\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/expositional/frameworks/llama_index/llama_index_agents.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7x7wjl4UX2gP"
   },
   "source": [
    "### Install TruLens and Llama-Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pn9BbqG3fKy1",
    "outputId": "3b827697-a069-4de0-fd71-9e4f460fe80c"
   },
   "outputs": [],
   "source": [
    "#! pip install trulens_eval==0.18.2 llama_index==0.9.11.post1 llama_hub==0.0.52 yelpapi==2.5.1 openai==1.3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fznbmy3UX2gR"
   },
   "outputs": [],
   "source": [
    "# If running from github repo, uncomment the below to setup paths.\n",
    "#from pathlib import Path\n",
    "#import sys\n",
    "#trulens_path = Path().cwd().parent.parent.parent.parent.resolve()\n",
    "#sys.path.append(str(trulens_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oY9A_hltX2gS"
   },
   "outputs": [],
   "source": [
    "# Setup OpenAI Agent\n",
    "import llama_index\n",
    "from llama_index.agent import OpenAIAgent\n",
    "import openai\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLgAZvErX2gS"
   },
   "outputs": [],
   "source": [
    "# Set your API keys. If you already have them in your var env., you can skip these steps.\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "os.environ[\"YELP_API_KEY\"] = \"...\"\n",
    "os.environ[\"YELP_CLIENT_ID\"] = \"...\"\n",
    "\n",
    "# If you already have keys in var env., use these to check instead:\n",
    "# from trulens_eval.keys import check_keys\n",
    "# check_keys(\"OPENAI_API_KEY\", \"YELP_API_KEY\", \"YELP_CLIENT_ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sZ9RF_6X2gT"
   },
   "source": [
    "### Set up our Llama-Index App\n",
    "\n",
    "For this app, we will use a tool from Llama-Index to connect to Yelp and allow the Agent to search for business and fetch reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5aUsWQlwX2gU"
   },
   "outputs": [],
   "source": [
    "# Import and initialize our tool spec\n",
    "from llama_hub.tools.yelp.base import YelpToolSpec\n",
    "from llama_index.tools.tool_spec.load_and_search.base import LoadAndSearchToolSpec\n",
    "\n",
    "# Add Yelp API key and client ID\n",
    "tool_spec = YelpToolSpec(\n",
    "    api_key=os.environ.get(\"YELP_API_KEY\"),\n",
    "    client_id=os.environ.get(\"YELP_CLIENT_ID\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pM8rXmCEX2gU"
   },
   "outputs": [],
   "source": [
    "gordon_ramsay_prompt = \"You answer questions about restaurants in the style of Gordon Ramsay, often insulting the asker.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LcgFJ7K5X2gV"
   },
   "outputs": [],
   "source": [
    "# Create the Agent with our tools\n",
    "tools = tool_spec.to_tool_list()\n",
    "agent = OpenAIAgent.from_tools([\n",
    "        *LoadAndSearchToolSpec.from_defaults(tools[0]).to_tool_list(),\n",
    "        *LoadAndSearchToolSpec.from_defaults(tools[1]).to_tool_list()\n",
    "    ],\n",
    "    verbose=True,\n",
    "    system_prompt=gordon_ramsay_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNd7EWzzX2gW"
   },
   "source": [
    "### Create a standalone GPT3.5 for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iyuYQ_j_g4Ms"
   },
   "outputs": [],
   "source": [
    "client = openai.OpenAI()\n",
    "\n",
    "chat_completion = client.chat.completions.create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1qTWpGxX2gW"
   },
   "outputs": [],
   "source": [
    "from trulens_eval.tru_custom_app import TruCustomApp, instrument\n",
    "\n",
    "class LLMStandaloneApp():\n",
    "    @instrument\n",
    "    def __call__(self, prompt):\n",
    "        return chat_completion(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                    {\"role\": \"system\", \"content\": gordon_ramsay_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "        ).choices[0].message.content\n",
    "\n",
    "llm_standalone = LLMStandaloneApp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2bxeKoPX2gX"
   },
   "source": [
    "## Evaluation and Tracking with TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lByWI1c8X2gX",
    "outputId": "d1158dc1-d08b-4e7b-e8e9-c58bd02ef63b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n",
      "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of `Tru` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "# imports required for tracking and evaluation\n",
    "from trulens_eval import Feedback, OpenAI, Tru, TruLlama, Select, OpenAI as fOpenAI\n",
    "from trulens_eval.feedback import GroundTruthAgreement, Groundedness\n",
    "\n",
    "tru = Tru()\n",
    "# tru.reset_database() # if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gK_dfR06X2gX"
   },
   "source": [
    "## Evaluation setup\n",
    "\n",
    "To set up our evaluation, we'll first create two new custom feedback functions: query_translation_score and ratings_usage. These are straight-forward prompts of the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ccpVccEgX2gX"
   },
   "outputs": [],
   "source": [
    "class OpenAI_custom(OpenAI):\n",
    "    def query_translation_score(self, question1: str, question2: str) -> float:\n",
    "        return float(chat_completion(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Your job is to rate how similar two quesitons are on a scale of 1 to 10. Respond with the number only.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"QUESTION 1: {question1}; QUESTION 2: {question2}\"}\n",
    "                ]\n",
    "        ).choices[0].message.content) / 10\n",
    "\n",
    "    def ratings_usage(self, last_context: str) -> float:\n",
    "        return float(chat_completion(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Your job is to respond with a '1' if the following statement mentions ratings or reviews, and a '0' if not.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"STATEMENT: {last_context}\"}\n",
    "            ]\n",
    "        ).choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LR7RpFwX2gY"
   },
   "source": [
    "Now that we have all of our feedback functions available, we can instantiate them. For many of our evals, we want to check on intermediate parts of our app such as the query passed to the yelp app, or the summarization of the Yelp content. We'll do so here using Select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4qKkPU4oX2gY",
    "outputId": "a6c2a5ef-9092-4bac-82cf-7a15fbbfa99e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In Query Translation, input question1 will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Query Translation, input question2 will be set to __record__.app.query[0].args.str_or_query_bundle .\n",
      "âœ… In Ratings Usage, input last_context will be set to __record__.app.query[0].rets.response .\n",
      "âœ… In Context Relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Context Relevance, input statement will be set to __record__.app.query[0].rets.response .\n",
      "âœ… In Groundedness, input source will be set to __record__.app.query[0].rets.response .\n",
      "âœ… In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "âœ… In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "# unstable: perhaps reduce temperature?\n",
    "\n",
    "custom = OpenAI_custom()\n",
    "# Input to tool based on trimmed user input.\n",
    "f_query_translation = Feedback(\n",
    "    custom.query_translation_score,\n",
    "    name=\"Query Translation\") \\\n",
    ".on_input() \\\n",
    ".on(Select.Record.app.query[0].args.str_or_query_bundle)\n",
    "\n",
    "f_ratings_usage = Feedback(\n",
    "    custom.ratings_usage,\n",
    "    name=\"Ratings Usage\") \\\n",
    ".on(Select.Record.app.query[0].rets.response)\n",
    "\n",
    "# Result of this prompt: Given the context information and not prior knowledge, answer the query.\n",
    "# Query: address of Gumbo Social\n",
    "# Answer: \"\n",
    "fopenai = fOpenAI()\n",
    "# Question/statement (context) relevance between question and last context chunk (i.e. summary)\n",
    "f_context_relevance = Feedback(\n",
    "    fopenai.qs_relevance,\n",
    "    name=\"Context Relevance\") \\\n",
    ".on_input() \\\n",
    ".on(Select.Record.app.query[0].rets.response)\n",
    "\n",
    "# Groundedness\n",
    "grounded = Groundedness(groundedness_provider=fopenai)\n",
    "\n",
    "f_groundedness = Feedback(\n",
    "    grounded.groundedness_measure,\n",
    "    name=\"Groundedness\") \\\n",
    ".on(Select.Record.app.query[0].rets.response) \\\n",
    ".on_output().aggregate(grounded.grounded_statements_aggregator)\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = Feedback(\n",
    "    fopenai.relevance,\n",
    "    name=\"Answer Relevance\"\n",
    ").on_input_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3wNcOTzX2gY"
   },
   "source": [
    "### Ground Truth Eval\n",
    "\n",
    "It's also useful in many cases to do ground truth eval with small golden sets. We'll do so here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NGkx8fi0X2gY",
    "outputId": "1152da3c-f42d-4815-a5ed-16dc39dcb9e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In Ground Truth Eval, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Ground Truth Eval, input response will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "golden_set = [\n",
    "    {\"query\": \"Hello there mister AI. What's the vibe like at oprhan andy's in SF?\", \"response\": \"welcoming and friendly\"},\n",
    "    {\"query\": \"Is park tavern in San Fran open yet?\", \"response\": \"Yes\"},\n",
    "    {\"query\": \"I'm in san francisco for the morning, does Juniper serve pastries?\", \"response\": \"Yes\"},\n",
    "    {\"query\": \"What's the address of Gumbo Social in San Francisco?\", \"response\": \"5176 3rd St, San Francisco, CA 94124\"},\n",
    "    {\"query\": \"What are the reviews like of Gola in SF?\", \"response\": \"Excellent, 4.6/5\"},\n",
    "    {\"query\": \"Where's the best pizza in New York City\", \"response\": \"Joe's Pizza\"},\n",
    "    {\"query\": \"What's the best diner in Toronto?\", \"response\": \"The George Street Diner\"}\n",
    "]\n",
    "\n",
    "f_groundtruth = Feedback(\n",
    "    GroundTruthAgreement(golden_set).agreement_measure,\n",
    "    name=\"Ground Truth Eval\") \\\n",
    ".on_input_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCcgA40zX2gY"
   },
   "source": [
    "### Run the dashboard\n",
    "\n",
    "By running the dashboard before we start to make app calls, we can see them come in 1 by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EwBDGkDaX2gZ",
    "outputId": "39e0e9e9-0a8f-4cde-94a7-4a832cb6ec38"
   },
   "outputs": [],
   "source": [
    "tru.run_dashboard(\n",
    "#     _dev=trulens_path, force=True  # if running from github\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8V7ch-kX2gZ"
   },
   "source": [
    "### Instrument Yelp App\n",
    "\n",
    "We can instrument our yelp app with TruLlama and utilize the full suite of evals we set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DGHRn6g7X2gZ"
   },
   "outputs": [],
   "source": [
    "tru_agent = TruLlama(agent,\n",
    "    app_id='YelpAgent',\n",
    "    tags = \"agent prototype\",\n",
    "    feedbacks = [\n",
    "        f_qa_relevance,\n",
    "        f_groundtruth,\n",
    "        f_context_relevance,\n",
    "        f_groundedness,\n",
    "        f_query_translation,\n",
    "        f_ratings_usage\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmKbsnVlX2gZ",
    "outputId": "46452f07-3a77-407a-8852-cb40d4d5cb6d"
   },
   "outputs": [],
   "source": [
    "tru_agent.print_instrumented()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKP5E4gQX2gZ"
   },
   "source": [
    "### Instrument Standalone LLM app.\n",
    "\n",
    "Since we don't have insight into the OpenAI innerworkings, we cannot run many of the evals on intermediate steps.\n",
    "\n",
    "We can still do QA relevance on input and output, and check for similarity of the answers compared to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DSjKuay-X2gZ"
   },
   "outputs": [],
   "source": [
    "tru_llm_standalone = TruCustomApp(\n",
    "    llm_standalone,\n",
    "    app_id=\"OpenAIChatCompletion\",\n",
    "    tags = \"comparison\",\n",
    "    feedbacks=[\n",
    "        f_qa_relevance,\n",
    "        f_groundtruth\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "idXs2pgHX2ga",
    "outputId": "b576a43c-872c-4102-9684-b2cceb115ded"
   },
   "outputs": [],
   "source": [
    "tru_llm_standalone.print_instrumented()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFFJEuM0X2ga"
   },
   "source": [
    "### Start using our apps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hRj2AiNX2ga"
   },
   "outputs": [],
   "source": [
    "prompt_set = [\n",
    "    \"What's the vibe like at oprhan andy's in SF?\",\n",
    "    \"What are the reviews like of Gola in SF?\",\n",
    "    \"Where's the best pizza in New York City\",\n",
    "    \"What's the address of Gumbo Social in San Francisco?\",\n",
    "    \"I'm in san francisco for the morning, does Juniper serve pastries?\",\n",
    "    \"What's the best diner in Toronto?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lX1RQ875X2ga",
    "outputId": "332ddfb8-e4fc-48ad-e3f6-c95942006e41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the vibe like at oprhan andy's in SF?\n",
      "STARTING TURN 1\n",
      "---------------\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: business_search with args: {\n",
      "  \"location\": \"San Francisco\",\n",
      "  \"term\": \"Orphan Andy's\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /tmp/llama_index...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got output: Content loaded! You can now search the information using read_business_search\n",
      "========================\n",
      "\n",
      "STARTING TURN 2\n",
      "---------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:trulens_eval.app:A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x7bd28d453700 is calling an instrumented method <function BaseQueryEngine.query at 0x7bd2ec904e50>. The path of this call may be incorrect.\n",
      "WARNING:trulens_eval.app:Guessing path of new object is app based on other object (0x7bd2ad7617e0) using this function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: read_business_search with args: {\n",
      "  \"query\": \"What's the vibe like at Orphan Andy's in SF?\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:trulens_eval.app:A new object of type <class 'llama_index.llms.openai.OpenAI'> at 0x7bd2a079c680 is calling an instrumented method <function llm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat at 0x7bd2c201f7f0>. The path of this call may be incorrect.\n",
      "WARNING:trulens_eval.app:Guessing path of new object is app._llm based on other object (0x7bd2ded85e40) using this function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got output: The vibe at Orphan Andy's in San Francisco is not provided in the given context information.\n",
      "========================\n",
      "\n",
      "STARTING TURN 3\n",
      "---------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:trulens_eval.feedback.groundedness:Feedback function `groundedness_measure` was renamed to `groundedness_measure_with_cot_reasons`. The new functionality of `groundedness_measure` function will no longer emit reasons as a lower cost option. It may have reduced accuracy due to not using Chain of Thought reasoning in the scoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the reviews like of Gola in SF?\n",
      "STARTING TURN 1\n",
      "---------------\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: business_search with args: {\n",
      "  \"location\": \"San Francisco\",\n",
      "  \"term\": \"Gola\"\n",
      "}\n",
      "Got output: Content loaded! You can now search the information using read_business_search\n",
      "========================\n",
      "\n",
      "STARTING TURN 2\n",
      "---------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:trulens_eval.app:A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x7bd296042890 is calling an instrumented method <function BaseQueryEngine.query at 0x7bd2ec904e50>. The path of this call may be incorrect.\n",
      "WARNING:trulens_eval.app:Guessing path of new object is app based on other object (0x7bd2ad7617e0) using this function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: read_business_search with args: {\n",
      "  \"query\": \"reviews of Gola in SF\"\n",
      "}\n",
      "Got output: Gola in San Francisco has received 54 reviews.\n",
      "========================\n",
      "\n",
      "STARTING TURN 3\n",
      "---------------\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: business_reviews with args: {\n",
      "  \"id\": \"Gola-san-francisco\"\n",
      "}\n",
      "Got output: Content loaded! You can now search the information using read_business_reviews\n",
      "========================\n",
      "\n",
      "STARTING TURN 4\n",
      "---------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:trulens_eval.app:A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x7bd295f131c0 is calling an instrumented method <function BaseQueryEngine.query at 0x7bd2ec904e50>. The path of this call may be incorrect.\n",
      "WARNING:trulens_eval.app:Guessing path of new object is app based on other object (0x7bd2ad7617e0) using this function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: read_business_reviews with args: {\n",
      "  \"query\": \"reviews of Gola in SF\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:trulens_eval.app:A new object of type <class 'llama_index.llms.openai.OpenAI'> at 0x7bd2961a76f0 is calling an instrumented method <function llm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat at 0x7bd2c201f7f0>. The path of this call may be incorrect.\n",
      "WARNING:trulens_eval.app:Guessing path of new object is app._llm based on other object (0x7bd2ded85e40) using this function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got output: There are several reviews of Gola in San Francisco. One reviewer mentioned that the food was average-above average, but the service was not great. Another reviewer mentioned that the dishes were amazing and they were impressed from the beginning to the end. Another reviewer described the food as absolutely delicious and flavorful, and mentioned that the restaurant is a gem in the Mission neighborhood. Overall, there are positive reviews of Gola in San Francisco.\n",
      "========================\n",
      "\n",
      "STARTING TURN 5\n",
      "---------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:trulens_eval.feedback.groundedness:Feedback function `groundedness_measure` was renamed to `groundedness_measure_with_cot_reasons`. The new functionality of `groundedness_measure` function will no longer emit reasons as a lower cost option. It may have reduced accuracy due to not using Chain of Thought reasoning in the scoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where's the best pizza in New York City\n",
      "STARTING TURN 1\n",
      "---------------\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: business_search with args: {\n",
      "  \"location\": \"New York City\",\n",
      "  \"term\": \"pizza\"\n",
      "}\n",
      "Got output: Content loaded! You can now search the information using read_business_search\n",
      "========================\n",
      "\n",
      "STARTING TURN 2\n",
      "---------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:trulens_eval.app:A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x7bd29616b1f0 is calling an instrumented method <function BaseQueryEngine.query at 0x7bd2ec904e50>. The path of this call may be incorrect.\n",
      "WARNING:trulens_eval.app:Guessing path of new object is app based on other object (0x7bd2ad7617e0) using this function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: read_business_search with args: {\n",
      "  \"query\": \"What are the best pizza places in New York City?\"\n",
      "}\n",
      "Got output: Some of the best pizza places in New York City include Joe's Pizza, Juliana's - Time Out Market, Scarr's Pizza, Grimaldi's Pizzeria, Rubirosa, and Lombardi's Pizza.\n",
      "========================\n",
      "\n",
      "STARTING TURN 3\n",
      "---------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:trulens_eval.feedback.groundedness:Feedback function `groundedness_measure` was renamed to `groundedness_measure_with_cot_reasons`. The new functionality of `groundedness_measure` function will no longer emit reasons as a lower cost option. It may have reduced accuracy due to not using Chain of Thought reasoning in the scoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the address of Gumbo Social in San Francisco?\n",
      "STARTING TURN 1\n",
      "---------------\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: business_search with args: {\n",
      "  \"location\": \"San Francisco\",\n",
      "  \"term\": \"Gumbo Social\"\n",
      "}\n",
      "Got output: Content loaded! You can now search the information using read_business_search\n",
      "========================\n",
      "\n",
      "STARTING TURN 2\n",
      "---------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:trulens_eval.app:A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x7bd295fba4d0 is calling an instrumented method <function BaseQueryEngine.query at 0x7bd2ec904e50>. The path of this call may be incorrect.\n",
      "WARNING:trulens_eval.app:Guessing path of new object is app based on other object (0x7bd2ad7617e0) using this function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: read_business_search with args: {\n",
      "  \"query\": \"What is the address of Gumbo Social in San Francisco?\"\n",
      "}\n",
      "Got output: The address of Gumbo Social in San Francisco is 5176 3rd St, San Francisco, CA 94124.\n",
      "========================\n",
      "\n",
      "STARTING TURN 3\n",
      "---------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:trulens_eval.feedback.groundedness:Feedback function `groundedness_measure` was renamed to `groundedness_measure_with_cot_reasons`. The new functionality of `groundedness_measure` function will no longer emit reasons as a lower cost option. It may have reduced accuracy due to not using Chain of Thought reasoning in the scoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm in san francisco for the morning, does Juniper serve pastries?\n",
      "STARTING TURN 1\n",
      "---------------\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: business_search with args: {\n",
      "  \"location\": \"san francisco\",\n",
      "  \"term\": \"Juniper\"\n",
      "}\n",
      "Got output: Error: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "========================\n",
      "\n",
      "STARTING TURN 2\n",
      "---------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:trulens_eval.feedback.groundedness:Feedback function `groundedness_measure` was renamed to `groundedness_measure_with_cot_reasons`. The new functionality of `groundedness_measure` function will no longer emit reasons as a lower cost option. It may have reduced accuracy due to not using Chain of Thought reasoning in the scoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the best diner in Toronto?\n",
      "STARTING TURN 1\n",
      "---------------\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: business_search with args: {\n",
      "  \"location\": \"Toronto\",\n",
      "  \"term\": \"diner\"\n",
      "}\n",
      "Got output: Content loaded! You can now search the information using read_business_search\n",
      "========================\n",
      "\n",
      "STARTING TURN 2\n",
      "---------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:trulens_eval.app:A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x7bd295fba740 is calling an instrumented method <function BaseQueryEngine.query at 0x7bd2ec904e50>. The path of this call may be incorrect.\n",
      "WARNING:trulens_eval.app:Guessing path of new object is app based on other object (0x7bd2ad7617e0) using this function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: read_business_search with args: {\n",
      "  \"query\": \"best diner in Toronto\"\n",
      "}\n",
      "Got output: White Lily Diner is the best diner in Toronto.\n",
      "========================\n",
      "\n",
      "STARTING TURN 3\n",
      "---------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:trulens_eval.feedback.groundedness:Feedback function `groundedness_measure` was renamed to `groundedness_measure_with_cot_reasons`. The new functionality of `groundedness_measure` function will no longer emit reasons as a lower cost option. It may have reduced accuracy due to not using Chain of Thought reasoning in the scoring.\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompt_set:\n",
    "    print(prompt)\n",
    "\n",
    "    with tru_llm_standalone as recording:\n",
    "        llm_standalone(prompt)\n",
    "    record_standalone = recording.get()\n",
    "\n",
    "    with tru_agent as recording:\n",
    "         agent.query(prompt)\n",
    "    record_agent = recording.get()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "7d153714b979d5e6d08dd8ec90712dd93bff2c9b6c1f0c118169738af3430cd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
