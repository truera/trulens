{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlamaIndex Hybrid Retriever + Reranking\n",
    "\n",
    "Hybrid Retrievers are a great way to combine the strenghts of different retrievers. Combined with filtering and reranking, this can be especially powerful in retrieving only the most relevant context from multiple methods. TruLens can take us even farther to highlight the strengths of each component retriever along with measuring the success of the hybrid retriever. This example walks through that process.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/expositional/frameworks/llama_index/llama_index_hybrid_retriever.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install trulens_eval llama_index llama-index-readers-file llama-index-llms-openai llama-index-retrievers-bm25 openai pypdf torch sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/snowday/lib/python3.11/site-packages/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n",
      "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of `Tru` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "# Imports main tools:\n",
    "from trulens_eval import TruLlama, Feedback, Huggingface, Tru\n",
    "from trulens_eval.schema.feedback import FeedbackResult\n",
    "tru = Tru()\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 20.7M  100 20.7M    0     0  27.4M      0 --:--:-- --:--:-- --:--:-- 27.4M      0      0 --:--:-- --:--:-- --:--:--     0\n"
     ]
    }
   ],
   "source": [
    "!curl https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf --output IPCC_AR6_WGII_Chapter03.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"IPCC_AR6_WGII_Chapter03.pdf\"]\n",
    ").load_data()\n",
    "\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "# initialize storage context (by default it's in-memory)\n",
    "storage_context = StorageContext.from_defaults()\n",
    "storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "index = VectorStoreIndex(\n",
    "    nodes=nodes,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retireve the top 10 most similar nodes using embeddings\n",
    "vector_retriever = VectorIndexRetriever(index)\n",
    "\n",
    "# retireve the top 2 most similar nodes using bm25\n",
    "bm25_retriever = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Hybrid (Custom) Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import BaseRetriever\n",
    "\n",
    "class HybridRetriever(BaseRetriever):\n",
    "    def __init__(self, vector_retriever, bm25_retriever):\n",
    "        self.vector_retriever = vector_retriever\n",
    "        self.bm25_retriever = bm25_retriever\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query, **kwargs):\n",
    "        bm25_nodes = self.bm25_retriever.retrieve(query, **kwargs)\n",
    "        vector_nodes = self.vector_retriever.retrieve(query, **kwargs)\n",
    "\n",
    "        # combine the two lists of nodes\n",
    "        all_nodes = []\n",
    "        node_ids = set()\n",
    "        for n in bm25_nodes + vector_nodes:\n",
    "            if n.node.node_id not in node_ids:\n",
    "                all_nodes.append(n)\n",
    "                node_ids.add(n.node.node_id)\n",
    "        return all_nodes\n",
    "\n",
    "index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "hybrid_retriever = HybridRetriever(vector_retriever, bm25_retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/snowday/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "reranker = SentenceTransformerRerank(top_n=2, model=\"BAAI/bge-reranker-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=hybrid_retriever,\n",
    "    node_postprocessors=[reranker]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n",
      "Config file already exists. Skipping writing process.\n",
      "Credentials file already exists. Skipping writing process.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dabee6e2c0e49fc995f5fbc8f45548a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valuâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard started at http://192.168.4.206:1234 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.run_dashboard(port=1234)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Context Relevance checks\n",
    "\n",
    "Include relevance checks for bm25, vector retrievers, hybrid retriever and the filtered hybrid retriever (after rerank and filter).\n",
    "\n",
    "This requires knowing the feedback selector for each. You can find this path by logging a run of your application and examining the application traces on the Evaluations page.\n",
    "\n",
    "Read more in our docs: https://www.trulens.org/trulens_eval/evaluation/feedback_selectors/selecting_components/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In BM25, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In BM25, input context will be set to __record__.app._retriever.bm25_retriever.retrieve.rets[:].node.text .\n",
      "âœ… In Vector, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Vector, input context will be set to __record__.app._retriever.vector_retriever._retrieve.rets[:].node.text .\n",
      "âœ… In Hybrid, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Hybrid, input context will be set to __record__.app._retriever.retrieve.rets[:].node.text .\n",
      "âœ… In Hybrid Filtered, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Hybrid Filtered, input context will be set to __record__.app._node_postprocessors[0]._postprocess_nodes.rets[:].node.text .\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval.feedback.provider import OpenAI\n",
    "from trulens_eval.schema.feedback import Select\n",
    "import numpy as np\n",
    "\n",
    "# Initialize provider class\n",
    "openai = OpenAI()\n",
    "\n",
    "bm25_context = Select.RecordCalls._retriever.bm25_retriever.retrieve.rets[:].node.text\n",
    "vector_context = Select.RecordCalls._retriever.vector_retriever._retrieve.rets[:].node.text\n",
    "hybrid_context = Select.RecordCalls._retriever.retrieve.rets[:].node.text\n",
    "hybrid_context_filtered = Select.RecordCalls._node_postprocessors[0]._postprocess_nodes.rets[:].node.text\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance_bm25 = (\n",
    "    Feedback(openai.context_relevance, name = \"BM25\")\n",
    "    .on_input()\n",
    "    .on(bm25_context)\n",
    "    .aggregate(np.mean)\n",
    "    )\n",
    "\n",
    "f_context_relevance_vector = (\n",
    "    Feedback(openai.context_relevance, name = \"Vector\")\n",
    "    .on_input()\n",
    "    .on(vector_context)\n",
    "    .aggregate(np.mean)\n",
    "    )\n",
    "\n",
    "f_context_relevance_hybrid = (\n",
    "    Feedback(openai.context_relevance, name = \"Hybrid\")\n",
    "    .on_input()\n",
    "    .on(hybrid_context)\n",
    "    .aggregate(np.mean)\n",
    "    )\n",
    "\n",
    "f_context_relevance_hybrid_filtered = (\n",
    "    Feedback(openai.context_relevance, name = \"Hybrid Filtered\")\n",
    "    .on_input()\n",
    "    .on(hybrid_context_filtered)\n",
    "    .aggregate(np.mean)\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add feedbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_recorder = TruLlama(query_engine,\n",
    "    app_id='Hybrid Retriever Query Engine',\n",
    "    feedbacks=[f_context_relevance_bm25, f_context_relevance_vector, f_context_relevance_hybrid, f_context_relevance_hybrid_filtered])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_recorder as recording:\n",
    "    response = query_engine.query(\"What is the impact of climate change on the ocean?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore in a Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n",
      "Config file already exists. Skipping writing process.\n",
      "Credentials file already exists. Skipping writing process.\n",
      "Dashboard already running at path:   Network URL: http://192.168.4.206:1234\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.run_dashboard() # open a local streamlit app to explore\n",
    "\n",
    "# tru.stop_dashboard() # stop if needed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can run `trulens-eval` from a command line in the same folder to start the dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback Guardrails: an alternative to reranking/filtering\n",
    "\n",
    "TruLens feedback functions can be used as context filters in place of reranking. This is great for cases when you don't want to deal with another model (the reranker) or in cases when the feedback function is better aligned to human scores than a reranker. Notably, this feedback function can be any model of your choice - this is a great use of small, lightweight models that don't add as much latency to your app.\n",
    "\n",
    "To illustrate this, we'll set up a new query engine with only the hybrid retriever (no reranking)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=hybrid_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll set up a feedback function and wrap the query engine with TruLens' `WithFeedbackFilterNodes`. This allows us to pass in any feedback function we'd like to use for filtering, even custom ones!\n",
    "\n",
    "In this example, we're using LLM-as-judge context relevance, but a small local model could be used here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.guardrails.llama import WithFeedbackFilterNodes\n",
    "\n",
    "feedback = (\n",
    "    Feedback(openai.context_relevance)\n",
    ")\n",
    "\n",
    "filtered_query_engine = WithFeedbackFilterNodes(query_engine,\n",
    "    feedback=feedback,\n",
    "    threshold=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up for recording\n",
    "\n",
    "Here we'll introduce one last variation of the context relevance feedback function, this one pointed at the returned source nodes from the query engine's `synthesize` method. This will accurately capture which retrieved context gets past the filter and to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In After guardrails, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In After guardrails, input context will be set to __record__.app.query_engine.synthesize.rets.source_nodes[:].node.text .\n"
     ]
    }
   ],
   "source": [
    "hybrid_context_filtered = Select.Record.app.query_engine.synthesize.rets.source_nodes[:].node.text\n",
    "\n",
    "\n",
    "f_context_relevance_afterguardrails = (\n",
    "    Feedback(openai.context_relevance, name = \"After guardrails\")\n",
    "    .on_input()\n",
    "    .on(hybrid_context_filtered)\n",
    "    .aggregate(np.mean)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_recorder = TruLlama(filtered_query_engine,\n",
    "    app_id='Hybrid Retriever Query Engine with Guardrails',\n",
    "    feedbacks=[f_context_relevance_afterguardrails])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_recorder as recording:\n",
    "    response = filtered_query_engine.query(\"What is the impact of climate change on the ocean\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "d5737f6101ac92451320b0e41890107145710b89f85909f3780d702e7818f973"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
