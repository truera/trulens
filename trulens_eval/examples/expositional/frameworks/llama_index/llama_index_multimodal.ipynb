{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Multi-Modal RAG\n",
    "\n",
    "In this notebook guide, weâ€™ll demonstrate how to evaluate a Llama-Index Multi-Modal RAG system with TruLens.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/expositional/frameworks/llama_index/llama_index_multimodal.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install trulens_eval llama_index ftfy regex tqdm git+https://github.com/openai/CLIP.git torch torchvision matplotlib scikit-image qdrant_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case: Spelling In ASL\n",
    "\n",
    "In this demonstration, we will build a RAG application for teaching how to sign the alphabet of the American Sign Language (ASL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_STR_TEMPLATE = \"How can I sign a {symbol}?.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images\n",
    "\n",
    "The images were taken from ASL-Alphabet Kaggle dataset. Note, that they were modified to simply include a label of the associated letter on the hand gesture image. These altered images are what we use as context to the user queries, and they can be downloaded from our google drive (see below cell, which you can uncomment to download the dataset directly from this notebook).\n",
    "\n",
    "## Text Context\n",
    "\n",
    "For text context, we use descriptions of each of the hand gestures sourced from https://www.deafblind.com/asl.html. We have conveniently stored these in a json file called asl_text_descriptions.json which is included in the zip download from our google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_notebook_data = True\n",
    "if download_notebook_data:\n",
    "    !wget \"https://www.dropbox.com/scl/fo/tpesl5m8ye21fqza6wq6j/h?rlkey=zknd9pf91w30m23ebfxiva9xn&dl=1\" -O asl_data.zip -q\n",
    "!unzip asl_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from llama_index.multi_modal_llms.generic_utils import (\n",
    "    load_image_urls,\n",
    ")\n",
    "from llama_index import SimpleDirectoryReader, Document\n",
    "\n",
    "# context images\n",
    "image_path = \"./asl_data/images\"\n",
    "image_documents = SimpleDirectoryReader(image_path).load_data()\n",
    "\n",
    "# context text\n",
    "with open(\"asl_data/asl_text_descriptions.json\") as json_file:\n",
    "    asl_text_descriptions = json.load(json_file)\n",
    "text_format_str = \"To sign {letter} in ASL: {desc}.\"\n",
    "text_documents = [\n",
    "    Document(text=text_format_str.format(letter=k, desc=v))\n",
    "    for k, v in asl_text_descriptions.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our documents in hand, we can create our MultiModalVectorStoreIndex. To do so, we parse our Documents into nodes and then simply pass these nodes to the MultiModalVectorStoreIndex constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.multi_modal.base import MultiModalVectorStoreIndex\n",
    "from llama_index.node_parser import SentenceSplitter\n",
    "\n",
    "node_parser = SentenceSplitter.from_defaults()\n",
    "image_nodes = node_parser.get_nodes_from_documents(image_documents)\n",
    "text_nodes = node_parser.get_nodes_from_documents(text_documents)\n",
    "\n",
    "asl_index = MultiModalVectorStoreIndex(image_nodes + text_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "## Set load_previously_generated_text_descriptions to True if you    ##\n",
    "## would rather use previously generated gpt-4v text descriptions    ##\n",
    "## that are included in the .zip download                            ##\n",
    "#######################################################################\n",
    "\n",
    "load_previously_generated_text_descriptions = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "from llama_index.schema import ImageDocument\n",
    "import tqdm\n",
    "\n",
    "if not load_previously_generated_text_descriptions:\n",
    "    # define our lmm\n",
    "    openai_mm_llm = OpenAIMultiModal(\n",
    "        model=\"gpt-4-vision-preview\", max_new_tokens=300\n",
    "    )\n",
    "\n",
    "    # make a new copy since we want to store text in its attribute\n",
    "    image_with_text_documents = SimpleDirectoryReader(image_path).load_data()\n",
    "\n",
    "    # get text desc and save to text attr\n",
    "    for img_doc in tqdm.tqdm(image_with_text_documents):\n",
    "        response = openai_mm_llm.complete(\n",
    "            prompt=\"Describe the images as an alternative text\",\n",
    "            image_documents=[img_doc],\n",
    "        )\n",
    "        img_doc.text = response.text\n",
    "\n",
    "    # save so don't have to incur expensive gpt-4v calls again\n",
    "    desc_jsonl = [\n",
    "        json.loads(img_doc.to_json()) for img_doc in image_with_text_documents\n",
    "    ]\n",
    "    with open(\"image_descriptions.json\", \"w\") as f:\n",
    "        json.dump(desc_jsonl, f)\n",
    "else:\n",
    "    # load up previously saved image descriptions and documents\n",
    "    with open(\"asl_data/image_descriptions.json\") as f:\n",
    "        image_descriptions = json.load(f)\n",
    "\n",
    "    image_with_text_documents = [\n",
    "        ImageDocument.from_dict(el) for el in image_descriptions\n",
    "    ]\n",
    "\n",
    "# parse into nodes\n",
    "image_with_text_nodes = node_parser.get_nodes_from_documents(\n",
    "    image_with_text_documents\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A keen reader will notice that we stored the text descriptions within the text field of an ImageDocument. As we did before, to create a MultiModalVectorStoreIndex, we'll need to parse the ImageDocuments as ImageNodes, and thereafter pass the nodes to the constructor.\n",
    "\n",
    "Note that when ImageNodess that have populated text fields are used to build a MultiModalVectorStoreIndex, we can choose to use this text to build embeddings on that will be used for retrieval. To so, we just specify the class attribute is_image_to_text to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_with_text_nodes = node_parser.get_nodes_from_documents(\n",
    "    image_with_text_documents\n",
    ")\n",
    "\n",
    "asl_text_desc_index = MultiModalVectorStoreIndex(\n",
    "    nodes=image_with_text_nodes + text_nodes, is_image_to_text=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Our Multi-Modal RAG Systems\n",
    "\n",
    "As in the text-only case, we need to \"attach\" a generator to our index (that can be used as a retriever) to finally assemble our RAG systems. In the multi-modal case however, our generators are Multi-Modal LLMs (or also often referred to as Large Multi-Modal Models or LMM for short). In this notebook, to draw even more comparisons on varied RAG systems, we will use GPT-4V. We can \"attach\" a generator and get an queryable interface for RAG by invoking the as_query_engine method of our indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "from llama_index.multi_modal_llms.replicate_multi_modal import (\n",
    "    ReplicateMultiModal,\n",
    ")\n",
    "from llama_index.prompts import PromptTemplate\n",
    "\n",
    "# define our QA prompt template\n",
    "qa_tmpl_str = (\n",
    "    \"Images of hand gestures for ASL are provided.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"If the images provided cannot help in answering the query\\n\"\n",
    "    \"then respond that you are unable to answer the query. Otherwise,\\n\"\n",
    "    \"using only the context provided, and not prior knowledge,\\n\"\n",
    "    \"provide an answer to the query.\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_tmpl = PromptTemplate(qa_tmpl_str)\n",
    "\n",
    "# define our lmms\n",
    "openai_mm_llm = OpenAIMultiModal(\n",
    "    model=\"gpt-4-vision-preview\",\n",
    "    max_new_tokens=300,\n",
    ")\n",
    "\n",
    "# define our RAG query engines\n",
    "rag_engines = {\n",
    "    \"mm_clip_gpt4v\": asl_index.as_query_engine(\n",
    "        multi_modal_llm=openai_mm_llm, text_qa_template=qa_tmpl\n",
    "    ),\n",
    "    \"mm_text_desc_gpt4v\": asl_text_desc_index.as_query_engine(\n",
    "        multi_modal_llm=openai_mm_llm, text_qa_template=qa_tmpl\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test drive our Multi-Modal RAG\n",
    "Let's take a test drive of one these systems. To pretty display the resonse, we make use of notebook utility function display_query_and_multimodal_response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter = \"R\"\n",
    "query = QUERY_STR_TEMPLATE.format(symbol=letter)\n",
    "response = rag_engines[\"mm_text_desc_gpt4v\"].query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.response.notebook_utils import (\n",
    "    display_query_and_multimodal_response,\n",
    ")\n",
    "\n",
    "display_query_and_multimodal_response(query, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Multi-Modal RAGs with TruLens\n",
    "\n",
    "Just like with text-based RAG systems, we can leverage the [RAG Triad](https://www.trulens.org/trulens_eval/core_concepts_rag_triad/) with TruLens to assess the quality of the RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Tru\n",
    "tru = Tru()\n",
    "tru.reset_database()\n",
    "\n",
    "tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the RAG Triad for evaluations\n",
    "\n",
    "First we need to define the feedback functions to use: answer relevance, context relevance and groundedness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Feedback\n",
    "from trulens_eval.feedback import Groundedness\n",
    "from trulens_eval.feedback.provider.openai import OpenAI as fOpenAI\n",
    "from trulens_eval import TruLlama\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Initialize provider class\n",
    "from openai import OpenAI\n",
    "openai_client = OpenAI()\n",
    "fopenai = fOpenAI(client = openai_client)\n",
    "\n",
    "grounded = Groundedness(groundedness_provider=fopenai)\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons, name = \"Groundedness\")\n",
    "    .on(TruLlama.select_source_nodes().node.text.collect())\n",
    "    .on_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = (\n",
    "    Feedback(fopenai.relevance_with_cot_reasons, name = \"Answer Relevance\")\n",
    "    .on_input_output()\n",
    ")\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(fopenai.qs_relevance_with_cot_reasons, name = \"Context Relevance\")\n",
    "    .on_input()\n",
    "    .on(TruLlama.select_source_nodes().node.text)\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "feedbacks = [f_groundedness, f_qa_relevance, f_context_relevance]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up TruLlama to log and evaluate rag engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_text_desc_gpt4v = TruLlama(rag_engines[\"mm_text_desc_gpt4v\"],\n",
    "                                 app_id = 'text-desc-gpt4v',\n",
    "                                 feedbacks=feedbacks)\n",
    "\n",
    "tru_mm_clip_gpt4v = TruLlama(rag_engines[\"mm_clip_gpt4v\"],\n",
    "                                 app_id = 'mm_clip_gpt4v',\n",
    "                                 feedbacks=feedbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the performance of the RAG on each letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_text_desc_gpt4v as recording:\n",
    "    for letter in letters:\n",
    "        query = QUERY_STR_TEMPLATE.format(symbol=letter)\n",
    "        response = rag_engines[\"mm_text_desc_gpt4v\"].query(query)\n",
    "\n",
    "with tru_mm_clip_gpt4v as recording:\n",
    "    for letter in letters:\n",
    "        query = QUERY_STR_TEMPLATE.format(symbol=letter)\n",
    "        response = rag_engines[\"mm_clip_gpt4v\"].query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_leaderboard(app_ids=['text-desc-gpt4v', 'mm_clip_gpt4v'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.run_dashboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
