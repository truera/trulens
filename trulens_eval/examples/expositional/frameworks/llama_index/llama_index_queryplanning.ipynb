{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Planning in LlamaIndex\n",
    "\n",
    "Query planning is a useful tool to leverage the ability of LLMs to structure the user inputs into multiple different queries, either sequentially or in parallel before answering the questions. This method improvers the response by allowing the question to be decomposed into smaller, more answerable questions.\n",
    "\n",
    "Sub-question queries are one such method. Sub-question queries decompose the user input into multiple different sub-questions. This is great for answering complex questions that require knowledge from different documents.\n",
    "\n",
    "Relatedly, there are a great deal of configurations for this style of application that must be selected. In this example, we'll iterate through several of these choices and evaluate each with TruLens.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/expositional/frameworks/llama_index/llama_index_queryplanning.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import from LlamaIndex and TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install trulens_eval llama_index==0.10.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, ServiceContext\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "\n",
    "from trulens import TruLlama, Feedback, Tru, feedback\n",
    "tru = Tru()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This is ONLY necessary in jupyter notebook.\n",
    "# Details: Jupyter runs an event-loop behind the scenes. \n",
    "#          This results in nested event-loops when we start an event-loop to make async queries.\n",
    "#          This is normally not allowed, we use nest_asyncio to allow it for convenience.  \n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set keys\n",
    "\n",
    "For this example we need an OpenAI key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up evaluation\n",
    "\n",
    "Here we'll use agreement with GPT-4 as our evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = feedback.OpenAI()\n",
    "model_agreement = Feedback(openai.model_agreement).on_input_output()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the dashboard\n",
    "\n",
    "By starting the dashboard ahead of time, we can watch as the evaluations get logged. This is especially useful for longer-running applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.run_dashboard()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\"https://www.gutenberg.org/files/11/11-h/11-h.htm\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set configuration space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through embeddings and chunk sizes, evaluating each response's agreement with chatgpt using TruLens\n",
    "embeddings = ['text-embedding-ada-001','text-embedding-ada-002']\n",
    "query_engine_types = ['VectorStoreIndex','SubQuestionQueryEngine']\n",
    "\n",
    "service_context=512"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set test prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set test prompts\n",
    "prompts = [\"Describe Alice's growth from meeting the White Rabbit to challenging the Queen of Hearts?\",\n",
    "            \"Relate aspects of enchantment to the nostalgia that Alice experiences in Wonderland. Why is Alice both fascinated and frustrated by her encounters below-ground?\",\n",
    "            \"Describe the White Rabbit's function in Alice.\",\n",
    "            \"Describe some of the ways that Carroll achieves humor at Alice's expense.\",\n",
    "            \"Compare the Duchess' lullaby to the 'You Are Old, Father William' verse\",\n",
    "            \"Compare the sentiment of the Mouse's long tale, the Mock Turtle's story and the Lobster-Quadrille.\",\n",
    "            \"Summarize the role of the mad hatter in Alice's journey\",\n",
    "            \"How does the Mad Hatter influence the arc of the story throughout?\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate through configruation space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for embedding in(embeddings):\n",
    "    for query_engine_type in query_engine_types:\n",
    "\n",
    "            # build index and query engine\n",
    "            index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "            # create embedding-based query engine from index\n",
    "            query_engine = index.as_query_engine(embed_model=embedding)\n",
    "\n",
    "            if query_engine_type == 'SubQuestionQueryEngine':\n",
    "                service_context = ServiceContext.from_defaults(chunk_size=512)\n",
    "                # setup base query engine as tool\n",
    "                query_engine_tools = [\n",
    "                    QueryEngineTool(\n",
    "                        query_engine=query_engine,\n",
    "                        metadata=ToolMetadata(name='Alice in Wonderland', description='THE MILLENNIUM FULCRUM EDITION 3.0')\n",
    "                    )\n",
    "                ]\n",
    "                query_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=query_engine_tools, service_context=service_context)\n",
    "            else:\n",
    "                pass         \n",
    "\n",
    "            tru_query_engine_recorder = TruLlama(app_id = f'{query_engine_type}_{embedding}', app = query_engine, feedbacks = [model_agreement])\n",
    "\n",
    "            # tru_query_engine_recorder as context manager\n",
    "            with tru_query_engine_recorder as recording:\n",
    "                 for prompt in prompts:\n",
    "                    query_engine.query(prompt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d153714b979d5e6d08dd8ec90712dd93bff2c9b6c1f0c118169738af3430cd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
