{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Comparison\n",
    "\n",
    "When building an LLM application we have hundreds of different models to choose from, all with different costs/latency and performance characteristics. Importantly, performance of LLMs can be heterogeneous across different use cases. Rather than relying on standard benchmarks or leaderboard performance, we want to evaluate an LLM for the use case we need.\n",
    "\n",
    "Doing this sort of comparison is a core use case of TruLens. In this example, we'll walk through how to build a simple langchain app and evaluate across 3 different models: small flan, large flan and text-turbo-3.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/expositional/frameworks/langchain/langchain_model_comparison.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install trulens_eval==0.11.0 langchain==0.0.283"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain import LLMChain\n",
    "# Imports from langchain to build app. You may need to install langchain first\n",
    "# with the following:\n",
    "# ! pip install langchain>=0.0.170\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "import numpy as np\n",
    "\n",
    "# Imports main tools:\n",
    "# Imports main tools:\n",
    "from trulens_eval import Feedback\n",
    "from trulens_eval import feedback\n",
    "from trulens_eval import FeedbackMode\n",
    "from trulens_eval import Huggingface\n",
    "from trulens_eval import Select\n",
    "from trulens_eval import TP\n",
    "from trulens_eval import Tru\n",
    "from trulens_eval import TruChain\n",
    "from trulens_eval.utils.langchain import WithFeedbackFilterDocuments\n",
    "\n",
    "tru = Tru()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set API Keys\n",
    "\n",
    "For this example, we need API keys for the Huggingface, HuggingFaceHub, and OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = \"...\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"...\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(\n",
    "        template=template,\n",
    "    input_variables=['question']\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up feedback functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API endpoints for models used in feedback functions:\n",
    "hugs = feedback.Huggingface()\n",
    "openai = feedback.OpenAI()\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = Feedback(openai.relevance).on_input_output()\n",
    "# By default this will evaluate feedback on main app input and main app output.\n",
    "\n",
    "all_feedbacks = [f_qa_relevance]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a couple sizes of Flan and ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub, LLMChain\n",
    "\n",
    "# initialize the models\n",
    "hub_llm_smallflan = HuggingFaceHub(\n",
    "        repo_id = 'google/flan-t5-small',\n",
    "    model_kwargs = {'temperature':1e-10}\n",
    ")\n",
    "\n",
    "hub_llm_largeflan = HuggingFaceHub(\n",
    "        repo_id = 'google/flan-t5-large',\n",
    "    model_kwargs = {'temperature':1e-10}\n",
    ")\n",
    "\n",
    "davinci = OpenAI(model_name='text-davinci-003')\n",
    "\n",
    "# create prompt template > LLM chain\n",
    "smallflan_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=hub_llm_smallflan\n",
    ")\n",
    "\n",
    "largeflan_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=hub_llm_largeflan\n",
    ")\n",
    "\n",
    "davinci_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=davinci\n",
    ")\n",
    "\n",
    "# Trulens instrumentation.\n",
    "smallflan_app_recorder = TruChain(\n",
    "        app_id=f\"small_flan/v1\",\n",
    "        app=smallflan_chain,\n",
    "        feedbacks=all_feedbacks\n",
    "    )\n",
    "\n",
    "largeflan_app_recorder = TruChain(\n",
    "        app_id=f\"large_flan/v1\",\n",
    "        app=largeflan_chain,\n",
    "        feedbacks=all_feedbacks\n",
    "    )\n",
    "\n",
    "davinci_app_recorder = TruChain(\n",
    "        app_id=f\"davinci/v1\",\n",
    "        app=davinci_chain,\n",
    "        feedbacks=all_feedbacks\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the application with all 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Who won the superbowl in 2010?\",\n",
    "    \"What is the capital of Thailand?\",\n",
    "    \"Who developed the theory of evolution by natural selection?\"\n",
    "    ]\n",
    "\n",
    "for prompt in prompts:\n",
    "    with smallflan_app_recorder as recording:\n",
    "        smallflan_chain(prompt)\n",
    "    with largeflan_app_recorder as recording:\n",
    "        largeflan_chain(prompt)\n",
    "    with davinci_app_recorder as recording:\n",
    "        davinci_chain(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the TruLens dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.run_dashboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d153714b979d5e6d08dd8ec90712dd93bff2c9b6c1f0c118169738af3430cd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
