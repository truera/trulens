{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _LangChain_ retrieval agent \n",
    "In this notebook, we are building a _LangChain_ agent to take in user input and figure out the best tool(s) to use via chain of thought (CoT) reasoning. \n",
    "\n",
    "Given we have more than one distinct tasks defined in the tools for our agent, one being summarization and another one, which generates multiple choice questions and corresponding answers, being more similar to traditional Natural Language Understanding (NLU), we will use to key evaluations for our agent: Tool Input and Tool Selection. Both will be defined with custom functions.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/expositional/frameworks/langchain/langchain_retrieval_agent.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install trulens_eval==0.20.3 langchain==0.0.335 unstructured==0.10.23 chromadb==0.4.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import openai\n",
    "\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import  RetrievalQA\n",
    "from langchain import OpenAI\n",
    "from langchain.agents import Tool\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define custom class that loads dcouments into local vector store.\n",
    "We are using Chroma, one of the open-source embedding database offerings, in the following example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorstoreManager:\n",
    "    def __init__(self):\n",
    "        self.vectorstore = None  # Vectorstore for the current conversation\n",
    "        self.all_document_splits = []  # List to hold all document splits added during a conversation\n",
    "\n",
    "    def initialize_vectorstore(self):\n",
    "        \"\"\"Initialize an empty vectorstore for the current conversation.\"\"\"\n",
    "        self.vectorstore = Chroma(\n",
    "            embedding_function=OpenAIEmbeddings(), \n",
    "        )\n",
    "        self.all_document_splits = []  # Reset the documents list for the new conversation\n",
    "        return self.vectorstore\n",
    "\n",
    "    def add_documents_to_vectorstore(self, url_lst: list):\n",
    "        \"\"\"Example assumes loading new documents from websites to the vectorstore during a conversation.\"\"\"\n",
    "        for doc_url in url_lst:\n",
    "            document_splits = self.load_and_split_document(doc_url)\n",
    "            self.all_document_splits.extend(document_splits)\n",
    "        \n",
    "        # Create a new Chroma instance with all the documents\n",
    "        self.vectorstore = Chroma.from_documents(\n",
    "            documents=self.all_document_splits, \n",
    "            embedding=OpenAIEmbeddings(), \n",
    "        )\n",
    "\n",
    "        return self.vectorstore\n",
    "\n",
    "    def get_vectorstore(self):\n",
    "        \"\"\"Provide the initialized vectorstore for the current conversation. If not initialized, do it first.\"\"\"\n",
    "        if self.vectorstore is None:\n",
    "            raise ValueError(\"Vectorstore is not initialized. Please initialize it first.\")\n",
    "        return self.vectorstore\n",
    "\n",
    "    @staticmethod\n",
    "    def load_and_split_document(url: str, chunk_size=1000, chunk_overlap=0):  \n",
    "        \"\"\"Load and split a document into chunks.\"\"\"\n",
    "        loader = WebBaseLoader(url)\n",
    "        splits = loader.load_and_split(RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap))\n",
    "        return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_URL = \"http://paulgraham.com/worked.html\"\n",
    "\n",
    "vectorstore_manager = VectorstoreManager()\n",
    "vec_store = vectorstore_manager.add_documents_to_vectorstore([DOC_URL])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up conversational agent with multiple tools.\n",
    "The tools are then selected based on the match between their names/descriptions and the user input, for document retrieval, summarization, and generation of question-answering pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "        model_name='gpt-3.5-turbo-16k',\n",
    "        temperature=0.0    \n",
    "    )\n",
    "\n",
    "conversational_memory = ConversationSummaryBufferMemory(\n",
    "    k=4,\n",
    "    max_token_limit=64,\n",
    "    llm=llm,\n",
    "    memory_key = \"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "retrieval_summarization_template = \"\"\"\n",
    "System: Follow these instructions below in all your responses:\n",
    "System: always try to retrieve documents as knowledge base or external data source from retriever (vector DB). \n",
    "System: If performing summarization, you will try to be as accurate and informational as possible.\n",
    "System: If providing a summary/key takeaways/highlights, make sure the output is numbered as bullet points.\n",
    "If you don't understand the source document or cannot find sufficient relevant context, be sure to ask me for more context information.\n",
    "{context}\n",
    "Question: {question}\n",
    "Action:\n",
    "\"\"\"\n",
    "question_generation_template = \"\"\"\n",
    "System: Based on the summarized context, you are expected to generate a specified number of multiple choice questions and their answers from the context to ensure understanding. Each question, unless specified otherwise, is expected to have 4 options and only correct answer.\n",
    "System: Questions should be in the format of numbered list.\n",
    "{context}\n",
    "Question: {question}\n",
    "Action:\n",
    "\"\"\"\n",
    "\n",
    "summarization_prompt = PromptTemplate(template=retrieval_summarization_template, input_variables=[\"question\", \"context\"])\n",
    "question_generator_prompt = PromptTemplate(template=question_generation_template, input_variables=[\"question\", \"context\"])\n",
    "\n",
    "# retrieval qa chain\n",
    "summarization_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vec_store.as_retriever(),\n",
    "        chain_type_kwargs={'prompt': summarization_prompt}\n",
    ")\n",
    "\n",
    "question_answering_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vec_store.as_retriever(),\n",
    "        chain_type_kwargs={'prompt': question_generator_prompt}\n",
    "    )\n",
    "\n",
    "\n",
    "tools = [\n",
    "        Tool(\n",
    "            name=\"Knowledge Base / retrieval from documents\",\n",
    "            func=summarization_chain.run,\n",
    "            description=\"useful for when you need to answer questions about the source document(s).\",\n",
    "        ),\n",
    "    \n",
    "        Tool(\n",
    "            name=\"Conversational agent to generate multiple choice questions and their answers about the summary of the source document(s)\",\n",
    "            func=question_answering_chain.run,\n",
    "            description=\"useful for when you need to have a conversation with a human and hold the memory of the current / previous conversation.\",\n",
    "        ),\n",
    "]\n",
    "agent = initialize_agent(\n",
    "        agent='chat-conversational-react-description',\n",
    "        tools=tools,\n",
    "        llm=llm,\n",
    "        memory=conversational_memory\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Tru\n",
    "tru = Tru()\n",
    "\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.feedback.provider import OpenAI as fOpenAI\n",
    "from trulens_eval.feedback import Feedback\n",
    "from trulens_eval import Select\n",
    "from trulens_eval import feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAI_custom(fOpenAI):\n",
    "    def query_translation(self, question1: str, question2: str) -> float:\n",
    "        return float(self.endpoint.client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Your job is to rate how similar two quesitons are on a scale of 0 to 10, where 0 is completely distinct and 10 is matching exactly. Respond with the number only.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"QUESTION 1: {question1}; QUESTION 2: {question2}\"}\n",
    "                ]\n",
    "        ).choices[0].message.content) / 10\n",
    "\n",
    "    def tool_selection(self, task: str, tool: str) -> float:\n",
    "        return float(self.endpoint.client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Your job is to rate if the TOOL is the right tool for the TASK, where 0 is the wrong tool and 10 is the perfect tool. Respond with the number only.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"TASK: {task}; TOOL: {tool}\"}\n",
    "            ]\n",
    "        ).choices[0].message.content) / 10\n",
    "        \n",
    "custom = OpenAI_custom()\n",
    "\n",
    "# Query translation feedback (custom) to evaluate the similarity between user's original question and the question genenrated by the agent after paraphrasing.\n",
    "f_query_translation = Feedback(\n",
    "    custom.query_translation, name=\"Tool Input\").on(Select.RecordCalls.agent.plan.args.kwargs.input).on(Select.RecordCalls.agent.plan.rets.tool_input)\n",
    "\n",
    "# Tool Selection (custom) to evaluate the tool/task fit\n",
    "f_tool_selection = Feedback(\n",
    "    custom.tool_selection, name=\"Tool Selection\").on(Select.RecordCalls.agent.plan.args.kwargs.input).on(Select.RecordCalls.agent.plan.rets.tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import TruChain\n",
    "from trulens_eval import FeedbackMode\n",
    "tru_agent = TruChain(agent, app_id = \"Conversational_Agent\", feedbacks = [f_query_translation, f_tool_selection])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompts = [\n",
    "    \"Please summarize the document to a short summary under 100 words\",\n",
    "    \"Give me 5 questions in multiple choice format based on the previous summary and give me their answers\" \n",
    "]\n",
    "\n",
    "with tru_agent as recording:\n",
    "    for prompt in user_prompts:\n",
    "        print(agent(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Trulens dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Tru\n",
    "tru = Tru()\n",
    "tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
