{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "368686b4-f487-4dd4-aeff-37823976529d"
      },
      "source": [
        "# Evaluate Multi-Modal LLM using Google's Gemini model for image understanding and multi-modal RAG\n",
        "\n",
        "In the first example, run and evaluate a multimodal Gemini model with a multimodal evaluator.\n",
        "\n",
        "In the second example, learn how to run semantic evaluations on a multi-modal RAG, including the RAG triad.\n",
        "\n",
        "Note: `google-generativeai` is only available for certain countries and regions. Original example attribution: LlamaIndex\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/expositional/models/gemini_multi_modal.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc691ca8"
      },
      "outputs": [],
      "source": [
        "#!pip install trulens-eval==0.20.3 llama-index 'google-generativeai>=0.3.0' matplotlib qdrant_client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4479bf64"
      },
      "source": [
        "##  Use Gemini to understand Images from URLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5455d8c6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d0d083e"
      },
      "source": [
        "## Initialize `GeminiMultiModal` and Load Images from URLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8725b6d2"
      },
      "outputs": [],
      "source": [
        "from llama_index.multi_modal_llms.gemini import GeminiMultiModal\n",
        "\n",
        "from llama_index.multi_modal_llms.generic_utils import (\n",
        "    load_image_urls,\n",
        ")\n",
        "\n",
        "image_urls = [\n",
        "    \"https://storage.googleapis.com/generativeai-downloads/data/scene.jpg\",\n",
        "    # Add yours here!\n",
        "]\n",
        "\n",
        "image_documents = load_image_urls(image_urls)\n",
        "\n",
        "gemini_pro = GeminiMultiModal(model_name=\"models/gemini-pro-vision\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup TruLens Instrumentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trulens_eval import TruCustomApp\n",
        "from trulens_eval import Tru\n",
        "from trulens_eval.tru_custom_app import instrument\n",
        "from trulens_eval import Provider\n",
        "from trulens_eval import Feedback\n",
        "from trulens_eval import Select\n",
        "\n",
        "tru = Tru()\n",
        "tru.reset_database()\n",
        "\n",
        "# create a custom class to instrument\n",
        "class Gemini:\n",
        "    @instrument\n",
        "    def complete(self, prompt, image_documents):\n",
        "        completion = gemini_pro.complete(\n",
        "            prompt=prompt,\n",
        "            image_documents=image_documents,\n",
        "        )\n",
        "        return completion\n",
        "\n",
        "gemini = Gemini()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup custom provider with Gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create a custom gemini feedback provider\n",
        "class Gemini_Provider(Provider):\n",
        "    def city_rating(self, image_url) -> float:\n",
        "        image_documents = load_image_urls([image_url])\n",
        "        city_score = float(gemini_pro.complete(prompt = \"Is the image of a city? Respond with the float likelihood from 0.0 (not city) to 1.0 (city).\",\n",
        "        image_documents=image_documents).text)\n",
        "        return city_score\n",
        "\n",
        "gemini_provider = Gemini_Provider()\n",
        "\n",
        "f_custom_function = Feedback(gemini_provider.city_rating, name = \"City Likelihood\").on(Select.Record.calls[0].args.image_documents[0].image_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test custom feedback function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gemini_provider.city_rating(image_url=\"https://storage.googleapis.com/generativeai-downloads/data/scene.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instrument custom app with TruLens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trulens_eval import TruCustomApp\n",
        "tru_gemini = TruCustomApp(gemini, app_id = \"gemini\", feedbacks = [f_custom_function])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with tru_gemini as recording:\n",
        "    gemini.complete(\n",
        "    prompt=\"Identify the city where this photo was taken.\",\n",
        "    image_documents=image_documents\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyR5IYXyRfaa"
      },
      "source": [
        "## Build Multi-Modal RAG for Restaurant Recommendation\n",
        "\n",
        "Our stack consists of TruLens + Gemini + LlamaIndex + Pydantic structured output capabilities.\n",
        "\n",
        "Pydantic structured output is great, "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download data to use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "input_image_path = Path(\"google_restaurants\")\n",
        "if not input_image_path.exists():\n",
        "    Path.mkdir(input_image_path)\n",
        "\n",
        "!wget \"https://docs.google.com/uc?export=download&id=1Pg04p6ss0FlBgz00noHAOAJ1EYXiosKg\" -O ./google_restaurants/miami.png\n",
        "!wget \"https://docs.google.com/uc?export=download&id=1dYZy17bD6pSsEyACXx9fRMNx93ok-kTJ\" -O ./google_restaurants/orlando.png\n",
        "!wget \"https://docs.google.com/uc?export=download&id=1ShPnYVc1iL_TA1t7ErCFEAHT74-qvMrn\" -O ./google_restaurants/sf.png\n",
        "!wget \"https://docs.google.com/uc?export=download&id=1WjISWnatHjwL4z5VD_9o09ORWhRJuYqm\" -O ./google_restaurants/toronto.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Pydantic Class for Strucutred Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class GoogleRestaurant(BaseModel):\n",
        "    \"\"\"Data model for a Google Restaurant.\"\"\"\n",
        "\n",
        "    restaurant: str\n",
        "    food: str\n",
        "    location: str\n",
        "    category: str\n",
        "    hours: str\n",
        "    price: str\n",
        "    rating: float\n",
        "    review: str\n",
        "    description: str\n",
        "    nearby_tourist_places: str\n",
        "\n",
        "\n",
        "google_image_url = \"./google_restaurants/miami.png\"\n",
        "image = Image.open(google_image_url).convert(\"RGB\")\n",
        "\n",
        "plt.figure(figsize=(16, 5))\n",
        "plt.imshow(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.multi_modal_llms import GeminiMultiModal\n",
        "from llama_index.program import MultiModalLLMCompletionProgram\n",
        "from llama_index.output_parsers import PydanticOutputParser\n",
        "\n",
        "prompt_template_str = \"\"\"\\\n",
        "    can you summarize what is in the image\\\n",
        "    and return the answer with json format \\\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def pydantic_gemini(\n",
        "    model_name, output_class, image_documents, prompt_template_str\n",
        "):\n",
        "    gemini_llm = GeminiMultiModal(\n",
        "        api_key=os.environ[\"GOOGLE_API_KEY\"], model_name=model_name\n",
        "    )\n",
        "\n",
        "    llm_program = MultiModalLLMCompletionProgram.from_defaults(\n",
        "        output_parser=PydanticOutputParser(output_class),\n",
        "        image_documents=image_documents,\n",
        "        prompt_template_str=prompt_template_str,\n",
        "        multi_modal_llm=gemini_llm,\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "    response = llm_program()\n",
        "    return response\n",
        "\n",
        "from llama_index import SimpleDirectoryReader\n",
        "\n",
        "google_image_documents = SimpleDirectoryReader(\n",
        "    \"./google_restaurants\"\n",
        ").load_data()\n",
        "\n",
        "results = []\n",
        "for img_doc in google_image_documents:\n",
        "    pydantic_response = pydantic_gemini(\n",
        "        \"models/gemini-pro-vision\",\n",
        "        GoogleRestaurant,\n",
        "        [img_doc],\n",
        "        prompt_template_str,\n",
        "    )\n",
        "    # only output the results for miami for example along with image\n",
        "    if \"miami\" in img_doc.image_path:\n",
        "        for r in pydantic_response:\n",
        "            print(r)\n",
        "    results.append(pydantic_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZWjzsSkRfaa"
      },
      "source": [
        "### Construct Text Nodes for Building Vector Store. Store metadata and description for each restaurant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBcrWwGYRfaa"
      },
      "outputs": [],
      "source": [
        "from llama_index.schema import TextNode\n",
        "\n",
        "nodes = []\n",
        "for res in results:\n",
        "    text_node = TextNode()\n",
        "    metadata = {}\n",
        "    for r in res:\n",
        "        # set description as text of TextNode\n",
        "        if r[0] == \"description\":\n",
        "            text_node.text = r[1]\n",
        "        else:\n",
        "            metadata[r[0]] = r[1]\n",
        "    text_node.metadata = metadata\n",
        "    nodes.append(text_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrDnWgtbRfah"
      },
      "source": [
        "### Using Gemini Embedding for building Vector Store for Dense retrieval. Index Restaurants as nodes into Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ueV_FenRfah"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import StorageContext, ServiceContext\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.embeddings import GeminiEmbedding\n",
        "from llama_index.llms import Gemini\n",
        "from llama_index.vector_stores import QdrantVectorStore\n",
        "import qdrant_client\n",
        "\n",
        "# Create a local Qdrant vector store\n",
        "client = qdrant_client.QdrantClient(path=\"qdrant_gemini_4\")\n",
        "\n",
        "vector_store = QdrantVectorStore(client=client, collection_name=\"collection\")\n",
        "\n",
        "# Using the embedding model to Gemini\n",
        "embed_model = GeminiEmbedding(\n",
        "    model_name=\"models/embedding-001\", api_key=os.environ[\"GOOGLE_API_KEY\"]\n",
        ")\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    llm=Gemini(), embed_model=embed_model\n",
        ")\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "index = VectorStoreIndex(\n",
        "    nodes=nodes,\n",
        "    service_context=service_context,\n",
        "    storage_context=storage_context,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geZEOLcaRfah"
      },
      "source": [
        "### Using Gemini to synthesize the results and recommend the restaurants to user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmYNzEOCRfah",
        "outputId": "5beb09a2-53ca-4011-ab1e-b0f2a4d63b80"
      },
      "outputs": [],
      "source": [
        "query_engine = index.as_query_engine(\n",
        "    similarity_top_k=1,\n",
        ")\n",
        "\n",
        "response = query_engine.query(\n",
        "    \"recommend an inexpensive Orlando restaurant for me and its nearby tourist places\"\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instrument and Evaluate `query_engine` with TruLens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.llms import Gemini\n",
        "\n",
        "from trulens_eval import Provider\n",
        "from trulens_eval import Feedback\n",
        "from trulens_eval import Select\n",
        "\n",
        "from trulens_eval import LiteLLM\n",
        "from google.cloud import aiplatform\n",
        "aiplatform.init(\n",
        "    project = \"trulens-testing\",\n",
        "    location=\"us-central1\"\n",
        ")\n",
        "\n",
        "gemini_provider = LiteLLM(model_engine=\"gemini-pro\")\n",
        "\n",
        "from trulens_eval.feedback import Groundedness\n",
        "import numpy as np\n",
        "\n",
        "grounded = Groundedness(groundedness_provider=gemini_provider)\n",
        "\n",
        "# Define a groundedness feedback function\n",
        "f_groundedness = (\n",
        "    Feedback(grounded.groundedness_measure_with_cot_reasons, name = \"Groundedness\")\n",
        "    .on(Select.RecordCalls._response_synthesizer.get_response.args.text_chunks[0].collect())\n",
        "    .on_output()\n",
        "    .aggregate(grounded.grounded_statements_aggregator)\n",
        ")\n",
        "\n",
        "# Question/answer relevance between overall question and answer.\n",
        "f_qa_relevance = (\n",
        "    Feedback(gemini_provider.relevance, name = \"Answer Relevance\")\n",
        "    .on_input()\n",
        "    .on_output()\n",
        ")\n",
        "\n",
        "# Question/statement relevance between question and each context chunk.\n",
        "f_context_relevance = (\n",
        "    Feedback(gemini_provider.qs_relevance, name = \"Context Relevance\")\n",
        "    .on_input()\n",
        "    .on(Select.RecordCalls._response_synthesizer.get_response.args.text_chunks[0])\n",
        "    .aggregate(np.mean)\n",
        ")\n",
        "\n",
        "import re\n",
        "gemini_text = Gemini()\n",
        "\n",
        "# create a custom gemini feedback provider to rate affordability. Do it with len() and math and also with an LLM.\n",
        "class Gemini_Provider(Provider):\n",
        "    def affordable_math(self, text: str) -> float:\n",
        "        \"\"\"\n",
        "        Count the number of money signs using len(). Then subtract 1 and divide by 3.\n",
        "        \"\"\"\n",
        "        affordability = 1 - (\n",
        "            (len(text) - 1)/3)\n",
        "        return affordability\n",
        "\n",
        "    def affordable_llm(self, text: str) -> float:\n",
        "        \"\"\"\n",
        "        Count the number of money signs using an LLM. Then subtract 1 and take the reciprocal.\n",
        "        \"\"\"\n",
        "        prompt = f\"Count the number of characters in the text: {text}. Then subtract 1 and divide the result by 3. Last subtract from 1. Final answer:\"\n",
        "        gemini_response = gemini_text.complete(prompt).text\n",
        "        # gemini is a bit verbose, so do some regex to get the answer out.\n",
        "        float_pattern = r'[-+]?\\d*\\.\\d+|\\d+'\n",
        "        float_numbers = re.findall(float_pattern, gemini_response)\n",
        "        rightmost_float = float(float_numbers[-1])\n",
        "        affordability = rightmost_float\n",
        "        return affordability\n",
        "\n",
        "gemini_provider_custom = Gemini_Provider()\n",
        "f_affordable_math = Feedback(gemini_provider_custom.affordable_math, name = \"Affordability - Math\").on(Select.RecordCalls.retriever._index.storage_context.vector_stores.default.query.rets.nodes[0].metadata.price)\n",
        "f_affordable_llm = Feedback(gemini_provider_custom.affordable_llm, name = \"Affordability - LLM\").on(Select.RecordCalls.retriever._index.storage_context.vector_stores.default.query.rets.nodes[0].metadata.price)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test the feedback function(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grounded.groundedness_measure_with_cot_reasons([\"\"\"('restaurant', 'La Mar by Gaston Acurio')\n",
        "('food', 'South American')\n",
        "('location', '500 Brickell Key Dr, Miami, FL 33131')\n",
        "('category', 'Restaurant')\n",
        "('hours', 'Open ⋅ Closes 11 PM')\n",
        "('price', 'Moderate')\n",
        "('rating', 4.4)\n",
        "('review', '4.4 (2,104)')\n",
        "('description', 'Chic waterfront find offering Peruvian & fusion fare, plus bars for cocktails, ceviche & anticucho.')\n",
        "('nearby_tourist_places', 'Brickell Key Park')\"\"\"], \"La Mar by Gaston Acurio is a delicious peruvian restaurant by the water\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gemini_provider.qs_relevance(\"I'm hungry for Peruvian, and would love to eat by the water. Can you recommend a dinner spot?\",\n",
        "\"\"\"('restaurant', 'La Mar by Gaston Acurio')\n",
        "('food', 'South American')\n",
        "('location', '500 Brickell Key Dr, Miami, FL 33131')\n",
        "('category', 'Restaurant')\n",
        "('hours', 'Open ⋅ Closes 11 PM')\n",
        "('price', 'Moderate')\n",
        "('rating', 4.4)\n",
        "('review', '4.4 (2,104)')\n",
        "('description', 'Chic waterfront find offering Peruvian & fusion fare, plus bars for cocktails, ceviche & anticucho.')\n",
        "('nearby_tourist_places', 'Brickell Key Park')\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gemini_provider.relevance(\"I'm hungry for Peruvian, and would love to eat by the water. Can you recommend a dinner spot?\",\n",
        "\"La Mar by Gaston Acurio is a delicious peruvian restaurant by the water\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gemini_provider_custom.affordable_math(\"$$\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gemini_provider_custom.affordable_llm(\"$$\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set up instrumentation and eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trulens_eval import TruLlama\n",
        "\n",
        "tru_query_engine_recorder = TruLlama(query_engine,\n",
        "    app_id='LlamaIndex_App1',\n",
        "    feedbacks = [f_affordable_math, f_affordable_llm, f_context_relevance, f_groundedness, f_qa_relevance])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tru.stop_dashboard(force=True)\n",
        "tru.run_dashboard()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run the app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with tru_query_engine_recorder as recording:\n",
        "    query_engine.query(\"recommend an american restaurant in Orlando for me and its nearby tourist places\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trulens_eval import Tru\n",
        "\n",
        "tru = Tru()\n",
        "tru.run_dashboard()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tru.get_leaderboard(app_ids=['LlamaIndex_App1'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "gemini_multi_modal.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
