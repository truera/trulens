{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f48bb4",
   "metadata": {},
   "source": [
    "### Using Open Source Models from Huggingface for Specialized Rag Analysis \n",
    "\n",
    "**Introduction:** in this notebook you can use a combination of open source models to evaluate a RAG application. This notebook uses a purpose built evaluation model from [vectara](https://huggingface.co/vectara/hallucination_evaluation_model). This model is purpose built to evaluate LLM hallucination by comparing the RAG retrieval output against the semantic search outputs. The LLM selected for retrieval is [NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO](https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO) which was available at the time. **you might need to adapt this end-point based on availability!** \n",
    "\n",
    "**ðŸ¤—Huggingface:** huggingface hosts a variety of models. We're using the specialized [e5-mistral embeddings model](https://huggingface.co/intfloat/e5-mistral-7b-instruct) from microsoft ai. This model requires custom code to take advantage of its embeddings specialized for technical tasks.  The great thing about community focal points like huggingface is that you can use the latest most interesting models , including those that use custom code and libraries , all hosted on huggingface for storage and serving.\n",
    "\n",
    "**End-To-End Open Source Pipelines Using Trulens:** the great thing about trulens is that you can build a complete end-to-end pipeline or application using only open source models, including for the evaluation. \n",
    "\n",
    "### Let's Get Started\n",
    "\n",
    "Now we need to install and import various trulens and application dependencies. Run the cell below to get started :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75918e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c8d8f9f9-1d63-40be-8d6f-96e4ba7d9906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"trulens\\trulens_eval\")\n",
    "from trulens_eval.feedback.provider.hugs import Huggingface\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84ce17c",
   "metadata": {},
   "source": [
    "## build an end to end application\n",
    "\n",
    "we need to wrap our application with trulens evaluators. Below we'll build a simple application that uses selected open source models that require custom code or using huggingface. \n",
    "\n",
    "### Import Libraries \n",
    "\n",
    "run the cell below to make the required imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c14b80ea-bc86-4045-8f68-a53dee91449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import os\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from dotenv import load_dotenv\n",
    "from langchain.llms import OpenAI\n",
    "import json\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from openai import OpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import openai\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c685a8",
   "metadata": {},
   "source": [
    "### Instantiate the document loader\n",
    "\n",
    "run the cells below to instantiate a document loader and load your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecca5ca8-b36b-4f69-a672-a610f539b8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('./data/', glob=\"./*.txt\", loader_cls=TextLoader)\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633a2c13",
   "metadata": {},
   "source": [
    "### Process the data\n",
    "\n",
    "as always, we need to process the data into text chunks for retrieval. Run the cells below :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e09940fd-ffd7-4b53-ab99-746e19c310b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1291\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=90, chunk_overlap=20)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fce4640e-680a-4714-8587-695b50d5c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_list = [doc.metadata for doc in texts]\n",
    "text_content_list = [doc.page_content for doc in texts]\n",
    "id_list=id_list = [\"doc\" + str(i + 1) for i in range(len(texts))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da2db2c",
   "metadata": {},
   "source": [
    "### Use Chroma as a Vector Store:\n",
    "\n",
    "Storing and indexing text chunks requires a lot of thought and engineering, that's why we're using chroma to store our embeddings. We're creating embeddings using the [e5 model served on huggingface](https://huggingface.co/spaces/Tonic1/e5) , then storing them using Chroma's vector store.   Run the cells below to instantiate Chroma and the embeddings client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7fd79c0-ee86-4ab5-b72e-9b568dbb18b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "# class E5EmbeddingFunction(EmbeddingFunction):\n",
    "#     def __init__(self, api_link: str, model_name: str):\n",
    "#         if not api_link:\n",
    "#             raise ValueError(\"Please provide a api end point.\")\n",
    "\n",
    "#         if not model_name:\n",
    "#             raise ValueError(\"Please provide the model name.\")\n",
    "#         self._api_link = api_link\n",
    "#         self._model_name = model_name\n",
    "\n",
    "class E5EmbeddingFunction(EmbeddingFunction):\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        try:\n",
    "            from gradio_client import Client\n",
    "            client = Client(\"Tonic1/e5\")\n",
    "            \n",
    "            # Send request with all input documents\n",
    "            response = client.predict(\"ArguAna\", input, api_name=\"/compute_embeddings\")\n",
    "            \n",
    "            if response:\n",
    "                # Extract embeddings from the response\n",
    "                embeddings_data = response.get('data', [])\n",
    "                embeddings = []\n",
    "                for embedding_list in embeddings_data:\n",
    "                    embeddings.extend(embedding_list)\n",
    "                    \n",
    "                return embeddings  # Return embeddings\n",
    "                \n",
    "            else:\n",
    "                print(\"Warning: Empty response received.\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in E5EmbeddingFunction: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbcc709",
   "metadata": {},
   "source": [
    "### Instantiate Chroma Client\n",
    "\n",
    "simply run the cells below to instantiate the chroma client used to retrieve text chunks using the Chroma API :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "add5ab4c-b06e-4479-927c-0f26eb9425b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client=chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fefed8f7-1424-4129-bc0a-61b968bc7f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "embedding_function =E5EmbeddingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a646906-e75c-425e-9b3c-464dd9ec51da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: Existing collection 'texts' deleted.\n"
     ]
    }
   ],
   "source": [
    "collection_name = \"texts\"\n",
    "existing_collections = [collection.name for collection in chroma_client.list_collections()]\n",
    "\n",
    "if collection_name in existing_collections:\n",
    "    chroma_client.delete_collection(collection_name)\n",
    "    print(f\"Info: Existing collection '{collection_name}' deleted.\")\n",
    "else:\n",
    "    print(\"No collection found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cec863",
   "metadata": {},
   "source": [
    "# Run Embeddings\n",
    "\n",
    "Now we're ready to send our text chunks and index our embeddings using Chroma Vector Store. Run the cels below to get started :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13a115b9-60d8-42c6-9b39-9997a6b7ed4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://tonic1-e5.hf.space âœ”\n",
      "Error in E5EmbeddingFunction: Unsupported protocol: sse_v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception occurred invoking consumer for subscription 0c3e53395c4f41598bc845cedf195f4bto topic persistent://default/default/68e3a204-ff98-4cc2-86b3-3fd28ddba269 object of type 'NoneType' has no len()\n"
     ]
    }
   ],
   "source": [
    "vector_store = chroma_client.get_or_create_collection(name=\"texts\",\n",
    "                                                      embedding_function=embedding_function)\n",
    "vector_store.add(ids=id_list, documents=text_content_list,metadatas=metadata_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86826e8",
   "metadata": {},
   "source": [
    "### Retrieve text Chunks based on a similarity search\n",
    "\n",
    "the great thing about embeddings is the speed of the similarity search. Get started below :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "48d9de8c-9688-4214-a2c8-8e0a4cf0ad2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectordb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is Slack\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms plan for incorporating AI into its platform?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mvectordb\u001b[49m\u001b[38;5;241m.\u001b[39msimilarity_search(query)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(docs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mpage_content)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectordb' is not defined"
     ]
    }
   ],
   "source": [
    "query = \"What is Slack's plan for incorporating AI into its platform?\"\n",
    "docs = vectordb.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b386a6-8610-43e8-baed-6c540bc35684",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = ''\n",
    "\n",
    "for doc in docs:\n",
    "    content += ''.join(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0927f18-fb41-415a-8290-036e4e6286cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69db7453-1a42-485f-b657-0a88ac399587",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_provider = Huggingface()\n",
    "score = huggingface_provider.hallucination_evaluator(response,content)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de43df1-2699-4c24-906d-f871f4d042fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api-inference.huggingface.co/models/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\n",
    "headers = {\n",
    "    \"Authorization\": \"Bearer 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\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "data = {\n",
    "    \"inputs\": \"Can you please let us know more details about your question?\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d32b8f62-e8e1-4919-9149-656f90d4dd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://tonic1-e5.hf.space âœ”\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unsupported protocol: sse_v2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Client\n\u001b[1;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m Client(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTonic1/e5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mArguAna\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m\t\u001b[49m\u001b[38;5;66;43;03m# Literal['ArguAna', 'ClimateFEVER', 'DBPedia', 'FEVER', 'FiQA2018', 'HotpotQA', 'MSMARCO', 'NFCorpus', 'NQ', 'QuoraRetrieval', 'SCIDOCS', 'SciFact', 'Touche2020', 'TRECCOVID']  in 'Select a Task' Dropdown component\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello!!\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m\t\u001b[49m\u001b[38;5;66;43;03m# str  in 'ðŸ“–Input Text' Textbox component\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mapi_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/compute_embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gradio_client/client.py:392\u001b[0m, in \u001b[0;36mClient.predict\u001b[0;34m(self, api_name, fn_index, *args)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoints[inferred_fn_index]\u001b[38;5;241m.\u001b[39mis_continuous:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    390\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call predict on this function as it may run forever. Use submit instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m     )\n\u001b[0;32m--> 392\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_index\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gradio_client/client.py:1578\u001b[0m, in \u001b[0;36mJob.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1563\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1565\u001b[0m \u001b[38;5;124;03m    Return the result of the call that the future represents. Raises CancelledError: If the future was cancelled, TimeoutError: If the future didn't finish executing before the given timeout, and Exception: If the call raised then that exception will be raised.\u001b[39;00m\n\u001b[1;32m   1566\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1576\u001b[0m \u001b[38;5;124;03m        >> 9\u001b[39;00m\n\u001b[1;32m   1577\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/Desktop/trulens/trulens/trulens_eval/trulens_eval/utils/python.py:294\u001b[0m, in \u001b[0;36m_future_target_wrapper\u001b[0;34m(stack, context, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var, value \u001b[38;5;129;01min\u001b[39;00m context\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    292\u001b[0m     var\u001b[38;5;241m.\u001b[39mset(value)\n\u001b[0;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gradio_client/client.py:970\u001b[0m, in \u001b[0;36mEndpoint.make_end_to_end_fn.<locals>._inner\u001b[0;34m(*data)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mserialize:\n\u001b[1;32m    969\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserialize(\u001b[38;5;241m*\u001b[39mdata)\n\u001b[0;32m--> 970\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    971\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_predictions(\u001b[38;5;241m*\u001b[39mpredictions)\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# Append final output only if not already present\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# for consistency between generators and not generators\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gradio_client/client.py:1007\u001b[0m, in \u001b[0;36mEndpoint.make_predict.<locals>._predict\u001b[0;34m(*data)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     result \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39msynchronize_async(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sse_fn_v1, helper, event_id)\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1007\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported protocol: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mValueError\u001b[0m: Unsupported protocol: sse_v2"
     ]
    }
   ],
   "source": [
    "from gradio_client import Client\n",
    "\n",
    "client = Client(\"Tonic1/e5\")\n",
    "result = client.predict(\n",
    "\t\t\"ArguAna\",\t# Literal['ArguAna', 'ClimateFEVER', 'DBPedia', 'FEVER', 'FiQA2018', 'HotpotQA', 'MSMARCO', 'NFCorpus', 'NQ', 'QuoraRetrieval', 'SCIDOCS', 'SciFact', 'Touche2020', 'TRECCOVID']  in 'Select a Task' Dropdown component\n",
    "\t\t\"Hello!!\",\t# str  in 'ðŸ“–Input Text' Textbox component\n",
    "\t\tapi_name=\"/compute_embeddings\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4d6a42-adc0-4f12-b546-42f4080bb3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
