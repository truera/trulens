{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe15e692-181e-4477-aa0c-8a3f1d8ea7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain==0.0.354\n",
    "!pip install langchain-community==0.0.20\n",
    "!pip install langchain-core==0.1.23\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffec4fe-7189-42d1-b3c9-04cf639c5699",
   "metadata": {},
   "source": [
    "cloned git repo of trueLens to Add Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c8d8f9f9-1d63-40be-8d6f-96e4ba7d9906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"trulens\\trulens_eval\")\n",
    "from trulens_eval.feedback.provider.hugs import Huggingface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c14b80ea-bc86-4045-8f68-a53dee91449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import os\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4c746a-b3c2-44c2-a30e-215bd349084a",
   "metadata": {},
   "source": [
    "Load your Data Documents Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecca5ca8-b36b-4f69-a672-a610f539b8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('./data/', glob=\"./*.txt\", loader_cls=TextLoader)\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54673c22-83ec-4063-92da-c9786d5395e9",
   "metadata": {},
   "source": [
    "Split the DocumentTEXT into text Chunks to feed in ChromaDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e09940fd-ffd7-4b53-ab99-746e19c310b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a34e8b5-ad49-4760-82cc-07188a879849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "inference_api_key =getpass.getpass(\"Enter your HF Inference API Key:\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d607657-b583-4e43-b6d7-9c3d2634b0b7",
   "metadata": {},
   "source": [
    "Add hugging face e5 embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a4d6a42-adc0-4f12-b546-42f4080bb3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "\n",
    "embedding_function = HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=inference_api_key, model_name=\"intfloat/multilingual-e5-large-instruct\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4cfb264-20d0-4b9f-aafd-a4f92a29c6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(texts, embedding_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8a0469-b3a7-41bd-995f-1383b0666373",
   "metadata": {},
   "source": [
    "Get relevant context Docs with respect to quert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5c340d1b-bb20-4905-b32d-dbb6bd33115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is SpaceX\"\n",
    "docs = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "77b386a6-8610-43e8-baed-6c540bc35684",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = ''\n",
    "\n",
    "for doc in docs:\n",
    "    content += ''.join(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea25219-979e-49d7-8d7c-32026f9794ab",
   "metadata": {},
   "source": [
    "Query the LLM giving it context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ec11c7f5-2768-4b4a-a406-b790d407b068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "Dorsey responded to more questions, including one where he discussed Twitter founder Jack Dorsey’s view on Musk’s ownership of Twitter. When asked if there is anything Dorsey admires about Musk, he answered, “The people he sometimes listens to.”\n",
      "\n",
      "Musk said in a recent TED interview that he can’t guarantee free speech on Twitter, despite his claims it’s the “digital town square.” Dorsey said he’s not surprised\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def query_model(content, query):\n",
    "    url = \"https://api-inference.huggingface.co/models/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer \",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"inputs\": f\"answer the following question from the information given Question:{query}\\nInformation:{content}\\n\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        response_data = response.json()\n",
    "\n",
    "        # Extract the generated text from the response\n",
    "        generated_text = response_data[0]['generated_text']\n",
    "        # Remove the input text from the generated text\n",
    "        response_text = generated_text[len(data['inputs']):]\n",
    "\n",
    "        return response_text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error:\", e)\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "context_info =content\n",
    "question = \"what is SpaceX\"\n",
    "result = query_model(context_info, question)\n",
    "if result:\n",
    "    print(\"Response:\", result)\n",
    "else:\n",
    "    print(\"No response received.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9d70fa-cc14-4014-ac13-db291446d75a",
   "metadata": {},
   "source": [
    "pass retrieval context docs and LLM reponce in HHEM evaluater to get relevance respoce between O to 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "69db7453-1a42-485f-b657-0a88ac399587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.258838027715683\n"
     ]
    }
   ],
   "source": [
    "huggingface_provider = Huggingface()\n",
    "score = huggingface_provider.hallucination_evaluator(result,content)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6ed47d-6557-49a7-8512-d420cd01e743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
