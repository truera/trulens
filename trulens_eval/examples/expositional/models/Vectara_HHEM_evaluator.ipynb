{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07349e67-8830-4cee-a520-c6a5e75bcbf9",
   "metadata": {},
   "source": [
    "### Vectara HHEM Evaluator Quickstart\n",
    "\n",
    "In this quickstart, you'll learn how to use the HHEM evaluator feedback function from TruLens in your application. The Vectra HHEM evaluator, or Hughes Hallucination Evaluation Model, is a tool used to determine if a summary produced by a large language model (LLM) might contain hallucinated information.\n",
    "\n",
    "- **Purpose:** The Vectra HHEM evaluator analyzes both inputs and assigns a score indicating the probability of response containing hallucinations.\n",
    "- **Score :** The returned value is a floating point number between zero and one that represents a boolean outcome : either a high likelihood of hallucination if the score is less than 0.5 or a low likelihood of hallucination if the score is more than 0.5 \n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/models/Vectara_HHEM_evaluator.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f894d9",
   "metadata": {},
   "source": [
    "### Install Dependencies\n",
    "\n",
    "Run the cells below to install the utilities we'll use in this notebook to demonstrate Vectara's HHEM model.\n",
    "- uncomment the cell below if you havent yet installed the langchain or TruEra's TruLens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a03458-3d25-455d-a353-b5fa0f1f54c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain==0.0.354 ,langchain-community==0.0.20 ,langchain-core==0.1.23,trulens_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6a8601",
   "metadata": {},
   "source": [
    "### Import Utilities\n",
    "\n",
    "we're using Langchain utilities to facilitate RAG retrieval and demonstrate Vectara's HHEM.\n",
    "- run the cells below to get started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c14b80ea-bc86-4045-8f68-a53dee91449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader,DirectoryLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import json,getpass,os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54673c22-83ec-4063-92da-c9786d5395e9",
   "metadata": {},
   "source": [
    "### PreProcess Your Data\n",
    "Run the cells below to split the Document TEXT into text Chunks to feed in ChromaDb.\n",
    "These are our primary sources for evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e09940fd-ffd7-4b53-ab99-746e19c310b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('./data/', glob=\"./*.txt\", loader_cls=TextLoader)\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d607657-b583-4e43-b6d7-9c3d2634b0b7",
   "metadata": {},
   "source": [
    "### e5 Embeddings\n",
    "e5 embeddings set the SOTA on BEIR and MTEB benchmarks by using only synthetic data and less than 1k training steps. this method achieves\n",
    "strong performance on highly competitive text embedding benchmarks without using any labeled data. Furthermore, when fine-tuned with a mixture of synthetic and labeled data, this model sets new state-of-the-art results on the BEIR and MTEB benchmarks.[Improving Text Embeddings with Large Language Models](https://arxiv.org/pdf/2401.00368.pdf). It also requires a unique prompting mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0104dec4-2473-4e28-847e-b129538bf996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your HF Inference API Key:\n",
      "\n",
      " ········\n"
     ]
    }
   ],
   "source": [
    "inference_api_key =getpass.getpass(\"Enter your HF Inference API Key:\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a4d6a42-adc0-4f12-b546-42f4080bb3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "\n",
    "embedding_function = HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=inference_api_key, model_name=\"intfloat/multilingual-e5-large-instruct\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1a05d0",
   "metadata": {},
   "source": [
    "### Initialize a Vector Store\n",
    "\n",
    "Here we're using Chroma , our standard solution for all vector store requirements.\n",
    "- run the cells below to initialize the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4cfb264-20d0-4b9f-aafd-a4f92a29c6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(texts, embedding_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9553a97-8221-4b5d-a846-87e719680388",
   "metadata": {},
   "source": [
    "### Wrap a Simple RAG application with TruLens\n",
    "- **Retrieval:** to get relevant docs from vector DB\n",
    "- **Generate completions:** to get response from LLM.\n",
    "\n",
    "run the cells below to create a RAG Class and Functions to Record the Context and LLM Response for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec11c7f5-2768-4b4a-a406-b790d407b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.tru_custom_app import instrument\n",
    "import requests\n",
    "\n",
    "class Rag:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @instrument\n",
    "    def retrieve(self, query: str) -> str:\n",
    "        docs = db.similarity_search(query)\n",
    "        # Concatenate the content of the documents\n",
    "        content = ''.join(doc.page_content for doc in docs)\n",
    "        return content\n",
    "        \n",
    "    @instrument\n",
    "    def generate_completion(self, content: str, query: str) -> str:\n",
    "        url = \"https://api-inference.huggingface.co/models/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\n",
    "        headers = {\n",
    "            \"Authorization\": \"Bearer your hf token\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        data = {\n",
    "            \"inputs\": f\"answer the following question from the information given Question:{query}\\nInformation:{content}\\n\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, json=data)\n",
    "            response.raise_for_status()\n",
    "            response_data = response.json()\n",
    "\n",
    "            # Extract the generated text from the response\n",
    "            generated_text = response_data[0]['generated_text']\n",
    "            # Remove the input text from the generated text\n",
    "            response_text = generated_text[len(data['inputs']):]\n",
    "\n",
    "            return response_text\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(\"Error:\", e)\n",
    "            return None\n",
    "            \n",
    "    @instrument\n",
    "    def query(self, query: str) -> str:\n",
    "        context_str = self.retrieve(query)\n",
    "        completion = self.generate_completion(context_str, query)\n",
    "        return completion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51682668",
   "metadata": {},
   "source": [
    "# Instantiate the applications above\n",
    "- run the cells below to start the applications above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97fc773a-fa13-4e79-bd05-832972beb006",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag1 = Rag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4118c2c6-6945-43e3-ba4b-9b5d2e683627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Feedback, Huggingface, Tru, Select\n",
    "tru = Tru()\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0c38b7-f9b3-4735-998f-e6de10f6d8d8",
   "metadata": {},
   "source": [
    "### Initialize HHEM Feedback Function\n",
    "HHEM takes two inputs:\n",
    "\n",
    "1. The summary/answer itself generated by LLM.\n",
    "2. The original source text that the LLM used to generate the summary/answer (retrieval context).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a80d8760-84a9-4ca2-8076-9f47a785f7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In HHEM_Score, input model_output will be set to __record__.app.generate_completion.rets .\n",
      "✅ In HHEM_Score, input retrieved_text_chunks will be set to __record__.app.retrieve.rets .\n"
     ]
    }
   ],
   "source": [
    "huggingface_provider = Huggingface()\n",
    "f_hhem_score=(\n",
    "    Feedback(huggingface_provider.hallucination_evaluator, name = \"HHEM_Score\")\n",
    "    .on(Select.RecordCalls.generate_completion.rets)\n",
    "    .on(Select.RecordCalls.retrieve.rets)   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c51143",
   "metadata": {},
   "source": [
    "### Record The HHEM Score\n",
    "- run the cell below to create a feedback function for Vectara's HHEM model's score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8631816-0f68-4fcd-bd35-8f82c09b8d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "feedbacks = [f_hhem_score]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860e441e-68a5-4f60-99f6-6b6808cb395c",
   "metadata": {},
   "source": [
    "### Wrap the custom RAG with TruCustomApp, add HHEM  feedback for evaluation\n",
    "- it's as simple as running the cell below to complete the application and feedback wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0079734d-abbe-47d4-a229-5b4ef843503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import TruCustomApp\n",
    "tru_rag = TruCustomApp(rag1,\n",
    "    app_id = 'RAG v1',\n",
    "    feedbacks =feedbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945891f8-0189-4d72-8f45-de5a384c4afc",
   "metadata": {},
   "source": [
    "### Run the App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "73cadc2e-f152-40a9-b39e-442ea4111cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_rag as recording:\n",
    "    rag1.query(\"What is Vint Cerf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "926ece5c-b5b9-4343-bb05-948a5b0efe90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context Relevance</th>\n",
       "      <th>HHEM_Score</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RAG v1</th>\n",
       "      <td>0.205199</td>\n",
       "      <td>0.133374</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Context Relevance  HHEM_Score  latency  total_cost\n",
       "app_id                                                    \n",
       "RAG v1           0.205199    0.133374     18.0         0.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.get_leaderboard(app_ids=[\"RAG v1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1a44fa-01b8-492f-997d-f9d37d9421ce",
   "metadata": {},
   "source": [
    "### Explore in a Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f69f90c6-34fb-492c-88b2-aa6b4859fe37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n",
      "Config file already exists. Skipping writing process.\n",
      "Credentials file already exists. Skipping writing process.\n",
      "Dashboard already running at path:   Network URL: http://192.168.0.104:8501\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.run_dashboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
