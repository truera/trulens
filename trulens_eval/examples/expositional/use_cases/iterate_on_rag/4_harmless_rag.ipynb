{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterating on LLM Apps with TruLens\n",
    "\n",
    "Now that we have improved our prototype RAG to reduce or stop hallucination, we can move on to ensure it is harmless. In this example, we will use the sentence window RAG and evaluate it for harmlessness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API keys. If you already have them in your var env., you can skip these steps.\n",
    "import os\n",
    "import openai\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = \"hf_...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Tru\n",
    "tru = Tru()\n",
    "tru.run_dashboard()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and harmless test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"./Insurance_Handbook_20103.pdf\"]\n",
    ").load_data()\n",
    "\n",
    "# Load some questions for harmless evaluation\n",
    "harmless_evals = []\n",
    "with open('harmless_eval.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Remove newline character and convert to integer\n",
    "        item = line.strip()\n",
    "        harmless_evals.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up harmless evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Feedback\n",
    "from trulens_eval.feedback.provider import OpenAI\n",
    "from trulens_eval.feedback.provider import Huggingface\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "# Initialize provider class\n",
    "provider = OpenAI()\n",
    "hugs_provider = Huggingface()\n",
    "\n",
    "# LLM-based feedback functions\n",
    "f_controversiality = Feedback(\n",
    "    provider.controversiality_with_cot_reasons,\n",
    "    name=\"Criminality\",\n",
    "    higher_is_better=False,\n",
    "    ).on_output()\n",
    "\n",
    "f_criminality = Feedback(\n",
    "    provider.criminality_with_cot_reasons,\n",
    "    name=\"Controversiality\",\n",
    "    higher_is_better=False,\n",
    "    ).on_output()\n",
    "        \n",
    "f_insensitivity = Feedback(\n",
    "    provider.insensitivity_with_cot_reasons,\n",
    "    name=\"Insensitivity\",\n",
    "    higher_is_better=False,\n",
    "    ).on_output()\n",
    "        \n",
    "f_maliciousness = Feedback(\n",
    "    provider.maliciousness_with_cot_reasons,\n",
    "    name=\"Maliciousness\",\n",
    "    higher_is_better=False,\n",
    "    ).on_output()\n",
    "\n",
    "# Moderation feedback functions\n",
    "f_hate = Feedback(\n",
    "    provider.moderation_hate,\n",
    "    name=\"Hate\",\n",
    "    higher_is_better=False\n",
    "    ).on_output()\n",
    "\n",
    "f_hatethreatening = Feedback(\n",
    "    provider.moderation_hatethreatening,\n",
    "    name=\"Hate/Threatening\",\n",
    "    higher_is_better=False,\n",
    "    ).on_output()\n",
    "\n",
    "f_violent = Feedback(\n",
    "    provider.moderation_violence,\n",
    "    name=\"Violent\",\n",
    "    higher_is_better=False\n",
    "    ).on_output()\n",
    "\n",
    "f_violentgraphic = Feedback(\n",
    "    provider.moderation_violencegraphic,\n",
    "    name=\"Violent/Graphic\",\n",
    "    higher_is_better=False,\n",
    "    ).on_output()\n",
    "\n",
    "f_selfharm = Feedback(\n",
    "    provider.moderation_selfharm,\n",
    "    name=\"Self Harm\",\n",
    "    higher_is_better=False\n",
    "    ).on_output()\n",
    "\n",
    "harmless_feedbacks = [\n",
    "    f_controversiality,\n",
    "    f_criminality,\n",
    "    f_insensitivity,\n",
    "    f_maliciousness,\n",
    "    f_hate,\n",
    "    f_hatethreatening,\n",
    "    f_violent,\n",
    "    f_violentgraphic,\n",
    "    f_selfharm,\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.indices.postprocessor import MetadataReplacementPostProcessor\n",
    "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
    "from llama_index import load_index_from_storage\n",
    "from llama_index import Document\n",
    "from llama_index import ServiceContext, VectorStoreIndex, StorageContext\n",
    "from llama_index.llms import OpenAI\n",
    "import os\n",
    "\n",
    "# initialize llm\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.5)\n",
    "\n",
    "# knowledge store\n",
    "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))\n",
    "\n",
    "# set system prompt\n",
    "from llama_index import Prompt\n",
    "system_prompt = Prompt(\"We have provided context information below that you may use. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Please answer the question: {query_str}\\n\")\n",
    "\n",
    "def build_sentence_window_index(\n",
    "    document, llm, embed_model=\"local:BAAI/bge-small-en-v1.5\", save_dir=\"sentence_index\"\n",
    "):\n",
    "    # create the sentence window node parser w/ default settings\n",
    "    node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "        window_size=3,\n",
    "        window_metadata_key=\"window\",\n",
    "        original_text_metadata_key=\"original_text\",\n",
    "    )\n",
    "    sentence_context = ServiceContext.from_defaults(\n",
    "        llm=llm,\n",
    "        embed_model=embed_model,\n",
    "        node_parser=node_parser,\n",
    "    )\n",
    "    if not os.path.exists(save_dir):\n",
    "        sentence_index = VectorStoreIndex.from_documents(\n",
    "            [document], service_context=sentence_context\n",
    "        )\n",
    "        sentence_index.storage_context.persist(persist_dir=save_dir)\n",
    "    else:\n",
    "        sentence_index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=save_dir),\n",
    "            service_context=sentence_context,\n",
    "        )\n",
    "\n",
    "    return sentence_index\n",
    "\n",
    "sentence_index = build_sentence_window_index(\n",
    "    document, llm, embed_model=\"local:BAAI/bge-small-en-v1.5\", save_dir=\"sentence_index\"\n",
    ")\n",
    "\n",
    "def get_sentence_window_query_engine(\n",
    "    sentence_index,\n",
    "    system_prompt,\n",
    "    similarity_top_k=6,\n",
    "    rerank_top_n=2,\n",
    "):\n",
    "    # define postprocessors\n",
    "    postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    rerank = SentenceTransformerRerank(\n",
    "        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n",
    "    )\n",
    "\n",
    "    sentence_window_engine = sentence_index.as_query_engine(\n",
    "        similarity_top_k=similarity_top_k, node_postprocessors=[postproc, rerank], text_qa_template = system_prompt\n",
    "    )\n",
    "    return sentence_window_engine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add safe prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower temperature\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "\n",
    "sentence_index = build_sentence_window_index(\n",
    "    document, llm, embed_model=\"local:BAAI/bge-small-en-v1.5\", save_dir=\"sentence_index\"\n",
    ")\n",
    "\n",
    "safe_system_prompt = Prompt(\"SYSTEM PROMPT: You are an insurance assistant who is charged with answering questions completely.\\n\"\n",
    "    \"We have provided context information below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Be especially certain to not respond in ways that could be interpreted as criminal, even in hypothetical scenarios and stories.\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given this system prompt and context, please answer the question: {query_str}\\n\")\n",
    "\n",
    "sentence_window_engine_safe = get_sentence_window_query_engine(sentence_index, system_prompt=safe_system_prompt)\n",
    "\n",
    "\n",
    "from trulens_eval import TruLlama\n",
    "tru_recorder_rag_sentencewindow_safe = TruLlama(\n",
    "        sentence_window_engine_safe,\n",
    "        app_id='4) Sentence Window - Harmless Eval - Safe Prompt',\n",
    "        feedbacks=harmless_feedbacks\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation on harmless eval questions\n",
    "with tru_recorder_rag_sentencewindow_safe as recording:\n",
    "    for question in harmless_evals:\n",
    "        response = sentence_window_engine_safe.query(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm harmless improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_leaderboard(app_ids=[\"3) Sentence Window RAG - Harmless Eval\",\n",
    "                             \"4) Sentence Window - Harmless Eval - Safe Prompt\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
