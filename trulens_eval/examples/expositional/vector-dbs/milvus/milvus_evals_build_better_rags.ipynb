{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruLens + Milvus\n",
    "\n",
    "Setup:\n",
    "To get up and running, you'll first need to install Docker and Milvus. Find instructions below:\n",
    "* Docker Compose ([Instructions](https://docs.docker.com/compose/install/))\n",
    "* Milvus Standalone ([Instructions](https://milvus.io/docs/install_standalone-docker.md))\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/expositional/vector-dbs/milvus/milvus_evals_build_better_rags.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Install dependencies\n",
    "Let's install some of the dependencies for this notebook if we don't have them already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install trulens-eval llama_index==0.8.4 pymilvus==2.3.0 nltk==3.8.1 html2text==2020.1.16 tenacity==8.2.3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add API keys\n",
    "For this quickstart, you will need Open AI and Huggingface keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import from LlamaIndex and TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.vector_stores import MilvusVectorStore\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    LLMPredictor,\n",
    "    ServiceContext\n",
    ")\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "from trulens import TruLlama, Feedback, Tru, feedback\n",
    "from trulens.feedback import Groundedness\n",
    "tru = Tru()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we need to load documents. We can use SimpleWebPageReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import WikipediaReader\n",
    "\n",
    "cities = [\n",
    "    \"Los Angeles\", \"Houston\", \"Honolulu\", \"Tucson\", \"Mexico City\", \n",
    "    \"Cincinatti\", \"Chicago\"\n",
    "]\n",
    "\n",
    "wiki_docs = []\n",
    "for city in cities:\n",
    "    try:\n",
    "        doc = WikipediaReader().load_data(pages=[city])\n",
    "        wiki_docs.extend(doc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading page for city {city}: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now write down our test prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "    \"What's the best national park near Honolulu\",\n",
    "    \"What are some famous universities in Tucson?\",\n",
    "    \"What bodies of water are near Chicago?\",\n",
    "    \"What is the name of Chicago's central business district?\",\n",
    "    \"What are the two most famous universities in Los Angeles?\",\n",
    "    \"What are some famous festivals in Mexico City?\",\n",
    "    \"What are some famous festivals in Los Angeles?\",\n",
    "    \"What professional sports teams are located in Los Angeles\",\n",
    "    \"How do you classify Houston's climate?\",\n",
    "    \"What landmarks should I know about in Cincinatti\"\n",
    "]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a prototype RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = MilvusVectorStore(index_params={\n",
    "        \"index_type\": \"IVF_FLAT\",\n",
    "        \"metric_type\": \"L2\"\n",
    "        },\n",
    "        search_params={\"nprobe\": 20},\n",
    "        overwrite=True)\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "embed_v12 = HuggingFaceEmbeddings(model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "storage_context = StorageContext.from_defaults(vector_store = vector_store)\n",
    "service_context = ServiceContext.from_defaults(embed_model = embed_v12, llm = llm)\n",
    "index = VectorStoreIndex.from_documents(wiki_docs,\n",
    "            service_context=service_context,\n",
    "            storage_context=storage_context)\n",
    "query_engine = index.as_query_engine(top_k = 5)\n",
    "\n",
    "@retry(stop=stop_after_attempt(10), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "def call_query_engine(prompt):\n",
    "        return query_engine.query(prompt)\n",
    "for prompt in test_prompts:\n",
    "    call_query_engine(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize OpenAI-based feedback function collection class:\n",
    "provider = feedback.OpenAI()\n",
    "\n",
    "# Define groundedness\n",
    "f_groundedness = (\n",
    "    Feedback(provider.groundedness_measure_with_cot_reasons, name = \"Groundedness\").on(\n",
    "    TruLlama.select_context()\n",
    "    ).on_output()\n",
    "    )\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_answer_relevance = (\n",
    "    Feedback(\n",
    "    provider.relevance_with_cot_reasons, name = \"Answer Relevance\")\n",
    "    .on_input_output()\n",
    "    )\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(provider.context_relevance_with_cot_reasons, name = \"Context Relevance\").on_input().on(\n",
    "    TruLlama.select_context()\n",
    "    ).aggregate(np.mean)\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_params = [\"IVF_FLAT\",\"HNSW\"]\n",
    "embed_v12 = HuggingFaceEmbeddings(model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "embed_ft3_v12 = HuggingFaceEmbeddings(model_name = \"Sprylab/paraphrase-multilingual-MiniLM-L12-v2-fine-tuned-3\")\n",
    "embed_ada = OpenAIEmbeddings(model_name = \"text-embedding-ada-002\")\n",
    "embed_models = [embed_v12, embed_ada]\n",
    "top_ks = [1,3]\n",
    "chunk_sizes = [200,500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "for index_param, embed_model, top_k, chunk_size in itertools.product(\n",
    "    index_params, embed_models, top_ks, chunk_sizes\n",
    "    ):\n",
    "    if embed_model == embed_v12:\n",
    "        embed_model_name = \"v12\"\n",
    "    elif embed_model == embed_ft3_v12:\n",
    "        embed_model_name = \"ft3_v12\"\n",
    "    elif embed_model == embed_ada:\n",
    "        embed_model_name = \"ada\"\n",
    "    vector_store = MilvusVectorStore(index_params={\n",
    "        \"index_type\": index_param,\n",
    "        \"metric_type\": \"L2\"\n",
    "        },\n",
    "        search_params={\"nprobe\": 20},\n",
    "        overwrite=True)\n",
    "    llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "    storage_context = StorageContext.from_defaults(vector_store = vector_store)\n",
    "    service_context = ServiceContext.from_defaults(embed_model = embed_model, llm = llm, chunk_size=chunk_size)\n",
    "    index = VectorStoreIndex.from_documents(wiki_docs,\n",
    "            service_context=service_context,\n",
    "            storage_context=storage_context)\n",
    "    query_engine = index.as_query_engine(similarity_top_k = top_k)\n",
    "    tru_query_engine = TruLlama(query_engine,\n",
    "                    feedbacks=[f_groundedness, f_qa_relevance, f_context_relevance],\n",
    "                    metadata={\n",
    "                        'index_param':index_param,\n",
    "                        'embed_model':embed_model_name,\n",
    "                        'top_k':top_k,\n",
    "                        'chunk_size':chunk_size\n",
    "                        })\n",
    "    @retry(stop=stop_after_attempt(10), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "    def call_tru_query_engine(prompt):\n",
    "        return tru_query_engine.query(prompt)\n",
    "    for prompt in test_prompts:\n",
    "        call_tru_query_engine(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore in a Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.run_dashboard() # open a local streamlit app to explore\n",
    "\n",
    "# tru.stop_dashboard() # stop if needed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can run `trulens-eval` from a command line in the same folder to start the dashboard."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or view results directly in your notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_records_and_feedback(app_ids=[])[0] # pass an empty list of app_ids to get all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('milvus')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "12da0033b6ee0a044900ff965f51baf1f826c79f2500e7fd02d2f79bac1ea7cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
