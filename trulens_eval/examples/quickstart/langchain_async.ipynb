{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Async\n",
    "\n",
    "One of the biggest pain-points developers discuss when trying to build useful LLM applications is latency; these applications often make multiple calls to LLM APIs, each one taking a few seconds. It can be quite a frustrating user experience to stare at a loading spinner for more than a couple seconds. Streaming helps reduce this perceived latency by returning the output of the LLM token by token, instead of all at once.\n",
    "\n",
    "This notebook demonstrates how to monitor a LangChain streaming app with TruLens.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/quickstart/langchain_async.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import from LangChain and TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install trulens_eval==0.14.0 langchain>=0.0.263"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact\n",
    "from ipywidgets import widgets\n",
    "from langchain import LLMChain\n",
    "from langchain import PromptTemplate\n",
    "from langchain.callbacks import AsyncIteratorCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "from trulens_eval import Feedback\n",
    "from trulens_eval import feedback\n",
    "from trulens_eval import Tru\n",
    "from trulens_eval.keys import check_keys\n",
    "import trulens_eval.utils.python  # makes sure asyncio gets instrumented\n",
    "\n",
    "import openai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### Add API keys\n",
    "For this example you will need Huggingface and OpenAI keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = \"...\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Async Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up an async callback.\n",
    "callback = AsyncIteratorCallbackHandler()\n",
    "\n",
    "chatllm = ChatOpenAI(\n",
    "    temperature=0.0,\n",
    "    streaming=True # important\n",
    ")\n",
    "llm = OpenAI(\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"human_input\",\n",
    "    llm=llm,\n",
    "    max_token_limit=50\n",
    ")\n",
    "\n",
    "# Setup a simple question/answer chain with streaming ChatOpenAI.\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"human_input\", \"chat_history\"],\n",
    "    template='''\n",
    "    You are having a conversation with a person. Make small talk.\n",
    "    {chat_history}\n",
    "        Human: {human_input}\n",
    "        AI:'''\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=chatllm, prompt=prompt, memory=memory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a language match feedback function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru = Tru()\n",
    "hugs = feedback.Huggingface()\n",
    "f_lang_match = Feedback(hugs.language_match).on_input_output()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up evaluation and tracking with TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to also get filled-in prompt templates in timeline:\n",
    "from trulens_eval.instruments import instrument\n",
    "instrument.method(PromptTemplate, \"format\")\n",
    "\n",
    "tc = tru.Chain(\n",
    "    chain,\n",
    "    feedbacks=[f_lang_match],\n",
    "    app_id=\"chat_with_memory\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.print_instrumented()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the TruLens dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.run_dashboard()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"Hi. How are you?\"\n",
    "\n",
    "with tc as recording:\n",
    "    task = asyncio.create_task(\n",
    "        chain.acall(\n",
    "            inputs=dict(human_input=message, chat_history=[]),\n",
    "            callbacks=[callback]\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Note, you either need to process all of the callback iterations or await task\n",
    "# for record to be available.\n",
    "\n",
    "async for token in callback.aiter():\n",
    "    print(token, end=\"\")\n",
    "\n",
    "# Make sure task was completed:\n",
    "await task\n",
    "record = recording.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d153714b979d5e6d08dd8ec90712dd93bff2c9b6c1f0c118169738af3430cd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
