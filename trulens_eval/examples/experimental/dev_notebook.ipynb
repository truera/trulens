{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev Notebook\n",
    "\n",
    "This notebook loads the version of trulens_eval from the enclosing repo folder. You can use this to debug or devlop trulens_eval features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip uninstall -y trulens_eval # ==0.18.2\n",
    "# ! pip list | grep trulens\n",
    "# ! pip install --upgrade pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# If running from github repo, can use this:\n",
    "sys.path.append(str(Path().cwd().parent.parent.resolve()))\n",
    "\n",
    "# Enables: Debugging printouts.\n",
    "\"\"\"\n",
    "import logging\n",
    "root = logging.getLogger()\n",
    "root.setLevel(logging.DEBUG)\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "root.addHandler(handler)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pydantic testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, PydanticUndefinedAnnotation, SerializeAsAny, field_validator, validator, model_validator, ValidationInfo\n",
    "from typing import Any\n",
    "\n",
    "from pydantic_core import PydanticUndefined\n",
    "\n",
    "class CustomLoader(BaseModel):\n",
    "    cls: Any\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        kwargs['cls'] = type(self)\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    @model_validator(mode='before')\n",
    "    @staticmethod\n",
    "    def my_model_validate(obj, info: ValidationInfo):\n",
    "        if not isinstance(obj, dict):\n",
    "            return obj\n",
    "\n",
    "        cls = obj['cls']\n",
    "        # print(cls, subcls, obj, info)\n",
    "\n",
    "        validated = dict()\n",
    "        for k, finfo in cls.model_fields.items():\n",
    "            print(k, finfo)\n",
    "            typ = finfo.annotation\n",
    "            val = finfo.get_default()\n",
    "\n",
    "            if val is PydanticUndefined:\n",
    "                val = obj[k]\n",
    "\n",
    "            print(typ, type(typ))\n",
    "            if isinstance(typ, type) \\\n",
    "            and issubclass(typ, CustomLoader) \\\n",
    "            and isinstance(val, dict) and \"cls\" in val:\n",
    "                subcls = val['cls']\n",
    "                val = subcls.model_validate(val)\n",
    "    \n",
    "            validated[k] = val\n",
    "            \n",
    "        return validated\n",
    "\n",
    "class SubModel(CustomLoader):\n",
    "    sm: int = 3\n",
    "\n",
    "class Model(CustomLoader):\n",
    "    m: int = 2\n",
    "    sub: SubModel\n",
    "\n",
    "class SubSubModelA(SubModel):\n",
    "    ssma: int = 42\n",
    "\n",
    "class SubModelA(SubModel):\n",
    "    sma: int = 0\n",
    "    subsub: SubSubModelA\n",
    "\n",
    "class SubModelB(SubModel):\n",
    "    smb: int = 1\n",
    "\n",
    "c = Model(sub=SubModelA(subsub=SubSubModelA()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.model_validate({'cls': Model, 'm': 2, 'sub': {'cls': SubModelA, 'sma':3, 'subsub': {'cls': SubSubModelA, 'ssma': 42}}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.model_validate({'c': 2, 'sub': {}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import as_completed\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "\n",
    "import dotenv\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from trulens_eval import Feedback\n",
    "from trulens_eval import Tru\n",
    "from trulens_eval.feedback.provider.endpoint.base import Endpoint\n",
    "from trulens_eval.feedback.provider.hugs import Dummy\n",
    "from trulens_eval.schema import Cost\n",
    "from trulens_eval.schema import FeedbackMode\n",
    "from trulens_eval.schema import Record\n",
    "from trulens_eval.tru_custom_app import TruCustomApp\n",
    "from trulens_eval.utils.threading import TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Tru and/or dashboard.\n",
    "\n",
    "tru = Tru()\n",
    "\n",
    "tru.reset_database()\n",
    "\n",
    "tru.start_dashboard(\n",
    "    force = True,\n",
    "    _dev=Path().cwd().parent.parent.resolve()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keys testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show keys.\n",
    "\n",
    "import os\n",
    "for k in os.environ:\n",
    "    if \"KEY\" in k:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bedrock testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Bedrock\n",
    "bedrock = Bedrock(\n",
    "    model_id = \"amazon.titan-tg1-large\",\n",
    "    region_name=\"us-west-2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Endpoint.print_instrumented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_response, cost = Endpoint.track_all_costs_tally(\n",
    "    thunk=lambda: bedrock.endpoint.client.invoke_model_with_response_stream(\n",
    "    body=json.dumps({'inputText': \"Hello there.\"}), modelId=\"amazon.titan-tg1-large\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Huggingface\n",
    "Huggingface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Endpoint.print_instrumented()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AzureOpenAI Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.keys import check_keys\n",
    "check_keys(\n",
    "    \"AZURE_OPENAI_API_KEY\",\n",
    "    \"AZURE_OPENAI_ENDPOINT\",\n",
    "    \"OPENAI_API_VERSION\",\n",
    "    \"OPENAI_API_TYPE\",\n",
    "    \"AZURE_OPENAI_DEPLOYMENT_NAME\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import AzureOpenAI as AzureOpenAIChat\n",
    "import os\n",
    "\n",
    "gpt_35_turbo = AzureOpenAIChat(\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    model=\"gpt-35-turbo\",\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2023-05-15\",\n",
    "    model_version=\"0613\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "c = gpt_35_turbo._get_client()\n",
    "gpt_35_turbo._get_credential_kwargs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.base_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from trulens_eval import feedback\n",
    "azopenai = feedback.AzureOpenAI(\n",
    "    deployment_name=os.environ['AZURE_OPENAI_DEPLOYMENT_NAME']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azopenai.endpoint.client.client_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# azopenai.relevance(prompt=\"Where is Germany?\", response=\"Germany is in Europe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reval = feedback.AzureOpenAI.model_validate(azopenai.model_dump())\n",
    "# reval.relevance(prompt=\"Where is Germany?\", response=\"Poland is in Europe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azureOpenAI = azopenai\n",
    "\n",
    "from trulens_eval.feedback.provider import AzureOpenAI\n",
    "from trulens_eval.feedback import Groundedness, GroundTruthAgreement\n",
    "from trulens_eval import TruLlama, Feedback\n",
    "from trulens_eval.app import App\n",
    "import numpy as np\n",
    "# Initialize provider class\n",
    "#azureOpenAI = AzureOpenAI(deployment_name=\"gpt-35-turbo\")\n",
    "\n",
    "grounded = Groundedness(groundedness_provider=azureOpenAI)\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons)\n",
    "    .on_input_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_answer_relevance = Feedback(azureOpenAI.relevance_with_cot_reasons).on_input_output()\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(azureOpenAI.qs_relevance_with_cot_reasons)\n",
    "    .on_input_output()\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "# GroundTruth for comparing the Answer to the Ground-Truth Answer\n",
    "#ground_truth_collection = GroundTruthAgreement(golden_set, provider=azureOpenAI)\n",
    "#f_answer_correctness = (\n",
    "#    Feedback(ground_truth_collection.agreement_measure)\n",
    "#    .on_input_output()\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_groundedness.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.utils.pyschema import WithClassInfo\n",
    "from trulens_eval.utils.serial import SerialModel\n",
    "from trulens_eval.feedback.groundedness import Groundedness\n",
    "import pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Groundedness.model_validate(grounded.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = Feedback.model_validate(f_groundedness.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2.implementation.obj.init_bindings.kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2.imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_serial(f):\n",
    "    print(\"Before serialization:\")\n",
    "    print(f.imp(\"Where is Poland?\", \"Poland is in Europe\"))\n",
    "    f_dump = f.model_dump()\n",
    "    f = Feedback.model_validate(f_dump)\n",
    "    print(\"After serialization:\")\n",
    "    print(f.imp(\"Where is Poland?\", \"Poland is in Europe\"))\n",
    "    return f\n",
    "\n",
    "f2 = test_serial(f_groundedness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_groundedness.imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2.imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_answer_relevance = Feedback(azureOpenAI.relevance_with_cot_reasons).on_input_output()\n",
    "\n",
    "# test without serialization\n",
    "print(f_answer_relevance.imp(prompt=\"Where is Germany?\", response=\"Germany is in Europe.\"))\n",
    "\n",
    "# serialize/deserialize\n",
    "f_answer_relevance2 = Feedback.model_validate(f_answer_relevance.model_dump())\n",
    "\n",
    "# test after deserialization\n",
    "print(f_answer_relevance2.imp(prompt=\"Where is Germany?\", response=\"Poland is in Europe.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = feedback.Feedback.model_validate(f.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr.imp(prompt=\"Where is Germany?\", response=\"Germany is in Europe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy endpoint testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = TP()\n",
    "\n",
    "d = Dummy(\n",
    "    loading_prob=0.1,\n",
    "    freeze_prob=0.0, # we expect requests to have their own timeouts so freeze should never happen\n",
    "    error_prob=0.01,\n",
    "    overloaded_prob=0.1,\n",
    "    rpm=6000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports from langchain to build app. You may need to install langchain first\n",
    "# with the following:\n",
    "# ! pip install langchain>=0.0.170\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "\n",
    "# Initialize Huggingface-based feedback function collection class:\n",
    "# bedrock = Bedrock(model_engine=\"Bedrock\", model_id = \"anthropic.claude-v2\", region_name=\"us-west-2\")\n",
    "\n",
    "# Define a language match feedback function using HuggingFace.\n",
    "f_relevance = Feedback(bedrock.relevance).on_input_output()\n",
    "# By default this will check language match on the main app input and main app\n",
    "# output.\n",
    "\n",
    "full_prompt = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        template=\n",
    "        \"Provide a helpful response with relevant background information for the following: {prompt}\",\n",
    "        input_variables=[\"prompt\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([full_prompt])\n",
    "\n",
    "llm = OpenAI(temperature=0.9, max_tokens=128)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=chat_prompt_template, verbose=True)\n",
    "\n",
    "tru_recorder = tru.Chain(\n",
    "    chain,\n",
    "    app_id='Chain1_ChatApplication',\n",
    "    feedbacks=[f_relevance]\n",
    ")\n",
    "\n",
    "with tru_recorder as recording:\n",
    "    llm_response = chain.run(\"What's the capital of the USA?\")\n",
    "\n",
    "display(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain expressions testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import TruChain, Tru\n",
    "tru = Tru()\n",
    "\n",
    "from langchain_core.runnables.base import RunnableSerializable\n",
    "from trulens_eval.instruments import instrument\n",
    "instrument.method(RunnableSerializable, \"invoke\")\n",
    "instrument.method(RunnableSerializable, \"ainvoke\")\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"what country is the city {city} in? respond in {language}\"\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain2_input = {\"person\": \"obama\", \"language\": \"spanish\"}\n",
    "\n",
    "chain2.invoke(chain2_input)\n",
    "\n",
    "tru_recorder = TruChain(\n",
    "    chain2,\n",
    "    app_id='Chain2'\n",
    ")\n",
    "\n",
    "with tru_recorder as recs:\n",
    "    llm_response = chain2.invoke({\"person\": \"obama\", \"language\": \"spanish\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = recs.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama-Index testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Feedback, Tru, TruLlama\n",
    "from trulens_eval.feedback import Groundedness\n",
    "from trulens_eval.feedback.provider.openai import OpenAI\n",
    "\n",
    "tru = Tru()\n",
    "\n",
    "from llama_index import VectorStoreIndex, QueryBundle\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "documents = SimpleWebPageReader(\n",
    "    html_to_text=True\n",
    ").load_data([\"http://paulgraham.com/worked.html\"])\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_query_engine_recorder = TruLlama(query_engine,\n",
    "    app_id='LlamaIndex_App1',\n",
    "    feedbacks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or as context manager\n",
    "with tru_query_engine_recorder as recording:\n",
    "    print(query_engine.query(QueryBundle(\"What did the author do growing up?\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = recording.get().layout_calls_as_app() # important\n",
    "from trulens_eval.utils.serial import Lens\n",
    "from trulens_eval.schema import Select\n",
    "all_args = next(Lens().app.query[0].args.str_or_query_bundle.get(rec))\n",
    "\n",
    "Select.render_for_dashboard(Select.RecordRets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruBasicApp testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import TruBasicApp\n",
    "\n",
    "SCRIPT_DIR = Path().cwd()\n",
    "dotenv.load_dotenv(SCRIPT_DIR / \"my.env\")\n",
    "\n",
    "tru = Tru(database_redact_keys=True)#database_url=os.environ.get(\"database_url\"))\n",
    "\n",
    "def llm_standalone(prompt):\n",
    "    return prompt\n",
    "\n",
    "f_sentiment = Feedback(bedrock.sentiment).on_output()\n",
    "\n",
    "recorder = TruBasicApp(llm_standalone, app=\"default\", feedbacks=[f_sentiment])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
