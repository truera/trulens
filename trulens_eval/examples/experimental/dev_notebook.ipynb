{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev Notebook\n",
    "\n",
    "This notebook loads the version of trulens_eval from the enclosing repo folder. You can use this to debug or devlop trulens_eval features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip uninstall -y trulens_eval\n",
    "# pip install git+https://github.com/truera/trulens@piotrm/azure_bugfixes#subdirectory=trulens_eval\n",
    "\n",
    "# trulens_eval notebook dev\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "base = Path().cwd()\n",
    "while not (base / \"trulens_eval\").exists():\n",
    "    base = base.parent\n",
    "\n",
    "print(base)\n",
    "\n",
    "# If running from github repo, can use this:\n",
    "sys.path.append(str(base))\n",
    "\n",
    "# Uncomment for more debugging printouts.\n",
    "\"\"\"\n",
    "import logging\n",
    "root = logging.getLogger()\n",
    "root.setLevel(logging.DEBUG)\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "root.addHandler(handler)\n",
    "\"\"\"\n",
    "\n",
    "from trulens_eval.keys import check_keys\n",
    "\n",
    "check_keys(\n",
    "    \"OPENAI_API_KEY\",\n",
    "    \"HUGGINGFACE_API_KEY\"\n",
    ")\n",
    "\n",
    "from trulens_eval import Tru\n",
    "tru = Tru()\n",
    "# tru.reset_database()\n",
    "\n",
    "# tru.run_dashboard(_dev=base, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.feedback.provider import LiteLLM\n",
    "from trulens_eval.feedback.provider.endpoint import Endpoint\n",
    "from litellm import completion\n",
    "\n",
    "from trulens_eval.keys import check_or_set_keys\n",
    "\n",
    "import os\n",
    "\n",
    "check_or_set_keys(\n",
    "    \"AZURE_OPENAI_ENDPOINT\",\n",
    "    \"AZURE_OPENAI_API_KEY\",\n",
    "    \"OPENAI_API_TYPE\",\n",
    "    \"OPENAI_API_VERSION\",\n",
    "    \"AZURE_OPENAI_DEPLOYMENT_NAME\"\n",
    ")\n",
    "\n",
    "import litellm\n",
    "from litellm.utils import get_llm_provider, completion_cost\n",
    "\n",
    "prov = get_llm_provider(model=\"azure/truera-gpt-35-turbo\")\n",
    "\n",
    "\"\"\"\n",
    "lm_res = litellm.completion(\n",
    "    model=\"azure/truera-gpt-35-turbo\",\n",
    "    api_base=\"https://truera-dev-azure-openai.openai.azure.com/\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"hello there\"}]\n",
    ")\n",
    "\"\"\"\n",
    "    #azure_client_params = { \n",
    "    #    \"api_key\": os.environ[\"AZURE_OPENAI_API_KEY\"], \n",
    "    #    \"azure_endpoint\": os.environ[\"AZURE_OPENAI_ENDPOINT\"], \n",
    "    #    \"api_version\": os.environ[\"OPENAI_API_VERSION\"], \n",
    "    #}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_llm_provider(\"azure/truera-gpt-35-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prov = LiteLLM(\n",
    "#    model_engine=\"azure/\" + os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    "#    completion_kwargs=dict(api_base=\"https://truera-dev-azure-openai.openai.azure.com/\")\n",
    "#)\n",
    "from trulens_eval.feedback.provider.bedrock import Bedrock\n",
    "from trulens_eval.feedback.provider import LiteLLM\n",
    "\n",
    "# Have to delete litellm endpoint singleton as it may have been created\n",
    "# with the wrong underlying litellm provider in a prior test.\n",
    "# Endpoint.delete_singleton_by_name(\"litellm\")\n",
    "\n",
    "#provider = LiteLLM(f\"bedrock/{Bedrock.DEFAULT_MODEL_ID}\")\n",
    "provider = Bedrock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "provider.endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider.sentiment(\"hello there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, costs = await Endpoint.atrack_all_costs_tally(prov.sentiment, text=\"This rocks!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.feedback.provider import OpenAI\n",
    "from trulens_eval.feedback.provider.endpoint import Endpoint\n",
    "from openai import OpenAI\n",
    "op = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op.chat.completions.create(\"Hello?\", stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.feedback.provider import Huggingface\n",
    "\n",
    "hugs = Huggingface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, cost = await Endpoint.atrack_all_costs_tally(hugs.positive_sentiment, text=\"This rocks!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import boto3\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.retrievers.bedrock import \\\n",
    "    AmazonKnowledgeBasesRetriever\n",
    "from langchain_community.retrievers.bedrock import RetrievalConfig\n",
    "\n",
    "\n",
    "class MyKnowledgeBaseRetriever(AmazonKnowledgeBasesRetriever): \n",
    "    def __init__(self, knowledge_base_id, region_name, retrieval_config):\n",
    "        super().__init__(\n",
    "            knowledge_base_id=knowledge_base_id,\n",
    "            region_name=region_name,\n",
    "            retrieval_config=retrieval_config\n",
    "        )\n",
    "        self.knowledge_base_id = knowledge_base_id\n",
    "        self.region_name = region_name\n",
    "        self.retrieval_config = retrieval_config\n",
    "\n",
    "    def get_relevant_documents(self, query):\n",
    "        # Create a session using your AWS credentials\n",
    "        session = boto3.Session(region_name=self.region_name)\n",
    "\n",
    "        # Create a client for the Bedrock Agent Runtime service\n",
    "        bedrock_client = session.client('bedrock-agent-runtime')\n",
    "\n",
    "        # Retrieve relevant documents from the Knowledge Base\n",
    "        response = bedrock_client.retrieve(\n",
    "            knowledgeBaseId=self.knowledge_base_id,\n",
    "            retrievalQuery={\n",
    "                'text': query\n",
    "            },\n",
    "            retrievalConfiguration=self.retrieval_config\n",
    "        )\n",
    "        \n",
    "        relevant_documents = response['retrievalResults']\n",
    "        docs = []\n",
    "\n",
    "        for result in relevant_documents:\n",
    "            content = result['content']['text']\n",
    "            location = result['location']['s3Location']['uri']\n",
    "            score = result['score']\n",
    "\n",
    "            doc = Document(page_content=content, metadata={'location': location, 'score': score})\n",
    "            docs.append(doc)\n",
    "\n",
    "        # Return the relevant documents\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "\n",
    "retriever = MyKnowledgeBaseRetriever(\n",
    "    knowledge_base_id=\"MY KNOWLEDGE BASE ID\",\n",
    "    region_name=\"us-east-1\",\n",
    "    retrieval_config={\"vectorSearchConfiguration\": {\"numberOfResults\": 6}}\n",
    ")\n",
    "\n",
    "# Create an instance of the Bedrock LLM\n",
    "llm = Bedrock(\n",
    "    model_id='meta.llama2-70b-chat-v1',\n",
    "    model_kwargs={\"temperature\": 0.1, \"top_p\": 0.9, \"max_gen_len\": 1200}\n",
    ")\n",
    "\n",
    "# Create an instance of the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Garbage collecting testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import TruChain, Tru\n",
    "tru = Tru()\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"what country is the city {city} in? respond in {language}\"\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain2_input = {\"person\": \"obama\", \"language\": \"spanish\"}\n",
    "# chain2.invoke(chain2_input)\n",
    "\n",
    "tru_recorder2 = TruChain(\n",
    "    chain2,\n",
    "    app_id='Chain1'\n",
    ")\n",
    "\n",
    "with tru_recorder2 as recs:\n",
    "    llm_response = chain2.invoke(chain2_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "tru_recorder2 = TruChain(\n",
    "    chain2,\n",
    "    app_id='Chain2'\n",
    ")\n",
    "\n",
    "with tru_recorder2 as recs:\n",
    "    llm_response = chain2.invoke(chain2_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "tru_recorder2 = TruChain(\n",
    "    chain2,\n",
    "    app_id='Chain3'\n",
    ")\n",
    "\n",
    "with tru_recorder2 as recs:\n",
    "    llm_response = chain2.invoke(chain2_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "tru_recorder2 = TruChain(\n",
    "    chain2,\n",
    "    app_id='Chain4'\n",
    ")\n",
    "\n",
    "with tru_recorder2 as recs:\n",
    "    llm_response = chain2.invoke(chain2_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "tru_recorder2 = TruChain(\n",
    "    chain2,\n",
    "    app_id='Chain5'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_recorder2 as recs:\n",
    "    llm_response = chain2.invoke(chain2_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import as_completed\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "\n",
    "import dotenv\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from trulens_eval import Feedback\n",
    "from trulens_eval import Tru\n",
    "from trulens_eval.feedback.provider.endpoint.base import Endpoint\n",
    "from trulens_eval.feedback.provider.hugs import Dummy\n",
    "from trulens_eval.schema import Cost\n",
    "from trulens_eval.schema import FeedbackMode\n",
    "from trulens_eval.schema import Record\n",
    "from trulens_eval.tru_custom_app import TruCustomApp\n",
    "from trulens_eval.utils.threading import TP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context selection tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from langchain_community.retrievers.bedrock import (\n",
    "    AmazonKnowledgeBasesRetriever,\n",
    "    RetrievalConfig,\n",
    ")\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "\n",
    "\n",
    "class MyKnowledgeBaseRetriever(AmazonKnowledgeBasesRetriever):\n",
    "\n",
    "    def __init__(self, knowledge_base_id, region_name, retrieval_config):\n",
    "        super().__init__(\n",
    "            knowledge_base_id=knowledge_base_id,\n",
    "            region_name=region_name,\n",
    "            retrieval_config=retrieval_config\n",
    "        )\n",
    "        self.knowledge_base_id = knowledge_base_id\n",
    "        self.region_name = region_name\n",
    "        self.retrieval_config = retrieval_config\n",
    "\n",
    "    def get_relevant_documents(self, query):\n",
    "        # Create a session using your AWS credentials\n",
    "        session = boto3.Session(region_name=self.region_name)\n",
    "\n",
    "        # Create a client for the Bedrock Agent Runtime service\n",
    "        bedrock_client = session.client('bedrock-agent-runtime')\n",
    "\n",
    "        # Retrieve relevant documents from the Knowledge Base\n",
    "        response = bedrock_client.retrieve(\n",
    "            knowledgeBaseId=self.knowledge_base_id,\n",
    "            retrievalQuery={'text': query},\n",
    "            retrievalConfiguration=self.retrieval_config\n",
    "        )\n",
    "\n",
    "        relevant_documents = response['retrievalResults']\n",
    "        docs = []\n",
    "\n",
    "        for result in relevant_documents:\n",
    "            content = result['content']['text']\n",
    "            location = result['location']['s3Location']['uri']\n",
    "            score = result['score']\n",
    "\n",
    "            doc = Document(\n",
    "                page_content=content,\n",
    "                metadata={\n",
    "                    'location': location,\n",
    "                    'score': score\n",
    "                }\n",
    "            )\n",
    "            docs.append(doc)\n",
    "\n",
    "        # Return the relevant documents\n",
    "        return docs\n",
    "\n",
    "\n",
    "# Configure the AWS credentials and region\n",
    "# config = Config(region_name='us-east-1')\n",
    "\n",
    "retriever = MyKnowledgeBaseRetriever(\n",
    "    knowledge_base_id=\"MY KNOWLEDGE BASE ID\",\n",
    "    region_name=\"us-east-1\",\n",
    "    retrieval_config={\"vectorSearchConfiguration\": {\n",
    "        \"numberOfResults\": 6\n",
    "    }}\n",
    ")\n",
    "\n",
    "# Create an instance of the Bedrock LLM\n",
    "llm = Bedrock(\n",
    "    model_id='meta.llama2-70b-chat-v1',\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.9,\n",
    "        \"max_gen_len\": 1200\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create an instance of the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, retriever=retriever, return_source_documents=True\n",
    ")\n",
    "\n",
    "#qa_chain.invoke(\"What steps should be taken during the demo ride in the buying cycle to improve the likelihood of closing the deal?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.app import App\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "tru = Tru()\n",
    "\n",
    "context = App.select_context(qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Tru and/or dashboard.\n",
    "\n",
    "tru = Tru()\n",
    "\n",
    "#tru.reset_database()\n",
    "\n",
    "tru.start_dashboard(\n",
    "    force = True,\n",
    "    _dev=Path().cwd().parent.parent.resolve()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pydantic testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from pydantic import Field\n",
    "from pydantic import field_validator\n",
    "from pydantic import model_validator\n",
    "from pydantic import PydanticUndefinedAnnotation\n",
    "from pydantic import SerializeAsAny\n",
    "from pydantic import ValidationInfo\n",
    "from pydantic import validator\n",
    "from pydantic_core import PydanticUndefined\n",
    "\n",
    "\n",
    "class CustomLoader(BaseModel):\n",
    "    cls: Any\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        kwargs['cls'] = type(self)\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    @model_validator(mode='before')\n",
    "    @staticmethod\n",
    "    def my_model_validate(obj, info: ValidationInfo):\n",
    "        if not isinstance(obj, dict):\n",
    "            return obj\n",
    "\n",
    "        cls = obj['cls']\n",
    "        # print(cls, subcls, obj, info)\n",
    "\n",
    "        validated = dict()\n",
    "        for k, finfo in cls.model_fields.items():\n",
    "            print(k, finfo)\n",
    "            typ = finfo.annotation\n",
    "            val = finfo.get_default()\n",
    "\n",
    "            if val is PydanticUndefined:\n",
    "                val = obj[k]\n",
    "\n",
    "            print(typ, type(typ))\n",
    "            if isinstance(typ, type) \\\n",
    "            and issubclass(typ, CustomLoader) \\\n",
    "            and isinstance(val, dict) and \"cls\" in val:\n",
    "                subcls = val['cls']\n",
    "                val = subcls.model_validate(val)\n",
    "    \n",
    "            validated[k] = val\n",
    "            \n",
    "        return validated\n",
    "\n",
    "class SubModel(CustomLoader):\n",
    "    sm: int = 3\n",
    "\n",
    "class Model(CustomLoader):\n",
    "    m: int = 2\n",
    "    sub: SubModel\n",
    "\n",
    "class SubSubModelA(SubModel):\n",
    "    ssma: int = 42\n",
    "\n",
    "class SubModelA(SubModel):\n",
    "    sma: int = 0\n",
    "    subsub: SubSubModelA\n",
    "\n",
    "class SubModelB(SubModel):\n",
    "    smb: int = 1\n",
    "\n",
    "c = Model(sub=SubModelA(subsub=SubSubModelA()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.model_validate({'cls': Model, 'm': 2, 'sub': {'cls': SubModelA, 'sma':3, 'subsub': {'cls': SubSubModelA, 'ssma': 42}}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.model_validate({'c': 2, 'sub': {}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keys testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show keys.\n",
    "\n",
    "import os\n",
    "for k in os.environ:\n",
    "    if \"KEY\" in k:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bedrock testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Bedrock\n",
    "bedrock = Bedrock(\n",
    "    model_id = \"amazon.titan-tg1-large\",\n",
    "    region_name=\"us-west-2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Endpoint.print_instrumented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_response, cost = Endpoint.track_all_costs_tally(\n",
    "    thunk=lambda: bedrock.endpoint.client.invoke_model_with_response_stream(\n",
    "    body=json.dumps({'inputText': \"Hello there.\"}), modelId=\"amazon.titan-tg1-large\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Huggingface\n",
    "Huggingface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Endpoint.print_instrumented()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AzureOpenAI Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.keys import check_keys\n",
    "check_keys(\n",
    "    \"AZURE_OPENAI_API_KEY\",\n",
    "    \"AZURE_OPENAI_ENDPOINT\",\n",
    "    \"OPENAI_API_VERSION\",\n",
    "    \"OPENAI_API_TYPE\",\n",
    "    \"AZURE_OPENAI_DEPLOYMENT_NAME\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import AzureOpenAI as AzureOpenAIChat\n",
    "import os\n",
    "\n",
    "gpt_35_turbo = AzureOpenAIChat(\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    model=\"gpt-35-turbo\",\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2023-05-15\",\n",
    "    model_version=\"0613\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "c = gpt_35_turbo._get_client()\n",
    "gpt_35_turbo._get_credential_kwargs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.base_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from trulens_eval import feedback\n",
    "azopenai = feedback.AzureOpenAI(\n",
    "    deployment_name=os.environ['AZURE_OPENAI_DEPLOYMENT_NAME']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azopenai.endpoint.client.client_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# azopenai.relevance(prompt=\"Where is Germany?\", response=\"Germany is in Europe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reval = feedback.AzureOpenAI.model_validate(azopenai.model_dump())\n",
    "# reval.relevance(prompt=\"Where is Germany?\", response=\"Poland is in Europe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azureOpenAI = azopenai\n",
    "\n",
    "from trulens_eval.feedback.provider import AzureOpenAI\n",
    "from trulens_eval.feedback import Groundedness, GroundTruthAgreement\n",
    "from trulens_eval import TruLlama, Feedback\n",
    "from trulens_eval.app import App\n",
    "import numpy as np\n",
    "# Initialize provider class\n",
    "#azureOpenAI = AzureOpenAI(deployment_name=\"gpt-35-turbo\")\n",
    "\n",
    "grounded = Groundedness(groundedness_provider=azureOpenAI)\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons)\n",
    "    .on_input_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_answer_relevance = Feedback(azureOpenAI.relevance_with_cot_reasons).on_input_output()\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(azureOpenAI.qs_relevance_with_cot_reasons)\n",
    "    .on_input_output()\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "# GroundTruth for comparing the Answer to the Ground-Truth Answer\n",
    "#ground_truth_collection = GroundTruthAgreement(golden_set, provider=azureOpenAI)\n",
    "#f_answer_correctness = (\n",
    "#    Feedback(ground_truth_collection.agreement_measure)\n",
    "#    .on_input_output()\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_groundedness.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.utils.pyschema import WithClassInfo\n",
    "from trulens_eval.utils.serial import SerialModel\n",
    "from trulens_eval.feedback.groundedness import Groundedness\n",
    "import pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Groundedness.model_validate(grounded.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = Feedback.model_validate(f_groundedness.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2.implementation.obj.init_bindings.kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2.imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_serial(f):\n",
    "    print(\"Before serialization:\")\n",
    "    print(f.imp(\"Where is Poland?\", \"Poland is in Europe\"))\n",
    "    f_dump = f.model_dump()\n",
    "    f = Feedback.model_validate(f_dump)\n",
    "    print(\"After serialization:\")\n",
    "    print(f.imp(\"Where is Poland?\", \"Germany is in Europe\"))\n",
    "    return f\n",
    "\n",
    "f2 = test_serial(f_groundedness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_groundedness.imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2.imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_answer_relevance = Feedback(azureOpenAI.relevance_with_cot_reasons).on_input_output()\n",
    "\n",
    "# test without serialization\n",
    "print(f_answer_relevance.imp(prompt=\"Where is Germany?\", response=\"Germany is in Europe.\"))\n",
    "\n",
    "# serialize/deserialize\n",
    "f_answer_relevance2 = Feedback.model_validate(f_answer_relevance.model_dump())\n",
    "\n",
    "# test after deserialization\n",
    "print(f_answer_relevance2.imp(prompt=\"Where is Germany?\", response=\"Poland is in Europe.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = feedback.Feedback.model_validate(f.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr.imp(prompt=\"Where is Germany?\", response=\"Germany is in Europe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy endpoint testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = TP()\n",
    "\n",
    "d = Dummy(\n",
    "    loading_prob=0.1,\n",
    "    freeze_prob=0.0, # we expect requests to have their own timeouts so freeze should never happen\n",
    "    error_prob=0.01,\n",
    "    overloaded_prob=0.1,\n",
    "    rpm=6000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports from langchain to build app. You may need to install langchain first\n",
    "# with the following:\n",
    "# ! pip install langchain>=0.0.170\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "\n",
    "# Initialize Huggingface-based feedback function collection class:\n",
    "# bedrock = Bedrock(model_engine=\"Bedrock\", model_id = \"anthropic.claude-v2\", region_name=\"us-west-2\")\n",
    "\n",
    "# Define a language match feedback function using HuggingFace.\n",
    "#f_relevance = Feedback(bedrock.relevance).on_input_output()\n",
    "# By default this will check language match on the main app input and main app\n",
    "# output.\n",
    "\n",
    "full_prompt = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        template=\n",
    "        \"Provide a helpful response with relevant background information for the following: {prompt}\",\n",
    "        input_variables=[\"prompt\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([full_prompt])\n",
    "\n",
    "llm = OpenAI(temperature=0.9, max_tokens=128)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=chat_prompt_template, verbose=True)\n",
    "\n",
    "tru_recorder1 = tru.Chain(\n",
    "    chain,\n",
    "    app_id='Chain1_ChatApplication',\n",
    "    #feedbacks=[f_relevance]\n",
    ")\n",
    "\n",
    "with tru_recorder1 as recording:\n",
    "    llm_response = chain.run(\"What's the capital of the USA?\")\n",
    "\n",
    "display(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain expressions testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import TruChain, Tru\n",
    "tru = Tru()\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"what country is the city {city} in? respond in {language}\"\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain2_input = {\"person\": \"obama\", \"language\": \"spanish\"}\n",
    "\n",
    "chain2.invoke(chain2_input)\n",
    "\n",
    "tru_recorder2 = TruChain(\n",
    "    chain2,\n",
    "    app_id='Chain2'\n",
    ")\n",
    "\n",
    "with tru_recorder2 as recs:\n",
    "    llm_response = chain2.invoke({\"person\": \"obama\", \"language\": \"spanish\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = recs.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record.calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama-Index testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Feedback, Tru, TruLlama\n",
    "from trulens_eval.feedback import Groundedness\n",
    "from trulens_eval.feedback.provider.openai import OpenAI\n",
    "\n",
    "tru = Tru()\n",
    "\n",
    "from llama_index import VectorStoreIndex, QueryBundle\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "documents = SimpleWebPageReader(\n",
    "    html_to_text=True\n",
    ").load_data([\"http://paulgraham.com/worked.html\"])\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_query_engine_recorder = TruLlama(query_engine,\n",
    "    app_id='LlamaIndex_App1',\n",
    "    feedbacks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or as context manager\n",
    "with tru_query_engine_recorder as recording:\n",
    "    print(query_engine.query(QueryBundle(\"What did the author do growing up?\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = recording.get().layout_calls_as_app() # important\n",
    "from trulens_eval.utils.serial import Lens\n",
    "from trulens_eval.schema import Select\n",
    "all_args = next(Lens().app.query[0].args.str_or_query_bundle.get(rec))\n",
    "\n",
    "Select.render_for_dashboard(Select.RecordRets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruBasicApp testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import TruBasicApp\n",
    "\n",
    "SCRIPT_DIR = Path().cwd()\n",
    "dotenv.load_dotenv(SCRIPT_DIR / \"my.env\")\n",
    "\n",
    "tru = Tru(database_redact_keys=True)#database_url=os.environ.get(\"database_url\"))\n",
    "\n",
    "def llm_standalone(prompt):\n",
    "    return prompt\n",
    "\n",
    "f_sentiment = Feedback(bedrock.sentiment).on_output()\n",
    "\n",
    "recorder = TruBasicApp(llm_standalone, app=\"default\", feedbacks=[f_sentiment])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
