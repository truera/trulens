{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev Notebook\n",
    "\n",
    "This notebook loads the version of trulens_eval from the enclosing repo folder. You can use this to debug or devlop trulens_eval features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip uninstall -y trulens_eval # ==0.18.2\n",
    "# ! pip list | grep trulens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# If running from github repo, can use this:\n",
    "sys.path.append(str(Path().cwd().parent.parent.resolve()))\n",
    "\n",
    "# Enables: Debugging printouts.\n",
    "\"\"\"\n",
    "import logging\n",
    "root = logging.getLogger()\n",
    "root.setLevel(logging.DEBUG)\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "root.addHandler(handler)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import as_completed\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "\n",
    "import dotenv\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from trulens_eval import Feedback\n",
    "from trulens_eval import Tru\n",
    "from trulens_eval.feedback.provider.endpoint.base import Endpoint\n",
    "from trulens_eval.feedback.provider.hugs import Dummy\n",
    "from trulens_eval.schema import Cost\n",
    "from trulens_eval.schema import FeedbackMode\n",
    "from trulens_eval.schema import Record\n",
    "from trulens_eval.tru_custom_app import TruCustomApp\n",
    "from trulens_eval.utils.threading import TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Tru and/or dashboard.\n",
    "\n",
    "tru = Tru()\n",
    "\n",
    "tru.reset_database()\n",
    "\n",
    "tru.start_dashboard(\n",
    "    force = True,\n",
    "    _dev=Path().cwd().parent.parent.resolve()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keys testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show keys.\n",
    "\n",
    "import os\n",
    "for k in os.environ:\n",
    "    if \"KEY\" in k:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bedrock testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Bedrock\n",
    "bedrock = Bedrock(\n",
    "    model_id = \"amazon.titan-tg1-large\",\n",
    "    region_name=\"us-west-2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Endpoint.print_instrumented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_response, cost = Endpoint.track_all_costs_tally(\n",
    "    thunk=lambda: bedrock.endpoint.client.invoke_model_with_response_stream(\n",
    "    body=json.dumps({'inputText': \"Hello there.\"}), modelId=\"amazon.titan-tg1-large\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Huggingface\n",
    "Huggingface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Endpoint.print_instrumented()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy endpoint testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = TP()\n",
    "\n",
    "d = Dummy(\n",
    "    loading_prob=0.1,\n",
    "    freeze_prob=0.0, # we expect requests to have their own timeouts so freeze should never happen\n",
    "    error_prob=0.01,\n",
    "    overloaded_prob=0.1,\n",
    "    rpm=6000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports from langchain to build app. You may need to install langchain first\n",
    "# with the following:\n",
    "# ! pip install langchain>=0.0.170\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "\n",
    "# Initialize Huggingface-based feedback function collection class:\n",
    "# bedrock = Bedrock(model_engine=\"Bedrock\", model_id = \"anthropic.claude-v2\", region_name=\"us-west-2\")\n",
    "\n",
    "# Define a language match feedback function using HuggingFace.\n",
    "f_relevance = Feedback(bedrock.relevance).on_input_output()\n",
    "# By default this will check language match on the main app input and main app\n",
    "# output.\n",
    "\n",
    "full_prompt = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        template=\n",
    "        \"Provide a helpful response with relevant background information for the following: {prompt}\",\n",
    "        input_variables=[\"prompt\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([full_prompt])\n",
    "\n",
    "llm = OpenAI(temperature=0.9, max_tokens=128)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=chat_prompt_template, verbose=True)\n",
    "\n",
    "tru_recorder = tru.Chain(\n",
    "    chain,\n",
    "    app_id='Chain1_ChatApplication',\n",
    "    feedbacks=[f_relevance]\n",
    ")\n",
    "\n",
    "with tru_recorder as recording:\n",
    "    llm_response = chain.run(\"What's the capital of the USA?\")\n",
    "\n",
    "display(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruBasicApp testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import TruBasicApp\n",
    "\n",
    "SCRIPT_DIR = Path().cwd()\n",
    "dotenv.load_dotenv(SCRIPT_DIR / \"my.env\")\n",
    "\n",
    "tru = Tru(database_redact_keys=True)#database_url=os.environ.get(\"database_url\"))\n",
    "\n",
    "def llm_standalone(prompt):\n",
    "    return prompt\n",
    "\n",
    "f_sentiment = Feedback(bedrock.sentiment).on_output()\n",
    "\n",
    "recorder = TruBasicApp(llm_standalone, app=\"default\", feedbacks=[f_sentiment])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
