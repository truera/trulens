{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyvHPNecnwvn"
      },
      "source": [
        "\n",
        "# MultiQueryRetriever implementation with trulens\n",
        "\n",
        "\n",
        "> IDistance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on “distance”. But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious.\n",
        "\n",
        "> The MultiQueryRetriever automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the MultiQueryRetriever might be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results.\n",
        "\n",
        "\n",
        "https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEU0MkJ2nwvq"
      },
      "outputs": [],
      "source": [
        "! pip install trulens_eval openai langchain chromadb langchainhub bs4 tiktoken langchain-core langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rMLbNqJWnwvr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#os.environ[\"OPENAI_API_KEY\"] = \"sk-\" #hide the key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jRTO0efnwvr"
      },
      "source": [
        "# Importing neccessary imports for the langchain and trulens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_cB0Hf_hnwvr"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from trulens_eval import Tru, TruChain, Feedback\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from trulens_eval.feedback.provider import OpenAI\n",
        "import logging\n",
        "from trulens_eval.app import App\n",
        "from trulens_eval.feedback import Groundedness\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain import hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpwjxWKfnwvs"
      },
      "source": [
        "# get and load data from lilianweng.github.io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "V1uoAdUgnwvs"
      },
      "outputs": [],
      "source": [
        "# Load blog post\n",
        "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
        "data = loader.load()\n",
        "\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
        "splits = text_splitter.split_documents(data)\n",
        "\n",
        "# VectorDB\n",
        "embedding = OpenAIEmbeddings()\n",
        "vectordb = Chroma.from_documents(documents=splits, embedding=embedding)\n",
        "\n",
        "QUERY_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
        "    different versions of the given user question to retrieve relevant documents from a vector\n",
        "    database. By generating multiple perspectives on the user question, your goal is to help\n",
        "    the user overcome some of the limitations of the distance-based similarity search.\n",
        "    Provide these alternative questions separated by newlines.\n",
        "    Original question: {question}\"\"\",\n",
        ")\n",
        "\n",
        "\n",
        "question = \"What are the approaches to Task Decomposition?\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz_hZTASnwvs"
      },
      "source": [
        "\n",
        "# Setup multiQueryRetrieval along with a LLM and and logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zI_70xUnnwvs"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature=0)\n",
        "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
        "    retriever=vectordb.as_retriever(), llm=llm, prompt=QUERY_PROMPT\n",
        ")\n",
        "\n",
        "logging.basicConfig()\n",
        "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-hcmeIYnwvt"
      },
      "source": [
        "# Setup trulens with MultiQueryRetriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yetykasnwvt"
      },
      "outputs": [],
      "source": [
        "tru = Tru()\n",
        "tru.reset_database()\n",
        "# Initialize provider class\n",
        "provider = OpenAI()\n",
        "\n",
        "# select context to be used in feedback. the location of context is app specific.\n",
        "\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever_from_llm | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "context = App.select_context(rag_chain)\n",
        "\n",
        "\n",
        "grounded = Groundedness(groundedness_provider=OpenAI())\n",
        "# Define a groundedness feedback function\n",
        "f_groundedness = (\n",
        "    Feedback(grounded.groundedness_measure_with_cot_reasons)\n",
        "    .on(context.collect()) # collect context chunks into a list\n",
        "    .on_output()\n",
        "    .aggregate(grounded.grounded_statements_aggregator)\n",
        ")\n",
        "\n",
        "# Question/answer relevance between overall question and answer.\n",
        "f_answer_relevance = (\n",
        "    Feedback(provider.relevance)\n",
        "    .on_input_output()\n",
        ")\n",
        "# Question/statement relevance between question and each context chunk.\n",
        "f_context_relevance = (\n",
        "    Feedback(provider.context_relevance_with_cot_reasons)\n",
        "    .on_input()\n",
        "    .on(context)\n",
        "    .aggregate(np.mean)\n",
        ")\n",
        "\n",
        "tru_recorder = TruChain(rag_chain,\n",
        "    app_id='MultiReg',\n",
        "    feedbacks=[f_answer_relevance, f_context_relevance, f_groundedness])\n",
        "\n",
        "response, tru_record = tru_recorder.with_record(rag_chain.invoke, \"What is Task Decomposition?\")\n",
        "\n",
        "tru.get_records_and_feedback(app_ids=[\"MultiReg\"])\n",
        "tru.get_leaderboard(app_ids=[\"MultiReg\"])\n",
        "tru.run_dashboard()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
