{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruBot\n",
    "\n",
    "Example setup and monitoring of a conversational bot with context made up of the\n",
    "TruEra website. This example requires either a pinecone vector db set up with\n",
    "some contexts to answer questions with or alternatively can use the local\n",
    "database for use with hnswlib provided here. To use hnswlib, some additional\n",
    "requirements need to be installed with pip. Regardless of the vector db\n",
    "provider, the example feedback functions here use openai and huggingface free\n",
    "inference APIs and need their respective keys to be provided in a .env file.\n",
    "\n",
    "## HNSWLIB additional requirements\n",
    "\n",
    "Run the following in your shell or the equivalent in the following cell to\n",
    "install additional requirements for use with HNSWLIB. This is not required if\n",
    "you are running this example with a pinecone db.\n",
    "\n",
    "```bash\n",
    "pip install docarray hnswlib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install docarray hnswlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# If running from github repo, can use this:\n",
    "sys.path.append(str(Path().cwd().parent.parent.resolve()))\n",
    "\n",
    "# Uncomment for more debugging printouts.\n",
    "\"\"\"\n",
    "import logging\n",
    "root = logging.getLogger()\n",
    "root.setLevel(logging.DEBUG)\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "root.addHandler(handler)\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API keys setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.keys import check_keys\n",
    "\n",
    "check_keys(\n",
    "    \"OPENAI_API_KEY\",\n",
    "    \"HUGGINGFACE_API_KEY\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import PrettyPrinter\n",
    "\n",
    "# Imports from langchain to build app:\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "import numpy as np\n",
    "\n",
    "# Imports main tools:\n",
    "from trulens_eval import Feedback\n",
    "from trulens_eval import feedback\n",
    "from trulens_eval import FeedbackMode\n",
    "from trulens_eval import Select\n",
    "from trulens_eval import TP\n",
    "from trulens_eval import Tru\n",
    "from trulens_eval.utils.langchain import WithFeedbackFilterDocuments\n",
    "\n",
    "pp = PrettyPrinter()\n",
    "\n",
    "# Tru object manages the database of apps, records, and feedbacks; and the\n",
    "# dashboard to display these.\n",
    "tru = Tru()\n",
    "\n",
    "# Start the dasshboard. If you running from github repo, you will need to adjust\n",
    "# the path the dashboard streamlit app starts in by providing the _dev argument.\n",
    "tru.start_dashboard(\n",
    "    force = True,\n",
    "    _dev=Path().cwd().parent.parent.resolve()\n",
    ")\n",
    "\n",
    "# If needed, you can reset the trulens_eval dashboard database by running the\n",
    "# below line:\n",
    "\n",
    "# tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select vector db provider. Pinecone requires setting up a pinecone database\n",
    "# first while the hnsw database is included with trulens_eval.\n",
    "# db_host = \"pinecone\"\n",
    "db_host = \"hnsw\"\n",
    "\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "app_id = \"TruBot\"\n",
    "\n",
    "# Embedding for vector db.\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-ada-002')  # 1536 dims\n",
    "\n",
    "if db_host == \"pinecone\":\n",
    "    check_keys(\n",
    "        \"PINECONE_API_KEY\",\n",
    "        \"PINECONE_ENV\"\n",
    "    )\n",
    "\n",
    "    # Pinecone configuration if using pinecone.\n",
    "\n",
    "    from langchain.vectorstores import Pinecone\n",
    "    import pinecone\n",
    "\n",
    "    pinecone.init(\n",
    "        api_key=os.environ.get(\"PINECONE_API_KEY\"),  # find at app.pinecone.io\n",
    "        environment=os.environ.get(\"PINECONE_ENV\")  # next to api key in console\n",
    "    )\n",
    "\n",
    "    # If using pinecone, make sure you create your index under name 'llmdemo' or\n",
    "    # change the below.\n",
    "\n",
    "    def get_doc_search():\n",
    "\n",
    "        docsearch = Pinecone.from_existing_index(\n",
    "            index_name=\"llmdemo\", embedding=embedding\n",
    "        )\n",
    "\n",
    "        return docsearch\n",
    "\n",
    "elif db_host == \"hnsw\":\n",
    "    # Local pinecone alternative. Requires precomputed 'hnswlib_truera' folder.\n",
    "\n",
    "    from langchain.vectorstores import DocArrayHnswSearch\n",
    "\n",
    "    def get_doc_search():\n",
    "        # We need to create this object in the thread in which it is used so we\n",
    "        # wrap it in this function for later usage.\n",
    "\n",
    "        docsearch = DocArrayHnswSearch.from_params(\n",
    "            embedding=embedding,\n",
    "            work_dir='hnswlib_trubot',\n",
    "            n_dim=1536,\n",
    "            max_elements=1024\n",
    "        )\n",
    "\n",
    "        return docsearch\n",
    "else:\n",
    "    raise RuntimeError(\"Unhandled db_host, select either 'pinecone' or 'hnsw'.\")\n",
    "\n",
    "# LLM for completing prompts, and other tasks.\n",
    "llm = OpenAI(temperature=0, max_tokens=256)\n",
    "\n",
    "# Construct feedback functions.\n",
    "\n",
    "# API endpoints for models used in feedback functions:\n",
    "hugs = feedback.Huggingface()\n",
    "openai = feedback.OpenAI()\n",
    "\n",
    "# Language match between question/answer.\n",
    "f_lang_match = Feedback(hugs.language_match).on_input_output()\n",
    "# By default this will evaluate feedback on main app input and main app output.\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = Feedback(openai.relevance).on_input_output()\n",
    "# By default this will evaluate feedback on main app input and main app output.\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_qs_relevance = feedback.Feedback(openai.qs_relevance).on_input().on(\n",
    "    Select.Record.app.combine_docs_chain._call.args.inputs.input_documents[:].page_content\n",
    ").aggregate(np.min)\n",
    "# First feedback argument is set to main app input, and the second is taken from\n",
    "# the context sources as passed to an internal `combine_docs_chain._call`.\n",
    "\n",
    "all_feedbacks = [f_lang_match, f_qa_relevance, f_qs_relevance]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruBot Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v1_new_conversation(feedback_mode=FeedbackMode.WITH_APP):\n",
    "    \"\"\"\n",
    "    Create a langchain app for a new conversation with a question-answering bot.\n",
    "\n",
    "    Feedback_mode controls when feedback is evaluated:\n",
    "\n",
    "    - FeedbackMode.WITH_APP -- app will wait until feedback is evaluated before\n",
    "      returning from calls.\n",
    "\n",
    "    - FeedbackMode.WITH_APP_THREAD -- app will return from calls and evaluate\n",
    "      feedback in a new thread.\n",
    "\n",
    "    - FeedbackMode.DEFERRED -- app will return and a separate runner thread (see\n",
    "      usage later in this notebook) will evaluate feedback.\n",
    "    \"\"\"\n",
    "\n",
    "    # Blank conversation memory.\n",
    "    memory = ConversationSummaryBufferMemory(\n",
    "        max_token_limit=650,\n",
    "        llm=llm,\n",
    "        memory_key=\"chat_history\",\n",
    "        output_key='answer'\n",
    "    )\n",
    "\n",
    "    docsearch = get_doc_search()\n",
    "\n",
    "    # Context retriever.\n",
    "    retriever = docsearch.as_retriever()\n",
    "\n",
    "    # Conversational app puts it all together.\n",
    "    app = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        memory=memory,\n",
    "        get_chat_history=lambda a: a,\n",
    "        max_tokens_limit=4096\n",
    "    )\n",
    "\n",
    "    # Trulens instrumentation.\n",
    "    tc = Tru().Chain(\n",
    "        app_id=f\"{app_id}/v1\",\n",
    "        chain=app,\n",
    "        feedbacks=all_feedbacks,\n",
    "        feedback_mode=feedback_mode, \n",
    "    )\n",
    "\n",
    "    return tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the app with fresh memory:\n",
    "\n",
    "tc1 = v1_new_conversation()\n",
    "\n",
    "# Call the app:\n",
    "\n",
    "res, record = tc1.call_with_record(\"Who is Shayak?\")\n",
    "res\n",
    "\n",
    "# Notice the `source_documents` returned include chunks about Shameek and the\n",
    "# answer includes bits about Shameek as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The feedback should already be present in the dashboard, but we can check the\n",
    "# qs_relevance here manually as well:\n",
    "feedback = f_qs_relevance.run(record=record, app=tc1)\n",
    "feedback.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now a question about QII (quantitative input influence is a base technology\n",
    "# employed in TruEra's products) question but in a non-English language:\n",
    "\n",
    "# Start a new conversation as the app keeps prior questions in its memory which\n",
    "# may cause you some testing woes.\n",
    "tc1 = v1_new_conversation()\n",
    "\n",
    "# res, record = tc1.call_with_record(\"Co jest QII?\") # Polish\n",
    "res, record = tc1.call_with_record(\"Was ist QII?\") # German\n",
    "res\n",
    "\n",
    "# Note here the response is in English. This example sometimes matches language\n",
    "# so other variants may need to be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language match failure can be seen using the f_lang_match (and is visible in\n",
    "# dashboard):\n",
    "feedback = f_lang_match.run(record=record, app=tc1)\n",
    "feedback.dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruBot Version 2 - Language match fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v2_new_conversation(feedback_mode=FeedbackMode.WITH_APP):\n",
    "    \"\"\"\n",
    "    Create a langchain app for a new conversation with a question-answering bot.\n",
    "    \"\"\"\n",
    "\n",
    "    # Blank conversation memory.\n",
    "    memory = ConversationSummaryBufferMemory(\n",
    "        max_token_limit=650,\n",
    "        llm=llm,\n",
    "        memory_key=\"chat_history\",\n",
    "        output_key='answer'\n",
    "    )\n",
    "\n",
    "    docsearch = get_doc_search()\n",
    "\n",
    "    # Context retriever.\n",
    "    retriever = docsearch.as_retriever()\n",
    "\n",
    "    # Conversational app puts it all together.\n",
    "    app = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        memory=memory,\n",
    "        get_chat_history=lambda a: a,\n",
    "        max_tokens_limit=4096\n",
    "    )\n",
    "\n",
    "    ### DIFFERENCES START HERE\n",
    "\n",
    "    # Need to copy these otherwise various apps will feature templates that\n",
    "    # point to the same objects.\n",
    "    app.combine_docs_chain.llm_chain.prompt = \\\n",
    "        app.combine_docs_chain.llm_chain.prompt.copy()\n",
    "    app.combine_docs_chain.document_prompt = \\\n",
    "        app.combine_docs_chain.document_prompt.copy()\n",
    "\n",
    "    # Language mismatch fix via a prompt adjustment:\n",
    "    app.combine_docs_chain.llm_chain.prompt.template = \\\n",
    "        \"Use the following pieces of context to answer the question at the end \" \\\n",
    "        \"in the same language as the question. If you don't know the answer, \" \\\n",
    "        \"just say that you don't know, don't try to make up an answer.\\n\\n\" \\\n",
    "        \"{context}\\n\\n\" \\\n",
    "        \"Question: {question}\\n\" \\\n",
    "        \"Helpful Answer: \"\n",
    "\n",
    "    ### END OF DIFFERENCES\n",
    "\n",
    "    # Trulens instrumentation.\n",
    "    tc = Tru().Chain(\n",
    "        app_id=f\"{app_id}/v2\",\n",
    "        chain=app,\n",
    "        feedbacks=all_feedbacks,\n",
    "        feedback_mode=feedback_mode\n",
    "    )\n",
    "\n",
    "    return tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the version 2 app:\n",
    "\n",
    "tc2 = v2_new_conversation()\n",
    "\n",
    "# Now the non-English question again:\n",
    "\n",
    "res, record = tc2.call_with_record(\"Was ist QII?\")\n",
    "res\n",
    "\n",
    "# Note that the response is now the appropriate language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the language match feedback is happy:\n",
    "\n",
    "feedback = f_lang_match.run(record=record, app=tc2)\n",
    "feedback.dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruBot Version 3: Context Filtering with Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v3_new_conversation(feedback_mode=FeedbackMode.WITH_APP):\n",
    "    \"\"\"\n",
    "    Create a langchain app for a new conversation with a question-answering bot.\n",
    "    \"\"\"\n",
    "\n",
    "    # Blank conversation memory.\n",
    "    memory = ConversationSummaryBufferMemory(\n",
    "        max_token_limit=650,\n",
    "        llm=llm,\n",
    "        memory_key=\"chat_history\",\n",
    "        output_key='answer'\n",
    "    )\n",
    "\n",
    "    docsearch = get_doc_search()\n",
    "\n",
    "    # Context retriever.\n",
    "    retriever = docsearch.as_retriever()\n",
    "\n",
    "    ### DIFFERENCES START HERE\n",
    "\n",
    "    # Modified retriever that first filters returned contexts using\n",
    "    # f_qs_relevance with a minimum relevance threshold (of 0.5):\n",
    "    retriever_filtered = WithFeedbackFilterDocuments.of_retriever(\n",
    "        retriever=retriever, feedback=f_qs_relevance, threshold = 0.5\n",
    "    )\n",
    "\n",
    "    ### END OF DIFFERENCES\n",
    "\n",
    "    # Conversational app puts it all together.\n",
    "    app = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever_filtered,\n",
    "        return_source_documents=True,\n",
    "        memory=memory,\n",
    "        get_chat_history=lambda a: a,\n",
    "        max_tokens_limit=4096\n",
    "    )\n",
    "\n",
    "    # Trulens instrumentation.\n",
    "    tc = Tru().Chain(\n",
    "        app_id=f\"{app_id}/v3\",\n",
    "        chain=app,\n",
    "        feedbacks=all_feedbacks,\n",
    "        feedback_mode=feedback_mode\n",
    "    )\n",
    "\n",
    "    return tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the version 3 app:\n",
    "\n",
    "tc3 = v3_new_conversation()\n",
    "\n",
    "# Call the app:\n",
    "\n",
    "res, record = tc3.call_with_record(\"Who is Shayak?\")\n",
    "res\n",
    "\n",
    "# Notice the `source_documents` returned now does not include the low-relevance\n",
    "# chunks and the answer likewise does not reference them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruBot Version 4: Lang match fix and context filter\n",
    "\n",
    "This is left as an exercise to the reader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v4_new_conversation(feedback_mode=FeedbackMode.WITH_APP):\n",
    "    \"\"\"\n",
    "    Create a langchain app for a new conversation with a question-answering bot.\n",
    "    \"\"\"\n",
    "\n",
    "    ### TO FILL IN HERE ###\n",
    "    app = ...\n",
    "    ### END OF TO FILL IN ###\n",
    "\n",
    "    # Trulens instrumentation.\n",
    "    tc = Tru().Chain(\n",
    "        app_id=f\"{app_id}/v4\",\n",
    "        chain=app,\n",
    "        feedbacks=all_feedbacks,\n",
    "        feedback_mode=feedback_mode\n",
    "    )\n",
    "\n",
    "    return tc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test conversations\n",
    "\n",
    "Lets try out the 3 (or 4) trubot versions on a collection of test instances\n",
    "about Shayak and some technical terms in several languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps = [\n",
    "    v1_new_conversation,\n",
    "    v2_new_conversation,\n",
    "    v3_new_conversation,\n",
    "    # v4_new_conversation # include this if you completed the exercise\n",
    "]\n",
    "\n",
    "questions = [\n",
    "    \"Who is Shayak?\", \"Wer ist Shayak?\", \"Kim jest Shayak?\", \"¿Quién es Shayak?\", \n",
    "    \"What is QII?\", \"Was ist QII?\", \"Co jest QII?\", \"¿Que es QII?\"\n",
    "]\n",
    "\n",
    "# Comment out the next two lines to try all of the version and question\n",
    "# combinations. Otherwise we select here only 2 questions and 2 models to start with.\n",
    "apps = apps[0:2]\n",
    "questions = questions[0:2]\n",
    "\n",
    "def test_app_on_question(app, question):\n",
    "    print(app.__name__, question)\n",
    "    app = app(feedback_mode=FeedbackMode.DEFERRED)\n",
    "    answer = app.call_with_record(question)\n",
    "    return answer\n",
    "\n",
    "# This asks all of the questions in parallel:\n",
    "for app in apps:\n",
    "    for question in questions:\n",
    "        TP().promise(\n",
    "            test_app_on_question,\n",
    "            app=app,\n",
    "            question=question\n",
    "        )\n",
    "\n",
    "TP().finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For deferred feedback evaluation. Start this:\n",
    "\n",
    "Tru().start_evaluator(restart=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
