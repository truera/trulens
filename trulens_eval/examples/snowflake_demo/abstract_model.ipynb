{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Optional, Sequence, Dict, Union\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "from trulens_eval.feedback import Feedback\n",
    "from trulens_eval.schema.feedback import FeedbackResult\n",
    "from trulens_eval import LiteLLM\n",
    "from trulens_eval.feedback.provider.base import LLMProvider\n",
    "from trulens_eval import Select\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig()\n",
    "logging.getLogger(\"proxy_model\").setLevel(logging.INFO)\n",
    "logger = logging.getLogger('proxy_model')\n",
    "logger.info('logging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterationFeedbackResults(BaseModel):\n",
    "    passed: bool\n",
    "    feedback_scores: dict[str, FeedbackResult]\n",
    "    source_data: dict[str, Optional[Union[Sequence[Dict], Sequence[str], str]]]\n",
    "\n",
    "class ProxyModelResponse(BaseModel):\n",
    "    response: str\n",
    "    all_responses: list[IterationFeedbackResults]\n",
    "    passed: bool\n",
    "\n",
    "    def print_conversation(self, n_iter=-1):\n",
    "        iteration = self.all_responses[n_iter]\n",
    "        for message in iteration.source_data['messages']:\n",
    "            print(f\"\\x1b[31m{message['role']}\\x1b[0m: {message['content']}\")\n",
    "        response = iteration.source_data['response']\n",
    "        if response:\n",
    "            print(f\"\\x1b[31m{'assistant'}\\x1b[0m: {response}\")\n",
    "    \n",
    "\n",
    "class ModelConfig(BaseModel):\n",
    "    provider: LLMProvider\n",
    "    n_retries: int = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProxyModel:\n",
    "    def __init__(self, model_configs: list[ModelConfig], feedbacks: list[Feedback], feedback_thresholds: Optional[Dict[str, float]] = None):\n",
    "        self.model_configs = model_configs\n",
    "        self.feedbacks = feedbacks\n",
    "        if not feedback_thresholds:\n",
    "            feedback_thresholds = {feedback.name: .5 for feedback in self.feedbacks}\n",
    "        self.feedback_thresholds = feedback_thresholds\n",
    "        self._validate_args()\n",
    "\n",
    "    def _validate_args(self):\n",
    "        assert len(self.feedback_thresholds) == len(self.feedbacks)\n",
    "        for model_config in self.model_configs:\n",
    "            assert isinstance(model_config, ModelConfig)\n",
    "        \n",
    "        for feedback in self.feedbacks:\n",
    "            assert isinstance(feedback, Feedback)\n",
    "            assert feedback.name in self.feedback_thresholds\n",
    "            assert 0 < self.feedback_thresholds[feedback.name] < 1\n",
    "\n",
    "    def _feedback_results_as_user_response(self, feedback_results: IterationFeedbackResults, prompt: str):\n",
    "        response_buffer = f\"Feedback: {'good response' if feedback_results.passed else \"bad response\"}\"\n",
    "        for name, result in feedback_results.feedback_scores.items():\n",
    "            response_buffer += f\"\\n{name} score: {result.result}/1.0\"\n",
    "            reasons = []\n",
    "            for call in result.calls:\n",
    "                if \"reason\" in call.meta:\n",
    "                    reasons.append(call.meta['reason'])\n",
    "            reasons_str = \"\\n - \" + \"\\n - \".join(reasons)\n",
    "            response_buffer += f\"\\nReasoning:\\n{reasons_str}\"\n",
    "        response_buffer += f\"\\nGiven this feedback, answer the prompt again. {prompt}\"\n",
    "        return response_buffer\n",
    "    \n",
    "    def _format_prompt(self, prompt: str, contexts: Sequence[str]):\n",
    "        context_str = \"\\n - \" + \"\\n - \".join(contexts)\n",
    "        return f\"Use the context to answer this prompt.\\nCONTEXT: {context_str}\\nPROMPT: {prompt}\"\n",
    "    \n",
    "    def rag_chat(self, prompt: str, contexts: Optional[Sequence[str]] = None, messages: Optional[Sequence[Dict]] = None, **kwargs):\n",
    "        if contexts:\n",
    "            prompt = self._format_prompt(prompt, contexts)\n",
    "        return self._create_chat_completion(prompt=prompt, messages=messages, contexts=contexts, **kwargs)\n",
    "\n",
    "    def _create_chat_completion(\n",
    "        self,\n",
    "        prompt: Optional[str] = None,\n",
    "        messages: Optional[Sequence[Dict]] = None,\n",
    "        **kwargs\n",
    "    ) -> ProxyModelResponse:\n",
    "        all_responses = []\n",
    "\n",
    "        messages = messages or []\n",
    "        if prompt is not None:\n",
    "            messages.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            })\n",
    "        \n",
    "        for model_config in self.model_configs:\n",
    "            for n_iter in range(1, model_config.n_retries + 1):\n",
    "                response = model_config.provider._create_chat_completion(messages=messages)\n",
    "                source_data = {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"messages\": messages,\n",
    "                    \"response\": response,\n",
    "                } | kwargs\n",
    "                feedback_results = self._score_feedback(source_data)\n",
    "                all_responses.append(feedback_results)\n",
    "                messages.append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": response\n",
    "                })\n",
    "                \n",
    "                if feedback_results.passed:\n",
    "                    logger.info(f\"({n_iter}/{model_config.n_retries}) {model_config.provider.model_engine} passed.\")\n",
    "                    break\n",
    "                \n",
    "                logger.info(f\"({n_iter}/{model_config.n_retries}) {model_config.provider.model_engine} did not pass feedback thresholds. Escalating\")\n",
    "                messages.append({\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": self._feedback_results_as_user_response(feedback_results=feedback_results, prompt=prompt)\n",
    "                })\n",
    "\n",
    "            if feedback_results.passed:\n",
    "                break\n",
    "            else:\n",
    "                logger.info(f\"{model_config.provider.model_engine} failed {model_config.n_retries} times. Escalating\")\n",
    "\n",
    "        return ProxyModelResponse(\n",
    "            response=response, \n",
    "            all_responses=all_responses, \n",
    "            passed=feedback_results.passed\n",
    "        )\n",
    "    \n",
    "    def _score_feedback(\n",
    "        self, \n",
    "        source_data: dict[str, str]\n",
    "    ) -> IterationFeedbackResults:\n",
    "        feedback_scores = {feedback.name: feedback.run(source_data=source_data) for feedback in self.feedbacks}\n",
    "        passed = True\n",
    "        for name, threshold in self.feedback_thresholds.items():\n",
    "            if feedback_scores[name].result is not None and feedback_scores[name].result < threshold:\n",
    "                passed = False\n",
    "                break\n",
    "        return IterationFeedbackResults(\n",
    "            source_data=source_data, \n",
    "            passed=passed, \n",
    "            feedback_scores=feedback_scores\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = [\n",
    "    ModelConfig(provider=LiteLLM(\"replicate/mistralai/mistral-7b-instruct-v0.2\"), n_retries=2),\n",
    "    ModelConfig(provider=LiteLLM(\"replicate/meta/llama-2-70b-chat\"), n_retries=2),\n",
    "    ModelConfig(provider=LiteLLM(\"azure/sfc-ml-sweden-gpt4-managed\", completion_kwargs={\"api_base\": \"https://sfc-apim-sweden.azure-api.net\"}), n_retries=2),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs[-1].provider._create_chat_completion(\"What is 2+2?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_provider = LiteLLM(\"azure/sfc-ml-sweden-gpt4-managed\", completion_kwargs={\"api_base\": \"https://sfc-apim-sweden.azure-api.net\"})\n",
    "feedbacks = [\n",
    "    Feedback(feedback_provider.relevance_with_cot_reasons, name=\"answer_relevance\").on(Select.Tru.prompt).on(Select.Tru.response),\n",
    "    # Feedback(feedback_provider.context_relevance_with_cot_reasons, name=\"context relevance\").on(Select.Tru.prompt).on(Select.Tru.contexts[:]),\n",
    "    # Feedback(feedback_provider.groundedness_measure_with_cot_reasons, name=\"groundedness\").on(Select.Tru.contexts[:]).on(Select.Tru.response)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ProxyModel(\n",
    "    model_configs=model_configs,\n",
    "    feedbacks=feedbacks,\n",
    "    feedback_thresholds={feedback.name: .9 for feedback in feedbacks}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = model.rag_chat(\n",
    "    prompt=\"Why was Franklin born?\", \n",
    "    contexts=[\n",
    "        \"Benjamin Franklin FRS FRSA FRSE (January 17, 1706 [O.S. January 6, 1705][Note 1] â€“ April 17, 1790) was an American polymath: a leading writer, scientist, inventor, statesman, diplomat, printer, publisher, and political philosopher.[1] Among the most influential intellectuals of his time, Franklin was one of the Founding Fathers of the United States; a drafter and signer of the Declaration of Independence; and the first postmaster general.[2]\",\n",
    "        \"Franklin became a successful newspaper editor and printer in Philadelphia, the leading city in the colonies, publishing the Pennsylvania Gazette at age 23.[3] He became wealthy publishing this and Poor Richard's Almanack, which he wrote under the pseudonym 'Richard Saunders'.[4] After 1767, he was associated with the Pennsylvania Chronicle, a newspaper known for its revolutionary sentiments and criticisms of the policies of the British Parliament and the Crown.[5]\",\n",
    "        \"Benjamin Franklin's father, Josiah Franklin, was a tallow chandler, soaper, and candlemaker. Josiah Franklin was born at Ecton, Northamptonshire, England, on December 23, 1657, the son of Thomas Franklin, a blacksmith and farmer, and his wife, Jane White. Benjamin's father and all four of his grandparents were born in England.\"\n",
    "    ]\n",
    ")\n",
    "resp.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.all_responses[0].feedback_scores['answer_relevance'].result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.all_responses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.print_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
