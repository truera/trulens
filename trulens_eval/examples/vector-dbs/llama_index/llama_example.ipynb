{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport logging\\n\\nroot = logging.getLogger()\\nroot.setLevel(logging.DEBUG)\\n\\nhandler = logging.StreamHandler(sys.stdout)\\nhandler.setLevel(logging.DEBUG)\\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\\nhandler.setFormatter(formatter)\\nroot.addHandler(handler)\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(Path().cwd().parent.parent.parent.resolve()))\n",
    "\n",
    "from pprint import PrettyPrinter\n",
    "pp = PrettyPrinter()\n",
    "\n",
    "# Uncomment to get more debugging printouts:\n",
    "\"\"\"\n",
    "import logging\n",
    "\n",
    "root = logging.getLogger()\n",
    "root.setLevel(logging.DEBUG)\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "root.addHandler(handler)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEY SET: OPENAI_API_KEY\n",
      "KEY SET: PINECONE_API_KEY\n",
      "KEY SET: PINECONE_ENV\n",
      "KEY SET: HUGGINGFACE_API_KEY\n",
      "KEY SET: SLACK_TOKEN\n",
      "KEY SET: SLACK_SIGNING_SECRET\n",
      "KEY SET: COHERE_API_KEY\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval.keys import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "# response = query_engine.query(\"What did the author do growing up?\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For aggregation,\n",
    "import numpy as np\n",
    "\n",
    "from trulens_eval import feedback, Feedback, Query, Tru, TruLlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c199d45f24e456da7f56bb28e2f0c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "huggingface api: 0requests [00:00, ?requests/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b1f31cc5b04435bd44b3a425be0262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openai api: 0requests [00:00, ?requests/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In language_match, input text1 will be set to *.__record__.main_input or `Query.RecordInput` .\n",
      "✅ In language_match, input text2 will be set to *.__record__.main_output or `Query.RecordOutput` .\n",
      "✅ In relevance, input prompt will be set to *.__record__.main_input or `Query.RecordInput` .\n",
      "✅ In relevance, input response will be set to *.__record__.main_output or `Query.RecordOutput` .\n",
      "✅ In qs_relevance, input question will be set to *.__record__.main_input or `Query.RecordInput` .\n",
      "✅ In qs_relevance, input statement will be set to *.__record__.app.query.rets.source_nodes[:].node.text .\n"
     ]
    }
   ],
   "source": [
    "# Construct feedback functions.\n",
    "\n",
    "hugs = feedback.Huggingface()\n",
    "openai = feedback.OpenAI()\n",
    "\n",
    "# Language match between question/answer.\n",
    "f_lang_match = Feedback(hugs.language_match).on_input_output()\n",
    "# By default this will evaluate feedback on main app input and main app output.\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = Feedback(openai.relevance).on_input_output()\n",
    "# Same default inputs as the above.\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_qs_relevance = Feedback(openai.qs_relevance).on_input().on(\n",
    "    TruLlama.select_source_nodes().node.text\n",
    ").aggregate(np.min)\n",
    "# First feedback arg is the main app input while the second is the context\n",
    "# chunks retrieved from the main app output, output of `query`.\n",
    "\n",
    "feedbacks = [\n",
    "#    f_lang_match, \n",
    "#    f_qa_relevance, \n",
    "#    f_qs_relevance\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ app app_hash_6e1ec7f7e22a6bb5b127530aec670f8b -> default.sqlite\n"
     ]
    }
   ],
   "source": [
    "l = TruLlama(app=query_engine, feedbacks=feedbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other component: *.__app__.app\n",
      "Other component: *.__app__.app._response_synthesizer\n",
      "Other component: *.__app__.app._response_synthesizer._response_builder\n",
      "Prompt component: *.__app__.app._response_synthesizer._response_builder._refine_template\n",
      "Prompt component: *.__app__.app._response_synthesizer._response_builder._refine_template.prompt\n",
      "Other component: *.__app__.app._response_synthesizer._response_builder.service_context\n",
      "Other component: *.__app__.app._response_synthesizer._response_builder.service_context.embed_model\n",
      "Other component: *.__app__.app._response_synthesizer._response_builder.service_context.llm_predictor\n",
      "LLM component: *.__app__.app._response_synthesizer._response_builder.service_context.llm_predictor.llm\n",
      "Other component: *.__app__.app._response_synthesizer._response_builder.service_context.node_parser\n",
      "Other component: *.__app__.app._response_synthesizer._response_builder.service_context.prompt_helper\n",
      "Prompt component: *.__app__.app._response_synthesizer._response_builder.text_qa_template\n",
      "Prompt component: *.__app__.app._response_synthesizer._response_builder.text_qa_template.prompt\n",
      "Other component: *.__app__.app.retriever\n",
      "Other component: *.__app__.app.retriever._index\n",
      "Other component: *.__app__.app.retriever._index.service_context\n",
      "Other component: *.__app__.app.retriever._index.service_context.embed_model\n",
      "Other component: *.__app__.app.retriever._index.service_context.llm_predictor\n",
      "LLM component: *.__app__.app.retriever._index.service_context.llm_predictor.llm\n",
      "Other component: *.__app__.app.retriever._index.service_context.node_parser\n",
      "Other component: *.__app__.app.retriever._index.service_context.prompt_helper\n",
      "Other component: *.__app__.app.retriever._index.vector_store\n",
      "Other component: *.__app__.app.retriever._service_context\n",
      "Other component: *.__app__.app.retriever._service_context.embed_model\n",
      "Other component: *.__app__.app.retriever._service_context.llm_predictor\n",
      "LLM component: *.__app__.app.retriever._service_context.llm_predictor.llm\n",
      "Other component: *.__app__.app.retriever._service_context.node_parser\n",
      "Other component: *.__app__.app.retriever._service_context.prompt_helper\n",
      "Other component: *.__app__.app.retriever._vector_store\n"
     ]
    }
   ],
   "source": [
    "# Show which components of the llama index app have been instrumented (will be\n",
    "# tracked as components in the dashboard).\n",
    "\n",
    "l.print_instrumented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1818 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "res, record = l.query_with_record(\"Who is Shayak?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ record record_hash_e463aa0f533d752b586a31c7b8385d34 from app_hash_6e1ec7f7e22a6bb5b127530aec670f8b -> default.sqlite\n"
     ]
    }
   ],
   "source": [
    "# Start the dashboard here:\n",
    "# proc = Tru().start_dashboard(force=True, _dev=Path.cwd().parent)\n",
    "\n",
    "# If using deferred feedback evaluation, need to start this too:\n",
    "# thread = Tru().start_evaluator(restart=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = f_qs_relevance.run(record=record, app=l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feedback_result_id': 'feedback_result_hash_00b769b19601e220a854aacd7a6f8257',\n",
       " 'record_id': 'record_hash_e463aa0f533d752b586a31c7b8385d34',\n",
       " 'feedback_definition_id': 'feedback_definition_hash_ca90883398099199aac8facf66f0aaae',\n",
       " 'last_ts': datetime.datetime(2023, 6, 20, 14, 19, 4, 536474),\n",
       " 'status': <FeedbackResultStatus.DONE: 'done'>,\n",
       " 'cost': {'n_tokens': 0, 'cost': 0.0},\n",
       " 'tags': '',\n",
       " 'name': 'qs_relevance',\n",
       " 'calls': [{'args': {'question': 'Who is Shayak?',\n",
       "    'statement': \"of my attention. It had already eaten Arc, and was in the process of eating essays too. Either YC was my life's work or I'd have to leave eventually. And it wasn't, so I would.\\n\\nIn the summer of 2012 my mother had a stroke, and the cause turned out to be a blood clot caused by colon cancer. The stroke destroyed her balance, and she was put in a nursing home, but she really wanted to get out of it and back to her house, and my sister and I were determined to help her do it. I used to fly up to Oregon to visit her regularly, and I had a lot of time to think on those flights. On one of them I realized I was ready to hand YC over to someone else.\\n\\nI asked Jessica if she wanted to be president, but she didn't, so we decided we'd try to recruit Sam Altman. We talked to Robert and Trevor and we agreed to make it a complete changing of the guard. Up till that point YC had been controlled by the original LLC we four had started. But we wanted YC to last for a long time, and to do that it couldn't be controlled by the founders. So if Sam said yes, we'd let him reorganize YC. Robert and I would retire, and Jessica and Trevor would become ordinary partners.\\n\\nWhen we asked Sam if he wanted to be president of YC, initially he said no. He wanted to start a startup to make nuclear reactors. But I kept at it, and in October 2013 he finally agreed. We decided he'd take over starting with the winter 2014 batch. For the rest of 2013 I left running YC more and more to Sam, partly so he could learn the job, and partly because I was focused on my mother, whose cancer had returned.\\n\\nShe died on January 15, 2014. We knew this was coming, but it was still hard when it did.\\n\\nI kept working on YC till March, to help get that batch of startups through Demo Day, then I checked out pretty completely. (I still talk to alumni and to new startups working on things I'm interested in, but that only takes a few hours a week.)\\n\\nWhat should I do next? Rtm's advice hadn't included anything about that. I wanted to do something completely different, so I decided I'd paint. I wanted to see how good I could get if I really focused on it. So the day after I stopped working on YC, I started painting. I was rusty and it took a while to get back into shape, but it was at least completely engaging. [18]\\n\\nI spent most of the rest of 2014 painting. I'd never been able to work so uninterruptedly before, and I got to be better than I had been. Not good enough, but better. Then in November, right in the middle of a painting, I ran out of steam. Up till that point I'd always been curious to see how the painting I was working on would turn out, but suddenly finishing this one seemed like a chore. So I stopped working on it and cleaned my brushes and haven't painted since. So far anyway.\\n\\nI realize that sounds rather wimpy. But attention is a zero sum game. If you can choose what to work on, and you choose a project that's not the best one (or at least a good one) for you, then it's getting in the way of another project that is. And at 50 there was some opportunity cost to screwing around.\\n\\nI started writing essays again, and wrote a bunch of new ones over the next few months. I even wrote a couple that weren't about startups. Then in March 2015 I started working on Lisp again.\\n\\nThe distinctive thing about Lisp is that its core is a language defined by writing an interpreter in itself. It wasn't originally intended as a programming language in the ordinary sense. It was meant to be a formal model of computation, an alternative to the Turing machine. If you want to write an interpreter for a language in itself, what's the minimum set of predefined operators you need? The Lisp that John McCarthy invented, or more accurately discovered, is an answer to that question. [19]\\n\\nMcCarthy didn't realize this Lisp could even be used to program computers\"},\n",
       "   'ret': 0.1},\n",
       "  {'args': {'question': 'Who is Shayak?',\n",
       "    'statement': 'documentary that showed Terry Winograd using SHRDLU. I haven\\'t tried rereading The Moon is a Harsh Mistress, so I don\\'t know how well it has aged, but when I read it I was drawn entirely into its world. It seemed only a matter of time before we\\'d have Mike, and when I saw Winograd using SHRDLU, it seemed like that time would be a few years at most. All you had to do was teach SHRDLU more words.\\n\\nThere weren\\'t any classes in AI at Cornell then, not even graduate classes, so I started trying to teach myself. Which meant learning Lisp, since in those days Lisp was regarded as the language of AI. The commonly used programming languages then were pretty primitive, and programmers\\' ideas correspondingly so. The default language at Cornell was a Pascal-like language called PL/I, and the situation was similar elsewhere. Learning Lisp expanded my concept of a program so fast that it was years before I started to have a sense of where the new limits were. This was more like it; this was what I had expected college to do. It wasn\\'t happening in a class, like it was supposed to, but that was ok. For the next couple years I was on a roll. I knew what I was going to do.\\n\\nFor my undergraduate thesis, I reverse-engineered SHRDLU. My God did I love working on that program. It was a pleasing bit of code, but what made it even more exciting was my belief — hard to imagine now, but not unique in 1985 — that it was already climbing the lower slopes of intelligence.\\n\\nI had gotten into a program at Cornell that didn\\'t make you choose a major. You could take whatever classes you liked, and choose whatever you liked to put on your degree. I of course chose \"Artificial Intelligence.\" When I got the actual physical diploma, I was dismayed to find that the quotes had been included, which made them read as scare-quotes. At the time this bothered me, but now it seems amusingly accurate, for reasons I was about to discover.\\n\\nI applied to 3 grad schools: MIT and Yale, which were renowned for AI at the time, and Harvard, which I\\'d visited because Rich Draves went there, and was also home to Bill Woods, who\\'d invented the type of parser I used in my SHRDLU clone. Only Harvard accepted me, so that was where I went.\\n\\nI don\\'t remember the moment it happened, or if there even was a specific moment, but during the first year of grad school I realized that AI, as practiced at the time, was a hoax. By which I mean the sort of AI in which a program that\\'s told \"the dog is sitting on the chair\" translates this into some formal representation and adds it to the list of things it knows.\\n\\nWhat these programs really showed was that there\\'s a subset of natural language that\\'s a formal language. But a very proper subset. It was clear that there was an unbridgeable gap between what they could do and actually understanding natural language. It was not, in fact, simply a matter of teaching SHRDLU more words. That whole way of doing AI, with explicit data structures representing concepts, was not going to work. Its brokenness did, as so often happens, generate a lot of opportunities to write papers about various band-aids that could be applied to it, but it was never going to get us Mike.\\n\\nSo I looked around to see what I could salvage from the wreckage of my plans, and there was Lisp. I knew from experience that Lisp was interesting for its own sake and not just for its association with AI, even though that was the main reason people cared about it at the time. So I decided to focus on Lisp. In fact, I decided to write a book about Lisp hacking. It\\'s scary to think how little I knew about Lisp hacking when I started writing that book. But there\\'s nothing like writing a book about something to help you learn it. The book, On Lisp, wasn\\'t published till 1993, but I wrote much of it in grad school.\\n\\nComputer Science is an uneasy alliance between two halves, theory and systems. The'},\n",
       "   'ret': 0.1}],\n",
       " 'result': 0.1,\n",
       " 'error': None}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
