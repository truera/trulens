,record,event_id,record_attributes,record_type,resource_attributes,start_timestamp,timestamp,trace
0,"{'name': 'llama_index.core.base.base_query_engine.BaseQueryEngine.query', 'kind': 1, 'parent_span_id': '', 'status': 'STATUS_CODE_UNSET'}",12662027539902430486,"{'name': 'llama_index.core.base.base_query_engine.BaseQueryEngine.query', 'ai.observability.span_type': 'record_root', 'ai.observability.app_name': 'Simple RAG', 'ai.observability.app_version': 'v1', 'ai.observability.record_id': 'da781265-5bfa-4ee4-9da8-8f37c10b0853', 'ai.observability.run.name': 'test run', 'ai.observability.input_id': '42', 'ai.observability.call.return': 'Context information is below.\n---------------------\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19].\n\npage_label: 3\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nTo facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: ', 'ai.observability.call.function': 'llama_index.core.base.base_query_engine.BaseQueryEngine.query', 'ai.observability.call.kwargs.str_or_query_bundle': 'What is multi-headed attention?', 'ai.observability.record_root.input': 'What is multi-headed attention?', 'ai.observability.record_root.output': 'Context information is below.\n---------------------\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19].\n\npage_label: 3\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nTo facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: '}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.32.1', 'service.name': 'trulens'}",2025-04-30 08:53:43.671847,2025-04-30 08:53:43.679143,"{'trace_id': '149232111067984740000449897198695001087', 'parent_id': '', 'span_id': '12662027539902430486'}"
1,"{'name': 'llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine.retrieve', 'kind': 1, 'parent_span_id': '12662027539902430486', 'status': 'STATUS_CODE_UNSET'}",11603158240846042077,"{'name': 'llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine.retrieve', 'ai.observability.span_type': 'retrieval', 'ai.observability.app_name': 'Simple RAG', 'ai.observability.app_version': 'v1', 'ai.observability.record_id': 'da781265-5bfa-4ee4-9da8-8f37c10b0853', 'ai.observability.run.name': 'test run', 'ai.observability.input_id': '42', 'ai.observability.call.return': ['Node ID: 00183213-440d-49b9-86da-15959db0f28b\nText: Recent work has achieved significant improvements in\ncomputational efficiency through factorization tricks [21] and\nconditional computation [32], while also improving model performance\nin case of the latter. The fundamental constraint of sequential\ncomputation, however, remains. Attention mechanisms have become an\nintegral part of compelling seq...\nScore:  0.900\n', 'Node ID: 1bbdc9cd-b8cb-43e4-a788-9484e9f74638\nText: To facilitate these residual connections, all sub-layers in the\nmodel, as well as the embedding layers, produce outputs of dimension\ndmodel = 512. Decoder: The decoder is also composed of a stack of N =\n6identical layers. In addition to the two sub-layers in each encoder\nlayer, the decoder inserts a third sub-layer, which performs multi-\nhead att...\nScore:  0.891\n', 'Node ID: 327aa2c2-21b0-416a-acfc-d61bd1895706\nText: Recurrent models typically factor computation along the symbol\npositions of the input and output sequences. Aligning the positions to\nsteps in computation time, they generate a sequence of hidden states\nht, as a function of the previous hidden state ht−1 and the input for\nposition t. This inherently sequential nature precludes\nparallelization wi...\nScore:  0.880\n'], 'ai.observability.call.function': 'llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine.retrieve', 'ai.observability.call.kwargs.query_bundle': 'What is multi-headed attention?', 'ai.observability.retrieval.retrieved_contexts': ['Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19].', 'To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack.', 'Recurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples.'], 'ai.observability.retrieval.query_text': 'What is multi-headed attention?'}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.32.1', 'service.name': 'trulens'}",2025-04-30 08:53:43.672027,2025-04-30 08:53:43.673582,"{'trace_id': '149232111067984740000449897198695001087', 'parent_id': '12662027539902430486', 'span_id': '11603158240846042077'}"
2,"{'name': 'llama_index.core.base.base_retriever.BaseRetriever.retrieve', 'kind': 1, 'parent_span_id': '11603158240846042077', 'status': 'STATUS_CODE_UNSET'}",16775352935683176830,"{'name': 'llama_index.core.base.base_retriever.BaseRetriever.retrieve', 'ai.observability.span_type': 'retrieval', 'ai.observability.app_name': 'Simple RAG', 'ai.observability.app_version': 'v1', 'ai.observability.record_id': 'da781265-5bfa-4ee4-9da8-8f37c10b0853', 'ai.observability.run.name': 'test run', 'ai.observability.input_id': '42', 'ai.observability.call.return': ['Node ID: 00183213-440d-49b9-86da-15959db0f28b\nText: Recent work has achieved significant improvements in\ncomputational efficiency through factorization tricks [21] and\nconditional computation [32], while also improving model performance\nin case of the latter. The fundamental constraint of sequential\ncomputation, however, remains. Attention mechanisms have become an\nintegral part of compelling seq...\nScore:  0.900\n', 'Node ID: 1bbdc9cd-b8cb-43e4-a788-9484e9f74638\nText: To facilitate these residual connections, all sub-layers in the\nmodel, as well as the embedding layers, produce outputs of dimension\ndmodel = 512. Decoder: The decoder is also composed of a stack of N =\n6identical layers. In addition to the two sub-layers in each encoder\nlayer, the decoder inserts a third sub-layer, which performs multi-\nhead att...\nScore:  0.891\n', 'Node ID: 327aa2c2-21b0-416a-acfc-d61bd1895706\nText: Recurrent models typically factor computation along the symbol\npositions of the input and output sequences. Aligning the positions to\nsteps in computation time, they generate a sequence of hidden states\nht, as a function of the previous hidden state ht−1 and the input for\nposition t. This inherently sequential nature precludes\nparallelization wi...\nScore:  0.880\n'], 'ai.observability.call.function': 'llama_index.core.base.base_retriever.BaseRetriever.retrieve', 'ai.observability.call.kwargs.str_or_query_bundle': 'What is multi-headed attention?', 'ai.observability.retrieval.retrieved_contexts': ['Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19].', 'To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack.', 'Recurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples.'], 'ai.observability.retrieval.query_text': 'What is multi-headed attention?'}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.32.1', 'service.name': 'trulens'}",2025-04-30 08:53:43.672072,2025-04-30 08:53:43.673419,"{'trace_id': '149232111067984740000449897198695001087', 'parent_id': '11603158240846042077', 'span_id': '16775352935683176830'}"
3,"{'name': 'llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever._retrieve', 'kind': 1, 'parent_span_id': '16775352935683176830', 'status': 'STATUS_CODE_UNSET'}",10435120658017545869,"{'name': 'llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever._retrieve', 'ai.observability.span_type': 'retrieval', 'ai.observability.app_name': 'Simple RAG', 'ai.observability.app_version': 'v1', 'ai.observability.record_id': 'da781265-5bfa-4ee4-9da8-8f37c10b0853', 'ai.observability.run.name': 'test run', 'ai.observability.input_id': '42', 'ai.observability.call.return': ['Node ID: 00183213-440d-49b9-86da-15959db0f28b\nText: Recent work has achieved significant improvements in\ncomputational efficiency through factorization tricks [21] and\nconditional computation [32], while also improving model performance\nin case of the latter. The fundamental constraint of sequential\ncomputation, however, remains. Attention mechanisms have become an\nintegral part of compelling seq...\nScore:  0.900\n', 'Node ID: 1bbdc9cd-b8cb-43e4-a788-9484e9f74638\nText: To facilitate these residual connections, all sub-layers in the\nmodel, as well as the embedding layers, produce outputs of dimension\ndmodel = 512. Decoder: The decoder is also composed of a stack of N =\n6identical layers. In addition to the two sub-layers in each encoder\nlayer, the decoder inserts a third sub-layer, which performs multi-\nhead att...\nScore:  0.891\n', 'Node ID: 327aa2c2-21b0-416a-acfc-d61bd1895706\nText: Recurrent models typically factor computation along the symbol\npositions of the input and output sequences. Aligning the positions to\nsteps in computation time, they generate a sequence of hidden states\nht, as a function of the previous hidden state ht−1 and the input for\nposition t. This inherently sequential nature precludes\nparallelization wi...\nScore:  0.880\n'], 'ai.observability.call.function': 'llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever._retrieve', 'ai.observability.call.kwargs.query_bundle': 'What is multi-headed attention?', 'ai.observability.retrieval.retrieved_contexts': ['Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19].', 'To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack.', 'Recurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples.'], 'ai.observability.retrieval.query_text': 'What is multi-headed attention?'}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.32.1', 'service.name': 'trulens'}",2025-04-30 08:53:43.672155,2025-04-30 08:53:43.673184,"{'trace_id': '149232111067984740000449897198695001087', 'parent_id': '16775352935683176830', 'span_id': '10435120658017545869'}"
4,"{'name': 'llama_index.core.response_synthesizers.compact_and_refine.CompactAndRefine.get_response', 'kind': 1, 'parent_span_id': '12662027539902430486', 'status': 'STATUS_CODE_UNSET'}",7262605714750521467,"{'name': 'llama_index.core.response_synthesizers.compact_and_refine.CompactAndRefine.get_response', 'ai.observability.span_type': 'unknown', 'ai.observability.app_name': 'Simple RAG', 'ai.observability.app_version': 'v1', 'ai.observability.record_id': 'da781265-5bfa-4ee4-9da8-8f37c10b0853', 'ai.observability.run.name': 'test run', 'ai.observability.input_id': '42', 'ai.observability.call.return': 'Context information is below.\n---------------------\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19].\n\npage_label: 3\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nTo facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: ', 'ai.observability.call.function': 'llama_index.core.response_synthesizers.compact_and_refine.CompactAndRefine.get_response', 'ai.observability.call.kwargs.query_str': 'What is multi-headed attention?', 'ai.observability.call.kwargs.text_chunks': ['page_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19].', 'page_label: 3\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nTo facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack.', 'page_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples.']}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.32.1', 'service.name': 'trulens'}",2025-04-30 08:53:43.673681,2025-04-30 08:53:43.678992,"{'trace_id': '149232111067984740000449897198695001087', 'parent_id': '12662027539902430486', 'span_id': '7262605714750521467'}"
5,"{'name': 'llama_index.core.response_synthesizers.refine.Refine.get_response', 'kind': 1, 'parent_span_id': '7262605714750521467', 'status': 'STATUS_CODE_UNSET'}",1410373575696979816,"{'name': 'llama_index.core.response_synthesizers.refine.Refine.get_response', 'ai.observability.span_type': 'unknown', 'ai.observability.app_name': 'Simple RAG', 'ai.observability.app_version': 'v1', 'ai.observability.record_id': 'da781265-5bfa-4ee4-9da8-8f37c10b0853', 'ai.observability.run.name': 'test run', 'ai.observability.input_id': '42', 'ai.observability.call.return': 'Context information is below.\n---------------------\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19].\n\npage_label: 3\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nTo facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: ', 'ai.observability.call.function': 'llama_index.core.response_synthesizers.refine.Refine.get_response', 'ai.observability.call.kwargs.query_str': 'What is multi-headed attention?', 'ai.observability.call.kwargs.text_chunks': ['page_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19].\n\npage_label: 3\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nTo facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples.']}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.32.1', 'service.name': 'trulens'}",2025-04-30 08:53:43.678337,2025-04-30 08:53:43.678960,"{'trace_id': '149232111067984740000449897198695001087', 'parent_id': '7262605714750521467', 'span_id': '1410373575696979816'}"
6,"{'name': 'llama_index.core.llms.mock.MockLLM.complete', 'kind': 1, 'parent_span_id': '1410373575696979816', 'status': 'STATUS_CODE_UNSET'}",10324387262068202018,"{'name': 'llama_index.core.llms.mock.MockLLM.complete', 'ai.observability.span_type': 'unknown', 'ai.observability.app_name': 'Simple RAG', 'ai.observability.app_version': 'v1', 'ai.observability.record_id': 'da781265-5bfa-4ee4-9da8-8f37c10b0853', 'ai.observability.run.name': 'test run', 'ai.observability.input_id': '42', 'ai.observability.call.return': 'Context information is below.\n---------------------\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19].\n\npage_label: 3\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nTo facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: ', 'ai.observability.call.function': 'llama_index.core.llms.mock.MockLLM.complete', 'ai.observability.call.kwargs.formatted': True, 'ai.observability.call.kwargs.args': ['Context information is below.\n---------------------\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19].\n\npage_label: 3\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nTo facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: '], 'ai.observability.call.kwargs.kwargs': '{""formatted"": true}'}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.32.1', 'service.name': 'trulens'}",2025-04-30 08:53:43.678756,2025-04-30 08:53:43.678900,"{'trace_id': '149232111067984740000449897198695001087', 'parent_id': '1410373575696979816', 'span_id': '10324387262068202018'}"
