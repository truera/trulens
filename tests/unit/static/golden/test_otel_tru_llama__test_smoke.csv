record,event_id,record_attributes,record_type,resource_attributes,start_timestamp,timestamp,trace
"{'name': 'llama_index.core.base.base_query_engine.BaseQueryEngine.query', 'kind': 1, 'parent_span_id': '', 'status': 'STATUS_CODE_UNSET'}",14023480616735475011,"{'ai.observability.span_type': 'record_root', 'ai.observability.app_name': 'Simple RAG', 'ai.observability.app_version': 'v1', 'ai.observability.app_id': 'app_hash_f68a15a65f74a98f9981e35440ef56ab', 'ai.observability.record_id': 'ffa8b184-1011-4e16-b3a6-9b872d19f37c', 'ai.observability.run.name': 'test run', 'ai.observability.input_id': '42', 'ai.observability.record_root.input': 'What is multi-headed attention?', 'ai.observability.record_root.output': 'Context information is below.\n---------------------\npage_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nThe input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices KandV.\n\npage_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nOn each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\ni=1qiki, has mean 0and variance dk.\n4\n\npage_label: 5\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nIn the embedding layers, we multiply those weights by√dmodel.\n5\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: ', 'ai.observability.call.return': 'Context information is below.\n---------------------\npage_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nThe input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices KandV.\n\npage_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nOn each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\ni=1qiki, has mean 0and variance dk.\n4\n\npage_label: 5\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nIn the embedding layers, we multiply those weights by√dmodel.\n5\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: ', 'ai.observability.call.function': 'llama_index.core.base.base_query_engine.BaseQueryEngine.query', 'ai.observability.call.kwargs.str_or_query_bundle': 'What is multi-headed attention?'}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.34.1', 'service.name': 'trulens', 'ai.observability.app_id': 'app_hash_f68a15a65f74a98f9981e35440ef56ab', 'ai.observability.app_name': 'Simple RAG', 'ai.observability.app_version': 'v1'}",2025-09-10 21:16:09.152696,2025-09-10 21:16:09.158006,"{'trace_id': '160222885005595993857975228876295133827', 'parent_id': '', 'span_id': '14023480616735475011'}"
"{'name': 'llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine.retrieve', 'kind': 1, 'parent_span_id': '14023480616735475011', 'status': 'STATUS_CODE_UNSET'}",17595210022942811015,"{'ai.observability.span_type': 'retrieval', 'ai.observability.app_name': 'Simple RAG', 'ai.observability.app_version': 'v1', 'ai.observability.app_id': 'app_hash_f68a15a65f74a98f9981e35440ef56ab', 'ai.observability.record_id': 'ffa8b184-1011-4e16-b3a6-9b872d19f37c', 'ai.observability.run.name': 'test run', 'ai.observability.input_id': '42', 'ai.observability.call.return': ['Node ID: b4fcd7c5-87da-405d-b95c-71c242fdc643\nText: The input consists of queries and keys of dimension dk, and\nvalues of dimension dv. We compute the dot products of the query with\nall keys, divide each by√dk, and apply a softmax function to obtain\nthe weights on the values. In practice, we compute the attention\nfunction on a set of queries simultaneously, packed together into a\nmatrix Q. The ke...\nScore:  0.913\n', 'Node ID: 46a9e6b5-ec70-49cf-acbd-40de18697715\nText: On each of these projected versions of queries, keys and values\nwe then perform the attention function in parallel, yielding dv-\ndimensional 4To illustrate why the dot products get large, assume that\nthe components of qandkare independent random variables with mean 0and\nvariance 1. Then their dot product, q·k=Pdk i=1qiki, has mean 0and\nvariance d...\nScore:  0.874\n', 'Node ID: ff58b4f1-b13d-4a25-970f-190c27517d5f\nText: In the embedding layers, we multiply those weights by√dmodel. 5\nScore:  0.871\n'], 'ai.observability.call.function': 'llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine.retrieve', 'ai.observability.call.kwargs.query_bundle': 'What is multi-headed attention?', 'ai.observability.retrieval.retrieved_contexts': ['The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices KandV.', 'On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\ni=1qiki, has mean 0and variance dk.\n4', 'In the embedding layers, we multiply those weights by√dmodel.\n5'], 'ai.observability.retrieval.query_text': 'What is multi-headed attention?'}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.34.1', 'service.name': 'trulens', 'ai.observability.app_id': 'app_hash_f68a15a65f74a98f9981e35440ef56ab', 'ai.observability.app_name': 'Simple RAG', 'ai.observability.app_version': 'v1'}",2025-09-10 21:16:09.152831,2025-09-10 21:16:09.154175,"{'trace_id': '160222885005595993857975228876295133827', 'parent_id': '14023480616735475011', 'span_id': '17595210022942811015'}"
"{'name': 'llama_index.core.base.base_retriever.BaseRetriever.retrieve', 'kind': 1, 'parent_span_id': '17595210022942811015', 'status': 'STATUS_CODE_UNSET'}",3648146369008799775,"{'ai.observability.span_type': 'retrieval', 'ai.observability.app_name': 'Simple RAG', 'ai.observability.app_version': 'v1', 'ai.observability.app_id': 'app_hash_f68a15a65f74a98f9981e35440ef56ab', 'ai.observability.record_id': 'ffa8b184-1011-4e16-b3a6-9b872d19f37c', 'ai.observability.run.name': 'test run', 'ai.observability.input_id': '42', 'ai.observability.call.return': ['Node ID: b4fcd7c5-87da-405d-b95c-71c242fdc643\nText: The input consists of queries and keys of dimension dk, and\nvalues of dimension dv. We compute the dot products of the query with\nall keys, divide each by√dk, and apply a softmax function to obtain\nthe weights on the values. In practice, we compute the attention\nfunction on a set of queries simultaneously, packed together into a\nmatrix Q. The ke...\nScore:  0.913\n', 'Node ID: 46a9e6b5-ec70-49cf-acbd-40de18697715\nText: On each of these projected versions of queries, keys and values\nwe then perform the attention function in parallel, yielding dv-\ndimensional 4To illustrate why the dot products get large, assume that\nthe components of qandkare independent random variables with mean 0and\nvariance 1. Then their dot product, q·k=Pdk i=1qiki, has mean 0and\nvariance d...\nScore:  0.874\n', 'Node ID: ff58b4f1-b13d-4a25-970f-190c27517d5f\nText: In the embedding layers, we multiply those weights by√dmodel. 5\nScore:  0.871\n'], 'ai.observability.call.function': 'llama_index.core.base.base_retriever.BaseRetriever.retrieve', 'ai.observability.call.kwargs.str_or_query_bundle': 'What is multi-headed attention?', 'ai.observability.retrieval.retrieved_contexts': ['The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices KandV.', 'On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\ni=1qiki, has mean 0and variance dk.\n4', 'In the embedding layers, we multiply those weights by√dmodel.\n5'], 'ai.observability.retrieval.query_text': 'What is multi-headed attention?'}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.34.1', 'service.name': 'trulens', 'ai.observability.app_id': 'app_hash_f68a15a65f74a98f9981e35440ef56ab', 'ai.observability.app_name': 'Simple RAG', 'ai.observability.app_version': 'v1'}",2025-09-10 21:16:09.152859,2025-09-10 21:16:09.153983,"{'trace_id': '160222885005595993857975228876295133827', 'parent_id': '17595210022942811015', 'span_id': '3648146369008799775'}"
"{'name': 'llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever._retrieve', 'kind': 1, 'parent_span_id': '3648146369008799775', 'status': 'STATUS_CODE_UNSET'}",6007849048954625769,"{'ai.observability.span_type': 'retrieval', 'ai.observability.app_name': 'Simple RAG', 'ai.observability.app_version': 'v1', 'ai.observability.app_id': 'app_hash_f68a15a65f74a98f9981e35440ef56ab', 'ai.observability.record_id': 'ffa8b184-1011-4e16-b3a6-9b872d19f37c', 'ai.observability.run.name': 'test run', 'ai.observability.input_id': '42', 'ai.observability.call.return': ['Node ID: b4fcd7c5-87da-405d-b95c-71c242fdc643\nText: The input consists of queries and keys of dimension dk, and\nvalues of dimension dv. We compute the dot products of the query with\nall keys, divide each by√dk, and apply a softmax function to obtain\nthe weights on the values. In practice, we compute the attention\nfunction on a set of queries simultaneously, packed together into a\nmatrix Q. The ke...\nScore:  0.913\n', 'Node ID: 46a9e6b5-ec70-49cf-acbd-40de18697715\nText: On each of these projected versions of queries, keys and values\nwe then perform the attention function in parallel, yielding dv-\ndimensional 4To illustrate why the dot products get large, assume that\nthe components of qandkare independent random variables with mean 0and\nvariance 1. Then their dot product, q·k=Pdk i=1qiki, has mean 0and\nvariance d...\nScore:  0.874\n', 'Node ID: ff58b4f1-b13d-4a25-970f-190c27517d5f\nText: In the embedding layers, we multiply those weights by√dmodel. 5\nScore:  0.871\n'], 'ai.observability.call.function': 'llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever._retrieve', 'ai.observability.call.kwargs.query_bundle': 'What is multi-headed attention?', 'ai.observability.retrieval.retrieved_contexts': ['The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices KandV.', 'On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\ni=1qiki, has mean 0and variance dk.\n4', 'In the embedding layers, we multiply those weights by√dmodel.\n5'], 'ai.observability.retrieval.query_text': 'What is multi-headed attention?'}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.34.1', 'service.name': 'trulens', 'ai.observability.app_id': 'app_hash_f68a15a65f74a98f9981e35440ef56ab', 'ai.observability.app_name': 'Simple RAG', 'ai.observability.app_version': 'v1'}",2025-09-10 21:16:09.152918,2025-09-10 21:16:09.153733,"{'trace_id': '160222885005595993857975228876295133827', 'parent_id': '3648146369008799775', 'span_id': '6007849048954625769'}"
"{'name': 'llama_index.core.response_synthesizers.compact_and_refine.CompactAndRefine.get_response', 'kind': 1, 'parent_span_id': '14023480616735475011', 'status': 'STATUS_CODE_UNSET'}",1518426318642164852,"{'ai.observability.span_type': 'unknown', 'ai.observability.app_name': 'Simple RAG', 'ai.observability.app_version': 'v1', 'ai.observability.app_id': 'app_hash_f68a15a65f74a98f9981e35440ef56ab', 'ai.observability.record_id': 'ffa8b184-1011-4e16-b3a6-9b872d19f37c', 'ai.observability.run.name': 'test run', 'ai.observability.input_id': '42', 'ai.observability.call.return': 'Context information is below.\n---------------------\npage_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nThe input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices KandV.\n\npage_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nOn each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\ni=1qiki, has mean 0and variance dk.\n4\n\npage_label: 5\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nIn the embedding layers, we multiply those weights by√dmodel.\n5\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: ', 'ai.observability.call.function': 'llama_index.core.response_synthesizers.compact_and_refine.CompactAndRefine.get_response', 'ai.observability.call.kwargs.query_str': 'What is multi-headed attention?', 'ai.observability.call.kwargs.text_chunks': ['page_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nThe input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices KandV.', 'page_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nOn each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\ni=1qiki, has mean 0and variance dk.\n4', 'page_label: 5\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nIn the embedding layers, we multiply those weights by√dmodel.\n5']}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.34.1', 'service.name': 'trulens', 'ai.observability.app_id': 'app_hash_f68a15a65f74a98f9981e35440ef56ab', 'ai.observability.app_name': 'Simple RAG', 'ai.observability.app_version': 'v1'}",2025-09-10 21:16:09.154255,2025-09-10 21:16:09.157920,"{'trace_id': '160222885005595993857975228876295133827', 'parent_id': '14023480616735475011', 'span_id': '1518426318642164852'}"
"{'name': 'llama_index.core.response_synthesizers.refine.Refine.get_response', 'kind': 1, 'parent_span_id': '1518426318642164852', 'status': 'STATUS_CODE_UNSET'}",16815555352443855247,"{'ai.observability.span_type': 'unknown', 'ai.observability.app_name': 'Simple RAG', 'ai.observability.app_version': 'v1', 'ai.observability.app_id': 'app_hash_f68a15a65f74a98f9981e35440ef56ab', 'ai.observability.record_id': 'ffa8b184-1011-4e16-b3a6-9b872d19f37c', 'ai.observability.run.name': 'test run', 'ai.observability.input_id': '42', 'ai.observability.call.return': 'Context information is below.\n---------------------\npage_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nThe input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices KandV.\n\npage_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nOn each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\ni=1qiki, has mean 0and variance dk.\n4\n\npage_label: 5\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nIn the embedding layers, we multiply those weights by√dmodel.\n5\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: ', 'ai.observability.call.function': 'llama_index.core.response_synthesizers.refine.Refine.get_response', 'ai.observability.call.kwargs.query_str': 'What is multi-headed attention?', 'ai.observability.call.kwargs.text_chunks': ['page_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nThe input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices KandV.\n\npage_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nOn each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\ni=1qiki, has mean 0and variance dk.\n4\n\npage_label: 5\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nIn the embedding layers, we multiply those weights by√dmodel.\n5']}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.34.1', 'service.name': 'trulens', 'ai.observability.app_id': 'app_hash_f68a15a65f74a98f9981e35440ef56ab', 'ai.observability.app_name': 'Simple RAG', 'ai.observability.app_version': 'v1'}",2025-09-10 21:16:09.157200,2025-09-10 21:16:09.157886,"{'trace_id': '160222885005595993857975228876295133827', 'parent_id': '1518426318642164852', 'span_id': '16815555352443855247'}"
"{'name': 'llama_index.core.llms.mock.MockLLM.complete', 'kind': 1, 'parent_span_id': '16815555352443855247', 'status': 'STATUS_CODE_UNSET'}",13208844754992149517,"{'ai.observability.span_type': 'unknown', 'ai.observability.app_name': 'Simple RAG', 'ai.observability.app_version': 'v1', 'ai.observability.app_id': 'app_hash_f68a15a65f74a98f9981e35440ef56ab', 'ai.observability.record_id': 'ffa8b184-1011-4e16-b3a6-9b872d19f37c', 'ai.observability.run.name': 'test run', 'ai.observability.input_id': '42', 'ai.observability.call.return': 'Context information is below.\n---------------------\npage_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nThe input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices KandV.\n\npage_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nOn each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\ni=1qiki, has mean 0and variance dk.\n4\n\npage_label: 5\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nIn the embedding layers, we multiply those weights by√dmodel.\n5\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: ', 'ai.observability.call.function': 'llama_index.core.llms.mock.MockLLM.complete', 'ai.observability.call.kwargs.formatted': True, 'ai.observability.call.kwargs.args': ['Context information is below.\n---------------------\npage_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nThe input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices KandV.\n\npage_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nOn each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\ni=1qiki, has mean 0and variance dk.\n4\n\npage_label: 5\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nIn the embedding layers, we multiply those weights by√dmodel.\n5\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: '], 'ai.observability.call.kwargs.kwargs': '{""formatted"": true}'}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.34.1', 'service.name': 'trulens', 'ai.observability.app_id': 'app_hash_f68a15a65f74a98f9981e35440ef56ab', 'ai.observability.app_name': 'Simple RAG', 'ai.observability.app_version': 'v1'}",2025-09-10 21:16:09.157620,2025-09-10 21:16:09.157824,"{'trace_id': '160222885005595993857975228876295133827', 'parent_id': '16815555352443855247', 'span_id': '13208844754992149517'}"
