,record,event_id,record_attributes,record_type,resource_attributes,start_timestamp,timestamp,trace
0,"{'name': 'root', 'kind': 1, 'parent_span_id': '', 'status': 'STATUS_CODE_UNSET'}",3567512205845083245,"{'name': 'root', 'ai.observability.span_type': 'record_root', 'ai.observability.domain': 'module', 'ai.observability.app_name': 'Simple RAG', 'ai.observability.app_version': 'v1', 'ai.observability.record_id': 'b5bd12f7-5b91-4a77-b0c6-6e990b02afbd', 'ai.observability.run_name': 'test run', 'ai.observability.input_id': '42', 'ai.observability.record_root.app_name': 'Simple RAG', 'ai.observability.record_root.app_version': 'v1', 'ai.observability.record_root.record_id': 'b5bd12f7-5b91-4a77-b0c6-6e990b02afbd'}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-29 13:14:09.261495,2025-01-29 13:14:09.263962,"{'trace_id': '132656670413087801624948638495435624870', 'parent_id': '', 'span_id': '3567512205845083245'}"
1,"{'name': 'llama_index.core.base.base_query_engine.BaseQueryEngine.query', 'kind': 1, 'parent_span_id': '3567512205845083245', 'status': 'STATUS_CODE_UNSET'}",13967326213225987754,"{'name': 'llama_index.core.base.base_query_engine.BaseQueryEngine.query', 'ai.observability.span_type': 'main', 'ai.observability.domain': 'module', 'ai.observability.app_name': 'Simple RAG', 'ai.observability.app_version': 'v1', 'ai.observability.record_id': 'b5bd12f7-5b91-4a77-b0c6-6e990b02afbd', 'ai.observability.run_name': 'test run', 'ai.observability.input_id': '42', 'ai.observability.main.main_input': 'What is multi-headed attention?', 'ai.observability.main.main_output': 'Context information is below.\n---------------------\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19].\n\npage_label: 3\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nTo facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: '}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-29 13:14:09.261544,2025-01-29 13:14:09.263957,"{'trace_id': '132656670413087801624948638495435624870', 'parent_id': '3567512205845083245', 'span_id': '13967326213225987754'}"
2,"{'name': 'llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine.retrieve', 'kind': 1, 'parent_span_id': '13967326213225987754', 'status': 'STATUS_CODE_UNSET'}",15236455958706521416,"{'name': 'llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine.retrieve', 'ai.observability.span_type': 'retrieval', 'ai.observability.domain': 'module', 'ai.observability.app_name': 'Simple RAG', 'ai.observability.app_version': 'v1', 'ai.observability.record_id': 'b5bd12f7-5b91-4a77-b0c6-6e990b02afbd', 'ai.observability.run_name': 'test run', 'ai.observability.input_id': '42', 'ai.observability.sem.retrieval.retrieved_contexts': ['Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19].', 'To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack.', 'Recurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples.'], 'ai.observability.sem.retrieval.query_text': 'What is multi-headed attention?'}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-29 13:14:09.261653,2025-01-29 13:14:09.262671,"{'trace_id': '132656670413087801624948638495435624870', 'parent_id': '13967326213225987754', 'span_id': '15236455958706521416'}"
3,"{'name': 'llama_index.core.base.base_retriever.BaseRetriever.retrieve', 'kind': 1, 'parent_span_id': '15236455958706521416', 'status': 'STATUS_CODE_UNSET'}",8176347238739647777,"{'name': 'llama_index.core.base.base_retriever.BaseRetriever.retrieve', 'ai.observability.span_type': 'retrieval', 'ai.observability.domain': 'module', 'ai.observability.app_name': 'Simple RAG', 'ai.observability.app_version': 'v1', 'ai.observability.record_id': 'b5bd12f7-5b91-4a77-b0c6-6e990b02afbd', 'ai.observability.run_name': 'test run', 'ai.observability.input_id': '42', 'ai.observability.sem.retrieval.retrieved_contexts': ['Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19].', 'To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack.', 'Recurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples.'], 'ai.observability.sem.retrieval.query_text': 'What is multi-headed attention?'}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-29 13:14:09.261672,2025-01-29 13:14:09.262637,"{'trace_id': '132656670413087801624948638495435624870', 'parent_id': '15236455958706521416', 'span_id': '8176347238739647777'}"
4,"{'name': 'llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever._retrieve', 'kind': 1, 'parent_span_id': '8176347238739647777', 'status': 'STATUS_CODE_UNSET'}",842599368731376487,"{'name': 'llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever._retrieve', 'ai.observability.span_type': 'retrieval', 'ai.observability.domain': 'module', 'ai.observability.app_name': 'Simple RAG', 'ai.observability.app_version': 'v1', 'ai.observability.record_id': 'b5bd12f7-5b91-4a77-b0c6-6e990b02afbd', 'ai.observability.run_name': 'test run', 'ai.observability.input_id': '42', 'ai.observability.sem.retrieval.retrieved_contexts': ['Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19].', 'To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack.', 'Recurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples.'], 'ai.observability.sem.retrieval.query_text': 'What is multi-headed attention?'}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-29 13:14:09.261741,2025-01-29 13:14:09.262539,"{'trace_id': '132656670413087801624948638495435624870', 'parent_id': '8176347238739647777', 'span_id': '842599368731376487'}"
5,"{'name': 'llama_index.core.response_synthesizers.compact_and_refine.CompactAndRefine.get_response', 'kind': 1, 'parent_span_id': '13967326213225987754', 'status': 'STATUS_CODE_UNSET'}",9169565173834679163,"{'name': 'llama_index.core.response_synthesizers.compact_and_refine.CompactAndRefine.get_response', 'ai.observability.span_type': 'unknown', 'ai.observability.domain': 'module', 'ai.observability.app_name': 'Simple RAG', 'ai.observability.app_version': 'v1', 'ai.observability.record_id': 'b5bd12f7-5b91-4a77-b0c6-6e990b02afbd', 'ai.observability.run_name': 'test run', 'ai.observability.input_id': '42', 'ai.observability.unknown.query_str': 'What is multi-headed attention?', 'ai.observability.unknown.text_chunks': ['page_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19].', 'page_label: 3\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nTo facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack.', 'page_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples.'], 'ai.observability.unknown.return': 'Context information is below.\n---------------------\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19].\n\npage_label: 3\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nTo facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: '}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-29 13:14:09.262746,2025-01-29 13:14:09.263820,"{'trace_id': '132656670413087801624948638495435624870', 'parent_id': '13967326213225987754', 'span_id': '9169565173834679163'}"
6,"{'name': 'llama_index.core.response_synthesizers.refine.Refine.get_response', 'kind': 1, 'parent_span_id': '9169565173834679163', 'status': 'STATUS_CODE_UNSET'}",2384986683601668390,"{'name': 'llama_index.core.response_synthesizers.refine.Refine.get_response', 'ai.observability.span_type': 'unknown', 'ai.observability.domain': 'module', 'ai.observability.app_name': 'Simple RAG', 'ai.observability.app_version': 'v1', 'ai.observability.record_id': 'b5bd12f7-5b91-4a77-b0c6-6e990b02afbd', 'ai.observability.run_name': 'test run', 'ai.observability.input_id': '42', 'ai.observability.unknown.query_str': 'What is multi-headed attention?', 'ai.observability.unknown.text_chunks': ['page_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19].\n\npage_label: 3\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nTo facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples.'], 'ai.observability.unknown.prev_response': 'None', 'ai.observability.unknown.return': 'Context information is below.\n---------------------\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19].\n\npage_label: 3\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nTo facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: '}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-29 13:14:09.263230,2025-01-29 13:14:09.263788,"{'trace_id': '132656670413087801624948638495435624870', 'parent_id': '9169565173834679163', 'span_id': '2384986683601668390'}"
7,"{'name': 'llama_index.core.llms.mock.MockLLM.complete', 'kind': 1, 'parent_span_id': '2384986683601668390', 'status': 'STATUS_CODE_UNSET'}",2634789606066267453,"{'name': 'llama_index.core.llms.mock.MockLLM.complete', 'ai.observability.span_type': 'unknown', 'ai.observability.domain': 'module', 'ai.observability.app_name': 'Simple RAG', 'ai.observability.app_version': 'v1', 'ai.observability.record_id': 'b5bd12f7-5b91-4a77-b0c6-6e990b02afbd', 'ai.observability.run_name': 'test run', 'ai.observability.input_id': '42', 'ai.observability.unknown.formatted': True, 'ai.observability.unknown.args': ['Context information is below.\n---------------------\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19].\n\npage_label: 3\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nTo facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: '], 'ai.observability.unknown.kwargs': ""{'formatted': True}"", 'ai.observability.unknown.return': 'Context information is below.\n---------------------\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19].\n\npage_label: 3\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nTo facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: '}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-29 13:14:09.263623,2025-01-29 13:14:09.263730,"{'trace_id': '132656670413087801624948638495435624870', 'parent_id': '2384986683601668390', 'span_id': '2634789606066267453'}"
