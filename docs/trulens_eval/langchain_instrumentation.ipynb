{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Integration\n",
    "\n",
    "TruLens provides TruChain, a deep integration with LangChain to allow you to inspect and evaluate the internals of your application built using LangChain.\n",
    "\n",
    "TruChain captures all of the metrics and metadata listed in the [instrumentation overview](../basic_instrumentation). In addition, TruChain instruments the following LangChain classes:\n",
    "\n",
    "## Instrumented Classes\n",
    "\n",
    "* langchain.chains.base.Chain\n",
    "* langchain.vectorstores.base.BaseRetriever\n",
    "* langchain.schema.BaseRetriever\n",
    "* langchain.llms.base.BaseLLM\n",
    "* langchain.prompts.base.BasePromptTemplate\n",
    "* langchain.schema.BaseMemory\n",
    "* langchain.schema.BaseChatMessageHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "Below is a quick example of usage. First, we'll create a standard LLMChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required imports\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "from trulens_eval import TruChain\n",
    "\n",
    "# typical langchain setup\n",
    "full_prompt = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        template=\n",
    "        \"Provide a helpful response with relevant background information for the following: {prompt}\",\n",
    "        input_variables=[\"prompt\"],\n",
    "    )\n",
    ")\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([full_prompt])\n",
    "\n",
    "llm = OpenAI(temperature=0.9, max_tokens=128)\n",
    "chain = LLMChain(llm=llm, prompt=chat_prompt_template, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instrument an LLM chain, all that's required is to wrap it using TruChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instrument with TruChain\n",
    "tru_recorder = TruChain(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the full quickstart available here: [LangChain Quickstart](../langchain_quickstart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async Support\n",
    "\n",
    "TruChain also provides async support for Langchain through the `acall` method. This allows you to track and evaluate async and streaming LangChain applications.\n",
    "\n",
    "As an example, below is an LLM chain set up with an async callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks import AsyncIteratorCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "\n",
    "from trulens_eval import TruChain\n",
    "\n",
    "# Set up an async callback.\n",
    "callback = AsyncIteratorCallbackHandler()\n",
    "\n",
    "# Setup a simple question/answer chain with streaming ChatOpenAI.\n",
    "prompt = PromptTemplate.from_template(\"Honestly answer this question: {question}.\")\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.0,\n",
    "    streaming=True, # important\n",
    "    callbacks=[callback] # callback can be here or below in acall_with_record\n",
    ")\n",
    "async_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have created the async LLM chain you can instrument it just as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async_tc_recorder = TruChain(async_chain)\n",
    "\n",
    "with async_tc_recorder as recording:\n",
    "    await async_chain.acall(inputs=dict(question=\"What is 1+2? Explain your answer.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more usage examples, check out the [LangChain examples directory](https://github.com/truera/trulens/tree/main/trulens_eval/examples/frameworks/langchain)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d153714b979d5e6d08dd8ec90712dd93bff2c9b6c1f0c118169738af3430cd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
