{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Integration\n",
    "\n",
    "TruLens provides TruChain, a deep integration with LangChain to allow you to inspect and evaluate the internals of your application built using LangChain.\n",
    "\n",
    "TruChain captures all of the metrics and metadata listed in the [instrumentation overview](../basic_instrumentation). In addition, TruChain instruments the following LangChain classes:\n",
    "\n",
    "## Instrumented Classes\n",
    "\n",
    "* langchain.chains.base.Chain\n",
    "* langchain.vectorstores.base.BaseRetriever\n",
    "* langchain.schema.BaseRetriever\n",
    "* langchain.llms.base.BaseLLM\n",
    "* langchain.prompts.base.BasePromptTemplate\n",
    "* langchain.schema.BaseMemory\n",
    "* langchain.schema.BaseChatMessageHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "Below is a quick example of usage. First, we'll create a standard LLMChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required imports\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.chat import HumanMessagePromptTemplate, ChatPromptTemplate, PromptTemplate\n",
    "from trulens_eval import TruChain\n",
    "\n",
    "# typical langchain setup\n",
    "full_prompt = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        template=\n",
    "        \"Provide a helpful response with relevant background information for the following: {prompt}\",\n",
    "        input_variables=[\"prompt\"],\n",
    "    )\n",
    ")\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([full_prompt])\n",
    "\n",
    "llm = OpenAI(temperature=0.9, max_tokens=128)\n",
    "chain = LLMChain(llm=llm, prompt=chat_prompt_template, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instrument an LLM chain, all that's required is to wrap it using TruChain. Then the instrumented chain can be used like the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instrument with TruChain\n",
    "truchain = TruChain(chain)\n",
    "\n",
    "# call the instrumented application just like the original\n",
    "llm_response = truchain(\"What would be a good company name for a company that makes colorful socks?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the full quickstart available here: [LangChain Quickstart](../langchain_quickstart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async Support\n",
    "\n",
    "TruChain also provides async support for Langchain through the `acall` method. This allows you to track and evaluate async and streaming LangChain applications.\n",
    "\n",
    "As an example, below is an LLM chain set up with an async callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from langchain import LLMChain\n",
    "from langchain import PromptTemplate\n",
    "from langchain.callbacks import AsyncIteratorCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "\n",
    "from trulens_eval import TruChain\n",
    "import trulens_eval.utils.python  # makes sure asyncio gets instrumented\n",
    "\n",
    "# Set up an async callback.\n",
    "callback = AsyncIteratorCallbackHandler()\n",
    "\n",
    "# Setup a simple question/answer chain with streaming ChatOpenAI.\n",
    "prompt = PromptTemplate.from_template(\"Honestly answer this question: {question}.\")\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.0,\n",
    "    streaming=True, # important\n",
    "    callbacks=[callback] # callback can be here or below in acall_with_record\n",
    ")\n",
    "async_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have created the async LLM chain you can instrument it just as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async_tc = TruChain(async_chain)\n",
    "\n",
    "# Create a task with the call to the chain, but don't wait for it yet.\n",
    "asyncio.create_task(\n",
    "    async_tc.acall_with_record(\n",
    "        inputs=dict(question=\"What is 1+2? Explain your answer.\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Instead wait for the callback's async generator, getting us each token as it comes in.\n",
    "async for token in callback.aiter():\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more usage examples, check out the [LangChain examples directory](https://github.com/truera/trulens/tree/main/trulens_eval/examples/frameworks/langchain)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d153714b979d5e6d08dd8ec90712dd93bff2c9b6c1f0c118169738af3430cd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
