{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ““ NeMo Guardrails Integration\n",
    "\n",
    "TruLens provides TruRails, an integration with NeMo Guardrails apps to allow you to\n",
    "inspect and evaluate the internals of your application built using nemo.\n",
    "This is done through the instrumentation of key nemo classes. To see a list\n",
    "of classes instrumented, see *Appendix: Instrumented Nemo Classes and\n",
    "Methods*.\n",
    "\n",
    "In addition to the default instrumentation, TruRails exposes the\n",
    "*select_context* method for evaluations that require access to retrieved\n",
    "context. Exposing *select_context* bypasses the need to know the json structure\n",
    "of your app ahead of time, and makes your evaluations re-usable across different\n",
    "apps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "Below is a quick example of usage. First, we'll create a standard Nemo app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml\n",
    "# Adapted from NeMo-Guardrails/nemoguardrails/examples/bots/abc/config.yml\n",
    "instructions:\n",
    "  - type: general\n",
    "    content: |\n",
    "      Below is a conversation between a user and a bot called the trulens Bot.\n",
    "      The bot is designed to answer questions about the trulens_eval python library.\n",
    "      The bot is knowledgeable about python.\n",
    "      If the bot does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "sample_conversation: |\n",
    "  user \"Hi there. Can you help me with some questions I have about trulens?\"\n",
    "    express greeting and ask for assistance\n",
    "  bot express greeting and confirm and offer assistance\n",
    "    \"Hi there! I'm here to help answer any questions you may have about the trulens. What would you like to know?\"\n",
    "\n",
    "models:\n",
    "  - type: main\n",
    "    engine: openai\n",
    "    model: gpt-3.5-turbo-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing config.co\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.co\n",
    "# Adapted from NeMo-Guardrails/tests/test_configs/with_kb_openai_embeddings/config.co\n",
    "define user ask capabilities\n",
    "  \"What can you do?\"\n",
    "  \"What can you help me with?\"\n",
    "  \"tell me what you can do\"\n",
    "  \"tell me about you\"\n",
    "\n",
    "define bot inform capabilities\n",
    "  \"I am an AI bot that helps answer questions about trulens_eval.\"\n",
    "\n",
    "define flow\n",
    "  user ask capabilities\n",
    "  bot inform capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small knowledge base from the root README file.\n",
    "\n",
    "! mkdir -p kb\n",
    "! cp ../../../../README.md kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4775dc92ba8a4097830daf3c8d479127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "config = RailsConfig.from_path(\".\")\n",
    "rails = LLMRails(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instrument an LLM chain, all that's required is to wrap it using TruChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import TruRails\n",
    "\n",
    "# instrument with TruRails\n",
    "tru_recorder = TruRails(\n",
    "    rails,\n",
    "    app_id = \"my first trurails app\", # optional\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To properly evaluate LLM apps we often need to point our evaluation at an\n",
    "internal step of our application, such as the retreived context. Doing so allows\n",
    "us to evaluate for metrics including context relevance and groundedness.\n",
    "\n",
    "For Nemo applications with a knowledge base, `select_context` can\n",
    "be used to access the retrieved text for evaluation.\n",
    "\n",
    "Example usage:\n",
    "\n",
    "```python\n",
    "context = TruRails.select_context(rails)\n",
    "\n",
    "f_context_relevance = (\n",
    "    Feedback(provider.qs_relevance)\n",
    "    .on_input()\n",
    "    .on(context)\n",
    "    .aggregate(np.mean)\n",
    "    )\n",
    "```\n",
    "\n",
    "For added flexibility, the select_context method is also made available through\n",
    "`trulens_eval.app.App`. This allows you to switch between frameworks without\n",
    "changing your context selector:\n",
    "\n",
    "```python\n",
    "from trulens_eval.app import App\n",
    "context = App.select_context(rag_chain)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Instrumented LangChain Classes and Methods\n",
    "\n",
    "The modules, classes, and methods that trulens instruments can be retrieved from\n",
    "the appropriate Instrument subclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module langchain*\n",
      "  Class langchain.agents.agent.BaseMultiActionAgent\n",
      "    Method plan: (self, intermediate_steps: 'List[Tuple[AgentAction, str]]', callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'Union[List[AgentAction], AgentFinish]'\n",
      "    Method aplan: (self, intermediate_steps: 'List[Tuple[AgentAction, str]]', callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'Union[List[AgentAction], AgentFinish]'\n",
      "  Class langchain.agents.agent.BaseSingleActionAgent\n",
      "    Method plan: (self, intermediate_steps: 'List[Tuple[AgentAction, str]]', callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'Union[AgentAction, AgentFinish]'\n",
      "    Method aplan: (self, intermediate_steps: 'List[Tuple[AgentAction, str]]', callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'Union[AgentAction, AgentFinish]'\n",
      "  Class langchain.chains.base.Chain\n",
      "    Method __call__: (self, inputs: Union[Dict[str, Any], Any], return_only_outputs: bool = False, callbacks: Union[List[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, *, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, run_name: Optional[str] = None, include_run_info: bool = False) -> Dict[str, Any]\n",
      "    Method invoke: (self, input: Dict[str, Any], config: Optional[langchain_core.runnables.config.RunnableConfig] = None, **kwargs: Any) -> Dict[str, Any]\n",
      "    Method ainvoke: (self, input: Dict[str, Any], config: Optional[langchain_core.runnables.config.RunnableConfig] = None, **kwargs: Any) -> Dict[str, Any]\n",
      "    Method run: (self, *args: Any, callbacks: Union[List[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, **kwargs: Any) -> Any\n",
      "    Method arun: (self, *args: Any, callbacks: Union[List[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, **kwargs: Any) -> Any\n",
      "    Method _call: (self, inputs: Dict[str, Any], run_manager: Optional[langchain_core.callbacks.manager.CallbackManagerForChainRun] = None) -> Dict[str, Any]\n",
      "    Method _acall: (self, inputs: Dict[str, Any], run_manager: Optional[langchain_core.callbacks.manager.AsyncCallbackManagerForChainRun] = None) -> Dict[str, Any]\n",
      "    Method acall: (self, inputs: Union[Dict[str, Any], Any], return_only_outputs: bool = False, callbacks: Union[List[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, *, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, run_name: Optional[str] = None, include_run_info: bool = False) -> Dict[str, Any]\n",
      "  Class langchain.memory.chat_memory.BaseChatMemory\n",
      "    Method save_context: (self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None\n",
      "    Method clear: (self) -> None\n",
      "  Class langchain_core.chat_history.BaseChatMessageHistory\n",
      "  Class langchain_core.documents.base.Document\n",
      "  Class langchain_core.language_models.base.BaseLanguageModel\n",
      "  Class langchain_core.language_models.llms.BaseLLM\n",
      "  Class langchain_core.load.serializable.Serializable\n",
      "  Class langchain_core.memory.BaseMemory\n",
      "    Method save_context: (self, inputs: 'Dict[str, Any]', outputs: 'Dict[str, str]') -> 'None'\n",
      "    Method clear: (self) -> 'None'\n",
      "  Class langchain_core.prompts.base.BasePromptTemplate\n",
      "  Class langchain_core.retrievers.BaseRetriever\n",
      "    Method _get_relevant_documents: (self, query: 'str', *, run_manager: 'CallbackManagerForRetrieverRun') -> 'List[Document]'\n",
      "    Method get_relevant_documents: (self, query: 'str', *, callbacks: 'Callbacks' = None, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, **kwargs: 'Any') -> 'List[Document]'\n",
      "    Method aget_relevant_documents: (self, query: 'str', *, callbacks: 'Callbacks' = None, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, **kwargs: 'Any') -> 'List[Document]'\n",
      "    Method _aget_relevant_documents: (self, query: 'str', *, run_manager: 'AsyncCallbackManagerForRetrieverRun') -> 'List[Document]'\n",
      "  Class langchain_core.runnables.base.RunnableSerializable\n",
      "  Class langchain_core.tools.BaseTool\n",
      "    Method _arun: (self, *args: 'Any', **kwargs: 'Any') -> 'Any'\n",
      "    Method _run: (self, *args: 'Any', **kwargs: 'Any') -> 'Any'\n",
      "\n",
      "Module nemoguardrails*\n",
      "  Class nemoguardrails.actions.action_dispatcher.ActionDispatcher\n",
      "    Method execute_action: (self, action_name: str, params: Dict[str, Any]) -> Tuple[Union[str, Dict[str, Any]], str]\n",
      "  Class nemoguardrails.actions.llm.generation.LLMGenerationActions\n",
      "    Method generate_user_intent: (self, events: List[dict], context: dict, config: nemoguardrails.rails.llm.config.RailsConfig, llm: Optional[langchain_core.language_models.llms.BaseLLM] = None, kb: Optional[nemoguardrails.kb.kb.KnowledgeBase] = None)\n",
      "    Method generate_next_step: (self, events: List[dict], llm: Optional[langchain_core.language_models.llms.BaseLLM] = None)\n",
      "    Method generate_bot_message: (self, events: List[dict], context: dict, llm: Optional[langchain_core.language_models.llms.BaseLLM] = None)\n",
      "    Method generate_value: (self, instructions: str, events: List[dict], var_name: Optional[str] = None, llm: Optional[langchain_core.language_models.llms.BaseLLM] = None)\n",
      "    Method generate_intent_steps_message: (self, events: List[dict], llm: Optional[langchain_core.language_models.llms.BaseLLM] = None, kb: Optional[nemoguardrails.kb.kb.KnowledgeBase] = None)\n",
      "  Class nemoguardrails.kb.kb.KnowledgeBase\n",
      "    Method search_relevant_chunks: (self, text, max_results: int = 3)\n",
      "  Class nemoguardrails.rails.llm.llmrails.LLMRails\n",
      "    Method generate: (self, prompt: Optional[str] = None, messages: Optional[List[dict]] = None, return_context: bool = False, options: Union[dict, nemoguardrails.rails.llm.options.GenerationOptions, NoneType] = None)\n",
      "    Method generate_async: (self, prompt: Optional[str] = None, messages: Optional[List[dict]] = None, options: Union[dict, nemoguardrails.rails.llm.options.GenerationOptions, NoneType] = None, streaming_handler: Optional[nemoguardrails.streaming.StreamingHandler] = None, return_context: bool = False) -> Union[str, dict, nemoguardrails.rails.llm.options.GenerationResponse, Tuple[dict, dict]]\n",
      "    Method stream_async: (self, prompt: Optional[str] = None, messages: Optional[List[dict]] = None) -> AsyncIterator[str]\n",
      "    Method generate_events: (self, events: List[dict]) -> List[dict]\n",
      "    Method generate_events_async: (self, events: List[dict]) -> List[dict]\n",
      "    Method _get_events_for_messages: (self, messages: List[dict])\n",
      "\n",
      "Module trulens_eval.*\n",
      "  Class trulens_eval.feedback.feedback.Feedback\n",
      "    Method __call__: (self, *args, **kwargs) -> 'Any'\n",
      "  Class trulens_eval.tru_rails.FeedbackActions\n",
      "  Class trulens_eval.utils.langchain.WithFeedbackFilterDocuments\n",
      "    Method _get_relevant_documents: (self, query: str, *, run_manager) -> List[langchain_core.documents.base.Document]\n",
      "    Method get_relevant_documents: (self, query: 'str', *, callbacks: 'Callbacks' = None, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, **kwargs: 'Any') -> 'List[Document]'\n",
      "    Method aget_relevant_documents: (self, query: 'str', *, callbacks: 'Callbacks' = None, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, **kwargs: 'Any') -> 'List[Document]'\n",
      "    Method _aget_relevant_documents: (self, query: 'str', *, run_manager: 'AsyncCallbackManagerForRetrieverRun') -> 'List[Document]'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval.tru_rails import RailsInstrument\n",
    "RailsInstrument().print_instrumentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrumenting other classes/methods.\n",
    "Additional classes and methods can be instrumented by use of the\n",
    "`trulens_eval.instruments.Instrument` methods and decorators. Examples of\n",
    "such usage can be found in the custom app used in the `custom_example.ipynb`\n",
    "notebook which can be found in\n",
    "`trulens_eval/examples/expositional/end2end_apps/custom_app/custom_app.py`. More\n",
    "information about these decorators can be found in the\n",
    "`docs/trulens_eval/tracking/instrumentation/index.ipynb` notebook.\n",
    "\n",
    "### Inspecting instrumentation\n",
    "The specific objects (of the above classes) and methods instrumented for a\n",
    "particular app can be inspected using the `App.print_instrumented` as\n",
    "exemplified in the next cell. Unlike `Instrument.print_instrumentation`, this\n",
    "function only shows what in an app was actually instrumented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components:\n",
      "\tTruRails (Other) at 0x2aa583d40 with path __app__\n",
      "\tLLMRails (Custom) at 0x10464b950 with path __app__.app\n",
      "\tKnowledgeBase (Custom) at 0x2a945d5d0 with path __app__.app.kb\n",
      "\tOpenAI (Custom) at 0x2a8f61c70 with path __app__.app.llm\n",
      "\tLLMGenerationActions (Custom) at 0x29c04c990 with path __app__.app.llm_generation_actions\n",
      "\tOpenAI (Custom) at 0x2a8f61c70 with path __app__.app.llm_generation_actions.llm\n",
      "\n",
      "Methods:\n",
      "Object at 0x29c04c990:\n",
      "\t<function LLMGenerationActions.generate_user_intent at 0x2a898fc40> with path __app__.app.llm_generation_actions\n",
      "\t<function LLMGenerationActions.generate_next_step at 0x2a898fd80> with path __app__.app.llm_generation_actions\n",
      "\t<function LLMGenerationActions.generate_bot_message at 0x2a898fec0> with path __app__.app.llm_generation_actions\n",
      "\t<function LLMGenerationActions.generate_value at 0x2a898ff60> with path __app__.app.llm_generation_actions\n",
      "\t<function LLMGenerationActions.generate_intent_steps_message at 0x2a89b8040> with path __app__.app.llm_generation_actions\n",
      "Object at 0x2a945d5d0:\n",
      "\t<function KnowledgeBase.search_relevant_chunks at 0x2a898cf40> with path __app__.app.kb\n",
      "Object at 0x10464b950:\n",
      "\t<function LLMRails.generate at 0x2a8db7b00> with path __app__.app\n",
      "\t<function LLMRails.generate_async at 0x2a8d6ab60> with path __app__.app\n",
      "\t<function LLMRails.stream_async at 0x2a8db7880> with path __app__.app\n",
      "\t<function LLMRails.generate_events at 0x2a8df80e0> with path __app__.app\n",
      "\t<function LLMRails.generate_events_async at 0x2a8df8040> with path __app__.app\n",
      "\t<function LLMRails._get_events_for_messages at 0x2a8d234c0> with path __app__.app\n",
      "Object at 0x104aa42d0:\n",
      "\t<function ActionDispatcher.execute_action at 0x2a8a044a0> with path __app__.app.runtime.action_dispatcher\n"
     ]
    }
   ],
   "source": [
    "tru_recorder.print_instrumented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d153714b979d5e6d08dd8ec90712dd93bff2c9b6c1f0c118169738af3430cd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
