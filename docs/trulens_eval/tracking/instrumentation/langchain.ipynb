{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ““ ðŸ¦œï¸ðŸ”— _LangChain_ Integration\n",
    "\n",
    "TruLens provides TruChain, a deep integration with _LangChain_ to allow you to\n",
    "inspect and evaluate the internals of your application built using _LangChain_.\n",
    "This is done through the instrumentation of key _LangChain_ classes. To see a list\n",
    "of classes instrumented, see *Appendix: Instrumented _LangChain_ Classes and\n",
    "Methods*.\n",
    "\n",
    "In addition to the default instrumentation, TruChain exposes the\n",
    "*select_context* method for evaluations that require access to retrieved\n",
    "context. Exposing *select_context* bypasses the need to know the json structure\n",
    "of your app ahead of time, and makes your evaluations re-usable across different\n",
    "apps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "Below is a quick example of usage. First, we'll create a standard LLMChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required imports\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "from trulens_eval import TruChain\n",
    "\n",
    "# typical LangChain rag setup\n",
    "full_prompt = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        template=\n",
    "        \"Provide a helpful response with relevant background information for the following: {prompt}\",\n",
    "        input_variables=[\"prompt\"],\n",
    "    )\n",
    ")\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([full_prompt])\n",
    "\n",
    "llm = OpenAI(temperature=0.9, max_tokens=128)\n",
    "chain = LLMChain(llm=llm, prompt=chat_prompt_template, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instrument an LLM chain, all that's required is to wrap it using TruChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n",
      "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of Tru` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "# instrument with TruChain\n",
    "tru_recorder = TruChain(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, LangChain apps defined with LangChain Expression Language (LCEL) are also supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "tru_recorder = TruChain(\n",
    "    chain,\n",
    "    app_id='Chain1_ChatApplication'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To properly evaluate LLM apps we often need to point our evaluation at an\n",
    "internal step of our application, such as the retreived context. Doing so allows\n",
    "us to evaluate for metrics including context relevance and groundedness.\n",
    "\n",
    "For LangChain applications where the BaseRetriever is used, `select_context` can\n",
    "be used to access the retrieved text for evaluation.\n",
    "\n",
    "Example usage:\n",
    "\n",
    "```python\n",
    "context = TruChain.select_context(rag_chain)\n",
    "\n",
    "f_context_relevance = (\n",
    "    Feedback(provider.qs_relevance)\n",
    "    .on_input()\n",
    "    .on(context)\n",
    "    .aggregate(np.mean)\n",
    "    )\n",
    "```\n",
    "\n",
    "For added flexibility, the select_context method is also made available through\n",
    "`trulens_eval.app.App`. This allows you to switch between frameworks without\n",
    "changing your context selector:\n",
    "\n",
    "```python\n",
    "from trulens_eval.app import App\n",
    "context = App.select_context(rag_chain)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the full quickstart available here: [LangChain Quickstart](../langchain_quickstart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async Support\n",
    "\n",
    "TruChain also provides async support for _LangChain_ through the `acall` method. This allows you to track and evaluate async and streaming _LangChain_ applications.\n",
    "\n",
    "As an example, below is an LLM chain set up with an async callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.callbacks import AsyncIteratorCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from trulens_eval import TruChain\n",
    "\n",
    "# Set up an async callback.\n",
    "callback = AsyncIteratorCallbackHandler()\n",
    "\n",
    "# Setup a simple question/answer chain with streaming ChatOpenAI.\n",
    "prompt = PromptTemplate.from_template(\"Honestly answer this question: {question}.\")\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.0,\n",
    "    streaming=True, # important\n",
    "    callbacks=[callback]\n",
    ")\n",
    "async_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have created the async LLM chain you can instrument it just as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "async_tc_recorder = TruChain(async_chain)\n",
    "\n",
    "with async_tc_recorder as recording:\n",
    "    await async_chain.ainvoke(input=dict(question=\"What is 1+2? Explain your answer.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more usage examples, check out the [LangChain examples directory](https://github.com/truera/trulens/tree/main/trulens_eval/examples/expositional/frameworks/langchain)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Instrumented LangChain Classes and Methods\n",
    "\n",
    "The modules, classes, and methods that trulens instruments can be retrieved from\n",
    "the appropriate Instrument subclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module langchain*\n",
      "  Class langchain.agents.agent.BaseMultiActionAgent\n",
      "    Method plan: (self, intermediate_steps: 'List[Tuple[AgentAction, str]]', callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'Union[List[AgentAction], AgentFinish]'\n",
      "    Method aplan: (self, intermediate_steps: 'List[Tuple[AgentAction, str]]', callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'Union[List[AgentAction], AgentFinish]'\n",
      "  Class langchain.agents.agent.BaseSingleActionAgent\n",
      "    Method plan: (self, intermediate_steps: 'List[Tuple[AgentAction, str]]', callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'Union[AgentAction, AgentFinish]'\n",
      "    Method aplan: (self, intermediate_steps: 'List[Tuple[AgentAction, str]]', callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'Union[AgentAction, AgentFinish]'\n",
      "  Class langchain.chains.base.Chain\n",
      "    Method __call__: (self, inputs: Union[Dict[str, Any], Any], return_only_outputs: bool = False, callbacks: Union[List[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, *, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, run_name: Optional[str] = None, include_run_info: bool = False) -> Dict[str, Any]\n",
      "    Method invoke: (self, input: Dict[str, Any], config: Optional[langchain_core.runnables.config.RunnableConfig] = None, **kwargs: Any) -> Dict[str, Any]\n",
      "    Method ainvoke: (self, input: Dict[str, Any], config: Optional[langchain_core.runnables.config.RunnableConfig] = None, **kwargs: Any) -> Dict[str, Any]\n",
      "    Method run: (self, *args: Any, callbacks: Union[List[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, **kwargs: Any) -> Any\n",
      "    Method arun: (self, *args: Any, callbacks: Union[List[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, **kwargs: Any) -> Any\n",
      "    Method _call: (self, inputs: Dict[str, Any], run_manager: Optional[langchain_core.callbacks.manager.CallbackManagerForChainRun] = None) -> Dict[str, Any]\n",
      "    Method _acall: (self, inputs: Dict[str, Any], run_manager: Optional[langchain_core.callbacks.manager.AsyncCallbackManagerForChainRun] = None) -> Dict[str, Any]\n",
      "    Method acall: (self, inputs: Union[Dict[str, Any], Any], return_only_outputs: bool = False, callbacks: Union[List[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, *, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, run_name: Optional[str] = None, include_run_info: bool = False) -> Dict[str, Any]\n",
      "  Class langchain.memory.chat_memory.BaseChatMemory\n",
      "    Method save_context: (self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None\n",
      "    Method clear: (self) -> None\n",
      "  Class langchain_core.chat_history.BaseChatMessageHistory\n",
      "  Class langchain_core.documents.base.Document\n",
      "  Class langchain_core.language_models.base.BaseLanguageModel\n",
      "  Class langchain_core.language_models.llms.BaseLLM\n",
      "  Class langchain_core.load.serializable.Serializable\n",
      "  Class langchain_core.memory.BaseMemory\n",
      "    Method save_context: (self, inputs: 'Dict[str, Any]', outputs: 'Dict[str, str]') -> 'None'\n",
      "    Method clear: (self) -> 'None'\n",
      "  Class langchain_core.prompts.base.BasePromptTemplate\n",
      "  Class langchain_core.retrievers.BaseRetriever\n",
      "    Method _get_relevant_documents: (self, query: 'str', *, run_manager: 'CallbackManagerForRetrieverRun') -> 'List[Document]'\n",
      "    Method get_relevant_documents: (self, query: 'str', *, callbacks: 'Callbacks' = None, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, **kwargs: 'Any') -> 'List[Document]'\n",
      "    Method aget_relevant_documents: (self, query: 'str', *, callbacks: 'Callbacks' = None, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, **kwargs: 'Any') -> 'List[Document]'\n",
      "    Method _aget_relevant_documents: (self, query: 'str', *, run_manager: 'AsyncCallbackManagerForRetrieverRun') -> 'List[Document]'\n",
      "  Class langchain_core.runnables.base.RunnableSerializable\n",
      "  Class langchain_core.tools.BaseTool\n",
      "    Method _arun: (self, *args: 'Any', **kwargs: 'Any') -> 'Any'\n",
      "    Method _run: (self, *args: 'Any', **kwargs: 'Any') -> 'Any'\n",
      "\n",
      "Module trulens_eval.*\n",
      "  Class trulens_eval.feedback.feedback.Feedback\n",
      "    Method __call__: (self, *args, **kwargs) -> 'Any'\n",
      "  Class trulens_eval.utils.langchain.WithFeedbackFilterDocuments\n",
      "    Method _get_relevant_documents: (self, query: str, *, run_manager) -> List[langchain_core.documents.base.Document]\n",
      "    Method get_relevant_documents: (self, query: 'str', *, callbacks: 'Callbacks' = None, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, **kwargs: 'Any') -> 'List[Document]'\n",
      "    Method aget_relevant_documents: (self, query: 'str', *, callbacks: 'Callbacks' = None, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, **kwargs: 'Any') -> 'List[Document]'\n",
      "    Method _aget_relevant_documents: (self, query: 'str', *, run_manager: 'AsyncCallbackManagerForRetrieverRun') -> 'List[Document]'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval.tru_chain import LangChainInstrument\n",
    "LangChainInstrument().print_instrumentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrumenting other classes/methods.\n",
    "Additional classes and methods can be instrumented by use of the\n",
    "`trulens_eval.instruments.Instrument` methods and decorators. Examples of\n",
    "such usage can be found in the custom app used in the `custom_example.ipynb`\n",
    "notebook which can be found in\n",
    "`trulens_eval/examples/expositional/end2end_apps/custom_app/custom_app.py`. More\n",
    "information about these decorators can be found in the\n",
    "`docs/trulens_eval/tracking/instrumentation/index.ipynb` notebook.\n",
    "\n",
    "### Inspecting instrumentation\n",
    "The specific objects (of the above classes) and methods instrumented for a\n",
    "particular app can be inspected using the `App.print_instrumented` as\n",
    "exemplified in the next cell. Unlike `Instrument.print_instrumentation`, this\n",
    "function only shows what in an app was actually instrumented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components:\n",
      "\tTruChain (Other) at 0x2b60a3660 with path __app__\n",
      "\tLLMChain (Other) at 0x2b5cdb3e0 with path __app__.app\n",
      "\tPromptTemplate (Custom) at 0x2b605e580 with path __app__.app.prompt\n",
      "\tChatOpenAI (Custom) at 0x2b5cdb4d0 with path __app__.app.llm\n",
      "\tStrOutputParser (Custom) at 0x2b60a3750 with path __app__.app.output_parser\n",
      "\n",
      "Methods:\n",
      "Object at 0x2b5cdb3e0:\n",
      "\t<function Chain.__call__ at 0x2a6c17560> with path __app__.app\n",
      "\t<function Chain.invoke at 0x2a6c16de0> with path __app__.app\n",
      "\t<function Chain.ainvoke at 0x2a6c16e80> with path __app__.app\n",
      "\t<function Chain.run at 0x2a6c17b00> with path __app__.app\n",
      "\t<function Chain.arun at 0x2a6c17d80> with path __app__.app\n",
      "\t<function LLMChain._call at 0x2a6c6c2c0> with path __app__.app\n",
      "\t<function LLMChain._acall at 0x2a6c6c860> with path __app__.app\n",
      "\t<function Chain.acall at 0x2a6c177e0> with path __app__.app\n",
      "\t<function Chain._call at 0x2a6c17380> with path __app__.app\n",
      "\t<function Chain._acall at 0x2a6c17420> with path __app__.app\n",
      "\t<function Runnable.invoke at 0x2a669ba60> with path __app__.app\n",
      "\t<function Runnable.ainvoke at 0x2a669bb00> with path __app__.app\n"
     ]
    }
   ],
   "source": [
    "async_tc_recorder.print_instrumented()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d153714b979d5e6d08dd8ec90712dd93bff2c9b6c1f0c118169738af3430cd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
