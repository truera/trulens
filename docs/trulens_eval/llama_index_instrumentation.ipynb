{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ““ Llama-Index Integration\n",
    "\n",
    "TruLens provides TruLlama, a deep integration with Llama-Index to allow you to\n",
    "inspect and evaluate the internals of your application built using Llama-Index.\n",
    "This is done through the instrumentation of key Llama-Index classes and methods.\n",
    "To see all classes and methods instrumented, see *Appendix: Llama-Index\n",
    "Instrumented Classes and Methods*.\n",
    "\n",
    "In addition to the default instrumentation, TruChain exposes the\n",
    "*select_context* and *select_source_nodes* methods for evaluations that require\n",
    "access to retrieved context or source nodes. Exposing these methods bypasses the\n",
    "need to know the json structure of your app ahead of time, and makes your\n",
    "evaluations re-usable across different apps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage\n",
    "\n",
    "Below is a quick example of usage. First, we'll create a standard Llama-Index query engine from Paul Graham's Essay, *What I Worked On* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\"http://paulgraham.com/worked.html\"]\n",
    ")\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instrument an Llama-Index query engine, all that's required is to wrap it using TruLlama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import TruLlama\n",
    "tru_query_engine_recorder = TruLlama(query_engine)\n",
    "\n",
    "with tru_query_engine_recorder as recording:\n",
    "    print(query_engine.query(\"What did the author do growing up?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To properly evaluate LLM apps we often need to point our evaluation at an\n",
    "internal step of our application, such as the retreived context. Doing so allows\n",
    "us to evaluate for metrics including context relevance and groundedness.\n",
    "\n",
    "For Llama-Index applications where the source nodes are used, `select_context`\n",
    "can be used to access the retrieved text for evaluation.\n",
    "\n",
    "Example usage:\n",
    "\n",
    "```python\n",
    "context = TruLlama.select_context(query_engine)\n",
    "\n",
    "f_context_relevance = (\n",
    "    Feedback(provider.qs_relevance)\n",
    "    .on_input()\n",
    "    .on(context)\n",
    "    .aggregate(np.mean)\n",
    "    )\n",
    "```\n",
    "\n",
    "For added flexibility, the select_context method is also made available through\n",
    "`trulens_eval.app.App`. This allows you to switch between frameworks without\n",
    "changing your context selector:\n",
    "\n",
    "```python\n",
    "from trulens_eval.app import App\n",
    "context = App.select_context(query_engine)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the full quickstart available here: [Llama-Index Quickstart](/trulens_eval/llama_index_quickstart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async Support\n",
    "TruLlama also provides async support for Llama-Index through the `aquery`,\n",
    "`achat`, and `astream_chat` methods. This allows you to track and evaluate async\n",
    "applciations.\n",
    "\n",
    "As an example, below is an Llama-Index async chat engine (`achat`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports main tools:\n",
    "from trulens_eval import TruLlama, Tru\n",
    "tru = Tru()\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\"http://paulgraham.com/worked.html\"]\n",
    ")\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "chat_engine = index.as_chat_engine(streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instrument an Llama-Index `achat` engine, all that's required is to wrap it using TruLlama - just like with the query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_chat_recorder = TruLlama(chat_engine)\n",
    "\n",
    "with tru_chat_recorder as recording:\n",
    "    llm_response_async = await chat_engine.aquery(\"What did the author do growing up?\")\n",
    "\n",
    "print(llm_response_async)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Support\n",
    "\n",
    "TruLlama also provides streaming support for Llama-Index. This allows you to track and evaluate streaming applications.\n",
    "\n",
    "As an example, below is an Llama-Index query engine with streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "from trulens_eval import TruLlama\n",
    "\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\"http://paulgraham.com/worked.html\"]\n",
    ")\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine(streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with other methods, just wrap your streaming query engine with TruLlama and operate like before.\n",
    "\n",
    "You can also print the response tokens as they are generated using the `response_gen` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_query_engine_recorder = TruLlama(query_engine)\n",
    "\n",
    "with tru_query_engine_recorder as recording:\n",
    "    response = query_engine.query(\"What did the author do growing up?\")\n",
    "\n",
    "for c in response.response_gen:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more usage examples, check out the [Llama-Index examples directory](https://github.com/truera/trulens/tree/main/trulens_eval/examples/frameworks/llama_index)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Llama-Index Instrumented Classes and Methods\n",
    "\n",
    "As of `trulens_eval` version 0.20.0, TruLlama instrumetns the follwowing classes by default:\n",
    "\n",
    "* `BaseComponent`\n",
    "* `BaseLLM`\n",
    "* `BaseQueryEngine`\n",
    "* `BaseRetriever`\n",
    "* `BaseIndex`\n",
    "* `BaseChatEngine`\n",
    "* `Prompt`\n",
    "* `llama_index.prompts.prompt_type.PromptType` # enum\n",
    "* `BaseQuestionGenerator`\n",
    "* `BaseSynthesizer`\n",
    "* `Refine`\n",
    "* `LLMPredictor`\n",
    "* `LLMMetadata`\n",
    "* `BaseLLMPredictor`\n",
    "* `VectorStore`\n",
    "* `ServiceContext`\n",
    "* `PromptHelper`\n",
    "* `BaseEmbedding`\n",
    "* `NodeParser`\n",
    "* `ToolMetadata`\n",
    "* `BaseTool`\n",
    "* `BaseMemory`\n",
    "* `WithFeedbackFilterNodes`\n",
    "\n",
    "\n",
    "TruLlama instruments the following methods:\n",
    "* `query`\n",
    "* `aquery`\n",
    "* `chat`\n",
    "* `achat`\n",
    "* `stream_chat`\n",
    "* `astream_chat`\n",
    "* `complete`\n",
    "* `stream_complete`\n",
    "* `acomplete`\n",
    "* `astream_complete`\n",
    "* `__call__`\n",
    "* `call`\n",
    "* `acall`\n",
    "* `put`\n",
    "* `get_response`\n",
    "* `predict`\n",
    "* `retrieve`\n",
    "* `synthesize`\n",
    "\n",
    "### Instrumenting other classes/methods.\n",
    "Additional classes and methods can be instrumented by use of the\n",
    "`trulens_eval.utils.instruments.Instrument` methods and decorators. Examples of\n",
    "such usage can be found in the custom app used in the `custom_example.ipynb`\n",
    "notebook which can be found in\n",
    "`trulens_eval/examples/expositional/end2end_apps/custom_app/custom_app.py`. More\n",
    "information about these decorators can be found in the\n",
    "`trulens_instrumentation.ipynb` notebook.\n",
    "\n",
    "### Inspecting instrumentation\n",
    "The specific objects (of the above classes) and methods instrumented for a\n",
    "particular app can be inspected using the `App.print_instrumented` as\n",
    "exemplified in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_query_engine_recorder.print_instrumented()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d153714b979d5e6d08dd8ec90712dd93bff2c9b6c1f0c118169738af3430cd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
