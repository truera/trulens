{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama-Index Integration\n",
    "\n",
    "TruLens provides TruLlama, a deep integration with Llama-Index to allow you to inspect and evaluate the internals of your application built using Llama-Index.\n",
    "\n",
    "TruLlama captures all of the metrics and metadata listed in the [instrumentation overview](../basic_instrumentation). In addition, TruLlama provides the `select_source_nodes` method to capture the source nodes of your query.\n",
    "\n",
    "## Supported methods\n",
    "TruLlama supports both sync and async modes using the following Llama-Index query engine methods:\n",
    "* `query`\n",
    "* `aquery`\n",
    "* `chat`\n",
    "* `achat`\n",
    "* `stream_chat`\n",
    "* `astream_chat`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage\n",
    "\n",
    "Below is a quick example of usage. First, we'll create a standard Llama-Index query engine from Paul Graham's Essay, *What I Worked On* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "from trulens_eval import TruLlama\n",
    "\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\"http://paulgraham.com/worked.html\"]\n",
    ")\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instrument an Llama-Index query engine, all that's required is to wrap it using TruLlama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_query_engine_recorder = TruLlama(query_engine)\n",
    "\n",
    "with tru_query_engine_recorder as recording:\n",
    "    print(query_engine.query(\"What did the author do growing up?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the full quickstart available here: [Llama-Index Quickstart](../llama_index_quickstart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async Support\n",
    "TruLlama also provides async support for Llama-Index through the `aquery`, `achat`, and `astream_chat` methods. This allows you to track and evaluate async applciations.\n",
    "\n",
    "As an example, below is an Llama-Index async chat engine (`achat`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports main tools:\n",
    "from trulens_eval import TruLlama, Feedback, Tru, feedback, Select\n",
    "tru = Tru()\n",
    "\n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\"http://paulgraham.com/worked.html\"]\n",
    ")\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "chat_engine = index.as_chat_engine(streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instrument an Llama-Index `achat` engine, all that's required is to wrap it using TruLlama - just like with the query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_chat_recorder = TruLlama(chat_engine)\n",
    "\n",
    "with tru_chat_recorder as recording:\n",
    "    llm_response_async = await chat_engine.aquery(\"What did the author do growing up?\")\n",
    "\n",
    "print(llm_response_async)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Support\n",
    "\n",
    "TruLlama also provides streaming support for Llama-Index. This allows you to track and evaluate streaming applications.\n",
    "\n",
    "As an example, below is an Llama-Index query engine with streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "from trulens_eval import TruLlama\n",
    "\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\"http://paulgraham.com/worked.html\"]\n",
    ")\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine(streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with other methods, just wrap your streaming query engine with TruLlama and operate like before.\n",
    "\n",
    "You can also print the response tokens as they are generated using the `response_gen` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_query_engine_recorder = TruLlama(query_engine)\n",
    "\n",
    "with tru_query_engine_recorder as recording:\n",
    "    response = query_engine.query(\"What did the author do growing up?\")\n",
    "\n",
    "for c in response.response_gen:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more usage examples, check out the [Llama-Index examples directory](https://github.com/truera/trulens/tree/main/trulens_eval/examples/frameworks/llama_index)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d153714b979d5e6d08dd8ec90712dd93bff2c9b6c1f0c118169738af3430cd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
