{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama-Index Integration\n",
    "\n",
    "TruLens provides TruLlama, a deep integration with Llama-Index to allow you to inspect and evaluate the internals of your application built using Llama-Index.\n",
    "\n",
    "TruLlama captures all of the metrics and metadata listed in the [instrumentation overview](../basic_instrumentation). In addition, TruLlama provides the `select_source_nodes` method to capture the source nodes of your query.\n",
    "\n",
    "## Supported methods\n",
    "TruLlama supports both sync and async modes using the following Llama-Index query engine methods:\n",
    "* `query`\n",
    "* `aquery`\n",
    "* `chat`\n",
    "* `achat`\n",
    "* `stream_chat`\n",
    "* `astream_chat`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage\n",
    "\n",
    "Below is a quick example of usage. First, we'll create a standard Llama-Index query engine from Paul Graham's Essay, *What I Worked On* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleWebPageReader\n",
    "from trulens_eval import TruLlama\n",
    "\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\"http://paulgraham.com/worked.html\"]\n",
    ")\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instrument an Llama-Index query engine, all that's required is to wrap it using TruLlama. Then the instrumented query engine can be used like the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_query_engine = TruLlama(query_engine)\n",
    "\n",
    "llm_response = tru_query_engine.query(\"What did the author do growing up?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the full quickstart available here: [Llama-Index Quickstart](../llama_index_quickstart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async Support\n",
    "TruLlama also provides async support for Llama-Index through the `aquery`, `achat`, and `astream_chat` methods. This allows you to track and evaluate async applciations.\n",
    "\n",
    "As an example, below is an Llama-Index async chat engine (`achat`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports main tools:\n",
    "from trulens_eval import TruLlama, Feedback, Tru, feedback, Select\n",
    "tru = Tru()\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleWebPageReader\n",
    "\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\"http://paulgraham.com/worked.html\"]\n",
    ")\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "chat_engine = index.as_chat_engine(streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instrument an Llama-Index `achat` engine, all that's required is to wrap it using TruLlama - just like with the query engine. Then the instrumented `achat` engine can be used like the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_achat_engine = TruLlama(chat_engine)\n",
    "\n",
    "# Instrumented query engine can operate like the original:\n",
    "llm_response_async = await tru_achat_engine.aquery(\"What did the author do growing up?\")\n",
    "\n",
    "print(llm_response_async)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Support\n",
    "\n",
    "TruLlama also provides streaming support for Llama-Index. This allows you to track and evaluate streaming applications.\n",
    "\n",
    "As an example, below is an Llama-Index query engine with streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleWebPageReader\n",
    "from trulens_eval import TruLlama\n",
    "\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\"http://paulgraham.com/worked.html\"]\n",
    ")\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine(streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with other methods, just wrap your streaming query engine with TruLlama and operate like before.\n",
    "\n",
    "You can also print the response tokens as they are generated using the `response_gen` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_query_engine = TruLlama(query_engine)\n",
    "\n",
    "response = tru_query_engine.query(\"What did the author do growing up?\")\n",
    "\n",
    "for c in response.response_gen:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more usage examples, check out the [Llama-Index examples directory](https://github.com/truera/trulens/tree/main/trulens_eval/examples/frameworks/llama_index)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d153714b979d5e6d08dd8ec90712dd93bff2c9b6c1f0c118169738af3430cd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
