{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ““ ðŸ¦™ LlamaIndex Integration\n",
    "\n",
    "TruLens provides TruLlama, a deep integration with LlamaIndex to allow you to\n",
    "inspect and evaluate the internals of your application built using LlamaIndex.\n",
    "This is done through the instrumentation of key LlamaIndex classes and methods.\n",
    "To see all classes and methods instrumented, see *Appendix: LlamaIndex\n",
    "Instrumented Classes and Methods*.\n",
    "\n",
    "In addition to the default instrumentation, TruChain exposes the\n",
    "*select_context* and *select_source_nodes* methods for evaluations that require\n",
    "access to retrieved context or source nodes. Exposing these methods bypasses the\n",
    "need to know the json structure of your app ahead of time, and makes your\n",
    "evaluations re-usable across different apps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage\n",
    "\n",
    "Below is a quick example of usage. First, we'll create a standard LlamaIndex query engine from Paul Graham's Essay, *What I Worked On* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\"http://paulgraham.com/worked.html\"]\n",
    ")\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instrument an LlamaIndex query engine, all that's required is to wrap it using TruLlama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n",
      "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of Tru` to prevent this.\n",
      "The author, growing up, worked on writing short stories and programming.\n"
     ]
    }
   ],
   "source": [
    "from trulens.ext.instrument.llamaindex import TruLlama\n",
    "\n",
    "tru_query_engine_recorder = TruLlama(query_engine)\n",
    "\n",
    "with tru_query_engine_recorder as recording:\n",
    "    print(query_engine.query(\"What did the author do growing up?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To properly evaluate LLM apps we often need to point our evaluation at an\n",
    "internal step of our application, such as the retreived context. Doing so allows\n",
    "us to evaluate for metrics including context relevance and groundedness.\n",
    "\n",
    "For LlamaIndex applications where the source nodes are used, `select_context`\n",
    "can be used to access the retrieved text for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from trulens.core import Feedback\n",
    "from trulens.ext.provider.openai import OpenAI\n",
    "\n",
    "provider = OpenAI()\n",
    "\n",
    "context = TruLlama.select_context(query_engine)\n",
    "\n",
    "f_context_relevance = (\n",
    "    Feedback(provider.context_relevance)\n",
    "    .on_input()\n",
    "    .on(context)\n",
    "    .aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For added flexibility, the select_context method is also made available through\n",
    "`trulens_eval.app.App`. This allows you to switch between frameworks without\n",
    "changing your context selector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core.app import App\n",
    "\n",
    "context = App.select_context(query_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the full quickstart available here: [LlamaIndex Quickstart](../../../getting_started/quickstarts/llama_index_quickstart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async Support\n",
    "TruLlama also provides async support for LlamaIndex through the `aquery`,\n",
    "`achat`, and `astream_chat` methods. This allows you to track and evaluate async\n",
    "applciations.\n",
    "\n",
    "As an example, below is an LlamaIndex async chat engine (`achat`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports main tools:\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "from trulens.core import Tru\n",
    "from trulens.ext.instrument.llamaindex import TruLlama\n",
    "\n",
    "tru = Tru()\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\"http://paulgraham.com/worked.html\"]\n",
    ")\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "chat_engine = index.as_chat_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instrument an LlamaIndex `achat` engine, all that's required is to wrap it using TruLlama - just like with the query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new object of type ChatMemoryBuffer at 0x2bf581210 is calling an instrumented method put. The path of this call may be incorrect.\n",
      "Guessing path of new object is app.memory based on other object (0x2bf5e5050) using this function.\n",
      "Could not determine main output from None.\n",
      "Could not determine main output from None.\n",
      "Could not determine main output from None.\n",
      "Could not determine main output from None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author worked on writing short stories and programming while growing up.\n"
     ]
    }
   ],
   "source": [
    "tru_chat_recorder = TruLlama(chat_engine)\n",
    "\n",
    "with tru_chat_recorder as recording:\n",
    "    llm_response_async = await chat_engine.achat(\n",
    "        \"What did the author do growing up?\"\n",
    "    )\n",
    "\n",
    "print(llm_response_async)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Support\n",
    "\n",
    "TruLlama also provides streaming support for LlamaIndex. This allows you to track and evaluate streaming applications.\n",
    "\n",
    "As an example, below is an LlamaIndex query engine with streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\"http://paulgraham.com/worked.html\"]\n",
    ")\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "chat_engine = index.as_chat_engine(streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with other methods, just wrap your streaming query engine with TruLlama and operate like before.\n",
    "\n",
    "You can also print the response tokens as they are generated using the `response_gen` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new object of type ChatMemoryBuffer at 0x2c1df9950 is calling an instrumented method put. The path of this call may be incorrect.\n",
      "Guessing path of new object is app.memory based on other object (0x2c08b04f0) using this function.\n",
      "Could not find usage information in openai response:\n",
      "<openai.Stream object at 0x2bf5f3ed0>\n",
      "Could not find usage information in openai response:\n",
      "<openai.Stream object at 0x2bf5f3ed0>\n"
     ]
    }
   ],
   "source": [
    "tru_chat_engine_recorder = TruLlama(chat_engine)\n",
    "\n",
    "with tru_chat_engine_recorder as recording:\n",
    "    response = chat_engine.stream_chat(\"What did the author do growing up?\")\n",
    "\n",
    "for c in response.response_gen:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more usage examples, check out the [LlamaIndex examples directory](https://github.com/truera/trulens/tree/main/trulens_eval/examples/frameworks/llama_index)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: LlamaIndex Instrumented Classes and Methods\n",
    "\n",
    "The modules, classes, and methods that trulens instruments can be retrieved from\n",
    "the appropriate Instrument subclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module langchain*\n",
      "  Class langchain.agents.agent.BaseMultiActionAgent\n",
      "    Method plan: (self, intermediate_steps: 'List[Tuple[AgentAction, str]]', callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'Union[List[AgentAction], AgentFinish]'\n",
      "    Method aplan: (self, intermediate_steps: 'List[Tuple[AgentAction, str]]', callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'Union[List[AgentAction], AgentFinish]'\n",
      "  Class langchain.agents.agent.BaseSingleActionAgent\n",
      "    Method plan: (self, intermediate_steps: 'List[Tuple[AgentAction, str]]', callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'Union[AgentAction, AgentFinish]'\n",
      "    Method aplan: (self, intermediate_steps: 'List[Tuple[AgentAction, str]]', callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'Union[AgentAction, AgentFinish]'\n",
      "  Class langchain.chains.base.Chain\n",
      "    Method invoke: (self, input: Dict[str, Any], config: Optional[langchain_core.runnables.config.RunnableConfig] = None, **kwargs: Any) -> Dict[str, Any]\n",
      "    Method ainvoke: (self, input: Dict[str, Any], config: Optional[langchain_core.runnables.config.RunnableConfig] = None, **kwargs: Any) -> Dict[str, Any]\n",
      "    Method run: (self, *args: Any, callbacks: Union[List[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, **kwargs: Any) -> Any\n",
      "    Method arun: (self, *args: Any, callbacks: Union[List[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, **kwargs: Any) -> Any\n",
      "    Method _call: (self, inputs: Dict[str, Any], run_manager: Optional[langchain_core.callbacks.manager.CallbackManagerForChainRun] = None) -> Dict[str, Any]\n",
      "    Method _acall: (self, inputs: Dict[str, Any], run_manager: Optional[langchain_core.callbacks.manager.AsyncCallbackManagerForChainRun] = None) -> Dict[str, Any]\n",
      "  Class langchain.memory.chat_memory.BaseChatMemory\n",
      "    Method save_context: (self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None\n",
      "    Method clear: (self) -> None\n",
      "  Class langchain_core.chat_history.BaseChatMessageHistory\n",
      "  Class langchain_core.documents.base.Document\n",
      "  Class langchain_core.language_models.base.BaseLanguageModel\n",
      "  Class langchain_core.language_models.llms.BaseLLM\n",
      "  Class langchain_core.load.serializable.Serializable\n",
      "  Class langchain_core.memory.BaseMemory\n",
      "    Method save_context: (self, inputs: 'Dict[str, Any]', outputs: 'Dict[str, str]') -> 'None'\n",
      "    Method clear: (self) -> 'None'\n",
      "  Class langchain_core.prompts.base.BasePromptTemplate\n",
      "  Class langchain_core.retrievers.BaseRetriever\n",
      "    Method _get_relevant_documents: (self, query: 'str', *, run_manager: 'CallbackManagerForRetrieverRun') -> 'List[Document]'\n",
      "    Method get_relevant_documents: (self, query: 'str', *, callbacks: 'Callbacks' = None, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, **kwargs: 'Any') -> 'List[Document]'\n",
      "    Method aget_relevant_documents: (self, query: 'str', *, callbacks: 'Callbacks' = None, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, **kwargs: 'Any') -> 'List[Document]'\n",
      "    Method _aget_relevant_documents: (self, query: 'str', *, run_manager: 'AsyncCallbackManagerForRetrieverRun') -> 'List[Document]'\n",
      "  Class langchain_core.runnables.base.RunnableSerializable\n",
      "  Class langchain_core.tools.BaseTool\n",
      "    Method _arun: (self, *args: 'Any', **kwargs: 'Any') -> 'Any'\n",
      "    Method _run: (self, *args: 'Any', **kwargs: 'Any') -> 'Any'\n",
      "\n",
      "Module llama_hub.*\n",
      "\n",
      "Module llama_index.*\n",
      "  Class llama_index.core.base.base_query_engine.BaseQueryEngine\n",
      "    Method query: (self, str_or_query_bundle: Union[str, llama_index.core.schema.QueryBundle]) -> Union[llama_index.core.base.response.schema.Response, llama_index.core.base.response.schema.StreamingResponse, llama_index.core.base.response.schema.PydanticResponse]\n",
      "    Method aquery: (self, str_or_query_bundle: Union[str, llama_index.core.schema.QueryBundle]) -> Union[llama_index.core.base.response.schema.Response, llama_index.core.base.response.schema.StreamingResponse, llama_index.core.base.response.schema.PydanticResponse]\n",
      "    Method retrieve: (self, query_bundle: llama_index.core.schema.QueryBundle) -> List[llama_index.core.schema.NodeWithScore]\n",
      "    Method synthesize: (self, query_bundle: llama_index.core.schema.QueryBundle, nodes: List[llama_index.core.schema.NodeWithScore], additional_source_nodes: Optional[Sequence[llama_index.core.schema.NodeWithScore]] = None) -> Union[llama_index.core.base.response.schema.Response, llama_index.core.base.response.schema.StreamingResponse, llama_index.core.base.response.schema.PydanticResponse]\n",
      "  Class llama_index.core.base.base_query_engine.QueryEngineComponent\n",
      "    Method _run_component: (self, **kwargs: Any) -> Any\n",
      "  Class llama_index.core.base.base_retriever.BaseRetriever\n",
      "    Method retrieve: (self, str_or_query_bundle: Union[str, llama_index.core.schema.QueryBundle]) -> List[llama_index.core.schema.NodeWithScore]\n",
      "    Method _retrieve: (self, query_bundle: llama_index.core.schema.QueryBundle) -> List[llama_index.core.schema.NodeWithScore]\n",
      "    Method _aretrieve: (self, query_bundle: llama_index.core.schema.QueryBundle) -> List[llama_index.core.schema.NodeWithScore]\n",
      "  Class llama_index.core.base.embeddings.base.BaseEmbedding\n",
      "  Class llama_index.core.base.llms.types.LLMMetadata\n",
      "  Class llama_index.core.chat_engine.types.BaseChatEngine\n",
      "    Method chat: (self, message: str, chat_history: Optional[List[llama_index.core.base.llms.types.ChatMessage]] = None) -> Union[llama_index.core.chat_engine.types.AgentChatResponse, llama_index.core.chat_engine.types.StreamingAgentChatResponse]\n",
      "    Method achat: (self, message: str, chat_history: Optional[List[llama_index.core.base.llms.types.ChatMessage]] = None) -> Union[llama_index.core.chat_engine.types.AgentChatResponse, llama_index.core.chat_engine.types.StreamingAgentChatResponse]\n",
      "    Method stream_chat: (self, message: str, chat_history: Optional[List[llama_index.core.base.llms.types.ChatMessage]] = None) -> llama_index.core.chat_engine.types.StreamingAgentChatResponse\n",
      "  Class llama_index.core.indices.base.BaseIndex\n",
      "  Class llama_index.core.indices.prompt_helper.PromptHelper\n",
      "  Class llama_index.core.memory.types.BaseMemory\n",
      "    Method put: (self, message: llama_index.core.base.llms.types.ChatMessage) -> None\n",
      "  Class llama_index.core.node_parser.interface.NodeParser\n",
      "  Class llama_index.core.postprocessor.types.BaseNodePostprocessor\n",
      "    Method _postprocess_nodes: (self, nodes: List[llama_index.core.schema.NodeWithScore], query_bundle: Optional[llama_index.core.schema.QueryBundle] = None) -> List[llama_index.core.schema.NodeWithScore]\n",
      "  Class llama_index.core.question_gen.types.BaseQuestionGenerator\n",
      "  Class llama_index.core.response_synthesizers.base.BaseSynthesizer\n",
      "  Class llama_index.core.response_synthesizers.refine.Refine\n",
      "    Method get_response: (self, query_str: str, text_chunks: Sequence[str], prev_response: Union[pydantic.v1.main.BaseModel, str, Generator[str, NoneType, NoneType], NoneType] = None, **response_kwargs: Any) -> Union[pydantic.v1.main.BaseModel, str, Generator[str, NoneType, NoneType]]\n",
      "  Class llama_index.core.schema.BaseComponent\n",
      "  Class llama_index.core.tools.types.BaseTool\n",
      "    Method __call__: (self, input: Any) -> llama_index.core.tools.types.ToolOutput\n",
      "  Class llama_index.core.tools.types.ToolMetadata\n",
      "  Class llama_index.core.vector_stores.types.VectorStore\n",
      "  Class llama_index.legacy.llm_predictor.base.BaseLLMPredictor\n",
      "    Method predict: (self, prompt: llama_index.legacy.prompts.base.BasePromptTemplate, **prompt_args: Any) -> str\n",
      "  Class llama_index.legacy.llm_predictor.base.LLMPredictor\n",
      "    Method predict: (self, prompt: llama_index.legacy.prompts.base.BasePromptTemplate, output_cls: Optional[pydantic.v1.main.BaseModel] = None, **prompt_args: Any) -> str\n",
      "\n",
      "Module trulens_eval.*\n",
      "  Class trulens_eval.feedback.feedback.Feedback\n",
      "    Method __call__: (self, *args, **kwargs) -> 'Any'\n",
      "  Class trulens_eval.utils.imports.llama_index.core.llms.base.BaseLLM\n",
      "    WARNING: this class could not be imported. It may have been (re)moved. Error:\n",
      "      > No module named 'llama_index.core.llms.base'\n",
      "  Class trulens_eval.utils.langchain.WithFeedbackFilterDocuments\n",
      "    Method _get_relevant_documents: (self, query: str, *, run_manager) -> List[langchain_core.documents.base.Document]\n",
      "    Method get_relevant_documents: (self, query: 'str', *, callbacks: 'Callbacks' = None, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, **kwargs: 'Any') -> 'List[Document]'\n",
      "    Method aget_relevant_documents: (self, query: 'str', *, callbacks: 'Callbacks' = None, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, **kwargs: 'Any') -> 'List[Document]'\n",
      "    Method _aget_relevant_documents: (self, query: 'str', *, run_manager: 'AsyncCallbackManagerForRetrieverRun') -> 'List[Document]'\n",
      "  Class trulens_eval.utils.llama.WithFeedbackFilterNodes\n",
      "    WARNING: this class could not be imported. It may have been (re)moved. Error:\n",
      "      > No module named 'llama_index.indices.vector_store'\n",
      "  Class trulens_eval.utils.python.EmptyType\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from trulens.ext.instrument.llamaindex import LlamaInstrument\n",
    "\n",
    "LlamaInstrument().print_instrumentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrumenting other classes/methods.\n",
    "Additional classes and methods can be instrumented by use of the\n",
    "`trulens_eval.instruments.Instrument` methods and decorators. Examples of\n",
    "such usage can be found in the custom app used in the `custom_example.ipynb`\n",
    "notebook which can be found in\n",
    "`trulens_eval/examples/expositional/end2end_apps/custom_app/custom_app.py`. More\n",
    "information about these decorators can be found in the\n",
    "`docs/trulens_eval/tracking/instrumentation/index.ipynb` notebook.\n",
    "\n",
    "### Inspecting instrumentation\n",
    "The specific objects (of the above classes) and methods instrumented for a\n",
    "particular app can be inspected using the `App.print_instrumented` as\n",
    "exemplified in the next cell. Unlike `Instrument.print_instrumentation`, this\n",
    "function only shows what in an app was actually instrumented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components:\n",
      "\tTruLlama (Other) at 0x2bf5d5d10 with path __app__\n",
      "\tOpenAIAgent (Other) at 0x2bf535a10 with path __app__.app\n",
      "\tChatMemoryBuffer (Other) at 0x2bf537210 with path __app__.app.memory\n",
      "\tSimpleChatStore (Other) at 0x2be6ef710 with path __app__.app.memory.chat_store\n",
      "\n",
      "Methods:\n",
      "Object at 0x2bf537210:\n",
      "\t<function ChatMemoryBuffer.put at 0x2b14c19e0> with path __app__.app.memory\n",
      "\t<function BaseMemory.put at 0x2b1448f40> with path __app__.app.memory\n",
      "Object at 0x2bf535a10:\n",
      "\t<function BaseQueryEngine.query at 0x2b137dc60> with path __app__.app\n",
      "\t<function BaseQueryEngine.aquery at 0x2b137e2a0> with path __app__.app\n",
      "\t<function AgentRunner.chat at 0x2bf5aa160> with path __app__.app\n",
      "\t<function AgentRunner.achat at 0x2bf5aa2a0> with path __app__.app\n",
      "\t<function AgentRunner.stream_chat at 0x2bf5aa340> with path __app__.app\n",
      "\t<function BaseQueryEngine.retrieve at 0x2b137e340> with path __app__.app\n",
      "\t<function BaseQueryEngine.synthesize at 0x2b137e3e0> with path __app__.app\n",
      "\t<function BaseChatEngine.chat at 0x2b1529f80> with path __app__.app\n",
      "\t<function BaseChatEngine.achat at 0x2b152a0c0> with path __app__.app\n",
      "\t<function BaseAgent.stream_chat at 0x2beb437e0> with path __app__.app\n",
      "\t<function BaseChatEngine.stream_chat at 0x2b152a020> with path __app__.app\n",
      "Object at 0x2c1df9950:\n",
      "\t<function ChatMemoryBuffer.put at 0x2b14c19e0> with path __app__.app.memory\n"
     ]
    }
   ],
   "source": [
    "tru_chat_engine_recorder.print_instrumented()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d153714b979d5e6d08dd8ec90712dd93bff2c9b6c1f0c118169738af3430cd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
