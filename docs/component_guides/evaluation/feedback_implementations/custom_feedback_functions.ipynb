{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "691ec232",
   "metadata": {},
   "source": [
    "# ðŸ““ Custom Feedback Functions\n",
    "\n",
    "Feedback functions are an extensible framework for evaluating LLMs.\n",
    "\n",
    "The primary motivations for customizing feedback functions are either to improve alignment of an existing feedback function, or to evaluate on a new axis not addressed by an out-of-the-box feedback function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e3d62f",
   "metadata": {},
   "source": [
    "## Improving feedback function alignment through customization\n",
    "\n",
    "Feedback functions can be customized through a number of parameter changes that influence score generation. For example, you can choose to run feedbacks with or without chain-of-thought reasoning, customize the output scale, or provide \"few-shot\" examples to guide alignment of a feedback function. All of these decisions affect the score generation and should be carefully tested and benchmarked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d99699d",
   "metadata": {},
   "source": [
    "### Chain-of-thought Reasoning\n",
    "\n",
    "Feedback functions can be run with chain-of-thought reasoning using their `\"with_cot_reasons\"` variant. Doing so provides both the benefit of a view into how the grading is performed, and improves alignment due to the auto-regressive nature of LLMs forcing the score to sequentially follow the reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2594092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import Feedback\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "provider = OpenAI(model_engine=\"gpt-4o\")\n",
    "\n",
    "provider.relevance(\n",
    "    \"What are the key considerations when starting a small business?\",\n",
    "    \"Find a mentor who can guide you through the early stages and help you navigate common challenges.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831743e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "provider.relevance_with_cot_reasons(\n",
    "    \"What are the key considerations when starting a small business?\",\n",
    "    \"Find a mentor who can guide you through the early stages and help you navigate common challenges.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e69f37d",
   "metadata": {},
   "source": [
    "### Output space\n",
    "\n",
    "The output space is another very important variable to consider. This allows you to trade-off between a score's accuracy and granularity. The larger the output space, the lower the accuracy.\n",
    "\n",
    "Output space can be modulated via the `min_score_val` and `max_score_val` keyword arguments.\n",
    "\n",
    "The output space currently allows three selections:\n",
    "- 0 or 1 (binary)\n",
    "- 0 to 3 (default)\n",
    "- 0 to 10\n",
    "\n",
    "While the output you see is always on a scale from 0 to 1, changing the output space changes the score range prompting given to the LLM judge. The score produced by the judge is then scaled down appropriately.\n",
    "\n",
    "For example, we can modulate the output space to 0-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6cfecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "provider.relevance(\n",
    "    \"What are the key considerations when starting a small business?\",\n",
    "    \"Find a mentor who can guide you through the early stages and help you navigate common challenges.\",\n",
    "    min_score_val=0,\n",
    "    max_score_val=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1290cbc",
   "metadata": {},
   "source": [
    "Or to binary scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796b042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "provider.relevance(\n",
    "    \"What are the key considerations when starting a small business?\",\n",
    "    \"Find a mentor who can guide you through the early stages and help you navigate common challenges.\",\n",
    "    min_score_val=0,\n",
    "    max_score_val=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24105b52",
   "metadata": {},
   "source": [
    "### Temperature\n",
    "\n",
    "When using LLMs, temperature is another parameter to be mindful of. Feedback functions default to a temperature of 0, but it can be useful in some cases to use higher temperatures, or even ensemble with feedback functions using different temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c28e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "provider.relevance(\n",
    "    \"What are the key considerations when starting a small business?\",\n",
    "    \"Find a mentor who can guide you through the early stages and help you navigate common challenges.\",\n",
    "    temperature=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb804240",
   "metadata": {},
   "source": [
    "### Groundedness configurations\n",
    "\n",
    "Groundedness has its own specific configurations that can be set with the `GroundednessConfigs` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a3c91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core.feedback import feedback\n",
    "\n",
    "groundedness_configs = feedback.GroundednessConfigs(\n",
    "    use_sent_tokenize=False, filter_trivial_statements=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9804e8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "provider.groundedness_measure_with_cot_reasons(\n",
    "    \"The First AFLâ€“NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles.\",\n",
    "    \"Hi, your football expert here. The first superbowl was held on Jan 15, 1967\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a547cfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "provider.groundedness_measure_with_cot_reasons(\n",
    "    \"The First AFLâ€“NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles.\",\n",
    "    \"Hi, your football expert here. The first superbowl was held on Jan 15, 1967\",\n",
    "    groundedness_configs=groundedness_configs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140c9429",
   "metadata": {},
   "source": [
    "## Custom Criteria\n",
    "\n",
    "To customize the LLM-judge prompting, you can override standard criteria with your own custom criteria.\n",
    "\n",
    "This can be useful to tailor LLM-judge prompting to your domain and improve alignment with human evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f795de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_relevance_criteria = \"\"\"\n",
    "A relevant response should provide a clear and concise answer to the question.\n",
    "\"\"\"\n",
    "\n",
    "provider.relevance(\n",
    "    \"What are the key considerations when starting a small business?\",\n",
    "    \"Find a mentor who can guide you through the early stages and help you navigate common challenges.\",\n",
    "    criteria=custom_relevance_criteria,\n",
    "    min_score_val=0,\n",
    "    max_score_val=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce721b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_sentiment_criteria = \"\"\"\n",
    "A positive sentiment should be expressed with an extremely encouraging and enthusiastic tone.\n",
    "\"\"\"\n",
    "\n",
    "provider.sentiment(\n",
    "    \"When you're ready to start your business, you'll be amazed at how much you can achieve!\",\n",
    "    criteria=custom_sentiment_criteria,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefe7bd1",
   "metadata": {},
   "source": [
    "### Few-shot examples\n",
    "\n",
    "You can also provide examples to customize feedback scoring to your domain.\n",
    "\n",
    "This is currently available only for the RAG triad feedback functions (answer relevance, context relevance, and groundedness)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f81f381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.feedback.v2 import feedback\n",
    "\n",
    "fewshot_relevance_examples_list = [\n",
    "    (\n",
    "        {\n",
    "            \"query\": \"What are the key considerations when starting a small business?\",\n",
    "            \"response\": \"You should focus on building relationships with mentors and industry leaders. Networking can provide insights, open doors to opportunities, and help you avoid common pitfalls.\",\n",
    "        },\n",
    "        3,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76133b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "provider.relevance(\n",
    "    \"What are the key considerations when starting a small business?\",\n",
    "    \"Find a mentor who can guide you through the early stages and help you navigate common challenges.\",\n",
    "    examples=fewshot_relevance_examples_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d69109",
   "metadata": {},
   "source": [
    "### Usage Options for Customized Feedback Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075b074c",
   "metadata": {},
   "source": [
    "Feedback customizations are available both directly (shown above) and through the `Feedback` class.\n",
    "\n",
    "Below is an example using the customizations via a feedback function instantiation that will run with typical TruLens recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555a53e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "provider = OpenAI(model_engine=\"gpt-4o\")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_answer_relevance = (\n",
    "    Feedback(\n",
    "        provider.relevance_with_cot_reasons,\n",
    "        name=\"Answer Relevance\",\n",
    "        examples=fewshot_relevance_examples_list,\n",
    "        criteria=custom_relevance_criteria,\n",
    "        min_score_val=0,\n",
    "        max_score_val=1,\n",
    "        temperature=0.9,\n",
    "    )\n",
    "    .on_input()\n",
    "    .on_output()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5533cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_answer_relevance(\n",
    "    \"What are the key considerations when starting a small business?\",\n",
    "    \"Find a mentor who can guide you through the early stages and help you navigate common challenges.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5712b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core.feedback import feedback\n",
    "\n",
    "groundedness_configs = feedback.GroundednessConfigs(\n",
    "    use_sent_tokenize=False, filter_trivial_statements=False\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_groundedness = (\n",
    "    Feedback(\n",
    "        provider.groundedness_measure_with_cot_reasons,\n",
    "        name=\"Groundedness\",\n",
    "        examples=fewshot_relevance_examples_list,\n",
    "        min_score_val=0,\n",
    "        max_score_val=1,\n",
    "        temperature=0.9,\n",
    "        groundedness_configs=groundedness_configs,\n",
    "    )\n",
    "    .on_input()\n",
    "    .on_output()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfed26d5",
   "metadata": {},
   "source": [
    "## Creating new custom feedback funcitons\n",
    "\n",
    "You can add your own feedback functions to evaluate the qualities required by your application in two steps: by creating a new provider class and feedback function in your notebook! If your contributions would be useful for others, we encourage you to contribute to TruLens!\n",
    "\n",
    "Feedback functions are organized by model provider into `Provider` classes.\n",
    "\n",
    "The process for adding new feedback functions is:\n",
    "1. Create a new `Provider` class or locate an existing one that applies to your feedback function. If your feedback function does not rely on a model provider, you can create a standalone class. Add the new feedback function method to your selected class. Your new method can either take a single text (`str`) as a parameter or both prompt (`str`) and response (`str`). It should return a float between 0 (worst) and 1 (best)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32ec934",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import Feedback\n",
    "from trulens.core import Provider\n",
    "\n",
    "\n",
    "class StandAlone(Provider):\n",
    "    def custom_feedback(self, my_text_field: str) -> float:\n",
    "        \"\"\"\n",
    "        A dummy function of text inputs to float outputs.\n",
    "\n",
    "        Parameters:\n",
    "            my_text_field (str): Text to evaluate.\n",
    "\n",
    "        Returns:\n",
    "            float: square length of the text\n",
    "        \"\"\"\n",
    "        return 1.0 / (1.0 + len(my_text_field) * len(my_text_field))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4056c677",
   "metadata": {},
   "source": [
    "2. Instantiate your provider and feedback functions. The feedback function is wrapped by the `Feedback` class which helps specify what will get sent to your function parameters (For example: `Select.RecordInput` or `Select.RecordOutput`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db77781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "standalone = StandAlone()\n",
    "f_custom_function = Feedback(standalone.custom_feedback).on_output()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66987343",
   "metadata": {},
   "source": [
    "3. Your feedback function is now ready to use just like the out of the box feedback functions. Below is an example of it being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4332174",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_custom_function(\"Hello, World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171cc0b7",
   "metadata": {},
   "source": [
    "## Extending existing providers.\n",
    "\n",
    "In addition to calling your own methods, you can also extend stock feedback providers (such as `OpenAI`, `AzureOpenAI`, or `Bedrock`) to custom feedback implementations. This can be especially useful for tweaking stock feedback functions, or running custom feedback function prompts while letting TruLens handle the backend LLM provider.\n",
    "\n",
    "This is done by subclassing the provider you wish to extend, using the `generate_score` method that runs the provided prompt with your specified provider, and extracting a float score from 0-1. Your prompt should request the LLM respond on the scale from 0 to 10, then the `generate_score` method will normalize to 0-1.\n",
    "\n",
    "See below for example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d420d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.providers.openai import AzureOpenAI\n",
    "\n",
    "\n",
    "class CustomAzureOpenAI(AzureOpenAI):\n",
    "    def style_check_professional(self, response: str) -> float:\n",
    "        \"\"\"\n",
    "        Custom feedback function to grade the professional style of the response, extending AzureOpenAI provider.\n",
    "\n",
    "        Args:\n",
    "            response (str): text to be graded for professional style.\n",
    "\n",
    "        Returns:\n",
    "            float: A value between 0 and 1. 0 being \"not professional\" and 1 being \"professional\".\n",
    "        \"\"\"\n",
    "        professional_prompt = str.format(\n",
    "            \"Please rate the professionalism of the following text on a scale from 0 to 10, where 0 is not at all professional and 10 is extremely professional: \\n\\n{}\",\n",
    "            response,\n",
    "        )\n",
    "        return self.generate_score(system_prompt=professional_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d621d70",
   "metadata": {},
   "source": [
    "Running \"chain of thought evaluations\" is another use case for extending providers. Doing so follows a similar process as above, where the base provider (such as `AzureOpenAI`) is subclassed.\n",
    "\n",
    "For this case, the method `generate_score_and_reasons` can be used to extract both the score and chain of thought reasons from the LLM response.\n",
    "\n",
    "To use this method, the prompt used should include the `COT_REASONS_TEMPLATE` available from the TruLens prompts library (`trulens.feedback.prompts`).\n",
    "\n",
    "See below for example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc024c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "from trulens.feedback import prompts\n",
    "\n",
    "\n",
    "class CustomAzureOpenAIReasoning(AzureOpenAI):\n",
    "    def context_relevance_with_cot_reasons_extreme(\n",
    "        self, question: str, context: str\n",
    "    ) -> Tuple[float, Dict]:\n",
    "        \"\"\"\n",
    "        Tweaked version of context relevance, extending AzureOpenAI provider.\n",
    "        A function that completes a template to check the relevance of the statement to the question.\n",
    "        Scoring guidelines for scores 5-8 are removed to push the LLM to more extreme scores.\n",
    "        Also uses chain of thought methodology and emits the reasons.\n",
    "\n",
    "        Args:\n",
    "            question (str): A question being asked.\n",
    "            context (str): A statement to the question.\n",
    "\n",
    "        Returns:\n",
    "            float: A value between 0 and 1. 0 being \"not relevant\" and 1 being \"relevant\".\n",
    "        \"\"\"\n",
    "\n",
    "        # remove scoring guidelines around middle scores\n",
    "        system_prompt = prompts.CONTEXT_RELEVANCE_SYSTEM.replace(\n",
    "            \"- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n\",\n",
    "            \"\",\n",
    "        )\n",
    "\n",
    "        user_prompt = str.format(\n",
    "            prompts.CONTEXT_RELEVANCE_USER, question=question, context=context\n",
    "        )\n",
    "        user_prompt = user_prompt.replace(\n",
    "            \"RELEVANCE:\", prompts.COT_REASONS_TEMPLATE\n",
    "        )\n",
    "\n",
    "        return self.generate_score_and_reasons(system_prompt, user_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit_trulens_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
