{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4ed9c783-b745-4c6a-b674-d8b6935dd62d"
    }
   },
   "source": [
    "# Attributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1cfd9f6d-1cb6-4fc1-a69b-0338d68ce62c"
    }
   },
   "source": [
    "## Model Wrappers\n",
    "\n",
    "In order to support a wide variety of backends with different interfaces for their respective models, TruLens uses its own `ModelWrapper` class which provides a general model interface to simplify the implementation of the API functions.\n",
    "To get the model wrapper, use the `get_model_wrapper` method in `trulens.nn.models`. A model wrapper class exists for each backend that converts a model in the respective backend's format to the general TruLens `ModelWrapper` interface. The wrappers are found in the `models` module, and any model defined using Keras, Pytorch, or Tensorflow should be wrapped before being used with the other API functions that require a model -- all other TruLens functionalities expect models to be an instance of `trulens.nn.models.ModelWrapper`.\n",
    "\n",
    "For more details on allowed parameters, see the [get_model_wrapper](https://truera.github.io/trulens/api/model_wrappers/) documentation.\n",
    "\n",
    "For this demo, we will be using a Pytorch model pre-trained on Imagenet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "6a7dd470-8385-4028-8b44-3f1e427af157"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from trulens.nn.models import get_model_wrapper\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.nn.attribution import InputAttribution, InternalInfluence\n",
    "from trulens.nn.attribution import IntegratedGradients\n",
    "from trulens.nn.attribution import Cut, InputCut, OutputCut\n",
    "from trulens.nn.distributions import LinearDoi, PointDoi\n",
    "\n",
    "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "bert = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
    "\n",
    "bt = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "sentences = [\"this is a sentence\", \"and so is this but this one is longer\"]\n",
    "inputs = bt(sentences, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "inputs['input_ids'] = inputs['input_ids'].to(device)\n",
    "inputs['attention_mask'] = inputs['attention_mask'].to(device)\n",
    "\n",
    "def as_tensor(thing):\n",
    "    if isinstance(thing, torch.Tensor):\n",
    "        return torch.clone(thing)\n",
    "    else:\n",
    "        return torch.Tensor(thing).type_as(thing.dtype).device(thing.device)\n",
    "\n",
    "def token_baseline(inputs, replace_map):\n",
    "    input_ids = as_tensor(inputs['input_ids']).to(device)\n",
    "    masks = as_tensor(inputs['attention_mask']).to(device)\n",
    "    for token, (replacement_id, replacement_mask) in replace_map.items():\n",
    "        ids = input_ids == token\n",
    "        input_ids[ids] = replacement_id\n",
    "        masks[ids] = replacement_mask\n",
    "\n",
    "    return dict(input_ids=input_ids, attention_mask=masks)\n",
    "\n",
    "inputs_baseline = token_baseline(inputs, {101: (0, 0), 102: (0,0)})\n",
    "\n",
    "# outs = bert(**inputs)\n",
    "\n",
    "model = get_model_wrapper(bert, input_shape=(None, bt.model_max_length))\n",
    "model.print_layer_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infl = IntegratedGradients(model, resolution=10)\n",
    "attrs_input = infl.attributions(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "class ToySentiment(nn.Module):\n",
    "    @staticmethod\n",
    "    def from_pretrained(model_code_path: Path) -> 'ToySentiment':\n",
    "        # Not strictly necessary to load or save given fixed parameters can be populated, but \n",
    "        # implementing this for better integration testing.\n",
    "        model = ToySentiment()\n",
    "        model.load_state_dict(torch.load(model_code_path))\n",
    "\n",
    "    def to_pretrained(self, model_code_path: Path) -> None:\n",
    "        torch.save(self.state_dict(), model_code_path)\n",
    "\n",
    "    def set_parameters(self) -> None:\n",
    "        \"\"\"Set model parameters as per fixed specification.\"\"\"\n",
    "\n",
    "        Wi = torch.zeros_like(self.lstm.weight_ih_l0) + 0.1\n",
    "        bi = torch.zeros_like(self.lstm.bias_ih_l0) + 0.1\n",
    "        Wh = torch.zeros_like(self.lstm.weight_hh_l0) + 0.1\n",
    "        bh = torch.zeros_like(self.lstm.bias_hh_l0) + 0.1\n",
    "\n",
    "        big = 20.0 # Multipliers to help dealing with LSTM sigmoids.\n",
    "        half = 8.0 # Intention here is that sigmoid((x*big) - half) is ~0 if x is ~0; and\n",
    "                    # ~1 when indicator is >~ 1.\n",
    "\n",
    "        hs = self.hidden_size\n",
    "\n",
    "        sneutral = 0\n",
    "        sgood = 1\n",
    "        sbad = 2\n",
    "        sconfused = 3\n",
    "\n",
    "        wneutral = self.vocab['neutral']\n",
    "        wgood = self.vocab['good']\n",
    "        wbad = self.vocab['bad']\n",
    "        \n",
    "        # make sure c gate is always big\n",
    "        bi[0:hs*3] = 100.0\n",
    "        bh[0:hs*3] = 100.0\n",
    "\n",
    "        # o gate weights:\n",
    "        Wi[3*hs,   wneutral] = big # read neutral word\n",
    "        Wi[3*hs+1, wgood] = big # read good word\n",
    "        Wi[3*hs+2, wbad] = big # read bad word\n",
    "        Wh[3*hs,   sneutral] = big # keep prior neutral, good, bad states\n",
    "        Wh[3*hs+1, sgood] = big # \n",
    "        Wh[3*hs+2, sbad] = big #\n",
    "        bi[3*hs:4*hs] = -half # sigmoid will be 0 unless one of the three words was read\n",
    "\n",
    "        # set \"good to bad\" confused if prior was good, and input was bad\n",
    "        Wh[3*hs+3, sgood] = big    # (prior state was good\n",
    "        Wi[3*hs+3, wbad] = big    #  and input was bad)\n",
    "        Wh[3*hs+3, sconfused] = 2*big  # or (was already in this confused state)\n",
    "        bh[3*hs+3] = -(half*2) # Want at least 2 of first two to fire, or just the last one to fire.\n",
    "\n",
    "        # set \"bad to good\" confused if prior was bad, and input was good\n",
    "        Wh[3*hs+4, sbad] = big     # (prior state was bad\n",
    "        Wi[3*hs+4, wgood] = big     #  and input was good)\n",
    "        Wh[3*hs+4, sconfused] = 2*big   # or (was already confused)\n",
    "        bh[3*hs+4] = -(half*2)  #\n",
    "\n",
    "        self.lstm.weight_hh_l0 = nn.Parameter(Wh)\n",
    "        self.lstm.bias_hh_l0 = nn.Parameter(bh)\n",
    "        self.lstm.weight_ih_l0 = nn.Parameter(Wi)\n",
    "        self.lstm.bias_ih_l0 = nn.Parameter(bi)\n",
    "\n",
    "        self.embedding.weight = nn.Parameter(torch.eye(self.emb_size))\n",
    "\n",
    "        self.lin.weight = nn.Parameter(torch.tensor(\n",
    "            [\n",
    "                [10.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                [0.0, 20.0, 0.0, 0.0, 0.0],\n",
    "                [0.0, 0.0, 20.0, 0.0, 0.0],\n",
    "                [0.0, 0.0, 0.0, 30.0, 30.0]\n",
    "            ]\n",
    "        ))\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "        self.vocab = {\"[UNKNOWN]\": 0, \"neutral\": 1, \"good\": 2, \"bad\": 3}\n",
    "\n",
    "        self.emb_size = len(self.vocab)\n",
    "\n",
    "        self.hidden_size = 5\n",
    "        # 5 states, one for neutral, one for positive, one for negative, and two for confused. Requiring two\n",
    "        # confused states for simplicity of the model; it is easier to encode semantics of confusion based on \n",
    "        # which initial positive/negative state was set first.\n",
    "\n",
    "        # Identity embedding, each vocab word has its own dimension where its presence is encoded.\n",
    "        self.embedding = nn.Embedding(\n",
    "            # padding_idx=0, \n",
    "            embedding_dim=self.emb_size, \n",
    "            num_embeddings=self.emb_size,\n",
    "            padding_idx=None,\n",
    "            max_norm=None,\n",
    "            norm_type=None\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.emb_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Linear layer to combine the two types of confused state and weight things so that\n",
    "        # confused outweighs positive and negative, while positive and negative outweigh neutral \n",
    "        # if more than one of these states is set.\n",
    "        self.lin = torch.nn.Linear(\n",
    "            in_features=self.hidden_size,\n",
    "            out_features=4,\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "        # self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "        # Finally add a softmax for classification.\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        # self.softmax = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "        self.set_parameters()\n",
    "\n",
    "    def forward(self, word_ids: torch.Tensor, embeds=None) -> torch.Tensor:\n",
    "        if word_ids is not None:\n",
    "            batch_size = word_ids.shape[0]\n",
    "        else:\n",
    "            batch_size = embeds.shape[0]\n",
    "\n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_size)\n",
    "        h0[0, 0, 0] = 1.0 # initial state is neutral\n",
    "        c0 = torch.zeros(1, batch_size, self.hidden_size)\n",
    "\n",
    "        if embeds is None:\n",
    "            embeds = self.embedding(word_ids)\n",
    "            \n",
    "        embeds.retain_grad()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeds, (h0, c0))\n",
    "        hn = hn[0,:,:]\n",
    "\n",
    "        lin_out = self.lin(hn)\n",
    "\n",
    "        probits = self.softmax(lin_out)\n",
    "        # preds = torch.argmax(probits, axis=1)\n",
    "        # pred_prob = torch.gather(probits, dim=1, index=preds[:,None])\n",
    "        # pred_prob.backward(torch.ones_like(pred_prob))\n",
    "        # attr = embeds.grad\n",
    "        # return preds, probits, attr\n",
    "\n",
    "        return probits\n",
    "\n",
    "\n",
    "    def input_of_text(self, texts: List[str]) -> torch.Tensor:\n",
    "        tokens = [self.tokenizer(text) for text in texts]\n",
    "\n",
    "        word_ids = [[(self.vocab[t] if t in self.vocab else 0) for t in token] for token in tokens]\n",
    "\n",
    "        return torch.Tensor(word_ids).int()\n",
    "\n",
    "\n",
    "def generate_dataset(n, l):\n",
    "    \"\"\"Generate random sentiment sentences and their labels.\"\"\"\n",
    "\n",
    "    ret = []\n",
    "    cls = []\n",
    "    for i in range(n):\n",
    "        sent = []\n",
    "        while len(sent) < l:\n",
    "            r = random.random()\n",
    "\n",
    "            word = \"neutral\"\n",
    "\n",
    "            if r > 0.9 and len(sent) > 0:\n",
    "                continue\n",
    "            elif r > 0.8:\n",
    "                word = \"good\"\n",
    "            elif r > 0.7:\n",
    "                word = \"bad\"\n",
    "\n",
    "            sent.append(word)\n",
    "\n",
    "        ret.append(\" \".join(sent))\n",
    "\n",
    "        gt = 0 # neutral\n",
    "        if \"good\" in sent and \"bad\" in sent:\n",
    "            gt = 3 # confused\n",
    "        elif \"good\" in sent:\n",
    "            gt = 1 # positive\n",
    "        elif \"bad\" in sent:\n",
    "            gt = 2 # negative\n",
    "\n",
    "        cls.append(gt)\n",
    "\n",
    "    return pd.DataFrame(dict(sentence=ret, sentiment=cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = ToySentiment()\n",
    "device = 'cpu'\n",
    "# Produce a wrapped model from the pytorch model.\n",
    "model = get_model_wrapper(pt, input_shape=(4), device=device, input_dtype=int, )\n",
    "\n",
    "# test_data = generate_dataset(5, 4)\n",
    "\n",
    "test_data = generate_dataset(16, 4)\n",
    "\n",
    "x_sentences = test_data['sentence']\n",
    "x_tokens = [pt.tokenizer(s) for s in x_sentences]\n",
    "x_ids = pt.input_of_text(x_sentences)\n",
    "labels = test_data['sentiment']\n",
    "\n",
    "# TODO: define \"EmbeddedInfluence\" for InputInfluence except with an initial embedding layer and initialize internal influence as below:\n",
    "from trulens.nn.quantities import LambdaQoI, ClassQoI\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "\n",
    "cls_names = {0:\"neutral\", 1:\"positive\", 2:\"negative\", 3:\"confused\"}\n",
    "\n",
    "infl = {}\n",
    "attrs = {}\n",
    "for cls in [0,1,2,3]:\n",
    "    infl[cls] = InternalInfluence(\n",
    "        model,\n",
    "        cuts=(Cut(\"embedding\"), Cut(\"softmax\")),\n",
    "        #doi=PointDoi(cut=Cut(\"embedding\")),\n",
    "        doi=LinearDoi(cut=Cut(\"embedding\"), resolution=100),\n",
    "        qoi=ClassQoI(cls),\n",
    "        multiply_activation=False\n",
    "    )\n",
    "    attrs[cls] = infl[cls].attributions(x_ids)\n",
    "\n",
    "\n",
    "preds = pt(x_ids)\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, (words, label, pred) in enumerate(zip(x_tokens, labels, preds)):\n",
    "    data.append([f\"<b>GT={cls_names[label]} PRED={cls_names[pred.argmax().item()]}</b>\"] + [\" \".join(words)])\n",
    "\n",
    "    for cls in [0,1,2,3]:\n",
    "        # print(f\"{cls_names[cls]} | \", end='')\n",
    "\n",
    "        row = [cls_names[cls]]\n",
    "\n",
    "        sent = \"\"\n",
    "\n",
    "        for word, attr in zip(words, attrs[cls][i]):\n",
    "            # print(f\"{word}[{attr.sum()}] \", end=\"\")\n",
    "            mag = attr.sum()\n",
    "            red = 0.0\n",
    "            green = 0.0\n",
    "            if mag > 0:\n",
    "                green = 1.0 # 0.5 + mag * 0.5\n",
    "                red = 1.0 - mag * 0.5\n",
    "            else:\n",
    "                red = 1.0\n",
    "                green = 1.0 + mag * 0.5\n",
    "                #red = 0.5 - mag * 0.5\n",
    "\n",
    "            blue = min(red, green)\n",
    "            # blue = 1.0 - max(red, green)\n",
    "\n",
    "            sent += f\"<span style='color: rgb({red*255}, {green*255}, {blue*255});'>{word}</span> \"\n",
    "\n",
    "        row.append(sent)\n",
    "        data.append(row)\n",
    "\n",
    "        # print()\n",
    "\n",
    "tab = tabulate.tabulate(data, tablefmt='html')\n",
    "display(HTML(tab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['sentence'][0] = \"neutral neutral neutral neutral\"\n",
    "test_data['sentence'][1] = \"good neutral neutral neutral\"\n",
    "test_data['sentence'][2] = \"neutral good neutral neutral\"\n",
    "test_data['sentence'][3] = \"neutral neutral good neutral\"\n",
    "test_data['sentence'][4] = \"neutral neutral neutral good\"\n",
    "test_data['sentiment'][0] = 0\n",
    "test_data['sentiment'][1:] = 1\n",
    "\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = generate_dataset(4, 2)\n",
    "pm = ToySentiment()\n",
    "pm.requires_grad_(True)\n",
    "pm.train()\n",
    "\n",
    "word_ids = pm.input_of_text(test_data['sentence'])\n",
    "# embeds = pm.embedding(word_ids)\n",
    "# embeds.retain_grad()\n",
    "\n",
    "probits = pm(word_ids=word_ids)\n",
    "\n",
    "print(probits)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "51eb71198507ab2c2a4108a27eda9d9658549732e67153fc0e371d8439827db7"
  },
  "kernelspec": {
   "display_name": "test-fresh-11-29",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
