{"cells":[{"cell_type":"markdown","metadata":{"id":"Tae8NK19MIuZ"},"source":["# Lab Week 2: NLP Example Usage and Stability\n","\n","In this lab we will try out an NLP model and discuss some aspects of NLP models that will come up later in the course as well as your NLP projects. The use of `trulens` for visualization will be introduced. This notebook also demonstrates some of the text processing that might be useful for some NLP projects though we will not go over those details during the lab sessions. In the process we will investigate the bias of a sentiment model towards/against particular baseball teams; by the end of the first lab we should find tweets that have strangely differing sentiments according to a model of interest on tweets suchs as:\n","\n","- \"*redsox* are really playing\"\n","- \"*yankees* are really playing\"\n","\n","Students should be able to:\n","\n","- realize that NLP models are easy to get and use, especially using `huggingface`,\n","- learn the structure of typical NLP pipelines,\n","- learn how to use `trulens` for visualization and attributions (next week), and\n","- use this notebook for ideas and/or coding tips when doing NLP projects.\n","\n","You need to run this notebook with a \"runtime\" feature a GPU. You can \"change runtime type\" in the \"Runtime\" menu above. Evaluate the next few cells but their details do not need discussion. If you run into a CUDA out-of-memory error, you might have to restart the runtime. This notebook uses a particular branch of `trulens` which needs to be installed from `git+https://github.com/truera/trulens.git@piotrm/vis/output-detect` as is done below. Feedback from students can inform main branch trulens features so send thoughts to `piotrm@truera.com`."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":134,"status":"ok","timestamp":1649709705806,"user":{"displayName":"Piotr Mardziel","userId":"13482056344261475057"},"user_tz":420},"id":"EEjog3a-MIuW"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36789,"status":"ok","timestamp":1649709742593,"user":{"displayName":"Piotr Mardziel","userId":"13482056344261475057"},"user_tz":420},"id":"14Wu9ybonM_y","outputId":"320df51f-5411-4bdb-f540-c0791c376f1a"},"outputs":[],"source":["# Some utilities to install things under colab. Run but ignore otherwise.\n","\n","import sys\n","import subprocess\n","from pathlib import Path\n","\n","# if running from within truera repo and want to use latest code, use this:\n","sys.path.insert(0, str(Path(\"../..\").resolve()))\n","\n","try: \n","  import trulens\n","except: \n","  !{sys.executable} -m pip install git+https://github.com/truera/trulens.git@piotrm/vis/output-detect\n","\n","from trulens.utils.colab import install_if_not_installed, load_or_make\n","\n","install_if_not_installed([\"transformers\", \"pandas\", \"numpy\", \"domonic\", \"parmap\"])\n","\n","# Configure trulens\n","import os\n","os.environ['TRULENS_BACKEND']='torch'\n","\n","from IPython.display import display\n","import matplotlib.pyplot as plt\n","import torch\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import re\n","\n","from torch.utils.data import DataLoader\n","from pandas import Series\n","from typing import Union\n","import parmap"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{"id":"t7AfMBMQMIuZ"},"source":["## Twitter Sentiment Model\n","\n","[Huggingface](https://huggingface.co/models) offers a variety of pre-trained NLP models to explore. We exemplify in this notebook a [transformer-based twitter sentiment classification model](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment). This model scores each tweet with three scores: \"negative\", \"neutral\", or \"positive\" sentiment.\n","\n","All it takes to download and instantiate a model from hugging face is these two lines:\n","\n","```[python]\n","model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n","tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","```\n","\n","The [pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines) interface can also be used to combine the two components, tokenizer and model, but in this lab we will be demonstrating the components so we will not use the pipeline.\n","\n","The other elements instantiated in the below cell indicate deployment of the model to a GPU and some annotations around the labels to use for the model's classification scores."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":177,"referenced_widgets":["7882b7eb6ddd4e09ad29753809f814c3","9d533d1740a24cc2bccfd4f99ba3bf23","08d1c47f12ca473f8f8ac80cb5184053","9f6aeb8e05564c6b9271852773eec58b","a833c877cd314ca8831425a1a57376b0","ad299b902b254b5dacc9f77793195d71","5b802c61b41a4178bb86896d76579825","81b8cd797a014713bed2f8fa90c1924e","1d97903892694c859f0d4647b557b5e0","fc5ae775dd26482e86ad66aaf989cfbd","92b4cc26d4d14a73ae0924a006cde30c","ead5041740684a5593ef05823dded92a","9b8fb420ebb94ce69591bdfda0255b03","8ea4e74ba70747cda40d2f2c722fec61","ec24d0959ceb4bc4ba20d0f3682aa7c5","aef547cd8845496f861b13541b1012da","4c6c42a90b5f45dfa03e9c911bedf0da","102f8566386447b78e75ddd051184a99","6c12589fcab04299b0176d74e73fb63b","73b9ed3ace19405a92d981dd4030a17f","574f6d86798a4df59e00c593500f2129","8830da740ecd4e8e901f18968abb8d41","6509af96a82846ffa1a62a5afaedf5f0","c19ba202dd6e49fdada51defb7684816","d0f3f7febd4847b2a3af723d4b2f584f","ca159130487045bbb7c03b42f68ac277","7140c885e55d44fb93cb3ea5299ae484","b3787a2725cb475d875282f790159285","185181475b144327947cfb18a67c0b6b","4480c9d03aa64d60aef96982f5277dc2","bbeca803194f4529a1912fe69c267f33","e0bbc25cdc814bdbb1773706b17c7b8d","ffb96905532b4c8fac016f8e149f586f","d8aef6e0f7464e1db62e6d7c980e194c","836b39843db94ad2b35d03c013867e9b","39566cce425d46e682b54bac1d00a913","683991be14cb4de1b43ce2a5022bc4df","c218226bb2004192882cf1a402f6552e","a23ff8e809934cbb928f9411c8addb71","f1ba1737d7324201b16ebd896f2fb549","19f441360c2a414a8ab2daad42132945","c4e6103c77bb4497b953e67d6ef1e755","112625d3cda147d5b44a9950b3af27b9","2e74d7c685d64447bb3f692f97c19f43","caecf052d5194ce88693c390d3306bb5","fb2e98fbfe364616aa27f45e7dbc608b","89a17cba612c4faba116e3585d92304c","7d940c75c973404e9242bb6a72049bf2","b687bf9aaeab43b8ae83776bbe1708a4","5affc7358d18440d8a23235309a5f4df","1a9cde3460b94ce5bd9f522bb1b0f27b","140a5c69009f41d6a05fc90f6e0df496","3523eb44ae704b698a18644da9e1000a","ca5d7e26941a4e6c895d0cb323503855","bbec5798cb804834b72671ee2646c9de"]},"executionInfo":{"elapsed":51537,"status":"ok","timestamp":1649709794127,"user":{"displayName":"Piotr Mardziel","userId":"13482056344261475057"},"user_tz":420},"id":"VD_4aRscMIua","outputId":"ed8b7103-958d-49f1-be34-132d246dd03e"},"outputs":[],"source":["from transformers import AutoModelForSequenceClassification\n","from transformers import AutoTokenizer\n","\n","# Wrap all of the necessary components.\n","class TwitterSentiment:\n","    MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n","\n","    # device = 'cpu'\n","    # Can also use cuda if available:\n","    device = 'cuda:0'\n","\n","    model = AutoModelForSequenceClassification.from_pretrained(MODEL).to(device)\n","\n","    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","    @staticmethod\n","    def tokenize(inputs):\n","        return TwitterSentiment \\\n","            .tokenizer(inputs, padding=True, return_tensors=\"pt\") \\\n","            .to(TwitterSentiment.device)\n","        # pt refers to pytorch tensor\n","\n","    labels = ['negative', 'neutral', 'positive']\n","\n","    NEGATIVE = labels.index('negative')\n","    NEUTRAL = labels.index('neutral')\n","    POSITIVE = labels.index('positive')\n","\n","task = TwitterSentiment()"]},{"cell_type":"markdown","metadata":{"id":"xx_SEKXzMIua"},"source":["This model quantifies tweets (or really any text you give it) according to its sentiment: positive, negative, or neutral. Lets try it out on some examples."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":154,"status":"ok","timestamp":1649709794274,"user":{"displayName":"Piotr Mardziel","userId":"13482056344261475057"},"user_tz":420},"id":"W_Ru8F0qMIua","outputId":"cec8eb9c-ac53-4060-f97c-c6a181a3ce38"},"outputs":[],"source":["sentences = [\"I'm so happy!\", \"I'm so sad!\", \"I cannot tell whether I should be happy or sad!\", \"meh\"]\n","\n","# Input sentences need to be tokenized first.\n","\n","inputs = task.tokenize(sentences)\n","\n","# The tokenizer gives us vocabulary indexes for each input token (in this case,\n","# words and some word parts like the \"'m\" part of \"I'm\" are tokens).\n","\n","print(inputs)\n","\n","# Decode helps inspecting the tokenization produced:\n","\n","print(task.tokenizer.batch_decode(torch.flatten(inputs['input_ids'])))\n","# Normally decode would give us a single string for each sentence but we would\n","# not be able to see some of the non-word tokens there. Flattening first gives\n","# us a string for each input_id."]},{"cell_type":"markdown","metadata":{"id":"nuQHQ6O61u89"},"source":["Some things to note:\n","\n","- To evaluate a model, we first need to \"tokenize\" a text into indexed tokens, the *tokenization*. We should see the tokens in the example sentences inside the first `tensor` shown in the output above.\n","\n","- While the token ids are not easy for us to inspect, we can use the tokenizer's `decode` method to invert the tokenization while keeping the token boundaries.\n","\n","- Some words may get split into multiple tokens and structures like contractions also make use of multiple tokens.\n","\n","- The tokenization includes special tokens such as seperator and padding tokens. Each text gets surrounded by \"s\" and \"/s\" tokens and each is padding with \"pad\" tokens so as to make each text of the same length.\n","\n","- You can ignore the \"attention_mask\" component of the tokenizer output for now.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9lkFqiioMIub"},"source":["## Displaying tokens using *trulens*\n","\n","The trulens library features some utilities for displaying tokenizations in a more readable manner. To use these, we use the `NLP` object as below. We need to configure it with regards to labels and a tokenizer's decode and encode methods."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":166},"executionInfo":{"elapsed":460,"status":"ok","timestamp":1649709794733,"user":{"displayName":"Piotr Mardziel","userId":"13482056344261475057"},"user_tz":420},"id":"3ZIcXhzwMIub","outputId":"29e433be-b238-4860-f6ad-e000e4cde9c5"},"outputs":[],"source":["from trulens.visualizations import NLP\n","\n","V = NLP(\n","    labels=task.labels,\n","    decode=lambda x: task.tokenizer.decode(x),\n","    tokenize=task.tokenize,\n","    # huggingface models can take as input the keyword args as per produced by their tokenizers.\n","\n","    input_accessor=lambda x: x['input_ids'],\n","    # for huggingface models, input/token ids are under input_ids key in the input dictionary\n","\n","    hidden_tokens=set([task.tokenizer.pad_token_id])\n","    # do not display these tokens\n",")\n","\n","V.tokens(sentences)#, show_id=True))"]},{"cell_type":"markdown","metadata":{"id":"BSryCBlu3PC_"},"source":["The underlines here span each separate token and we use the `hidden_tokens` argument to not render the padding tokens."]},{"cell_type":"markdown","metadata":{"id":"3awoYmKuMIuc"},"source":["## Evaluating the model\n","\n","Evaluating huggingface models is straight-forward if we use the structure (dictionary) produced by the tokenizer."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":227,"status":"ok","timestamp":1649709794959,"user":{"displayName":"Piotr Mardziel","userId":"13482056344261475057"},"user_tz":420},"id":"KGuXdyCOMIuc","outputId":"23c43bea-cead-4aa0-bb6d-b5e415408c7f"},"outputs":[],"source":["outputs = task.model(**inputs)\n","\n","print(outputs)\n","\n","# From logits we can extract the most likely class for each sentence and its readable label.\n","\n","predictions = [task.labels[i] for i in outputs.logits.argmax(axis=1)]\n","\n","for sentence, logits, prediction in zip(sentences, outputs.logits, predictions):\n","    print(logits.to('cpu').detach().numpy(), prediction, sentence)"]},{"cell_type":"markdown","metadata":{"id":"yOnYXWdR4Mva"},"source":["Things to discuss here:\n","\n","- The actual inputs to the model are the contents of the tokenizer output dictionary which is primarily the `input_ids` tensor with integer indices into some vocabulary, one for each token. The inputs are thus discrete, integer, non-floating values. What does this imply regarding the explanation techniques we could deploy here?\n","\n","- The output scores are \"logits\", they are not normalized. "]},{"cell_type":"markdown","metadata":{"id":"jM4vXbxAMIuc"},"source":["Trulens can also help us view these results in a more readable manner. But first we need to tell it about the model by wrappig it in a libray specific container:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":150,"status":"ok","timestamp":1649709795108,"user":{"displayName":"Piotr Mardziel","userId":"13482056344261475057"},"user_tz":420},"id":"EyrOrrPbMIuc","outputId":"cf680439-9dfa-4b51-d7b9-3de8f50fa9b2"},"outputs":[],"source":["from trulens.nn.models import get_model_wrapper\n","\n","task.wrapper = get_model_wrapper(task.model, input_shape=(None, task.tokenizer.model_max_length), device=task.device)"]},{"cell_type":"markdown","metadata":{"id":"aWpHu1k8MIuc"},"source":["We also need to indicate how to retrieve the logits from the output of the model as in the `output_accessor` parameter below:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":178},"executionInfo":{"elapsed":201,"status":"ok","timestamp":1649709795308,"user":{"displayName":"Piotr Mardziel","userId":"13482056344261475057"},"user_tz":420},"id":"mBGuF-9mMIuc","outputId":"edbd11b4-df11-4d9b-9b4d-c238719fd0f3"},"outputs":[],"source":["V = NLP(\n","    wrapper=task.wrapper,\n","    labels=task.labels,\n","    decode=lambda x: task.tokenizer.decode(x),\n","    tokenize=task.tokenize,\n","    # huggingface models can take as input the keyword args as per produced by their tokenizers.\n","\n","    input_accessor=lambda x: x['input_ids'],\n","    # for huggingface models, input/token ids are under input_ids key in the input dictionary\n","\n","    output_accessor=lambda x: x['logits'],\n","    # and logits under 'logits' key in the output dictionary\n","\n","    hidden_tokens=set([task.tokenizer.pad_token_id])\n","    # do not display these tokens\n",")\n","\n","V.tokens(sentences, show_id=True)"]},{"cell_type":"markdown","metadata":{"id":"A9vi9ugc31gC"},"source":["The `show_id` argument lets us display the token indices alongside the human-readable form. Additionally, if a model `wrapper` is provided to the visualizer, the score visualization on the left of each text is will be shown. The scores visualized are normalized to \"probits\" if they are not already."]},{"cell_type":"markdown","metadata":{"id":"fowC0g2YMIud"},"source":["## Exploring real-world tweets\n","\n","Lets try out the sentiment model on some real-world tweets. We first load it the CSV file to a pandas `DataFrame`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":441},"executionInfo":{"elapsed":12812,"status":"ok","timestamp":1649709808117,"user":{"displayName":"Piotr Mardziel","userId":"13482056344261475057"},"user_tz":420},"id":"jol1rniCjWbK","outputId":"fff3f336-d0c4-4570-f987-a60ef6a52b76"},"outputs":[],"source":["# Only needed if dataset not already present.\n","\n","def download_tweets(filename):\n","  subprocess.call(\n","      [\"wget\", \"http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\"]\n","  )\n","  subprocess.call(\n","      [\"unzip\", \"trainingandtestdata.zip\"]\n","  )\n","\n","tweets = load_or_make(\n","    filename=Path(\"training.1600000.processed.noemoticon.csv\"),\n","    loader=lambda filename: pd.read_csv(filename, encoding='ISO-8859-1', header=None, names=[\"polarity\", \"id\", \"timestamp\", \"query\", \"user\", \"text\"]),\n","    downloader=download_tweets\n",")\n","\n","tweets"]},{"cell_type":"markdown","metadata":{"id":"2zxQ4NdBMIud"},"source":["Lets take a look at the model's predictions on some of these tweets. Note that emojis were stripped from the dataset. These missing tokens are shown as ï¿½."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"elapsed":497,"status":"ok","timestamp":1649709808611,"user":{"displayName":"Piotr Mardziel","userId":"13482056344261475057"},"user_tz":420},"id":"0C2Q4UPlMIud","outputId":"0dcd6ada-04f8-46b3-e825-c466f49bf28b"},"outputs":[],"source":["some_tweets = list(tweets['text'][0:10])\n","\n","V.tokens(some_tweets)"]},{"cell_type":"markdown","metadata":{"id":"pPCR5_m3MIud"},"source":["Lets explore tweets that mention baseball teams. Below we have some utilities that find tweets that contain team names and create team-less versions of them where the team name is replaced with `:team:`. We can use such tweets to investigate the sensitivity of the sentiment model towards particular teams.\n","\n","The processing here might be useful for students taking on NLP projects. No need to discuss during the lab."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":103,"status":"ok","timestamp":1649709808713,"user":{"displayName":"Piotr Mardziel","userId":"13482056344261475057"},"user_tz":420},"id":"jrIowDR2MIud"},"outputs":[],"source":["def to_team(team: str):\n","    \"\"\"Replaces all instances of ':team:' with the given `team` in the given list of tweets.\"\"\"\n","\n","    def f(tweets: Union[Series, np.ndarray]):\n","        if isinstance(tweets, pd.Series):\n","            return tweets.map(subst(\":team:\", team))\n","        if isinstance(tweets, np.ndarray):\n","            return np.vectorize(subst(\":team:\", team))(tweets)\n","        raise ValueError(\"I don't know\")\n","    return f\n","\n","def word_pattern(word):\n","    \"\"\"Create a pattern that matches the given `word` as long as it is not immediately next to an alpha-numeric character.\"\"\"\n","    return \"(?<!\\w)\" + re.escape(word) + \"(?!\\w)\"\n","\n","def subst(thing_from: str, thing_to: str):\n","    pat = re.compile(word_pattern(thing_from), re.IGNORECASE)\n","    def f(context: str):\n","        return pat.sub(thing_to, context)\n","    return f\n","\n","def contains(s, reg):\n","    return reg.search(s) is not None\n","\n","def extract_teams(teams):\n","    \"\"\"Create a method that extracts tweets that contain mentions of any of the terms in the given `teams`.\"\"\"\n","\n","    pattern = \"(?<!\\w)(\" + '|'.join(map(re.escape, teams)) + \")(?!\\w)\"\n","    reg = re.compile(pattern, re.IGNORECASE)\n","\n","    def f(tweets: Series):\n","        indices = np.array(parmap.map(contains, tweets.to_numpy(), reg))\n","\n","        ret = tweets[indices]\n","\n","        for team in teams:\n","            ret = ret.map(subst(team, \":team:\"))\n","\n","        return ret\n","\n","    return f"]},{"cell_type":"markdown","metadata":{"id":"H8xPHcZ2MIue"},"source":["This cell may take a minute to run. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":117536,"status":"ok","timestamp":1649709926346,"user":{"displayName":"Piotr Mardziel","userId":"13482056344261475057"},"user_tz":420},"id":"saT3kaLOMIue","outputId":"b2257953-d5fa-4fb1-c1e6-72d329ac9410"},"outputs":[],"source":["team_tweets = load_or_make(\n","    filename=Path(\"team_tweets.csv\"),\n","    loader=lambda filename: pd.read_csv(filename, encoding='ISO-8859-1')['text'],\n","    maker=lambda: extract_teams([\n","        \"diamondbacks\", \"braves\", \"orioles\", \"redsox\", \"red sox\", \"cubs\", \"whitesox\", \"white sox\", \"reds\", \n","        \"guardians\", \"rockies\", \"tigers\", \"astros\", \"royals\", \"dodgers\", \"marlins\", \"brewers\", \"twins\", \n","        \"mets\", \"yankees\", \"athletics\", \"phillies\", \"pirates\", \"padres\", \"giants\", \"mariners\", \"cardinals\", \n","        \"rays\", \"rangers\", \"jays\", \"nationals\"\n","      ]\n","       # ['redsox', 'red sox', 'yankees'],\n","      )(tweets['text']),\n","    saver = lambda filename, series: series.to_frame(name=\"text\").to_csv(filename)\n",")\n","\n","print(f\"found {len(team_tweets)} team tweets\")\n","\n","for tweet in team_tweets[0:10]:\n","  print(tweet)"]},{"cell_type":"markdown","metadata":{"id":"nnhXfnMnMIue"},"source":["Lets now focus on two particular teams and compare the sentiment model on them. We start by creating versions of the team tweets with a particular team filled in to where the `:team:` marker was. We can visualize pairs of such tweets that differ in team via another trulens utility."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":664},"executionInfo":{"elapsed":692,"status":"ok","timestamp":1649709927035,"user":{"displayName":"Piotr Mardziel","userId":"13482056344261475057"},"user_tz":420},"id":"MCHMwc3QMIue","outputId":"5879f22f-d3fd-46af-b39e-6bed93b20975"},"outputs":[],"source":["teams = ['redsox', 'yankees']\n","tweets_for_team = {team: to_team(team)(team_tweets) for team in teams}\n","\n","V.tokens_stability(\n","    texts1=list(tweets_for_team[teams[0]][0:10]),\n","    texts2=list(tweets_for_team[teams[1]][0:10])\n",")"]},{"cell_type":"markdown","metadata":{"id":"55zcv0mGMIue"},"source":["Lets inspect the distribution of logits accross tweets of the two teams.\n","\n","Skip the details of this cell can during the lab."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":346},"id":"5AOu_T7mMIue","outputId":"7f6950aa-de73-4b40-d218-f5dabcc882bb"},"outputs":[],"source":["# First a method to help us evaluate the model on a large collection of instances.\n","def eval_batched(data: Series, batch_size=128):\n","    \"\"\"Evaluate the model `task.model` on given `data` tokenized by\n","    `task.tokenizer` in a set of batches. Return the logits.\"\"\"\n","\n","    # Might need to cleanup GPU ram:\n","    torch.cuda.empty_cache()\n","\n","    sentences = DataLoader(data.to_numpy(), batch_size=batch_size)\n","\n","    all_logits = []\n","\n","    for batch in sentences:\n","        tokens = task.tokenizer(batch, padding=True, return_tensors='pt').to(task.device)\n","        logits = task.model(**tokens)['logits'].detach().to('cpu')\n","        del tokens\n","        all_logits += logits\n","\n","    returning = np.stack(list(map(torch.Tensor.numpy, all_logits)))\n","\n","    # Might need to cleanup GPU ram:\n","    torch.cuda.empty_cache()\n","\n","    return returning\n","\n","# Then get the logits for each team variant's tweets.\n","logits_for_team = {team: eval_batched(tweets) for team, tweets in tweets_for_team.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nA-P-EfVMIue"},"outputs":[],"source":["amin = min(logits.min() for logits in logits_for_team.values())\n","amax = max(logits.max() for logits in logits_for_team.values())\n","\n","colors = {teams[0]: 'red', teams[1]: 'blue'}\n","\n","# Create a figure showing the histogram of logits for each of the three classes for all of the teams in `teams`.\n","\n","fig, axs = plt.subplots(3,1, figsize=(10,10))\n","for idx, label in zip([task.NEGATIVE,task.NEUTRAL,task.POSITIVE], task.labels):\n","    for team, logits in logits_for_team.items():\n","        axs[idx].hist(logits[:, idx], bins=10, alpha=0.25, label=f\"{team} {label}\", color=colors[team], range=(amin, amax))\n","    axs[idx].legend()"]},{"cell_type":"markdown","metadata":{"id":"gp4TmXJPMIue"},"source":["We should see that there are minor differences in the score distributions for the sentiments across the two teams, \"redsox\" and \"yankees\".\n","\n","Are there any individual tweets that have particularly disparate logits? We can use `argsort` to get relevant indices into the lists of tweets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7UU5o76-MIue"},"outputs":[],"source":["# Get the index of tweets sorted by absolute difference in logits across teams.\n","sort_idx = np.argsort(abs(logits_for_team[teams[0]] - logits_for_team[teams[1]]).sum(axis=1))[::-1]"]},{"cell_type":"markdown","metadata":{"id":"O2PT2sTz6p15"},"source":["Trulens also provides a side-by-side rendering of texts with scores for exactly the situation of inspecting instabilities like the one we found."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lASql_t9MIue"},"outputs":[],"source":["team_tweets_np = team_tweets.to_numpy()\n","\n","V.tokens_stability(\n","    texts1 = list(tweets_for_team[teams[0]].to_numpy()[sort_idx[0:10]]),\n","    texts2 = list(tweets_for_team[teams[1]].to_numpy()[sort_idx[0:10]])\n",")"]},{"cell_type":"markdown","metadata":{"id":"d8ZliY7uMIue"},"source":["# Lab Week 3: NLP Attribution\n","\n","This week we will go over how to use trulens for computing gradient-based attributions for the NLP model covered last week. We will also cover several elements of tuning such attributions that may be relevant to particular use-cases. Specifically we will learn about:\n","\n","- How to determine the embedding layer of a model. This will be needed for applying attributions to NLP as per the lecture on \"Gradients in Explanations\".\n","- Basics of using trulens to retrieve attributions.\n","- Distributions of interest.\n","- Quantities of interest.\n","- How to construct baselines for integrated gradients for NLP."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"30PyW9x_MIuf"},"outputs":[],"source":["from trulens.nn.quantities import ClassQoI\n","from trulens.nn.attribution import IntegratedGradients, InputAttribution\n","from trulens.nn.attribution import Cut, OutputCut\n","from trulens.nn.distributions import GaussianDoi"]},{"cell_type":"markdown","metadata":{"id":"LA9pHlxxMIuf","nbpresent":{"id":"4ed9c783-b745-4c6a-b674-d8b6935dd62d"}},"source":["## Attributions\n","\n","Applying integrated gradents to the sentiment model is similar to non-NLP cases except special considerations need to be made for the cuts used as the targets of the attribution (i.e. what do we want to assign importance to). As you may have noted above, the model takes as input integer indexes associated with tokens. As we cannot take gradient with respect to these, we use an alternative: the embedding representation of those same inputs. To instantiate trulens with this regard, we need to find inspect the layer names inside our model:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["task.wrapper.print_layer_names()"]},{"cell_type":"markdown","metadata":{"id":"4Q7SPVX5MIuf"},"source":["### Parameters\n","\n","Above, `roberta_embeddings_word_embeddings` is the layer that produces a continuous representation of each input token so we will use that layer as the one defining the **distribution of interest**. While most neural NLP models contain a token embedding, the layer name will differ.\n","\n","The second thing to note is the form of model outputs. Specifically, outputs are structures which contain a 'logits' attribute that stores the model scores.\n","\n","Putting these things together, we instantiate `InputAttribution` to attribute each embedding dimension to the maximum of logit outputs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Z_IUQmdMIuf"},"outputs":[],"source":["# By default, the quantity of interest is max over logits. We need to, however, provide the means of accessing logits from the model \n","# outputs via the accessor parameter:\n","\n","infl_max = InputAttribution(\n","    model = task.wrapper,\n","    doi='point',\n","    doi_cut=Cut('roberta_embeddings_word_embeddings'),\n","    # qoi='max', # this is the default\n","    qoi_cut=OutputCut(accessor=lambda o: o['logits'])\n",")"]},{"cell_type":"markdown","metadata":{"id":"jt07WWRpMIuf"},"source":["Getting attributions uses the same call as model evaluation."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sentence_attributions = infl_max.attributions(**inputs)\n","\n","for token_ids, attribution in zip(inputs['input_ids'], sentence_attributions):\n","\n","    for token_id, embedding_attribution in zip(token_ids, attribution):\n","        # Note that each `word_attr` has a magnitude for each of the embedding\n","        # dimensions, of which there are many. We aggregate them for easier\n","        # interpretation and display. You can uncomment the below line to see\n","        # the raw per-dimension attributions.\n","    \n","        #  print(embedding_attribution)\n","\n","        attr = embedding_attribution.sum()\n","\n","        word = task.tokenizer.decode(token_id)\n","\n","        print(f\"{word}({attr:0.3f})\", end=' ')\n","\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"f4VdmCWCMIuf"},"source":["A listing as above is not very readable so Trulens comes with some utilities to present token influences a bit more concisely. The setup is the same as was last week."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uflSpKxmMIuf"},"outputs":[],"source":["V = NLP(\n","    wrapper=task.wrapper,\n","    labels=task.labels,\n","    decode=lambda x: task.tokenizer.decode(x),\n","    tokenize=task.tokenize,\n","    # huggingface models can take as input the keyword args as per produced by their tokenizers.\n","\n","    input_accessor=lambda x: x['input_ids'],\n","    # for huggingface models, input/token ids are under input_ids key in the input dictionary\n","\n","    output_accessor=lambda x: x['logits'],\n","    # and logits under 'logits' key in the output dictionary\n","\n","    hidden_tokens=set([task.tokenizer.pad_token_id])\n","    # do not display these tokens\n",")\n","\n","display(\n","    V.tokens(sentences, attributor=infl_max)\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Attributions are, by default, indicated by color and colored bars above (positive) and below (negative) each token. There seems to be barely any attribution shown. Lets take a look at some of the tweets from the real-world dataset."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["display(\n","    V.tokens(list(tweets_for_team[teams[0]][0:10]), attributor=infl_max)\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Distributions of interest\n","\n","The attribution above does not seem very enlightening. Part of the issue is that the \"point\" distribution of interest is equivalent to a simple saliency, or just the gradient of the quantity of interest with respect to model inputs (or embeddings in this case). Using trulens, we have some better options to chose from:\n","\n","- `PointDoi` - saliency as just described.\n","- `GaussianDoi` - averages out the gradients in the viscinity of the explained point with. The viscinity is sampled from a guassian distribution.\n","- `LinearDoi` - averages the gradients for inputs spanning a baseline interpolating all the way to the explained points. This is equivalent to Integrated Gradients.\n","- custom - you can also specify your own distribution of interest by subclassing any of the above or the `trulens.nn.distributions.DoI` class.\n","\n","Lets try these out."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from trulens.nn.distributions import PointDoi, GaussianDoi, LinearDoi\n","\n","attr_cut = Cut('roberta_embeddings_word_embeddings')\n","\n","# Each doi needs to be told about the layer we are attributing to.\n","common_args = dict(cut=attr_cut)\n","\n","dois = [\n","    PointDoi(**common_args),\n","    GaussianDoi(var=0.01, resolution=10, **common_args), # resolution is the number of samples to take\n","    LinearDoi(resolution=20, **common_args) # same\n","]\n","\n","for doi in dois:\n","    print(doi)\n","\n","    infl = InputAttribution(\n","        model = task.wrapper,\n","        doi=doi,\n","        doi_cut=attr_cut,\n","        qoi=ClassQoI(task.POSITIVE),\n","        qoi_cut=OutputCut(accessor=lambda o: o['logits'])\n","    )\n","    display(\n","        V.tokens(list(tweets_for_team[teams[0]][0:3]), attributor=infl)\n","    )"]},{"cell_type":"markdown","metadata":{},"source":["Ingore the other parameters of the printed DoI we did not discuss, they will not play a role for us in these labs. Note, however, the various differences in the attributions produced. Which of these attributions seems most accurate to what you think should be happening for classifying sentiment?"]},{"cell_type":"markdown","metadata":{},"source":["## Quantities of interest\n","\n","The other main parameter to tune in attributions is the quantity being explained. Above we used the logits of the predicted class but instead we have several options:\n","\n","- `MaxClassQoI` - as just noted,\n","- `ClassQoI` - logits of a specific class,\n","- `ComparativeQoI` - logits of one specific class minus logits of another,\n","- `LambdaQoI` - anything else definable from model outputs."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from trulens.nn.quantities import MaxClassQoI, ClassQoI, ComparativeQoI, LambdaQoI\n","\n","attr_cut = Cut('roberta_embeddings_word_embeddings')\n","\n","# Each doi needs to be told about the layer we are attributing to.\n","common_args = dict(cut=attr_cut)\n","\n","doi = LinearDoi(resolution=20, **common_args)\n","\n","# For a custom qoi, lets create one that gets the highest logit and outputs only\n","# its value. There are subtle differences between this and 'max' which we will\n","# not get into and one thing that trulens presently does not give us access to\n","# in these custom QoIs is the prediction of the instance being explained. While\n","# we use argmax here, it applies to instances produced by the distribution of\n","# interest which may or may not have a prediction matching the instance being\n","# explained.\n","def custom_qoi(logits):\n","    preds = logits.argmax(axis=1, keepdims=True)\n","    predicted_logits = logits.gather(1, preds)\n","    return predicted_logits.sum() # adding up over the entire batch\n","\n","qois = [\n","    MaxClassQoI(),\n","    ClassQoI(task.POSITIVE),\n","    LambdaQoI(custom_qoi),\n","    ClassQoI(task.NEGATIVE),\n","    ComparativeQoI(task.POSITIVE, task.NEGATIVE),\n","]\n","\n","for qoi in qois:\n","    print(qoi)\n","\n","    infl = InputAttribution(\n","        model = task.wrapper,\n","        doi=doi,\n","        doi_cut=attr_cut,\n","        qoi=qoi,\n","        qoi_cut=OutputCut(accessor=lambda o: o['logits'])\n","    )\n","\n","    display(\n","        V.tokens(list(tweets_for_team[teams[0]][0:3]), attributor=infl)\n","    )"]},{"cell_type":"markdown","metadata":{"id":"DHCneJalMIuf"},"source":["## Baselines\n","\n","We see in the above results that special tokens such as the sentence end **&lt;/s&gt;** contributes are found to contribute a lot to the model outputs. While this may be useful in some contexts, we are more interested in the contributions of the actual words in these sentences. To focus on the words more, we need to adjust the **baseline** used in the integrated gradients computation. By default in the instantiation so far, the baseline for each token is a zero vector of the same shape as its embedding. Special tokens, however, do not have zero embeddings. Lets take a look at pad and the sentence tokens."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"padding token=\", task.tokenizer.pad_token_id)\n","print(\"beginning of sentence token=\", task.tokenizer.bos_token_id)\n","print(\"end of sentence token=\", task.tokenizer.eos_token_id)\n","\n","tokens = torch.tensor([task.tokenizer.bos_token_id, task.tokenizer.pad_token_id, task.tokenizer.eos_token_id]).to(task.device)\n","\n","embeddings = task.model.get_input_embeddings()(tokens).detach().cpu().numpy()\n","\n","print(\"magnitudes=\", np.linalg.norm(embeddings, axis=1, ord=2))"]},{"cell_type":"markdown","metadata":{},"source":["We see that these tokens, by their magnitudes, not close to the zero vector. By comparison, here are the magnitudes of the tokens making up the sample sentences we worked with at the beginning of the labs:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["embeddings = task.model.get_input_embeddings()(inputs['input_ids']).detach().cpu().numpy()\n","\n","print(\"magnitudes=\", np.linalg.norm(embeddings, axis=2, ord=2))"]},{"cell_type":"markdown","metadata":{},"source":["By making the basaeline be identicaly to the explained instances on special tokens, we can rid their impact from our measurement. Trulens provides a utility for this purpose in terms of `token_baseline` which constructs for you the methods to compute the appropriate baseline. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BddGpF2bMIuf"},"outputs":[],"source":["from trulens.utils.nlp import token_baseline\n","\n","inputs_baseline_ids, inputs_baseline_embeddings = token_baseline(\n","    keep_tokens=set([task.tokenizer.cls_token_id, task.tokenizer.bos_token_id, task.tokenizer.eos_token_id]),\n","    # Which tokens to preserve.\n","\n","    replacement_token=task.tokenizer.pad_token_id,\n","    # What to replace tokens with.\n","\n","    input_accessor=lambda x: x.kwargs['input_ids'],\n","\n","    ids_to_embeddings=task.model.get_input_embeddings()\n","    # Callable to produce embeddings from token ids.\n",")"]},{"cell_type":"markdown","metadata":{"id":"nhWT-AkCMIuf"},"source":["We can now inspect the baselines on some example sentences. The first method returned by `token_baseline` gives us token ids to inspect while the second gives us the embeddings of the baseline which we will pass to the attributions method."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from trulens.utils.typing import ModelInputs\n","\n","print(\"originals=\", task.tokenizer.batch_decode(inputs['input_ids']))\n","\n","baseline_word_ids = inputs_baseline_ids(model_inputs=ModelInputs(kwargs=inputs))\n","print(\"baselines=\", task.tokenizer.batch_decode(baseline_word_ids))\n","\n","baseline_embeddings = inputs_baseline_embeddings(model_inputs=ModelInputs(kwargs=inputs))\n","# These are not useful to print.\n","# print(\"embeddings=\", baseline_embeddings)"]},{"cell_type":"markdown","metadata":{},"source":["Note that special tokens are not replaced in the baseline while actual content words are replaced with \"pad\". Lets try some attributions again on these baselines."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# del infl_positive_baseline\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LQuMoECVMIuf"},"outputs":[],"source":["infl_positive_baseline = IntegratedGradients(\n","    model = task.wrapper,\n","    resolution=20,\n","    baseline = inputs_baseline_embeddings,\n","    doi_cut=Cut('roberta_embeddings_word_embeddings'),\n","    qoi=ClassQoI(task.POSITIVE),\n","    qoi_cut=OutputCut(accessor=lambda o: o['logits']),\n","    return_doi=True\n",")\n","\n","print(\"QOI = POSITIVE WITH BASELINE\")\n","V.tokens(list(tweets_for_team[teams[0]][0:10]), attributor=infl_positive_baseline)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["res = infl_positive_baseline._attributions(**inputs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from trulens.utils.nlp import token_baseline_swap\n","\n","inputs_swap_baseline_ids, inputs_swap_baseline_embeddings = token_baseline_swap(\n","    # token1 = 205, # \" good\"\n","    token1 = 8396, # \"good\"\n","    # token2 = 1099, # \" bad\"\n","    token2 = 10999, # \"bad\"\n","    # Which tokens to preserve.\n","\n","    input_accessor=lambda x: x.kwargs['input_ids'],\n","\n","    ids_to_embeddings=task.model.get_input_embeddings()\n","    # Callable to produce embeddings from token ids.\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["task.tokenizer.vocab['good']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from trulens.utils.typing import ModelInputs\n","\n","sentences = [\"good\", \"bad\"]\n","\n","# Input sentences need to be tokenized first.\n","\n","inputs = task.tokenize(sentences)\n","\n","# The tokenizer gives us vocabulary indexes for each input token (in this case,\n","# words and some word parts like the \"'m\" part of \"I'm\" are tokens).\n","\n","print(inputs)\n","\n","# Decode helps inspecting the tokenization produced:\n","\n","print(task.tokenizer.batch_decode(torch.flatten(inputs['input_ids'])))\n","# Normally decode would give us a single string for each sentence but we would\n","# not be able to see some of the non-word tokens there. Flattening first gives\n","# us a string for each input_id.\n","\n","print(\"originals=\", task.tokenizer.batch_decode(inputs['input_ids']))\n","\n","baseline_word_ids = inputs_swap_baseline_ids(model_inputs=ModelInputs(kwargs=inputs))\n","print(\"baselines=\", task.tokenizer.batch_decode(baseline_word_ids))\n","\n","baseline_embeddings = inputs_swap_baseline_embeddings(model_inputs=ModelInputs(kwargs=inputs))\n","# These are not useful to print.\n","# print(\"embeddings=\", baseline_embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(res.attributions.shape)\n","print(res.gradients.shape)\n","print(res.interventions.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["inputs.input_ids.shape"]},{"cell_type":"markdown","metadata":{"id":"7nmjmrmbMIuf"},"source":["As we see, the baseline eliminated the measurement of contribution of the special tokens."]},{"cell_type":"markdown","metadata":{},"source":["# Lab Week 4: Embeddings, Baselines, Paths\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import AutoModelForSequenceClassification\n","from transformers import AutoTokenizer\n","from fastTSNE import TSNE\n","from fastTSNE.callbacks import ErrorLogger\n","import matplotlib.pyplot as plt\n","from IPython import display\n","from sklearn.decomposition import PCA\n","import numpy as np\n","\n","# ! pip install jupyter ipywidgets ipympl mpld3\n","# ! pip install tsnecuda\n","# ! pip install tsnecuda==3.0.1+cu113 -f https://tsnecuda.isx.ai/tsnecuda_stable.html\n","# from sklearn.manifold import TSNE\n","# from tsnecuda import TSNE\n","\n","# %matplotlib ipympl\n","# import mpld3\n","# mpld3.enable_notebook()\n","\n","# Wrap all of the necessary components.\n","class TwitterSentiment:\n","    MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n","\n","    # device = 'cpu'\n","    # Can also use cuda if available:\n","    device = 'cuda:0'\n","\n","    model = AutoModelForSequenceClassification.from_pretrained(MODEL).to(device)\n","\n","    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","    @staticmethod\n","    def tokenize(inputs):\n","        return TwitterSentiment \\\n","            .tokenizer(inputs, padding=True, return_tensors=\"pt\") \\\n","            .to(TwitterSentiment.device)\n","        # pt refers to pytorch tensor\n","\n","    labels = ['negative', 'neutral', 'positive']\n","\n","    NEGATIVE = labels.index('negative')\n","    NEUTRAL = labels.index('neutral')\n","    POSITIVE = labels.index('positive')\n","\n","task = TwitterSentiment()\n","embedder = task.model.roberta.embeddings.word_embeddings\n","embeddings = embedder.weight.detach().cpu().numpy()\n","\n","from trulens.nn.models import get_model_wrapper\n","\n","task.wrapper = get_model_wrapper(task.model, input_shape=(None, task.tokenizer.model_max_length), device=task.device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def closest_token(embeddings, emb):\n","    diffs = embeddings - emb\n","    # print(diffs.shape)\n","    distances = np.linalg.norm(diffs, ord=2, axis=1)\n","    # print(distances.shape)\n","    closest = np.argsort(distances)\n","    # print(closest.shape)\n","    return closest[0], distances[closest[0]]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["idx, dist = closest_token(embeddings, np.zeros(768))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(task.tokenizer.decode(idx), dist)"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from trulens.nn.distributions import PointDoi, GaussianDoi, LinearDoi\n","from trulens.utils.typing import ModelInputs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sample_inputs = task.tokenizer(\"I'm so happy!\", return_tensors='pt').to(\"cuda\")\n","sample_ids = sample_inputs['input_ids'].cuda()[0]\n","sample_embs = embedder(sample_ids.cuda())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(sample_ids.shape)\n","print(sample_embs.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from trulens.utils.nlp import token_baseline\n","\n","inputs_baseline_ids, inputs_baseline_embeddings = token_baseline(\n","    keep_tokens=set([task.tokenizer.cls_token_id, task.tokenizer.bos_token_id, task.tokenizer.eos_token_id]),\n","    # Which tokens to preserve.\n","\n","    replacement_token=task.tokenizer.pad_token_id,\n","    # What to replace tokens with.\n","\n","    input_accessor=lambda x: x.kwargs['input_ids'],\n","    # input_accessor = lambda x: x, \n","\n","    ids_to_embeddings=task.model.get_input_embeddings()\n","    # Callable to produce embeddings from token ids.\n",")\n","\n","# path = LinearDoi(resolution=11, baseline=inputs_baseline_embeddings)(sample_embs, model_inputs=ModelInputs(args=[], kwargs=sample_inputs))\n","path = GaussianDoi(resolution=11, var=0.25)(sample_embs)#, model_inputs=ModelInputs(args=[], kwargs=sample_inputs))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["infl = IntegratedGradients(\n","    model = task.wrapper,\n","    resolution=40,\n","    baseline = inputs_swap_baseline_embeddings,\n","    doi_cut=Cut('roberta_embeddings_word_embeddings'),\n","    qoi=ClassQoI(task.POSITIVE),\n","    qoi_cut=OutputCut(accessor=lambda o: o['logits']),\n","    return_doi=True\n",")\n","\n","def tokens_over_doi(attr, texts, doi):\n","    sample_inputs = task.tokenizer(texts, return_tensors='pt').to(\"cuda\")\n","\n","    sample_ids = sample_inputs['input_ids'].cuda()\n","    sample_embs = embedder(sample_ids.cuda()).cpu().detach().numpy()\n","\n","    pieces = attr._attributions(**sample_inputs)\n","\n","    attrs = pieces.attributions\n","    grads = pieces.gradients\n","    intervs = pieces.interventions\n","\n","    for sentence, sentence_ids, attr, grad, interv, base_embs in zip(texts, sample_ids, attrs, grads, intervs, sample_embs):\n","\n","        print(sentence)\n","        print(interv.shape)\n","        print(grad.shape)\n","        print(base_embs.shape)\n","\n","        path = interv\n","        # path = doi(sentence_embs)\n","\n","        for i in range(len(path)):\n","            \n","            grad_aggr = (grad[i][0][0] * base_embs).sum(axis=1)\n","\n","            print(f\"  {grad_aggr.sum():0.6f} \", end='')\n","\n","            for word_idx in range(len(sentence_ids)):\n","                word_emb = base_embs[word_idx]\n","\n","                word_id = sentence_ids[word_idx]\n","                word_token = task.tokenizer.decode(word_id)\n","\n","                grad_word = grad_aggr[word_idx]\n","\n","                interv_emb = path[i][0][word_idx]\n","                # interv_emb = path[i][0][word_idx]\n","                close_id, close_dist = closest_token(embeddings, interv_emb)\n","                # print(close_id)\n","                close_emb = embeddings[close_id]\n","                close_token = task.tokenizer.decode(close_id)\n","\n","                print(f\"{close_token}({grad_word:0.6f})\", end=' ')\n","            \n","            print()\n","\n","            # print(word_emb[0:2], word_id, word_token, close_emb[0:2], close_id, close_token, close_dist, interv_emb[0:2])\n","\n","tokens_over_doi(attr=infl, texts=[\"good\"], doi=GaussianDoi(resolution=11, var=0.25))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pca = PCA(n_components=10)\n","pca.fit(embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["emb_reduced = pca.transform(embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%matplotlib ipympl\n","plt.ion()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, ax = plt.subplots(1,1, figsize=(8,8))\n","\n","errors = []\n","\n","def showme(iteration, error, embedding):\n","    ax.clear()\n","    # ax.scatter(embedding[:,0], embedding[:,1], s=0.5, m=',')\n","    ax.scatter(embedding[:,0], embedding[:,1], s=1.0, marker='.', alpha=0.5)\n","\n","    errors.append(error)\n","\n","    # ax[1].clear()\n","    # ax[1].plot(errors)\n","\n","    fig.canvas.draw()\n","    fig.canvas.flush_events()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tsne = TSNE(n_components=2, n_jobs=-1, n_iter=100, callbacks=showme, callbacks_every_iters=10, neighbors='exact', negative_gradient_method='fft')\n","\n","emb = tsne.prepare_initial(emb_reduced[0:2000])\n","\n","tsne.callbacks = [showme]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["emb = emb.optimize(n_iter=1000)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["toks = [task.tokenizer.decode(i, clean_up_tokenization_spaces=True).replace(\" \", \"_\") for i in range(len(emb))]\n","\n","annots = []\n","\n","for i, e in enumerate(emb):\n","    tok = toks[i]\n","    if len(annots) <= i:\n","        annots.append(ax.annotate(tok, xy=e, alpha=0.5))#, fontsize=1)\n","    else:\n","        annots[i].xy = e\n","    # print(i, e, tok)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["emb.transform()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig.canvas.draw()\n","fig.canvas.flush_events()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","def map_to_2d(embs):\n","    embs_pca = pca.transform(embs)\n","    embs_2d = emb.transform(embs_pca)\n","    return embs_2d\n","\n","# map_to_2d(np.zeros((1, 768)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["emin = embeddings.min()\n","emax = embeddings.max()\n","\n","axxs = np.zeros((101,768))\n","\n","axxs[:,0] = np.linspace(emin, emax, 101)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["axis_in_2d = map_to_2d(axxs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ax.plot(axis_in_2d[:,0], axis_in_2d[:,1])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["! export LD_LIBRARY_PATH=/usr/local/cuda-11/lib64\n","import os, sys\n","\n","print(os.environ['LD_LIBRARY_PATH'])\n","\n","import tsnecuda\n","tsnecuda.test()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import umap\n","u = umap.UMAP()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["u.fit(emb_reduced)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["u.embedding_"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.scatter(u.embedding_[:,0], u.embedding_[:,1], s=1, marker='.')"]}],"metadata":{"accelerator":"GPU","anaconda-cloud":{},"colab":{"collapsed_sections":["d8ZliY7uMIue","LA9pHlxxMIuf","4Q7SPVX5MIuf","DHCneJalMIuf"],"name":"cs329t_labs.ipynb","provenance":[]},"interpreter":{"hash":"51eb71198507ab2c2a4108a27eda9d9658549732e67153fc0e371d8439827db7"},"kernelspec":{"display_name":"test-fresh-11-29","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"08d1c47f12ca473f8f8ac80cb5184053":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_81b8cd797a014713bed2f8fa90c1924e","max":747,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1d97903892694c859f0d4647b557b5e0","value":747}},"102f8566386447b78e75ddd051184a99":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"112625d3cda147d5b44a9950b3af27b9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"140a5c69009f41d6a05fc90f6e0df496":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"185181475b144327947cfb18a67c0b6b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"19f441360c2a414a8ab2daad42132945":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a9cde3460b94ce5bd9f522bb1b0f27b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d97903892694c859f0d4647b557b5e0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2e74d7c685d64447bb3f692f97c19f43":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3523eb44ae704b698a18644da9e1000a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"39566cce425d46e682b54bac1d00a913":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_19f441360c2a414a8ab2daad42132945","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c4e6103c77bb4497b953e67d6ef1e755","value":456318}},"4480c9d03aa64d60aef96982f5277dc2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c6c42a90b5f45dfa03e9c911bedf0da":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"574f6d86798a4df59e00c593500f2129":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5affc7358d18440d8a23235309a5f4df":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b802c61b41a4178bb86896d76579825":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6509af96a82846ffa1a62a5afaedf5f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c19ba202dd6e49fdada51defb7684816","IPY_MODEL_d0f3f7febd4847b2a3af723d4b2f584f","IPY_MODEL_ca159130487045bbb7c03b42f68ac277"],"layout":"IPY_MODEL_7140c885e55d44fb93cb3ea5299ae484"}},"683991be14cb4de1b43ce2a5022bc4df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_112625d3cda147d5b44a9950b3af27b9","placeholder":"â","style":"IPY_MODEL_2e74d7c685d64447bb3f692f97c19f43","value":" 446k/446k [00:00&lt;00:00, 1.10MB/s]"}},"6c12589fcab04299b0176d74e73fb63b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7140c885e55d44fb93cb3ea5299ae484":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73b9ed3ace19405a92d981dd4030a17f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7882b7eb6ddd4e09ad29753809f814c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9d533d1740a24cc2bccfd4f99ba3bf23","IPY_MODEL_08d1c47f12ca473f8f8ac80cb5184053","IPY_MODEL_9f6aeb8e05564c6b9271852773eec58b"],"layout":"IPY_MODEL_a833c877cd314ca8831425a1a57376b0"}},"7d940c75c973404e9242bb6a72049bf2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca5d7e26941a4e6c895d0cb323503855","placeholder":"â","style":"IPY_MODEL_bbec5798cb804834b72671ee2646c9de","value":" 150/150 [00:00&lt;00:00, 3.89kB/s]"}},"81b8cd797a014713bed2f8fa90c1924e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"836b39843db94ad2b35d03c013867e9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a23ff8e809934cbb928f9411c8addb71","placeholder":"â","style":"IPY_MODEL_f1ba1737d7324201b16ebd896f2fb549","value":"Downloading: 100%"}},"8830da740ecd4e8e901f18968abb8d41":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"89a17cba612c4faba116e3585d92304c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_140a5c69009f41d6a05fc90f6e0df496","max":150,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3523eb44ae704b698a18644da9e1000a","value":150}},"8ea4e74ba70747cda40d2f2c722fec61":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c12589fcab04299b0176d74e73fb63b","max":498679497,"min":0,"orientation":"horizontal","style":"IPY_MODEL_73b9ed3ace19405a92d981dd4030a17f","value":498679497}},"92b4cc26d4d14a73ae0924a006cde30c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b8fb420ebb94ce69591bdfda0255b03":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c6c42a90b5f45dfa03e9c911bedf0da","placeholder":"â","style":"IPY_MODEL_102f8566386447b78e75ddd051184a99","value":"Downloading: 100%"}},"9d533d1740a24cc2bccfd4f99ba3bf23":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad299b902b254b5dacc9f77793195d71","placeholder":"â","style":"IPY_MODEL_5b802c61b41a4178bb86896d76579825","value":"Downloading: 100%"}},"9f6aeb8e05564c6b9271852773eec58b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc5ae775dd26482e86ad66aaf989cfbd","placeholder":"â","style":"IPY_MODEL_92b4cc26d4d14a73ae0924a006cde30c","value":" 747/747 [00:00&lt;00:00, 4.83kB/s]"}},"a23ff8e809934cbb928f9411c8addb71":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a833c877cd314ca8831425a1a57376b0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad299b902b254b5dacc9f77793195d71":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aef547cd8845496f861b13541b1012da":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3787a2725cb475d875282f790159285":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b687bf9aaeab43b8ae83776bbe1708a4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bbec5798cb804834b72671ee2646c9de":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bbeca803194f4529a1912fe69c267f33":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c19ba202dd6e49fdada51defb7684816":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b3787a2725cb475d875282f790159285","placeholder":"â","style":"IPY_MODEL_185181475b144327947cfb18a67c0b6b","value":"Downloading: 100%"}},"c218226bb2004192882cf1a402f6552e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4e6103c77bb4497b953e67d6ef1e755":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ca159130487045bbb7c03b42f68ac277":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0bbc25cdc814bdbb1773706b17c7b8d","placeholder":"â","style":"IPY_MODEL_ffb96905532b4c8fac016f8e149f586f","value":" 878k/878k [00:00&lt;00:00, 1.16MB/s]"}},"ca5d7e26941a4e6c895d0cb323503855":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"caecf052d5194ce88693c390d3306bb5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fb2e98fbfe364616aa27f45e7dbc608b","IPY_MODEL_89a17cba612c4faba116e3585d92304c","IPY_MODEL_7d940c75c973404e9242bb6a72049bf2"],"layout":"IPY_MODEL_b687bf9aaeab43b8ae83776bbe1708a4"}},"d0f3f7febd4847b2a3af723d4b2f584f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4480c9d03aa64d60aef96982f5277dc2","max":898822,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bbeca803194f4529a1912fe69c267f33","value":898822}},"d8aef6e0f7464e1db62e6d7c980e194c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_836b39843db94ad2b35d03c013867e9b","IPY_MODEL_39566cce425d46e682b54bac1d00a913","IPY_MODEL_683991be14cb4de1b43ce2a5022bc4df"],"layout":"IPY_MODEL_c218226bb2004192882cf1a402f6552e"}},"e0bbc25cdc814bdbb1773706b17c7b8d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ead5041740684a5593ef05823dded92a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9b8fb420ebb94ce69591bdfda0255b03","IPY_MODEL_8ea4e74ba70747cda40d2f2c722fec61","IPY_MODEL_ec24d0959ceb4bc4ba20d0f3682aa7c5"],"layout":"IPY_MODEL_aef547cd8845496f861b13541b1012da"}},"ec24d0959ceb4bc4ba20d0f3682aa7c5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_574f6d86798a4df59e00c593500f2129","placeholder":"â","style":"IPY_MODEL_8830da740ecd4e8e901f18968abb8d41","value":" 476M/476M [00:31&lt;00:00, 34.1MB/s]"}},"f1ba1737d7324201b16ebd896f2fb549":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb2e98fbfe364616aa27f45e7dbc608b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5affc7358d18440d8a23235309a5f4df","placeholder":"â","style":"IPY_MODEL_1a9cde3460b94ce5bd9f522bb1b0f27b","value":"Downloading: 100%"}},"fc5ae775dd26482e86ad66aaf989cfbd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffb96905532b4c8fac016f8e149f586f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
