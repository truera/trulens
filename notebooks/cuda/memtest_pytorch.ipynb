{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the same model as `nlp_demo_pytorch` but tests trulens' memory suggestions and rebatching capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/piotrm/anaconda3/envs/python37_pytorch_cuda/lib/python3.7/site-packages (4.17.0)\n",
      "Requirement already satisfied: requests in /home/piotrm/anaconda3/envs/python37_pytorch_cuda/lib/python3.7/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /home/piotrm/anaconda3/envs/python37_pytorch_cuda/lib/python3.7/site-packages (from transformers) (0.11.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/piotrm/anaconda3/envs/python37_pytorch_cuda/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/piotrm/anaconda3/envs/python37_pytorch_cuda/lib/python3.7/site-packages (from transformers) (4.63.0)\n",
      "Requirement already satisfied: sacremoses in /home/piotrm/anaconda3/envs/python37_pytorch_cuda/lib/python3.7/site-packages (from transformers) (0.0.49)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/piotrm/anaconda3/envs/python37_pytorch_cuda/lib/python3.7/site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/piotrm/anaconda3/envs/python37_pytorch_cuda/lib/python3.7/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/piotrm/anaconda3/envs/python37_pytorch_cuda/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in /home/piotrm/anaconda3/envs/python37_pytorch_cuda/lib/python3.7/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/piotrm/anaconda3/envs/python37_pytorch_cuda/lib/python3.7/site-packages (from transformers) (4.11.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/piotrm/anaconda3/envs/python37_pytorch_cuda/lib/python3.7/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/piotrm/anaconda3/envs/python37_pytorch_cuda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/piotrm/anaconda3/envs/python37_pytorch_cuda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/piotrm/anaconda3/envs/python37_pytorch_cuda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.7.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/piotrm/anaconda3/envs/python37_pytorch_cuda/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/piotrm/anaconda3/envs/python37_pytorch_cuda/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/piotrm/anaconda3/envs/python37_pytorch_cuda/lib/python3.7/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/piotrm/anaconda3/envs/python37_pytorch_cuda/lib/python3.7/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: click in /home/piotrm/anaconda3/envs/python37_pytorch_cuda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in /home/piotrm/anaconda3/envs/python37_pytorch_cuda/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: six in /home/piotrm/anaconda3/envs/python37_pytorch_cuda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotrm/anaconda3/envs/python37_pytorch_cuda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Use this if running this notebook from within its place in the truera repository.\n",
    "sys.path.insert(0, \"../..\")\n",
    "\n",
    "# Or otherwise install trulens.\n",
    "# !{sys.executable} -m pip install trulens\n",
    "\n",
    "# Install transformers / huggingface.\n",
    "!{sys.executable} -m pip install transformers\n",
    "\n",
    "from IPython.display import display\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Wrap all of the necessary components.\n",
    "class TwitterSentiment:\n",
    "    MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "\n",
    "    # device = 'cpu'\n",
    "    # Can also use cuda if available:\n",
    "    device = 'cuda:1'\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    labels = ['negative', 'neutral', 'positive']\n",
    "\n",
    "    NEGATIVE = labels.index('negative')\n",
    "    NEUTRAL = labels.index('neutral')\n",
    "    POSITIVE = labels.index('positive')\n",
    "\n",
    "task = TwitterSentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 1                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  488178 KB |  488178 KB |  488178 KB |       0 B  |\n",
      "|       from large pool |  487680 KB |  487680 KB |  487680 KB |       0 B  |\n",
      "|       from small pool |     498 KB |     498 KB |     498 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  488178 KB |  488178 KB |  488178 KB |       0 B  |\n",
      "|       from large pool |  487680 KB |  487680 KB |  487680 KB |       0 B  |\n",
      "|       from small pool |     498 KB |     498 KB |     498 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  542720 KB |  542720 KB |  542720 KB |       0 B  |\n",
      "|       from large pool |  540672 KB |  540672 KB |  540672 KB |       0 B  |\n",
      "|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   54541 KB |   54554 KB |  265207 KB |  210665 KB |\n",
      "|       from large pool |   52992 KB |   52992 KB |  263162 KB |  210170 KB |\n",
      "|       from small pool |    1549 KB |    2045 KB |    2045 KB |     495 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     203    |     203    |     203    |       0    |\n",
      "|       from large pool |      75    |      75    |      75    |       0    |\n",
      "|       from small pool |     128    |     128    |     128    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     203    |     203    |     203    |       0    |\n",
      "|       from large pool |      75    |      75    |      75    |       0    |\n",
      "|       from small pool |     128    |     128    |     128    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      21    |      21    |      21    |       0    |\n",
      "|       from large pool |      20    |      20    |      20    |       0    |\n",
      "|       from small pool |       1    |       1    |       1    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      19    |      19    |      20    |       1    |\n",
      "|       from large pool |      18    |      18    |      19    |       1    |\n",
      "|       from small pool |       1    |       1    |       1    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=task.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   0,  100,  437,   98, 1372,  328,    2,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   0,  100,  437,   98, 5074,  328,    2,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   0,  100, 1395, 1137,  549,   38,  197,   28, 1372,   50, 5074,  328,\n",
      "            2],\n",
      "        [   0, 1794,  298,    2,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1]], device='cuda:1'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')}\n",
      "['<s>', 'I', \"'m\", ' so', ' happy', '!', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<s>', 'I', \"'m\", ' so', ' sad', '!', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<s>', 'I', ' cannot', ' tell', ' whether', ' I', ' should', ' be', ' happy', ' or', ' sad', '!', '</s>', '<s>', 'me', 'h', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"I'm so happy!\", \"I'm so sad!\", \"I cannot tell whether I should be happy or sad!\", \"meh\"]\n",
    "\n",
    "# Input sentences need to be tokenized first.\n",
    "\n",
    "inputs = task.tokenizer(sentences, padding=True, return_tensors=\"pt\").to(task.device) # pt refers to pytorch tensor\n",
    "\n",
    "# The tokenizer gives us vocabulary indexes for each input token (in this case,\n",
    "# words and some word parts like the \"'m\" part of \"I'm\" are tokens).\n",
    "\n",
    "print(inputs)\n",
    "\n",
    "# Decode helps inspecting the tokenization produced:\n",
    "\n",
    "print(task.tokenizer.batch_decode(torch.flatten(inputs['input_ids'])))\n",
    "# Normally decode would give us a single string for each sentence but we would\n",
    "# not be able to see some of the non-word tokens there. Flattening first gives\n",
    "# us a string for each input_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: lib level=1\n",
      "INFO: root level=30\n",
      "INFO: Detected pytorch backend for <class 'transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification'>.\n",
      "INFO: Changing backend from None to Backend.PYTORCH.\n",
      "INFO: If this seems incorrect, you can force the correct backend by passing the `backend` parameter directly into your get_model_wrapper call.\n",
      "DEBUG: Input dtype was not passed in. Defaulting to `torch.float32`.\n"
     ]
    }
   ],
   "source": [
    "from trulens.nn.models import get_model_wrapper\n",
    "from trulens.nn.quantities import ClassQoI\n",
    "from trulens.nn.attribution import IntegratedGradients\n",
    "from trulens.nn.attribution import Cut, OutputCut\n",
    "from trulens.utils.typing import ModelInputs\n",
    "\n",
    "task.wrapper = get_model_wrapper(task.model, input_shape=(None, task.tokenizer.model_max_length), device=task.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.visualizations import NLP\n",
    "\n",
    "V = NLP(\n",
    "    wrapper=task.wrapper,\n",
    "    labels=task.labels,\n",
    "    decode=lambda x: task.tokenizer.decode(x),\n",
    "    tokenize=lambda sentences: ModelInputs(kwargs=task.tokenizer(sentences, padding=True, return_tensors='pt')).map(lambda t: t.to(task.device)),\n",
    "    # huggingface models can take as input the keyword args as per produced by their tokenizers.\n",
    "\n",
    "    input_accessor=lambda x: x.kwargs['input_ids'],\n",
    "    # for huggingface models, input/token ids are under input_ids key in the input dictionary\n",
    "\n",
    "    output_accessor=lambda x: x['logits'],\n",
    "    # and logits under 'logits' key in the output dictionary\n",
    "\n",
    "    hidden_tokens=set([task.tokenizer.pad_token_id])\n",
    "    # do not display these tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.utils.nlp import token_baseline\n",
    "\n",
    "inputs_baseline_ids, inputs_baseline_embeddings = token_baseline(\n",
    "    keep_tokens=set([task.tokenizer.cls_token_id, task.tokenizer.sep_token_id]),\n",
    "    # Which tokens to preserve.\n",
    "\n",
    "    replacement_token=task.tokenizer.pad_token_id,\n",
    "    # What to replace tokens with.\n",
    "\n",
    "    input_accessor=lambda x: x.kwargs['input_ids'],\n",
    "\n",
    "    ids_to_embeddings=task.model.get_input_embeddings()\n",
    "    # Callable to produce embeddings from token ids.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QOI = POSITIVE WITH BASELINE\n"
     ]
    },
    {
     "ename": "OutOfMemory",
     "evalue": "Ran out of memory ({'device': 'cuda:1'}). Consider reducing memory-impactful parameters.\n\tfloat size = 4 (torch.float32); consider changing to a smaller type.\n\tbatch size = 4; consider reducing the size of the batch you send to the attributions method.\n\tdistribution of interest size = 1000; consider reducing intervention resolution.\n\tcombined batch size = 4000; consider reducing batch size, intervention size\n\trebatch_size = 900; consider reducing this AttributionMethod constructor parameter (default is same as combined batch size).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/repos/trulens/trulens/nn/backend/__init__.py\u001b[0m in \u001b[0;36mmemory_suggestions\u001b[0;34m(call_before, call_after, *settings, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/trulens/trulens/nn/models/pytorch.py\u001b[0m in \u001b[0;36m_qoi_bprop\u001b[0;34m(self, qoi, model_inputs, doi_cut, to_cut, attribution_cut, intervention)\u001b[0m\n\u001b[1;32m    424\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mmemory_suggestions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m                     \u001b[0mgrads_for_qoi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqoi_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/trulens/trulens/nn/backend/pytorch_backend/pytorch.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(scalar, wrt)\u001b[0m\n\u001b[1;32m     98\u001b[0m     grads = torch.autograd.grad(\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mscalar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_unused\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     )\n",
      "\u001b[0;32m~/anaconda3/envs/python37_pytorch_cuda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         inputs, allow_unused, accumulate_grad=False)\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 1; 23.70 GiB total capacity; 17.94 GiB already allocated; 43.00 MiB free; 18.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutOfMemory\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/repos/trulens/trulens/nn/backend/__init__.py\u001b[0m in \u001b[0;36mmemory_suggestions\u001b[0;34m(call_before, call_after, *settings, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/trulens/trulens/nn/attribution.py\u001b[0m in \u001b[0;36mattributions\u001b[0;34m(self, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    344\u001b[0m                     \u001b[0mintervention\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mintervention_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                     \u001b[0mdoi_cut\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoi_cut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m                 )\n",
      "\u001b[0;32m~/repos/trulens/trulens/nn/models/pytorch.py\u001b[0m in \u001b[0;36m_qoi_bprop\u001b[0;34m(self, qoi, model_inputs, doi_cut, to_cut, attribution_cut, intervention)\u001b[0m\n\u001b[1;32m    433\u001b[0m                     )\n\u001b[0;32m--> 434\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/trulens/trulens/nn/models/pytorch.py\u001b[0m in \u001b[0;36m_qoi_bprop\u001b[0;34m(self, qoi, model_inputs, doi_cut, to_cut, attribution_cut, intervention)\u001b[0m\n\u001b[1;32m    424\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mmemory_suggestions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m                     \u001b[0mgrads_for_qoi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqoi_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python37_pytorch_cuda/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/trulens/trulens/nn/backend/__init__.py\u001b[0m in \u001b[0;36mmemory_suggestions\u001b[0;34m(call_before, call_after, *settings, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"out of memory\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# cuda out of memory exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mOutOfMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemory\u001b[0m: Ran out of memory ({'device': 'cuda:1'}). Consider reducing memory-impactful parameters.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutOfMemory\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2409237/612150027.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"QOI = POSITIVE WITH BASELINE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_attribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfl_positive_baseline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/trulens/trulens/visualizations.py\u001b[0m in \u001b[0;36mtoken_attribution\u001b[0;34m(self, texts, attr)\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_on\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1258\u001b[0;31m         \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_on\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattributions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/trulens/trulens/utils/typing.py\u001b[0m in \u001b[0;36mcall_on\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;34m\"\"\"Call the given method with the contained arguments.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/trulens/trulens/nn/attribution.py\u001b[0m in \u001b[0;36mattributions\u001b[0;34m(self, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0;31m# important to cast to numpy inside loop:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m                 \u001b[0mqoi_grads_expanded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqoi_grads_expanded_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0mnum_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqoi_grads_expanded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python37_pytorch_cuda/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/trulens/trulens/nn/backend/__init__.py\u001b[0m in \u001b[0;36mmemory_suggestions\u001b[0;34m(call_before, call_after, *settings, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOutOfMemory\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOutOfMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemory\u001b[0m: Ran out of memory ({'device': 'cuda:1'}). Consider reducing memory-impactful parameters.\n\tfloat size = 4 (torch.float32); consider changing to a smaller type.\n\tbatch size = 4; consider reducing the size of the batch you send to the attributions method.\n\tdistribution of interest size = 1000; consider reducing intervention resolution.\n\tcombined batch size = 4000; consider reducing batch size, intervention size\n\trebatch_size = 900; consider reducing this AttributionMethod constructor parameter (default is same as combined batch size).\n"
     ]
    }
   ],
   "source": [
    "# This cell is likely to fail given the huge effective batch_size.\n",
    "\n",
    "task.wrapper._model.eval()\n",
    "\n",
    "infl_positive_baseline = IntegratedGradients(\n",
    "    model = task.wrapper,\n",
    "    resolution=1000,\n",
    "    baseline = inputs_baseline_embeddings,\n",
    "    doi_cut=Cut('roberta_embeddings_word_embeddings'),\n",
    "    qoi=ClassQoI(task.POSITIVE),\n",
    "    qoi_cut=OutputCut(accessor=lambda o: o['logits']),\n",
    "    rebatch_size=900\n",
    ")\n",
    "\n",
    "print(\"QOI = POSITIVE WITH BASELINE\")\n",
    "display(V.token_attribution(sentences, infl_positive_baseline))\n",
    "\n",
    "print(torch.cuda.memory_summary(device=task.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QOI = POSITIVE WITH BASELINE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style='padding: 2px; margin: 2px; background: gray; border-radius: 4px;'>positive:&nbsp;<span title='0.000' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 255.0, 255.0);'>&lt;s&gt;</span><span title='-0.316' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 214.73826599121094, 214.73826599121094);'>I</span><span title='0.711' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(164.3207550048828, 255.0, 164.3207550048828);'>&#x27;m</span>&nbsp;<span title='1.272' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(92.87504577636719, 255.0, 92.87504577636719);'>so</span>&nbsp;<span title='2.218' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(-27.763256072998047, 255.0, -27.763256072998047);'>happy</span><span title='0.417' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(201.85568237304688, 255.0, 201.85568237304688);'>!</span><span title='0.000' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 255.0, 255.0);'>&lt;/s&gt;</span></span><br/><br/><span style='padding: 2px; margin: 2px; background: gray; border-radius: 4px;'>negative:&nbsp;<span title='0.000' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 255.0, 255.0);'>&lt;s&gt;</span><span title='-0.018' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 252.69183349609375, 252.69183349609375);'>I</span><span title='-0.138' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 237.4199981689453, 237.4199981689453);'>&#x27;m</span>&nbsp;<span title='-0.597' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 178.87100219726562, 178.87100219726562);'>so</span>&nbsp;<span title='-0.759' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 158.25433349609375, 158.25433349609375);'>sad</span><span title='-0.398' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 204.19796752929688, 204.19796752929688);'>!</span><span title='0.000' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 255.0, 255.0);'>&lt;/s&gt;</span></span><br/><br/><span style='padding: 2px; margin: 2px; background: gray; border-radius: 4px;'>negative:&nbsp;<span title='0.000' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 255.0, 255.0);'>&lt;s&gt;</span><span title='-0.103' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 241.8904266357422, 241.8904266357422);'>I</span>&nbsp;<span title='-0.583' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 180.72738647460938, 180.72738647460938);'>cannot</span>&nbsp;<span title='-0.058' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 247.5995635986328, 247.5995635986328);'>tell</span>&nbsp;<span title='-0.560' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 183.59408569335938, 183.59408569335938);'>whether</span>&nbsp;<span title='0.311' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(215.3888702392578, 255.0, 215.3888702392578);'>I</span>&nbsp;<span title='0.093' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(243.15040588378906, 255.0, 243.15040588378906);'>should</span>&nbsp;<span title='0.353' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(210.03558349609375, 255.0, 210.03558349609375);'>be</span>&nbsp;<span title='0.856' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(145.90750122070312, 255.0, 145.90750122070312);'>happy</span>&nbsp;<span title='0.107' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(241.33053588867188, 255.0, 241.33053588867188);'>or</span>&nbsp;<span title='-0.531' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 187.35049438476562, 187.35049438476562);'>sad</span><span title='-0.181' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 231.87429809570312, 231.87429809570312);'>!</span><span title='0.000' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 255.0, 255.0);'>&lt;/s&gt;</span></span><br/><br/><span style='padding: 2px; margin: 2px; background: gray; border-radius: 4px;'>neutral:&nbsp;<span title='0.000' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 255.0, 255.0);'>&lt;s&gt;</span><span title='0.433' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(199.77516174316406, 255.0, 199.77516174316406);'>me</span><span title='-0.748' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 159.62142944335938, 159.62142944335938);'>h</span><span title='0.000' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 255.0, 255.0);'>&lt;/s&gt;</span></span><br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 1                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 1            |        cudaMalloc retries: 3         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    7419 MB |   18367 MB |  208235 MB |  200815 MB |\n",
      "|       from large pool |    7385 MB |   18333 MB |  204000 MB |  196614 MB |\n",
      "|       from small pool |      34 MB |     217 MB |    4234 MB |    4200 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    7419 MB |   18367 MB |  208235 MB |  200815 MB |\n",
      "|       from large pool |    7385 MB |   18333 MB |  204000 MB |  196614 MB |\n",
      "|       from small pool |      34 MB |     217 MB |    4234 MB |    4200 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   12520 MB |   19330 MB |   24362 MB |   11842 MB |\n",
      "|       from large pool |   12300 MB |   19254 MB |   23992 MB |   11692 MB |\n",
      "|       from small pool |     220 MB |     220 MB |     370 MB |     150 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  282773 KB |     956 MB |  134311 MB |  134035 MB |\n",
      "|       from large pool |  280852 KB |     954 MB |  129489 MB |  129215 MB |\n",
      "|       from small pool |    1921 KB |      60 MB |    4821 MB |    4819 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     612    |    1818    |   52475    |   51863    |\n",
      "|       from large pool |     224    |     812    |   26060    |   25836    |\n",
      "|       from small pool |     388    |    1594    |   26415    |   26027    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     612    |    1818    |   52475    |   51863    |\n",
      "|       from large pool |     224    |     812    |   26060    |   25836    |\n",
      "|       from small pool |     388    |    1594    |   26415    |   26027    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     518    |     518    |     729    |     211    |\n",
      "|       from large pool |     408    |     408    |     544    |     136    |\n",
      "|       from small pool |     110    |     110    |     185    |      75    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |     148    |     272    |   25823    |   25675    |\n",
      "|       from large pool |     129    |     203    |   17163    |   17034    |\n",
      "|       from small pool |      19    |     111    |    8660    |    8641    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This cell, however, should complete.\n",
    "\n",
    "task.wrapper._model.eval()\n",
    "\n",
    "infl_positive_baseline = IntegratedGradients(\n",
    "    model = task.wrapper,\n",
    "    resolution=1000,\n",
    "    baseline = inputs_baseline_embeddings,\n",
    "    doi_cut=Cut('roberta_embeddings_word_embeddings'),\n",
    "    qoi=ClassQoI(task.POSITIVE),\n",
    "    qoi_cut=OutputCut(accessor=lambda o: o['logits']),\n",
    "    rebatch_size=100\n",
    ")\n",
    "\n",
    "print(\"QOI = POSITIVE WITH BASELINE\")\n",
    "display(V.token_attribution(sentences, infl_positive_baseline))\n",
    "\n",
    "print(torch.cuda.memory_summary(device=task.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "51eb71198507ab2c2a4108a27eda9d9658549732e67153fc0e371d8439827db7"
  },
  "kernelspec": {
   "display_name": "test-fresh-11-29",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
