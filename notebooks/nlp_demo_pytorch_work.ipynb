{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Use this if running this notebook from within its place in the truera repository.\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "# Or otherwise install trulens.\n",
    "# !{sys.executable} -m pip install trulens\n",
    "\n",
    "# Install transformers / huggingface.\n",
    "!{sys.executable} -m pip install transformers pandas numpy\n",
    "\n",
    "import os\n",
    "os.environ['TRULENS_BACKEND']='torch'\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from pandas import Series\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Week 2: NLP Example Usage and Stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Sentiment Model\n",
    "\n",
    "[Huggingface](https://huggingface.co/models) offers a variety of pre-trained NLP models to explore. We exemplify in this notebook a [transformer-based twitter sentiment classification model](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment). Before getting started, familiarize yourself with the general Truera API as demonstrated in the [intro notebook using pytorch](intro_demo_pytorch.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Wrap all of the necessary components.\n",
    "class TwitterSentiment:\n",
    "    MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "\n",
    "    # device = 'cpu'\n",
    "    # Can also use cuda if available:\n",
    "    device = 'cuda:0'\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL).to(device)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    @staticmethod\n",
    "    def tokenize(inputs):\n",
    "        return TwitterSentiment \\\n",
    "            .tokenizer(inputs, padding=True, return_tensors=\"pt\") \\\n",
    "            .to(TwitterSentiment.device)\n",
    "        # pt refers to pytorch tensor\n",
    "\n",
    "    labels = ['negative', 'neutral', 'positive']\n",
    "\n",
    "    NEGATIVE = labels.index('negative')\n",
    "    NEUTRAL = labels.index('neutral')\n",
    "    POSITIVE = labels.index('positive')\n",
    "\n",
    "task = TwitterSentiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model quantifies tweets (or really any text you give it) according to its sentiment: positive, negative, or neutral. Lets try it out on some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"I'm so happy!\", \"I'm so sad!\", \"I cannot tell whether I should be happy or sad!\", \"meh\"]\n",
    "\n",
    "# Input sentences need to be tokenized first.\n",
    "\n",
    "inputs = task.tokenize(sentences)\n",
    "\n",
    "# The tokenizer gives us vocabulary indexes for each input token (in this case,\n",
    "# words and some word parts like the \"'m\" part of \"I'm\" are tokens).\n",
    "\n",
    "print(inputs)\n",
    "\n",
    "# Decode helps inspecting the tokenization produced:\n",
    "\n",
    "print(task.tokenizer.batch_decode(torch.flatten(inputs['input_ids'])))\n",
    "# Normally decode would give us a single string for each sentence but we would\n",
    "# not be able to see some of the non-word tokens there. Flattening first gives\n",
    "# us a string for each input_id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying tokens using *trulens*\n",
    "\n",
    "The trulens library features some utilities for displaying tokenizations. To use these, we use the `NLP` object as below. We need to configure it with regards to labels and a tokenizer's decode and encode methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.visualizations import NLP\n",
    "\n",
    "V = NLP(\n",
    "    labels=task.labels,\n",
    "    decode=lambda x: task.tokenizer.decode(x),\n",
    "    tokenize=task.tokenize,\n",
    "    # huggingface models can take as input the keyword args as per produced by their tokenizers.\n",
    "\n",
    "    input_accessor=lambda x: x['input_ids'],\n",
    "    # for huggingface models, input/token ids are under input_ids key in the input dictionary\n",
    "\n",
    "    hidden_tokens=set([task.tokenizer.pad_token_id])\n",
    "    # do not display these tokens\n",
    ")\n",
    "\n",
    "display(V.tokens(sentences))#, show_id=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "\n",
    "Evaluating huggingface models is straight-forward if we use the structure produced by the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = task.model(**inputs)\n",
    "\n",
    "print(outputs)\n",
    "\n",
    "# From logits we can extract the most likely class for each sentence and its readable label.\n",
    "\n",
    "predictions = [task.labels[i] for i in outputs.logits.argmax(axis=1)]\n",
    "\n",
    "for sentence, logits, prediction in zip(sentences, outputs.logits, predictions):\n",
    "    print(logits.to('cpu').detach().numpy(), prediction, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trulens can also help us view these results in a more readable manner. But first we need to tell it about the model by wrappig it in a libray specific container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.nn.models import get_model_wrapper\n",
    "\n",
    "task.wrapper = get_model_wrapper(task.model, input_shape=(None, task.tokenizer.model_max_length), device=task.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to indicate how to retrieve the logits from the output of the model as in the `output_accessor` parameter below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = NLP(\n",
    "    wrapper=task.wrapper,\n",
    "    labels=task.labels,\n",
    "    decode=lambda x: task.tokenizer.decode(x),\n",
    "    tokenize=task.tokenize,\n",
    "    # huggingface models can take as input the keyword args as per produced by their tokenizers.\n",
    "\n",
    "    input_accessor=lambda x: x['input_ids'],\n",
    "    # for huggingface models, input/token ids are under input_ids key in the input dictionary\n",
    "\n",
    "    output_accessor=lambda x: x['logits'],\n",
    "    # and logits under 'logits' key in the output dictionary\n",
    "\n",
    "    hidden_tokens=set([task.tokenizer.pad_token_id])\n",
    "    # do not display these tokens\n",
    ")\n",
    "\n",
    "display(V.tokens(sentences, show_id=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring real-world tweets\n",
    "\n",
    "Lets try out the sentiment model on some real-world tweets. We first load it the CSV file to a pandas `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\n",
    "    Path(\"resources\") / \"training.1600000.processed.noemoticon.csv\",\n",
    "    encoding='ISO-8859-1',\n",
    "    header=None,\n",
    "    names=[\"polarity\", \"id\", \"timestamp\", \"query\", \"user\", \"text\"]\n",
    ")\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the model's predictions on some of these tweets. Note that emojis were stripped from the dataset. These missing tokens are shown as ï¿½."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_tweets = list(tweets['text'][0:10])\n",
    "\n",
    "display(V.tokens(some_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets explore tweets that mention baseball teams. Below we have some utilities that find tweets that contain team names and create team-less versions of them where the team name is replaced with `:team:`. We can use such tweets to investigate the sensitivity of the sentiment model towards particular teams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_team(team: str):\n",
    "    \"\"\"Replaces all instances of ':team:' with the given `team` in the given list of tweets.\"\"\"\n",
    "\n",
    "    def f(tweets: Union[Series, np.ndarray]):\n",
    "        if isinstance(tweets, pd.Series):\n",
    "            return tweets.map(subst(\":team:\", team))\n",
    "        if isinstance(tweets, np.ndarray):\n",
    "            return np.vectorize(subst(\":team:\", team))(tweets)\n",
    "        raise ValueError(\"I don't know\")\n",
    "    return f\n",
    "\n",
    "def word_pattern(word):\n",
    "    \"\"\"Create a pattern that matches the given `word` as long as it is not immediately next to an alpha-numeric character.\"\"\"\n",
    "    return \"(?<!\\w)\" + re.escape(word) + \"(?!\\w)\"\n",
    "\n",
    "def subst(thing_from: str, thing_to: str):\n",
    "    pat = re.compile(word_pattern(thing_from), re.IGNORECASE)\n",
    "    def f(context: str):\n",
    "        return pat.sub(thing_to, context)\n",
    "    return f\n",
    "\n",
    "def extract_teams(teams):\n",
    "    \"\"\"Create a method that extracts tweets that contain mentions of any of the terms in the given `teams`.\"\"\"\n",
    "\n",
    "    pattern = '|'.join(map(word_pattern, teams))\n",
    "    reg = re.compile(pattern, re.IGNORECASE)\n",
    "\n",
    "    def f(tweets: Series):\n",
    "        indices = tweets.str.contains(reg)\n",
    "        ret = tweets[indices]\n",
    "\n",
    "        for team in teams:\n",
    "            ret = ret.map(subst(team, \":team:\"))\n",
    "\n",
    "        return ret\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell may take a minute to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_tweets = extract_teams([\n",
    "    \"diamondbacks\", \"braves\", \"orioles\", \"redsox\", \"red sox\", \"cubs\", \"whitesox\", \"white sox\", \"reds\", \n",
    "    \"guardians\", \"rockies\", \"tigers\", \"astros\", \"royals\", \"dodgers\", \"marlins\", \"brewers\", \"twins\", \n",
    "    \"mets\", \"yankees\", \"athletics\", \"phillies\", \"pirates\", \"padres\", \"giants\", \"mariners\", \"cardinals\", \n",
    "    \"rays\", \"rangers\", \"jays\", \"nationals\"\n",
    "    ]# ['redsox', 'red sox', 'yankees'],\n",
    "    )(tweets['text'])\n",
    "\n",
    "print(f\"found {len(team_tweets)} team tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now focus on two particular teams and compare the sentiment model on them. We start by creating versions of the team tweets with a particular team filled in to where the `:team:` marker was. We can visualize pairs of such tweets that differ in team via another trulens utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = ['redsox', 'yankees']\n",
    "tweets_for_team = {team: to_team(team)(team_tweets) for team in teams}\n",
    "\n",
    "display(V.tokens_stability(\n",
    "    texts1=list(tweets_for_team[teams[0]][0:10]),\n",
    "    texts2=list(tweets_for_team[teams[1]][0:10])\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets inspect the distribution of logits accross tweets of the two teams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First a method to help us evaluate the model on a large collection of instances.\n",
    "def eval_batched(data: Series, batch_size=16):\n",
    "    \"\"\"Evaluate the model `task.model` on given `data` tokenized by `task.tokenizer` in a set of batches. Return the logits.\"\"\"\n",
    "    sentences = DataLoader(data.to_numpy(), batch_size=batch_size)\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    for batch in sentences:\n",
    "        tokens = task.tokenizer(batch, padding=True, return_tensors='pt').to(task.device)\n",
    "        logits = task.model(**tokens)['logits'].detach().to('cpu')\n",
    "        del tokens\n",
    "        all_logits += logits\n",
    "\n",
    "    return np.stack(list(map(torch.Tensor.numpy, all_logits)))\n",
    "\n",
    "# Then get the logits for each team variant's tweets.\n",
    "logits_for_team = {team: eval_batched(tweets) for team, tweets in tweets_for_team.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amin = min(logits.min() for logits in logits_for_team.values())\n",
    "amax = max(logits.max() for logits in logits_for_team.values())\n",
    "\n",
    "colors = {teams[0]: 'red', teams[1]: 'blue'}\n",
    "\n",
    "# Create a figure showing the histogram of logits for each of the three classes for all of the teams in `teams`.\n",
    "\n",
    "fig, axs = plt.subplots(3,1, figsize=(10,10))\n",
    "for idx, label in zip([task.NEGATIVE,task.NEUTRAL,task.POSITIVE], task.labels):\n",
    "    for team, logits in logits_for_team.items():\n",
    "        axs[idx].hist(logits[:, idx], bins=10, alpha=0.25, label=f\"{team} {label}\", color=colors[team], range=(amin, amax))\n",
    "    axs[idx].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any individual tweets that have particularly disparate logits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index of tweets sorted by absolute difference in logits across teams.\n",
    "sort_idx = np.argsort(abs(logits_for_team[teams[0]] - logits_for_team[teams[1]]).sum(axis=1))[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_tweets_np = team_tweets.to_numpy()\n",
    "\n",
    "V.tokens_stability(\n",
    "    texts1 = list(tweets_for_team[teams[0]].to_numpy()[sort_idx[0:10]]),\n",
    "    texts2 = list(tweets_for_team[teams[1]].to_numpy()[sort_idx[0:10]])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Week 3: NLP Attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating huggingface models is straight-forward if we use the structure produced by the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.nn.quantities import ClassQoI\n",
    "from trulens.nn.attribution import IntegratedGradients, InputAttribution\n",
    "from trulens.nn.attribution import Cut, OutputCut\n",
    "from trulens.nn.distributions import GaussianDoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4ed9c783-b745-4c6a-b674-d8b6935dd62d"
    }
   },
   "source": [
    "## Attributions\n",
    "\n",
    "Applying integrated gradents to the sentiment model is similar as in the prior notebooks except special considerations need to be made for the cuts used as the targets of the attribution (i.e. what do we want to assign importance to). As you may have noted above, the model takes as input integer indexes associated with tokens. As we cannot take gradient with respect to these, we use an alternative: the embedding representation of those same inputs. To instantiate trulens with this regard, we need to find inspect the layer names inside our model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "Above, `roberta_embeddings_word_embeddings` is the layer that produces a continuous representation of each input token so we will use that layer as the one defining the **distribution of interest**. While most neural NLP models contain a token embedding, the layer name will differ.\n",
    "\n",
    "The second thing to note is the form of model outputs. Specifically, outputs are structures which contain a 'logits' attribute that stores the model scores.\n",
    "\n",
    "Putting these things together, we instantiate `IntegratedGradients` to attribute each embedding dimension to the maximum class (i.e. the predicted class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively we can look at a particular class:\n",
    "\n",
    "infl_positive = IntegratedGradients(\n",
    "    model = task.wrapper,\n",
    "    resolution=10,\n",
    "    doi_cut=Cut('roberta_embeddings_word_embeddings'),\n",
    "    qoi=ClassQoI(task.POSITIVE),\n",
    "    qoi_cut=OutputCut(accessor=lambda o: o['logits'])\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "# Alternatively we can look at a particular class:\n",
    "infl_positive = InputAttribution(\n",
    "    model = task.wrapper,\n",
    "    doi='point',\n",
    "    # doi=GaussianDoi(var=0.001, resolution=10, cut=Cut('roberta_embeddings_word_embeddings')),\n",
    "    doi_cut=Cut('roberta_embeddings_word_embeddings'),\n",
    "    qoi=ClassQoI(task.POSITIVE),\n",
    "    qoi_cut=OutputCut(accessor=lambda o: o['logits'])\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting attributions uses the same call as model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A listing as above is not very readable so Trulens comes with some utilities to present token influences a bit more concisely. First we need to set up a few parameters to make use of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.visualizations import NLP\n",
    "\n",
    "V = NLP(\n",
    "    wrapper=task.wrapper,\n",
    "    labels=task.labels,\n",
    "    decode=lambda x: task.tokenizer.decode(x),\n",
    "    tokenize=task.tokenize,\n",
    "    # huggingface models can take as input the keyword args as per produced by their tokenizers.\n",
    "\n",
    "    input_accessor=lambda x: x['input_ids'],\n",
    "    # for huggingface models, input/token ids are under input_ids key in the input dictionary\n",
    "\n",
    "    output_accessor=lambda x: x['logits'],\n",
    "    # and logits under 'logits' key in the output dictionary\n",
    "\n",
    "    hidden_tokens=set([task.tokenizer.pad_token_id])\n",
    "    # do not display these tokens\n",
    ")\n",
    "\n",
    "display(\n",
    "    V.tokens(list(tweets_for_team[teams[0]][0:10]), attributor=infl_positive)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n",
    "\n",
    "We see in the above results that special tokens such as the sentence end **&lt;/s&gt;** contributes are found to contribute a lot to the model outputs. While this may be useful in some contexts, we are more interested in the contributions of the actual words in these sentences. To focus on the words more, we need to adjust the **baseline** used in the integrated gradients computation. By default in the instantiation so far, the baseline for each token is a zero vector of the same shape as its embedding. By making the basaeline be identicaly to the explained instances on special tokens, we can rid their impact from our measurement. Trulens provides a utility for this purpose in terms of `token_baseline` which constructs for you the methods to compute the appropriate baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.utils.nlp import token_baseline\n",
    "\n",
    "inputs_baseline_ids, inputs_baseline_embeddings = token_baseline(\n",
    "    keep_tokens=set([task.tokenizer.cls_token_id, task.tokenizer.sep_token_id]),\n",
    "    # Which tokens to preserve.\n",
    "\n",
    "    replacement_token=task.tokenizer.pad_token_id,\n",
    "    # What to replace tokens with.\n",
    "\n",
    "    input_accessor=lambda x: x.kwargs['input_ids'],\n",
    "\n",
    "    ids_to_embeddings=task.model.get_input_embeddings()\n",
    "    # Callable to produce embeddings from token ids.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect the baselines on some example sentences. The first method returned by `token_baseline` gives us token ids to inspect while the second gives us the embeddings of the baseline which we will pass to the attributions method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infl_positive_baseline = IntegratedGradients(\n",
    "    model = task.wrapper,\n",
    "    resolution=50,\n",
    "    baseline = inputs_baseline_embeddings,\n",
    "    doi_cut=Cut('roberta_embeddings_word_embeddings'),\n",
    "    qoi=ClassQoI(task.POSITIVE),\n",
    "    qoi_cut=OutputCut(accessor=lambda o: o['logits'])\n",
    ")\n",
    "\n",
    "print(\"QOI = POSITIVE WITH BASELINE\")\n",
    "V.tokens(list(tweets_for_team[teams[0]][0:10]), attributor=infl_positive_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the baseline eliminated the measurement of contribution of the special tokens."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "51eb71198507ab2c2a4108a27eda9d9658549732e67153fc0e371d8439827db7"
  },
  "kernelspec": {
   "display_name": "test-fresh-11-29",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
