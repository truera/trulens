{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/piotrm/anaconda3/envs/python37_pytorch/lib/python3.7/site-packages (4.17.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/piotrm/anaconda3/envs/python37_pytorch/lib/python3.7/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: filelock in /home/piotrm/anaconda3/envs/python37_pytorch/lib/python3.7/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: requests in /home/piotrm/anaconda3/envs/python37_pytorch/lib/python3.7/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/piotrm/anaconda3/envs/python37_pytorch/lib/python3.7/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/piotrm/anaconda3/envs/python37_pytorch/lib/python3.7/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/piotrm/anaconda3/envs/python37_pytorch/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/piotrm/anaconda3/envs/python37_pytorch/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: sacremoses in /home/piotrm/anaconda3/envs/python37_pytorch/lib/python3.7/site-packages (from transformers) (0.0.47)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/piotrm/anaconda3/envs/python37_pytorch/lib/python3.7/site-packages (from transformers) (4.10.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/piotrm/anaconda3/envs/python37_pytorch/lib/python3.7/site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /home/piotrm/anaconda3/envs/python37_pytorch/lib/python3.7/site-packages (from transformers) (0.11.6)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /home/piotrm/anaconda3/envs/python37_pytorch/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/piotrm/anaconda3/envs/python37_pytorch/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/piotrm/anaconda3/envs/python37_pytorch/lib/python3.7/site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /home/piotrm/anaconda3/envs/python37_pytorch/lib/python3.7/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/piotrm/anaconda3/envs/python37_pytorch/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/piotrm/anaconda3/envs/python37_pytorch/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: joblib in /home/piotrm/anaconda3/envs/python37_pytorch/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: six in /home/piotrm/anaconda3/envs/python37_pytorch/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in /home/piotrm/anaconda3/envs/python37_pytorch/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/piotrm/anaconda3/envs/python37_pytorch/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Use this if running this notebook from within its place in the truera repository.\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "# Or otherwise install trulens.\n",
    "# !{sys.executable} -m pip install trulens\n",
    "\n",
    "# Install transformers / huggingface.\n",
    "!{sys.executable} -m pip install transformers\n",
    "\n",
    "from IPython.display import display\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Model\n",
    "\n",
    "[Huggingface](https://huggingface.co/models) offers a variety of pre-trained NLP models to explore. We exemplify in this notebook a [transformer-based twitter sentiment classification model](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment). Before getting started, familiarize yourself with the general Truera API as demonstrated in the [intro notebook using pytorch](intro_demo_pytorch.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Wrap all of the necessary components.\n",
    "class TwitterSentiment:\n",
    "    MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "\n",
    "    # device = 'cpu'\n",
    "    # Can also use cuda if available:\n",
    "    device = 'cuda:1'\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    labels = ['negative', 'neutral', 'positive']\n",
    "\n",
    "    NEGATIVE = labels.index('negative')\n",
    "    NEUTRAL = labels.index('neutral')\n",
    "    POSITIVE = labels.index('positive')\n",
    "\n",
    "task = TwitterSentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 1                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  488178 KB |  488178 KB |  488178 KB |       0 B  |\n",
      "|       from large pool |  487680 KB |  487680 KB |  487680 KB |       0 B  |\n",
      "|       from small pool |     498 KB |     498 KB |     498 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  488178 KB |  488178 KB |  488178 KB |       0 B  |\n",
      "|       from large pool |  487680 KB |  487680 KB |  487680 KB |       0 B  |\n",
      "|       from small pool |     498 KB |     498 KB |     498 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  542720 KB |  542720 KB |  542720 KB |       0 B  |\n",
      "|       from large pool |  540672 KB |  540672 KB |  540672 KB |       0 B  |\n",
      "|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   54541 KB |   54554 KB |  265207 KB |  210665 KB |\n",
      "|       from large pool |   52992 KB |   52992 KB |  263162 KB |  210170 KB |\n",
      "|       from small pool |    1549 KB |    2045 KB |    2045 KB |     495 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     203    |     203    |     203    |       0    |\n",
      "|       from large pool |      75    |      75    |      75    |       0    |\n",
      "|       from small pool |     128    |     128    |     128    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     203    |     203    |     203    |       0    |\n",
      "|       from large pool |      75    |      75    |      75    |       0    |\n",
      "|       from small pool |     128    |     128    |     128    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      21    |      21    |      21    |       0    |\n",
      "|       from large pool |      20    |      20    |      20    |       0    |\n",
      "|       from small pool |       1    |       1    |       1    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      19    |      19    |      20    |       1    |\n",
      "|       from large pool |      18    |      18    |      19    |       1    |\n",
      "|       from small pool |       1    |       1    |       1    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=task.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model quantifies tweets (or really any text you give it) according to its sentiment: positive, negative, or neutral. Lets try it out on some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   0,  100,  437,   98, 1372,  328,    2,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   0,  100,  437,   98, 5074,  328,    2,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   0,  100, 1395, 1137,  549,   38,  197,   28, 1372,   50, 5074,  328,\n",
      "            2],\n",
      "        [   0, 1794,  298,    2,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1]], device='cuda:1'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')}\n",
      "['<s>', 'I', \"'m\", ' so', ' happy', '!', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<s>', 'I', \"'m\", ' so', ' sad', '!', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<s>', 'I', ' cannot', ' tell', ' whether', ' I', ' should', ' be', ' happy', ' or', ' sad', '!', '</s>', '<s>', 'me', 'h', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"I'm so happy!\", \"I'm so sad!\", \"I cannot tell whether I should be happy or sad!\", \"meh\"]\n",
    "\n",
    "# Input sentences need to be tokenized first.\n",
    "\n",
    "inputs = task.tokenizer(sentences, padding=True, return_tensors=\"pt\").to(task.device) # pt refers to pytorch tensor\n",
    "\n",
    "# The tokenizer gives us vocabulary indexes for each input token (in this case,\n",
    "# words and some word parts like the \"'m\" part of \"I'm\" are tokens).\n",
    "\n",
    "print(inputs)\n",
    "\n",
    "# Decode helps inspecting the tokenization produced:\n",
    "\n",
    "print(task.tokenizer.batch_decode(torch.flatten(inputs['input_ids'])))\n",
    "# Normally decode would give us a single string for each sentence but we would\n",
    "# not be able to see some of the non-word tokens there. Flattening first gives\n",
    "# us a string for each input_id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating huggingface models is straight-forward if we use the structure produced by the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3216, -0.8766,  4.0705],\n",
      "        [ 2.5746, -0.4018, -2.1475],\n",
      "        [ 0.5976,  0.3778, -0.7692],\n",
      "        [-0.2266,  0.6010, -0.2010]], device='cuda:1',\n",
      "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "[-2.3215542 -0.8765632  4.070538 ] positive I'm so happy!\n",
      "[ 2.5745866  -0.40179724 -2.1475189 ] negative I'm so sad!\n",
      "[ 0.59755296  0.37775993 -0.7692062 ] negative I cannot tell whether I should be happy or sad!\n",
      "[-0.22658335  0.60099953 -0.20098346] neutral meh\n"
     ]
    }
   ],
   "source": [
    "outputs = task.model(**inputs)\n",
    "\n",
    "print(outputs)\n",
    "\n",
    "# From logits we can extract the most likely class for each sentence and its readable label.\n",
    "\n",
    "predictions = [task.labels[i] for i in outputs.logits.argmax(axis=1)]\n",
    "\n",
    "for sentence, logits, prediction in zip(sentences, outputs.logits, predictions):\n",
    "    print(logits.to('cpu').detach().numpy(), prediction, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Wrapper\n",
    "\n",
    "As in the prior notebooks, we need to wrap the pytorch model with the appropriate Trulens functionality. Here we specify the maximum input size (in terms of tokens) each tweet may have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: lib level=1\n",
      "INFO: root level=30\n",
      "INFO: Detected pytorch backend for <class 'transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification'>.\n",
      "INFO: Changing backend from None to Backend.PYTORCH.\n",
      "INFO: If this seems incorrect, you can force the correct backend by passing the `backend` parameter directly into your get_model_wrapper call.\n",
      "DEBUG: Input dtype was not passed in. Defaulting to `torch.float32`.\n"
     ]
    }
   ],
   "source": [
    "from trulens.nn.models import get_model_wrapper\n",
    "from trulens.nn.quantities import ClassQoI\n",
    "from trulens.nn.attribution import IntegratedGradients\n",
    "from trulens.nn.attribution import Cut, OutputCut\n",
    "from trulens.utils.typing import ModelInputs\n",
    "\n",
    "task.wrapper = get_model_wrapper(task.model, input_shape=(None, task.tokenizer.model_max_length), device=task.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4ed9c783-b745-4c6a-b674-d8b6935dd62d"
    }
   },
   "source": [
    "# Attributions\n",
    "\n",
    "Applying integrated gradents to the sentiment model is similar as in the prior notebooks except special considerations need to be made for the cuts used as the targets of the attribution (i.e. what do we want to assign importance to). As you may have noted above, the model takes as input integer indexes associated with tokens. As we cannot take gradient with respect to these, we use an alternative: the embedding representation of those same inputs. To instantiate trulens with this regard, we need to find inspect the layer names inside our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'roberta_embeddings_word_embeddings':\tEmbedding(50265, 768, padding_idx=1)\n",
      "'roberta_embeddings_position_embeddings':\tEmbedding(514, 768, padding_idx=1)\n",
      "'roberta_embeddings_token_type_embeddings':\tEmbedding(1, 768)\n",
      "'roberta_embeddings_LayerNorm':\tLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "'roberta_embeddings_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_0_attention_self_query':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_0_attention_self_key':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_0_attention_self_value':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_0_attention_self_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_0_attention_output_dense':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_0_attention_output_LayerNorm':\tLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "'roberta_encoder_layer_0_attention_output_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_0_intermediate_dense':\tLinear(in_features=768, out_features=3072, bias=True)\n",
      "'roberta_encoder_layer_0_intermediate_intermediate_act_fn':\tGELUActivation()\n",
      "'roberta_encoder_layer_0_output_dense':\tLinear(in_features=3072, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_0_output_LayerNorm':\tLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "'roberta_encoder_layer_0_output_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_1_attention_self_query':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_1_attention_self_key':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_1_attention_self_value':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_1_attention_self_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_1_attention_output_dense':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_1_attention_output_LayerNorm':\tLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "'roberta_encoder_layer_1_attention_output_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_1_intermediate_dense':\tLinear(in_features=768, out_features=3072, bias=True)\n",
      "'roberta_encoder_layer_1_intermediate_intermediate_act_fn':\tGELUActivation()\n",
      "'roberta_encoder_layer_1_output_dense':\tLinear(in_features=3072, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_1_output_LayerNorm':\tLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "'roberta_encoder_layer_1_output_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_2_attention_self_query':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_2_attention_self_key':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_2_attention_self_value':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_2_attention_self_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_2_attention_output_dense':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_2_attention_output_LayerNorm':\tLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "'roberta_encoder_layer_2_attention_output_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_2_intermediate_dense':\tLinear(in_features=768, out_features=3072, bias=True)\n",
      "'roberta_encoder_layer_2_intermediate_intermediate_act_fn':\tGELUActivation()\n",
      "'roberta_encoder_layer_2_output_dense':\tLinear(in_features=3072, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_2_output_LayerNorm':\tLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "'roberta_encoder_layer_2_output_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_3_attention_self_query':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_3_attention_self_key':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_3_attention_self_value':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_3_attention_self_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_3_attention_output_dense':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_3_attention_output_LayerNorm':\tLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "'roberta_encoder_layer_3_attention_output_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_3_intermediate_dense':\tLinear(in_features=768, out_features=3072, bias=True)\n",
      "'roberta_encoder_layer_3_intermediate_intermediate_act_fn':\tGELUActivation()\n",
      "'roberta_encoder_layer_3_output_dense':\tLinear(in_features=3072, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_3_output_LayerNorm':\tLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "'roberta_encoder_layer_3_output_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_4_attention_self_query':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_4_attention_self_key':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_4_attention_self_value':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_4_attention_self_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_4_attention_output_dense':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_4_attention_output_LayerNorm':\tLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "'roberta_encoder_layer_4_attention_output_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_4_intermediate_dense':\tLinear(in_features=768, out_features=3072, bias=True)\n",
      "'roberta_encoder_layer_4_intermediate_intermediate_act_fn':\tGELUActivation()\n",
      "'roberta_encoder_layer_4_output_dense':\tLinear(in_features=3072, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_4_output_LayerNorm':\tLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "'roberta_encoder_layer_4_output_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_5_attention_self_query':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_5_attention_self_key':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_5_attention_self_value':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_5_attention_self_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_5_attention_output_dense':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_5_attention_output_LayerNorm':\tLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "'roberta_encoder_layer_5_attention_output_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_5_intermediate_dense':\tLinear(in_features=768, out_features=3072, bias=True)\n",
      "'roberta_encoder_layer_5_intermediate_intermediate_act_fn':\tGELUActivation()\n",
      "'roberta_encoder_layer_5_output_dense':\tLinear(in_features=3072, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_5_output_LayerNorm':\tLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "'roberta_encoder_layer_5_output_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_6_attention_self_query':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_6_attention_self_key':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_6_attention_self_value':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_6_attention_self_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_6_attention_output_dense':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_6_attention_output_LayerNorm':\tLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "'roberta_encoder_layer_6_attention_output_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_6_intermediate_dense':\tLinear(in_features=768, out_features=3072, bias=True)\n",
      "'roberta_encoder_layer_6_intermediate_intermediate_act_fn':\tGELUActivation()\n",
      "'roberta_encoder_layer_6_output_dense':\tLinear(in_features=3072, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_6_output_LayerNorm':\tLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "'roberta_encoder_layer_6_output_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_7_attention_self_query':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_7_attention_self_key':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_7_attention_self_value':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_7_attention_self_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_7_attention_output_dense':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_7_attention_output_LayerNorm':\tLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "'roberta_encoder_layer_7_attention_output_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_7_intermediate_dense':\tLinear(in_features=768, out_features=3072, bias=True)\n",
      "'roberta_encoder_layer_7_intermediate_intermediate_act_fn':\tGELUActivation()\n",
      "'roberta_encoder_layer_7_output_dense':\tLinear(in_features=3072, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_7_output_LayerNorm':\tLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "'roberta_encoder_layer_7_output_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_8_attention_self_query':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_8_attention_self_key':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_8_attention_self_value':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_8_attention_self_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_8_attention_output_dense':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_8_attention_output_LayerNorm':\tLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "'roberta_encoder_layer_8_attention_output_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_8_intermediate_dense':\tLinear(in_features=768, out_features=3072, bias=True)\n",
      "'roberta_encoder_layer_8_intermediate_intermediate_act_fn':\tGELUActivation()\n",
      "'roberta_encoder_layer_8_output_dense':\tLinear(in_features=3072, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_8_output_LayerNorm':\tLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "'roberta_encoder_layer_8_output_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_9_attention_self_query':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_9_attention_self_key':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_9_attention_self_value':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_9_attention_self_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_9_attention_output_dense':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_9_attention_output_LayerNorm':\tLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "'roberta_encoder_layer_9_attention_output_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_9_intermediate_dense':\tLinear(in_features=768, out_features=3072, bias=True)\n",
      "'roberta_encoder_layer_9_intermediate_intermediate_act_fn':\tGELUActivation()\n",
      "'roberta_encoder_layer_9_output_dense':\tLinear(in_features=3072, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_9_output_LayerNorm':\tLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "'roberta_encoder_layer_9_output_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_10_attention_self_query':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_10_attention_self_key':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_10_attention_self_value':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_10_attention_self_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_10_attention_output_dense':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_10_attention_output_LayerNorm':\tLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "'roberta_encoder_layer_10_attention_output_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_10_intermediate_dense':\tLinear(in_features=768, out_features=3072, bias=True)\n",
      "'roberta_encoder_layer_10_intermediate_intermediate_act_fn':\tGELUActivation()\n",
      "'roberta_encoder_layer_10_output_dense':\tLinear(in_features=3072, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_10_output_LayerNorm':\tLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "'roberta_encoder_layer_10_output_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_11_attention_self_query':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_11_attention_self_key':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_11_attention_self_value':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_11_attention_self_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_11_attention_output_dense':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_11_attention_output_LayerNorm':\tLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "'roberta_encoder_layer_11_attention_output_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'roberta_encoder_layer_11_intermediate_dense':\tLinear(in_features=768, out_features=3072, bias=True)\n",
      "'roberta_encoder_layer_11_intermediate_intermediate_act_fn':\tGELUActivation()\n",
      "'roberta_encoder_layer_11_output_dense':\tLinear(in_features=3072, out_features=768, bias=True)\n",
      "'roberta_encoder_layer_11_output_LayerNorm':\tLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "'roberta_encoder_layer_11_output_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'classifier_dense':\tLinear(in_features=768, out_features=768, bias=True)\n",
      "'classifier_dropout':\tDropout(p=0.1, inplace=False)\n",
      "'classifier_out_proj':\tLinear(in_features=768, out_features=3, bias=True)\n"
     ]
    }
   ],
   "source": [
    "task.wrapper.print_layer_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Above, `roberta_embeddings_word_embeddings` is the layer that produces a continuous representation of each input token so we will use that layer as the one defining the **distribution of interest**. While most neural NLP models contain a token embedding, the layer name will differ.\n",
    "\n",
    "The second thing to note is the form of model outputs. Specifically, outputs are structures which contain a 'logits' attribute that stores the model scores.\n",
    "\n",
    "Putting these things together, we instantiate `IntegratedGradients` to attribute each embedding dimension to the maximum class (i.e. the predicted class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "infl_max = IntegratedGradients(\n",
    "    model = task.wrapper,\n",
    "    doi_cut=Cut('roberta_embeddings_word_embeddings'),\n",
    "    qoi_cut=OutputCut(accessor=lambda o: o['logits'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively we can look at a particular class:\n",
    "\n",
    "infl_positive = IntegratedGradients(\n",
    "    model = task.wrapper,\n",
    "    doi_cut=Cut('roberta_embeddings_word_embeddings'),\n",
    "    qoi=ClassQoI(task.POSITIVE),\n",
    "    qoi_cut=OutputCut(accessor=lambda o: o['logits'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting attributions uses the same call as model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_doi= 50\n",
      "<s>(0.014) I(0.140) 'm(0.083)  so(-0.013)  happy(0.119) !(0.001) </s>(-0.124) <pad>(0.000) <pad>(0.000) <pad>(0.000) <pad>(0.000) <pad>(0.000) <pad>(0.000) \n",
      "<s>(0.000) I(0.099) 'm(-0.009)  so(0.290)  sad(0.023) !(-0.041) </s>(-0.011) <pad>(0.000) <pad>(0.000) <pad>(0.000) <pad>(0.000) <pad>(0.000) <pad>(0.000) \n",
      "<s>(-0.007) I(0.039)  cannot(0.016)  tell(0.012)  whether(0.134)  I(-0.046)  should(0.021)  be(-0.064)  happy(-0.206)  or(-0.128)  sad(-0.138) !(-0.115) </s>(-0.009) \n",
      "<s>(0.048) me(-0.091) h(-0.289) </s>(-0.005) <pad>(0.000) <pad>(0.000) <pad>(0.000) <pad>(0.000) <pad>(0.000) <pad>(0.000) <pad>(0.000) <pad>(0.000) <pad>(0.000) \n"
     ]
    }
   ],
   "source": [
    "attrs = infl_max.attributions(**inputs)\n",
    "\n",
    "for token_ids, token_attr in zip(inputs['input_ids'], attrs):\n",
    "    for token_id, token_attr in zip(token_ids, token_attr):\n",
    "        # Not that each `word_attr` has a magnitude for each of the embedding\n",
    "        # dimensions, of which there are many. We aggregate them for easier\n",
    "        # interpretation and display.\n",
    "        attr = token_attr.sum()\n",
    "\n",
    "        word = task.tokenizer.decode(token_id)\n",
    "\n",
    "        print(f\"{word}({attr:0.3f})\", end=' ')\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A listing as above is not very readable so Trulens comes with some utilities to present token influences a bit more concisely. First we need to set up a few parameters to make use of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QOI = MAX PREDICTION\n",
      "n_doi= 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style='padding: 2px; margin: 2px; background: gray; border-radius: 4px;'>positive:&nbsp;<span title='0.035' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(250.49364415928721, 255.0, 250.49364415928721);'>&lt;s&gt;</span><span title='0.189' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(230.89225631207228, 255.0, 230.89225631207228);'>I</span><span title='0.274' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(220.091273188591, 255.0, 220.091273188591);'>&#x27;m</span>&nbsp;<span title='0.342' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(211.375270485878, 255.0, 211.375270485878);'>so</span>&nbsp;<span title='0.525' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(188.05622577667236, 255.0, 188.05622577667236);'>happy</span><span title='0.227' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(226.0474117845297, 255.0, 226.0474117845297);'>!</span><span title='0.046' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(249.13927238434553, 255.0, 249.13927238434553);'>&lt;/s&gt;</span></span><br/><br/><span style='padding: 2px; margin: 2px; background: gray; border-radius: 4px;'>negative:&nbsp;<span title='-0.011' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 253.5875089187175, 253.5875089187175);'>&lt;s&gt;</span><span title='0.118' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(239.9258621223271, 255.0, 239.9258621223271);'>I</span><span title='-0.036' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 250.40093008428812, 250.40093008428812);'>&#x27;m</span>&nbsp;<span title='0.169' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(233.49639147520065, 255.0, 233.49639147520065);'>so</span>&nbsp;<span title='0.216' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(227.49701119959354, 255.0, 227.49701119959354);'>sad</span><span title='0.038' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(250.18345065414906, 255.0, 250.18345065414906);'>!</span><span title='-0.130' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 238.46501626074314, 238.46501626074314);'>&lt;/s&gt;</span></span><br/><br/><span style='padding: 2px; margin: 2px; background: gray; border-radius: 4px;'>negative:&nbsp;<span title='-0.036' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 250.45792940072715, 250.45792940072715);'>&lt;s&gt;</span><span title='-0.012' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 253.48674830049276, 253.48674830049276);'>I</span>&nbsp;<span title='-0.209' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 228.3550051599741, 228.3550051599741);'>cannot</span>&nbsp;<span title='0.010' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(253.77556652761996, 255.0, 253.77556652761996);'>tell</span>&nbsp;<span title='0.044' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(249.36077674850821, 255.0, 249.36077674850821);'>whether</span>&nbsp;<span title='-0.026' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 251.68066911399364, 251.68066911399364);'>I</span>&nbsp;<span title='0.063' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(247.0179250650108, 255.0, 247.0179250650108);'>should</span>&nbsp;<span title='0.017' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(252.840729765594, 255.0, 252.840729765594);'>be</span>&nbsp;<span title='-0.224' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 226.40772745013237, 226.40772745013237);'>happy</span>&nbsp;<span title='-0.139' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 237.2582485154271, 237.2582485154271);'>or</span>&nbsp;<span title='-0.505' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 190.56427359580994, 190.56427359580994);'>sad</span><span title='-0.135' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 237.81594458967447, 237.81594458967447);'>!</span><span title='0.023' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(252.12978910189122, 255.0, 252.12978910189122);'>&lt;/s&gt;</span></span><br/><br/><span style='padding: 2px; margin: 2px; background: gray; border-radius: 4px;'>neutral:&nbsp;<span title='-0.020' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 252.38770168740302, 252.38770168740302);'>&lt;s&gt;</span><span title='-0.253' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 222.72670157253742, 222.72670157253742);'>me</span><span title='-0.444' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 198.35510574281216, 198.35510574281216);'>h</span><span title='0.310' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(215.41490972042084, 255.0, 215.41490972042084);'>&lt;/s&gt;</span></span><br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QOI = POSITIVE\n",
      "n_doi= 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style='padding: 2px; margin: 2px; background: gray; border-radius: 4px;'>positive:&nbsp;<span title='0.045' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(249.20680806040764, 255.0, 249.20680806040764);'>&lt;s&gt;</span><span title='0.263' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(221.52336314320564, 255.0, 221.52336314320564);'>I</span><span title='0.557' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(184.00947332382202, 255.0, 184.00947332382202);'>&#x27;m</span>&nbsp;<span title='1.090' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(115.98301559686661, 255.0, 115.98301559686661);'>so</span>&nbsp;<span title='1.789' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(26.897449493408203, 255.0, 26.897449493408203);'>happy</span><span title='0.748' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(159.6369655430317, 255.0, 159.6369655430317);'>!</span><span title='0.409' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(202.8601098060608, 255.0, 202.8601098060608);'>&lt;/s&gt;</span></span><br/><br/><span style='padding: 2px; margin: 2px; background: gray; border-radius: 4px;'>negative:&nbsp;<span title='-0.027' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 251.62062141112983, 251.62062141112983);'>&lt;s&gt;</span><span title='0.271' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(220.47349087893963, 255.0, 220.47349087893963);'>I</span><span title='0.174' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(232.7512514591217, 255.0, 232.7512514591217);'>&#x27;m</span>&nbsp;<span title='-0.255' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 222.53427229821682, 222.53427229821682);'>so</span>&nbsp;<span title='-0.725' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 162.55865156650543, 162.55865156650543);'>sad</span><span title='0.035' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(250.57123362086713, 255.0, 250.57123362086713);'>!</span><span title='-2.539' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, -68.73010396957397, -68.73010396957397);'>&lt;/s&gt;</span></span><br/><br/><span style='padding: 2px; margin: 2px; background: gray; border-radius: 4px;'>negative:&nbsp;<span title='-0.060' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 247.3380973096937, 247.3380973096937);'>&lt;s&gt;</span><span title='-0.117' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 240.12802932411432, 240.12802932411432);'>I</span>&nbsp;<span title='-1.062' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 119.5821064710617, 119.5821064710617);'>cannot</span>&nbsp;<span title='0.095' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(242.83318491652608, 255.0, 242.83318491652608);'>tell</span>&nbsp;<span title='-0.231' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 225.55634133517742, 225.55634133517742);'>whether</span>&nbsp;<span title='0.043' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(249.549810430035, 255.0, 249.549810430035);'>I</span>&nbsp;<span title='0.261' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(221.7280201613903, 255.0, 221.7280201613903);'>should</span>&nbsp;<span title='0.188' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(231.04615185409784, 255.0, 231.04615185409784);'>be</span>&nbsp;<span title='0.922' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(137.39305034279823, 255.0, 137.39305034279823);'>happy</span>&nbsp;<span title='0.191' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(230.60255225747824, 255.0, 230.60255225747824);'>or</span>&nbsp;<span title='-0.300' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 216.75815664231777, 216.75815664231777);'>sad</span><span title='0.185' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(231.4130848646164, 255.0, 231.4130848646164);'>!</span><span title='0.051' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(248.44863996841013, 255.0, 248.44863996841013);'>&lt;/s&gt;</span></span><br/><br/><span style='padding: 2px; margin: 2px; background: gray; border-radius: 4px;'>neutral:&nbsp;<span title='-0.042' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 249.59817993454635, 249.59817993454635);'>&lt;s&gt;</span><span title='0.330' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(212.9694825410843, 255.0, 212.9694825410843);'>me</span><span title='0.145' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(236.4543218538165, 255.0, 236.4543218538165);'>h</span><span title='0.091' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(243.4056052006781, 255.0, 243.4056052006781);'>&lt;/s&gt;</span></span><br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trulens.visualizations import NLP\n",
    "\n",
    "V = NLP(\n",
    "    wrapper=task.wrapper,\n",
    "    labels=task.labels,\n",
    "    decode=lambda x: task.tokenizer.decode(x),\n",
    "    tokenize=lambda sentences: ModelInputs(kwargs=task.tokenizer(sentences, padding=True, return_tensors='pt')).map(lambda t: t.to(task.device)),\n",
    "    # huggingface models can take as input the keyword args as per produced by their tokenizers.\n",
    "\n",
    "    input_accessor=lambda x: x.kwargs['input_ids'],\n",
    "    # for huggingface models, input/token ids are under input_ids key in the input dictionary\n",
    "\n",
    "    output_accessor=lambda x: x['logits'],\n",
    "    # and logits under 'logits' key in the output dictionary\n",
    "\n",
    "    hidden_tokens=set([task.tokenizer.pad_token_id])\n",
    "    # do not display these tokens\n",
    ")\n",
    "\n",
    "print(\"QOI = MAX PREDICTION\")\n",
    "display(V.token_attribution(sentences, infl_max))\n",
    "\n",
    "print(\"QOI = POSITIVE\")\n",
    "display(V.token_attribution(sentences, infl_positive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines\n",
    "\n",
    "We see in the above results that special tokens such as the sentence end **&lt;/s&gt;** contributes are found to contribute a lot to the model outputs. While this may be useful in some contexts, we are more interested in the contributions of the actual words in these sentences. To focus on the words more, we need to adjust the **baseline** used in the integrated gradients computation. By default in the instantiation so far, the baseline for each token is a zero vector of the same shape as its embedding. By making the basaeline be identicaly to the explained instances on special tokens, we can rid their impact from our measurement. Trulens provides a utility for this purpose in terms of `token_baseline` which constructs for you the methods to compute the appropriate baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.utils.nlp import token_baseline\n",
    "\n",
    "inputs_baseline_ids, inputs_baseline_embeddings = token_baseline(\n",
    "    keep_tokens=set([task.tokenizer.cls_token_id, task.tokenizer.sep_token_id]),\n",
    "    # Which tokens to preserve.\n",
    "\n",
    "    replacement_token=task.tokenizer.pad_token_id,\n",
    "    # What to replace tokens with.\n",
    "\n",
    "    input_accessor=lambda x: x.kwargs['input_ids'],\n",
    "\n",
    "    ids_to_embeddings=task.model.get_input_embeddings()\n",
    "    # Callable to produce embeddings from token ids.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect the baselines on some example sentences. The first method returned by `token_baseline` gives us token ids to inspect while the second gives us the embeddings of the baseline which we will pass to the attributions method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "originals= [\"<s>I'm so happy!</s><pad><pad><pad><pad><pad><pad>\", \"<s>I'm so sad!</s><pad><pad><pad><pad><pad><pad>\", '<s>I cannot tell whether I should be happy or sad!</s>', '<s>meh</s><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "baselines= ['<s><pad><pad><pad><pad><pad></s><pad><pad><pad><pad><pad><pad>', '<s><pad><pad><pad><pad><pad></s><pad><pad><pad><pad><pad><pad>', '<s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad></s>', '<s><pad><pad></s><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n"
     ]
    }
   ],
   "source": [
    "print(\"originals=\", task.tokenizer.batch_decode(inputs['input_ids']))\n",
    "\n",
    "baseline_word_ids = inputs_baseline_ids(model_inputs=ModelInputs(args=[], kwargs=inputs))\n",
    "print(\"baselines=\", task.tokenizer.batch_decode(baseline_word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QOI = POSITIVE WITH BASELINE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style='padding: 2px; margin: 2px; background: gray; border-radius: 4px;'>positive:&nbsp;<span title='0.000' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 255.0, 255.0);'>&lt;s&gt;</span><span title='-0.281' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 219.18037086725235, 219.18037086725235);'>I</span><span title='0.216' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(227.45206531137228, 255.0, 227.45206531137228);'>&#x27;m</span>&nbsp;<span title='0.226' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(226.2147795036435, 255.0, 226.2147795036435);'>so</span>&nbsp;<span title='1.172' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(105.58505594730377, 255.0, 105.58505594730377);'>happy</span><span title='0.439' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(199.07565347850323, 255.0, 199.07565347850323);'>!</span><span title='0.000' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 255.0, 255.0);'>&lt;/s&gt;</span></span><br/><br/><span style='padding: 2px; margin: 2px; background: gray; border-radius: 4px;'>negative:&nbsp;<span title='0.000' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 255.0, 255.0);'>&lt;s&gt;</span><span title='-0.267' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 220.91910436749458, 220.91910436749458);'>I</span><span title='0.507' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(190.34263908863068, 255.0, 190.34263908863068);'>&#x27;m</span>&nbsp;<span title='-1.132' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 110.71785092353821, 110.71785092353821);'>so</span>&nbsp;<span title='-3.011' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, -128.92230927944183, -128.92230927944183);'>sad</span><span title='-0.656' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 171.39703050255775, 171.39703050255775);'>!</span><span title='0.000' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 255.0, 255.0);'>&lt;/s&gt;</span></span><br/><br/><span style='padding: 2px; margin: 2px; background: gray; border-radius: 4px;'>negative:&nbsp;<span title='0.000' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 255.0, 255.0);'>&lt;s&gt;</span><span title='0.041' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(249.8301399126649, 255.0, 249.8301399126649);'>I</span>&nbsp;<span title='-0.149' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 236.04804765433073, 236.04804765433073);'>cannot</span>&nbsp;<span title='-0.035' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 250.54307095706463, 250.54307095706463);'>tell</span>&nbsp;<span title='-0.409' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 202.90961354970932, 202.90961354970932);'>whether</span>&nbsp;<span title='0.356' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(209.59629714488983, 255.0, 209.59629714488983);'>I</span>&nbsp;<span title='0.028' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(251.4744632039219, 255.0, 251.4744632039219);'>should</span>&nbsp;<span title='0.431' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(200.07797129452229, 255.0, 200.07797129452229);'>be</span>&nbsp;<span title='0.785' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(154.94632244110107, 255.0, 154.94632244110107);'>happy</span>&nbsp;<span title='0.105' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(241.62366706877947, 255.0, 241.62366706877947);'>or</span>&nbsp;<span title='-0.603' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 178.14284652471542, 178.14284652471542);'>sad</span><span title='-0.023' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 252.08854777738452, 252.08854777738452);'>!</span><span title='0.000' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 255.0, 255.0);'>&lt;/s&gt;</span></span><br/><br/><span style='padding: 2px; margin: 2px; background: gray; border-radius: 4px;'>neutral:&nbsp;<span title='0.000' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 255.0, 255.0);'>&lt;s&gt;</span><span title='0.246' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(223.61143469810486, 255.0, 223.61143469810486);'>me</span><span title='0.014' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(253.23852187022567, 255.0, 253.23852187022567);'>h</span><span title='0.000' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb(255.0, 255.0, 255.0);'>&lt;/s&gt;</span></span><br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "infl_positive_baseline = IntegratedGradients(\n",
    "    model = task.wrapper,\n",
    "    resolution=25,\n",
    "    baseline = inputs_baseline_embeddings,\n",
    "    doi_cut=Cut('roberta_embeddings_word_embeddings'),\n",
    "    qoi=ClassQoI(task.POSITIVE),\n",
    "    qoi_cut=OutputCut(accessor=lambda o: o['logits'])\n",
    ")\n",
    "\n",
    "#with device(device=task.device):\n",
    "print(\"QOI = POSITIVE WITH BASELINE\")\n",
    "display(V.token_attribution(sentences, infl_positive_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the baseline eliminated the measurement of contribution of the special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QOI = POSITIVE WITH BASELINE\n"
     ]
    },
    {
     "ename": "OutOfMemory",
     "evalue": "Ran out of memory ({'device': device(type='cuda', index=1)}). Consider reducing memory-impactful parameters.\n  batch size = 4\n  distribution of interest (per instance) size = 300\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/repos/trulens/trulens/nn/backend/__init__.py\u001b[0m in \u001b[0;36mgrace\u001b[0;34m(call_before, call_after, *settings, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/trulens/trulens/nn/models/pytorch.py\u001b[0m in \u001b[0;36mqoi_bprop\u001b[0;34m(self, qoi, model_args, model_kwargs, doi_cut, to_cut, attribution_cut, intervention)\u001b[0m\n\u001b[1;32m    564\u001b[0m                     \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_flat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqoi_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m                 ] if isinstance(qoi_out, DATA_CONTAINER_TYPE) else B.gradient(scalarize(qoi_out), z_flat)\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/trulens/trulens/nn/backend/pytorch_backend/pytorch.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(scalar, wrt)\u001b[0m\n\u001b[1;32m     87\u001b[0m     grads = torch.autograd.grad(\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mscalar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_unused\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     )\n",
      "\u001b[0;32m~/anaconda3/envs/python37_pytorch/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         inputs, allow_unused, accumulate_grad=False)\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 1; 23.70 GiB total capacity; 20.95 GiB already allocated; 40.00 MiB free; 21.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutOfMemory\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/repos/trulens/trulens/nn/backend/__init__.py\u001b[0m in \u001b[0;36mgrace\u001b[0;34m(call_before, call_after, *settings, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/trulens/trulens/nn/attribution.py\u001b[0m in \u001b[0;36mattributions\u001b[0;34m(self, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m                 \u001b[0mintervention\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m                 \u001b[0mdoi_cut\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoi_cut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m             )\n",
      "\u001b[0;32m~/repos/trulens/trulens/nn/models/pytorch.py\u001b[0m in \u001b[0;36mqoi_bprop\u001b[0;34m(self, qoi, model_args, model_kwargs, doi_cut, to_cut, attribution_cut, intervention)\u001b[0m\n\u001b[1;32m    564\u001b[0m                     \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_flat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqoi_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m                 ] if isinstance(qoi_out, DATA_CONTAINER_TYPE) else B.gradient(scalarize(qoi_out), z_flat)\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python37_pytorch/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/trulens/trulens/nn/backend/__init__.py\u001b[0m in \u001b[0;36mgrace\u001b[0;34m(call_before, call_after, *settings, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"out of memory\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mOutOfMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemory\u001b[0m: Ran out of memory ({'device': device(type='cuda', index=1)}). Consider reducing memory-impactful parameters.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutOfMemory\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2886942/2800263333.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"QOI = POSITIVE WITH BASELINE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_attribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfl_positive_baseline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/trulens/trulens/visualizations.py\u001b[0m in \u001b[0;36mtoken_attribution\u001b[0;34m(self, texts, attr)\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_on\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m         \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_on\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattributions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/trulens/trulens/utils/typing.py\u001b[0m in \u001b[0;36mcall_on\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;34m\"\"\"Call the given method with the contained arguments.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/trulens/trulens/nn/attribution.py\u001b[0m in \u001b[0;36mattributions\u001b[0;34m(self, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mto_cut\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_cut\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                 \u001b[0mintervention\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m                 \u001b[0mdoi_cut\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoi_cut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m             )\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python37_pytorch/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/trulens/trulens/nn/backend/__init__.py\u001b[0m in \u001b[0;36mgrace\u001b[0;34m(call_before, call_after, *settings, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOutOfMemory\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         raise OutOfMemory(\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0msettings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         )\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemory\u001b[0m: Ran out of memory ({'device': device(type='cuda', index=1)}). Consider reducing memory-impactful parameters.\n  batch size = 4\n  distribution of interest (per instance) size = 300\n"
     ]
    }
   ],
   "source": [
    "infl_positive_baseline = IntegratedGradients(\n",
    "    model = task.wrapper,\n",
    "    resolution=300,\n",
    "    baseline = inputs_baseline_embeddings,\n",
    "    doi_cut=Cut('roberta_embeddings_word_embeddings'),\n",
    "    qoi=ClassQoI(task.POSITIVE),\n",
    "    qoi_cut=OutputCut(accessor=lambda o: o['logits'])\n",
    ")\n",
    "\n",
    "print(\"QOI = POSITIVE WITH BASELINE\")\n",
    "display(V.token_attribution(sentences, infl_positive_baseline))\n",
    "\n",
    "print(torch.cuda.memory_summary(device=task.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary(device=task.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "51eb71198507ab2c2a4108a27eda9d9658549732e67153fc0e371d8439827db7"
  },
  "kernelspec": {
   "display_name": "test-fresh-11-29",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
