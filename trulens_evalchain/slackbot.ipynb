{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slackbot-related work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 3), match=' 5 '>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pat_1_10 = re.compile(r\"\\s*([1-9][0-9]*)\\s*\")\n",
    "\n",
    "def _re_1_10_rating(str_val):\n",
    "    matches = pat_1_10.fullmatch(str_val)\n",
    "    print(matches)\n",
    "    if not matches:\n",
    "        print(f\"WARNING: 1-10 rating regex failed to match on: '{str_val}'\")\n",
    "        return -10 # so this will be reported as -1 after division by 10\n",
    "    \n",
    "    return int(matches.group())\n",
    "\n",
    "_re_1_10_rating(\" 5 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got OPENAI_API_KEY\n",
      "got PINECONE_API_KEY\n",
      "got PINECONE_ENV\n",
      "got HUGGINGFACE_API_KEY\n",
      "got SLACK_TOKEN\n",
      "got SLACK_SIGNING_SECRET\n",
      "got COHERE_API_KEY\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'trulens_evalchain.trulens_evalchain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrulens_evalchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtru_db\u001b[39;00m \u001b[39mimport\u001b[39;00m Record, LocalTinyDB, TruDB, LocalSQLite\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrulens_evalchain\u001b[39;00m \u001b[39mimport\u001b[39;00m tru\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrulens_evalchain\u001b[39;00m \u001b[39mimport\u001b[39;00m tru_feedback\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m \u001b[39mimport\u001b[39;00m JSON\n",
      "File \u001b[0;32m~/repos/trulens_private/trulens_evalchain/trulens_evalchain/tru.py:16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrulens_evalchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtru_db\u001b[39;00m \u001b[39mimport\u001b[39;00m json_default\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrulens_evalchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtru_db\u001b[39;00m \u001b[39mimport\u001b[39;00m LocalSQLite\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrulens_evalchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtru_chain\u001b[39;00m \u001b[39mimport\u001b[39;00m TruChain\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrulens_evalchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtru_feedback\u001b[39;00m \u001b[39mimport\u001b[39;00m Feedback\n\u001b[1;32m     19\u001b[0m lms \u001b[39m=\u001b[39m LocalSQLite()\n",
      "File \u001b[0;32m~/repos/trulens_private/trulens_evalchain/trulens_evalchain/tru_chain.py:131\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any, Callable, Dict, List, Optional, Sequence, Tuple, Union\n\u001b[1;32m    129\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrulens_evalchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrulens_evalchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtru_db\u001b[39;00m \u001b[39mimport\u001b[39;00m noserio\n\u001b[1;32m    133\u001b[0m langchain\u001b[39m.\u001b[39mverbose \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m Chain\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'trulens_evalchain.trulens_evalchain'"
     ]
    }
   ],
   "source": [
    "from trulens_evalchain.tru_db import Record, LocalTinyDB, TruDB, LocalSQLite\n",
    "from trulens_evalchain import tru\n",
    "from trulens_evalchain import tru_feedback\n",
    "from IPython.display import JSON\n",
    "from ipywidgets import widgets\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: will not be able to serialize object of type <class 'langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain'> because it has memory.\n",
      "WARNING: will not be able to serialize object of type <class 'langchain.chains.conversational_retrieval.base.BaseConversationalRetrievalChain'> because it has memory.\n",
      "WARNING: will not be able to serialize object of type <class 'langchain.chains.base.Chain'> because it has memory.\n",
      "WARNING: do not know how to instrument <langchain.vectorstores.pinecone.Pinecone object at 0x2880e5d00>\n",
      "WARNING: Don't know how to dictify an object '<class 'openai.api_resources.com' of type '<class 'type'>'.\n",
      "WARNING: Don't know how to dictify an object '<class 'langchain.schema.SystemM' of type '<class 'pydantic.main.ModelMetaclass'>'.\n",
      "WARNING: Don't know how to dictify an object '<class 'openai.api_resources.com' of type '<class 'type'>'.\n",
      "WARNING: Don't know how to dictify an object '<class 'openai.api_resources.com' of type '<class 'type'>'.\n",
      "WARNING: Don't know how to dictify an object '<function <lambda> at 0x17def60d' of type '<class 'function'>'.\n",
      "WARNING: Don't know how to dictify an object '<langchain.vectorstores.pinecone' of type '<class 'langchain.vectorstores.pinecone.Pinecone'>'.\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms.base import BaseLLM\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "\n",
    "from trulens_evalchain import tru\n",
    "from trulens_evalchain import tru_chain\n",
    "from trulens_evalchain.keys import *\n",
    "from trulens_evalchain.keys import PINECONE_API_KEY\n",
    "from trulens_evalchain.keys import PINECONE_ENV\n",
    "\n",
    "# Set up GPT-3 model\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "\n",
    "chain_id = \"TruBot_relevance\"\n",
    "\n",
    "# Pinecone configuration.\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "    environment=PINECONE_ENV  # next to api key in console\n",
    ")\n",
    "\n",
    "identity = lambda h: h\n",
    "\n",
    "# Embedding needed for Pinecone vector db.\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-ada-002')  # 1536 dims\n",
    "docsearch = Pinecone.from_existing_index(\n",
    "    index_name=\"llmdemo\", embedding=embedding\n",
    ")\n",
    "retriever = docsearch.as_retriever()\n",
    "\n",
    "# LLM for completing prompts, and other tasks.\n",
    "llm = OpenAI(temperature=0, max_tokens=128)\n",
    "\n",
    "# Conversation memory.\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    max_token_limit=650,\n",
    "    llm=llm,\n",
    "    memory_key=\"chat_history\",\n",
    "    output_key='answer'\n",
    ")\n",
    "\n",
    "# Conversational chain puts it all together.\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    memory=memory,\n",
    "    get_chat_history=identity,\n",
    "    max_tokens_limit=4096\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "# Language mismatch fix:\n",
    "chain.combine_docs_chain.llm_chain.prompt.template = \\\n",
    "    \"Use the following pieces of context to answer the question at the end \" \\\n",
    "    \"in the same language as the question. If you don't know the answer, \" \\\n",
    "    \"just say that you don't know, don't try to make up an answer.\\n\\n\" \\\n",
    "    \"{context}\\n\\n\" \\\n",
    "    \"Question: {question}\\n\" \\\n",
    "    \"Helpful Answer: \"\n",
    "\"\"\"\n",
    "\n",
    "# Contexts fix\n",
    "chain.combine_docs_chain.llm_chain.prompt.template = \\\n",
    "    \"Use only the relevant contexts to answer the question at the end \" \\\n",
    "    \". Some pieces of context may not be relevant. If you don't know the answer, \" \\\n",
    "    \"just say that you don't know, don't try to make up an answer.\\n\\n\" \\\n",
    "    \"Contexts: \\n{context}\\n\\n\" \\\n",
    "    \"Question: {question}\\n\" \\\n",
    "    \"Helpful Answer: \"\n",
    "\n",
    "chain.combine_docs_chain.document_prompt.template=\"\\tContext: {page_content}\"\n",
    "\n",
    "# Trulens instrumentation.\n",
    "tc = tru_chain.TruChain(chain, chain_id=chain_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nan' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39meval\u001b[39;49m(\u001b[39mstr\u001b[39;49m(\u001b[39mfloat\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mnan\u001b[39;49m\u001b[39m\"\u001b[39;49m)))\n",
      "File \u001b[0;32m<string>:1\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nan' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tContext: When Shayak started building production grade machine learning models for algorithmic trading 10 years ago, he realized the need for putting the ‘science’ back in ‘data science’. Since then, he has been building systems and leading research to make machine learning and big data systems more explainable, privacy compliant, and fair. Shayak’s research at Carnegie Mellon University introduced a number of pioneering breakthroughs to the field of explainable AI. Shayak obtained his PhD in Computer Science from Carnegie Mellon University and BTech in Computer Science from the Indian Institute of Technology, Delhi.\n",
      "\n",
      "\tContext: When Shayak started building production grade machine learning models for algorithmic trading 10 years ago, he realized the need for putting the ‘science’ back in ‘data science’. Since then, he has been building systems and leading research to make machine learning and big data systems more explainable, privacy compliant, and fair. Shayak’s research at Carnegie Mellon University introduced a number of pioneering breakthroughs to the field of explainable AI. Shayak obtained his PhD in Computer Science from Carnegie Mellon University and BTech in Computer Science from the Indian Institute of Technology, Delhi.\n",
      "\n",
      "\tContext: Most recently, Shameek was Group Chief Data Officer at Standard Chartered Bank, where he helped the bank explore and adopt AI in multiple areas (e.g., credit, financial crime compliance, customer analytics, surveillance), and shaped the bank’s internal approach to responsible AI\n",
      "\n",
      "\tContext: Shameek has spent most of his career in driving responsible adoption of data analytics/ AI in the financial services industry. Most recently, Shameek was Group Chief Data Officer at Standard Chartered Bank, where he helped the bank explore and adopt AI in multiple areas and shaped the bank’s internal approach to responsible AI. He plays an active role in the future of AI as a member of the Bank of England’s AI Public-Private Forum and the OECD Global Partnership on AI.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\tContext: When Shayak started building production grade machine learning models for algorithmic trading 10 years ago, he realized the need for putting the ‘science’ back in ‘data science’. Since then, he has been building systems and leading research to make machine learning and big data systems more explainable, privacy compliant, and fair. Shayak’s research at Carnegie Mellon University introduced a number of pioneering breakthroughs to the field of explainable AI. Shayak obtained his PhD in Computer Science from Carnegie Mellon University and BTech in Computer Science from the Indian Institute of Technology, Delhi.\\n\\n\\tContext: When Shayak started building production grade machine learning models for algorithmic trading 10 years ago, he realized the need for putting the ‘science’ back in ‘data science’. Since then, he has been building systems and leading research to make machine learning and big data systems more explainable, privacy compliant, and fair. Shayak’s research at Carnegie Mellon University introduced a number of pioneering breakthroughs to the field of explainable AI. Shayak obtained his PhD in Computer Science from Carnegie Mellon University and BTech in Computer Science from the Indian Institute of Technology, Delhi.\\n\\n\\tContext: Most recently, Shameek was Group Chief Data Officer at Standard Chartered Bank, where he helped the bank explore and adopt AI in multiple areas (e.g., credit, financial crime compliance, customer analytics, surveillance), and shaped the bank’s internal approach to responsible AI\\n\\n\\tContext: Shameek has spent most of his career in driving responsible adoption of data analytics/ AI in the financial services industry. Most recently, Shameek was Group Chief Data Officer at Standard Chartered Bank, where he helped the bank explore and adopt AI in multiple areas and shaped the bank’s internal approach to responsible AI. He plays an active role in the future of AI as a member of the Bank of England’s AI Public-Private Forum and the OECD Global Partnership on AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tContext Piece: When Shayak started building production grade machine learning models for algorithmic trading 10 years ago, he realized the need for putting the ‘science’ back in ‘data science’. Since then, he has been building systems and leading research to make machine learning and big data systems more explainable, privacy compliant, and fair. Shayak’s research at Carnegie Mellon University introduced a number of pioneering breakthroughs to the field of explainable AI. Shayak obtained his PhD in Computer Science from Carnegie Mellon University and BTech in Computer Science from the Indian Institute of Technology, Delhi..\n",
      "\n",
      "\tContext Piece: When Shayak started building production grade machine learning models for algorithmic trading 10 years ago, he realized the need for putting the ‘science’ back in ‘data science’. Since then, he has been building systems and leading research to make machine learning and big data systems more explainable, privacy compliant, and fair. Shayak’s research at Carnegie Mellon University introduced a number of pioneering breakthroughs to the field of explainable AI. Shayak obtained his PhD in Computer Science from Carnegie Mellon University and BTech in Computer Science from the Indian Institute of Technology, Delhi..\n",
      "\n",
      "\tContext Piece: Most recently, Shameek was Group Chief Data Officer at Standard Chartered Bank, where he helped the bank explore and adopt AI in multiple areas (e.g., credit, financial crime compliance, customer analytics, surveillance), and shaped the bank’s internal approach to responsible AI.\n",
      "\n",
      "\tContext Piece: Shameek has spent most of his career in driving responsible adoption of data analytics/ AI in the financial services industry. Most recently, Shameek was Group Chief Data Officer at Standard Chartered Bank, where he helped the bank explore and adopt AI in multiple areas and shaped the bank’s internal approach to responsible AI. He plays an active role in the future of AI as a member of the Bank of England’s AI Public-Private Forum and the OECD Global Partnership on AI..\n"
     ]
    }
   ],
   "source": [
    "print(\"\\tContext Piece: When Shayak started building production grade machine learning models for algorithmic trading 10 years ago, he realized the need for putting the ‘science’ back in ‘data science’. Since then, he has been building systems and leading research to make machine learning and big data systems more explainable, privacy compliant, and fair. Shayak’s research at Carnegie Mellon University introduced a number of pioneering breakthroughs to the field of explainable AI. Shayak obtained his PhD in Computer Science from Carnegie Mellon University and BTech in Computer Science from the Indian Institute of Technology, Delhi..\\n\\n\\tContext Piece: When Shayak started building production grade machine learning models for algorithmic trading 10 years ago, he realized the need for putting the ‘science’ back in ‘data science’. Since then, he has been building systems and leading research to make machine learning and big data systems more explainable, privacy compliant, and fair. Shayak’s research at Carnegie Mellon University introduced a number of pioneering breakthroughs to the field of explainable AI. Shayak obtained his PhD in Computer Science from Carnegie Mellon University and BTech in Computer Science from the Indian Institute of Technology, Delhi..\\n\\n\\tContext Piece: Most recently, Shameek was Group Chief Data Officer at Standard Chartered Bank, where he helped the bank explore and adopt AI in multiple areas (e.g., credit, financial crime compliance, customer analytics, surveillance), and shaped the bank’s internal approach to responsible AI.\\n\\n\\tContext Piece: Shameek has spent most of his career in driving responsible adoption of data analytics/ AI in the financial services industry. Most recently, Shameek was Group Chief Data Officer at Standard Chartered Bank, where he helped the bank explore and adopt AI in multiple areas and shaped the bank’s internal approach to responsible AI. He plays an active role in the future of AI as a member of the Bank of England’s AI Public-Private Forum and the OECD Global Partnership on AI..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Don't know how to dictify an object '<class 'openai.api_resources.com' of type '<class 'type'>'.\n",
      "WARNING: Don't know how to dictify an object '<class 'langchain.schema.SystemM' of type '<class 'pydantic.main.ModelMetaclass'>'.\n",
      "WARNING: Don't know how to dictify an object '<class 'openai.api_resources.com' of type '<class 'type'>'.\n",
      "WARNING: Don't know how to dictify an object '<class 'openai.api_resources.com' of type '<class 'type'>'.\n",
      "WARNING: Don't know how to dictify an object '<function <lambda> at 0x17977516' of type '<class 'function'>'.\n",
      "WARNING: Don't know how to dictify an object '<langchain.vectorstores.pinecone' of type '<class 'langchain.vectorstores.pinecone.Pinecone'>'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'callbacks': None,\n",
       " 'callback_manager': None,\n",
       " 'verbose': False,\n",
       " 'chain': {'memory': {'human_prefix': 'Human',\n",
       "   'ai_prefix': 'AI',\n",
       "   'llm': {'cache': None,\n",
       "    'verbose': False,\n",
       "    'callbacks': None,\n",
       "    'callback_manager': None,\n",
       "    'client': {'_NON_SERIALIZED_OBJECT': {'class': 'type',\n",
       "      'module': 'builtins',\n",
       "      'bases': ['object']}},\n",
       "    'model_name': 'text-davinci-003',\n",
       "    'temperature': 0.0,\n",
       "    'max_tokens': 128,\n",
       "    'top_p': 1,\n",
       "    'frequency_penalty': 0,\n",
       "    'presence_penalty': 0,\n",
       "    'n': 1,\n",
       "    'best_of': 1,\n",
       "    'model_kwargs': {},\n",
       "    'openai_api_key': None,\n",
       "    'openai_api_base': None,\n",
       "    'openai_organization': None,\n",
       "    'batch_size': 20,\n",
       "    'request_timeout': None,\n",
       "    'logit_bias': {},\n",
       "    'max_retries': 6,\n",
       "    'streaming': False,\n",
       "    'allowed_special': [],\n",
       "    'disallowed_special': 'all'},\n",
       "   'prompt': {'input_variables': ['summary', 'new_lines'],\n",
       "    'output_parser': None,\n",
       "    'partial_variables': {},\n",
       "    'template': 'Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n{summary}\\n\\nNew lines of conversation:\\n{new_lines}\\n\\nNew summary:',\n",
       "    'template_format': 'f-string',\n",
       "    'validate_template': True},\n",
       "   'summary_message_cls': {'_NON_SERIALIZED_OBJECT': {'class': 'ModelMetaclass',\n",
       "     'module': 'pydantic.main',\n",
       "     'bases': ['ABCMeta']}},\n",
       "   'chat_memory': {'messages': []},\n",
       "   'output_key': 'answer',\n",
       "   'input_key': None,\n",
       "   'return_messages': False,\n",
       "   'max_token_limit': 650,\n",
       "   'moving_summary_buffer': '',\n",
       "   'memory_key': 'chat_history'},\n",
       "  'callbacks': None,\n",
       "  'callback_manager': None,\n",
       "  'verbose': False,\n",
       "  'combine_docs_chain': {'memory': None,\n",
       "   'callbacks': None,\n",
       "   'callback_manager': None,\n",
       "   'verbose': False,\n",
       "   'input_key': 'input_documents',\n",
       "   'output_key': 'output_text',\n",
       "   'llm_chain': {'memory': None,\n",
       "    'callbacks': None,\n",
       "    'callback_manager': None,\n",
       "    'verbose': False,\n",
       "    'prompt': {'input_variables': ['context', 'question'],\n",
       "     'output_parser': None,\n",
       "     'partial_variables': {},\n",
       "     'template': \"Use only the relevant contexts to answer the question at the end . Some pieces of context may not be relevant. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nContexts: \\n{context}\\n\\nQuestion: {question}\\nHelpful Answer: \",\n",
       "     'template_format': 'f-string',\n",
       "     'validate_template': True},\n",
       "    'llm': {'cache': None,\n",
       "     'verbose': False,\n",
       "     'callbacks': None,\n",
       "     'callback_manager': None,\n",
       "     'client': {'_NON_SERIALIZED_OBJECT': {'class': 'type',\n",
       "       'module': 'builtins',\n",
       "       'bases': ['object']}},\n",
       "     'model_name': 'text-davinci-003',\n",
       "     'temperature': 0.0,\n",
       "     'max_tokens': 128,\n",
       "     'top_p': 1,\n",
       "     'frequency_penalty': 0,\n",
       "     'presence_penalty': 0,\n",
       "     'n': 1,\n",
       "     'best_of': 1,\n",
       "     'model_kwargs': {},\n",
       "     'openai_api_key': None,\n",
       "     'openai_api_base': None,\n",
       "     'openai_organization': None,\n",
       "     'batch_size': 20,\n",
       "     'request_timeout': None,\n",
       "     'logit_bias': {},\n",
       "     'max_retries': 6,\n",
       "     'streaming': False,\n",
       "     'allowed_special': [],\n",
       "     'disallowed_special': 'all'},\n",
       "    'output_key': 'text'},\n",
       "   'document_prompt': {'input_variables': ['page_content'],\n",
       "    'output_parser': None,\n",
       "    'partial_variables': {},\n",
       "    'template': '\\tContext: {page_content}',\n",
       "    'template_format': 'f-string',\n",
       "    'validate_template': True},\n",
       "   'document_variable_name': 'context',\n",
       "   'document_separator': '\\n\\n'},\n",
       "  'question_generator': {'memory': None,\n",
       "   'callbacks': None,\n",
       "   'callback_manager': None,\n",
       "   'verbose': False,\n",
       "   'prompt': {'input_variables': ['chat_history', 'question'],\n",
       "    'output_parser': None,\n",
       "    'partial_variables': {},\n",
       "    'template': 'Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:',\n",
       "    'template_format': 'f-string',\n",
       "    'validate_template': True},\n",
       "   'llm': {'cache': None,\n",
       "    'verbose': False,\n",
       "    'callbacks': None,\n",
       "    'callback_manager': None,\n",
       "    'client': {'_NON_SERIALIZED_OBJECT': {'class': 'type',\n",
       "      'module': 'builtins',\n",
       "      'bases': ['object']}},\n",
       "    'model_name': 'text-davinci-003',\n",
       "    'temperature': 0.0,\n",
       "    'max_tokens': 128,\n",
       "    'top_p': 1,\n",
       "    'frequency_penalty': 0,\n",
       "    'presence_penalty': 0,\n",
       "    'n': 1,\n",
       "    'best_of': 1,\n",
       "    'model_kwargs': {},\n",
       "    'openai_api_key': None,\n",
       "    'openai_api_base': None,\n",
       "    'openai_organization': None,\n",
       "    'batch_size': 20,\n",
       "    'request_timeout': None,\n",
       "    'logit_bias': {},\n",
       "    'max_retries': 6,\n",
       "    'streaming': False,\n",
       "    'allowed_special': [],\n",
       "    'disallowed_special': 'all'},\n",
       "   'output_key': 'text'},\n",
       "  'output_key': 'answer',\n",
       "  'return_source_documents': True,\n",
       "  'get_chat_history': {'_NON_SERIALIZED_OBJECT': {'class': 'function',\n",
       "    'module': 'builtins',\n",
       "    'bases': ['object']}},\n",
       "  'retriever': {'vectorstore': {'_NON_SERIALIZED_OBJECT': {'class': 'Pinecone',\n",
       "     'module': 'langchain.vectorstores.pinecone',\n",
       "     'bases': ['VectorStore']}},\n",
       "   'search_type': 'similarity',\n",
       "   'search_kwargs': {}},\n",
       "  'max_tokens_limit': 4096},\n",
       " 'chain_id': 'TruBot_langprompt',\n",
       " 'recording': False}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc.chain_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling wrapped chain.\n",
      "WARNING: Don't know how to dictify an object '<class 'openai.api_resources.com' of type '<class 'type'>'.\n",
      "WARNING: Don't know how to dictify an object '<class 'langchain.schema.SystemM' of type '<class 'pydantic.main.ModelMetaclass'>'.\n",
      "WARNING: Don't know how to dictify an object '<class 'openai.api_resources.com' of type '<class 'type'>'.\n",
      "WARNING: Don't know how to dictify an object '<class 'openai.api_resources.com' of type '<class 'type'>'.\n",
      "WARNING: Don't know how to dictify an object '<function <lambda> at 0x17def60d' of type '<class 'function'>'.\n",
      "WARNING: Don't know how to dictify an object '<langchain.vectorstores.pinecone' of type '<class 'langchain.vectorstores.pinecone.Pinecone'>'.\n"
     ]
    }
   ],
   "source": [
    "res, record = tc(dict(question=\"Who is Shayak?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Shayak is a computer scientist who obtained his PhD in Computer Science from Carnegie Mellon University and BTech in Computer Science from the Indian Institute of Technology, Delhi. He has been building systems and leading research to make machine learning and big data systems more explainable, privacy compliant, and fair for the past 10 years.\n"
     ]
    }
   ],
   "source": [
    "print(res['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'callbacks': None,\n",
       " 'callback_manager': None,\n",
       " 'verbose': False,\n",
       " 'chain': {'memory': {'human_prefix': 'Human',\n",
       "   'ai_prefix': 'AI',\n",
       "   'llm': {'cache': None,\n",
       "    'verbose': False,\n",
       "    'callbacks': None,\n",
       "    'callback_manager': None,\n",
       "    'client': {'_NON_SERIALIZED_OBJECT': {'class': 'type',\n",
       "      'module': 'builtins',\n",
       "      'bases': ['object']}},\n",
       "    'model_name': 'text-davinci-003',\n",
       "    'temperature': 0.0,\n",
       "    'max_tokens': 128,\n",
       "    'top_p': 1,\n",
       "    'frequency_penalty': 0,\n",
       "    'presence_penalty': 0,\n",
       "    'n': 1,\n",
       "    'best_of': 1,\n",
       "    'model_kwargs': {},\n",
       "    'openai_api_key': None,\n",
       "    'openai_api_base': None,\n",
       "    'openai_organization': None,\n",
       "    'batch_size': 20,\n",
       "    'request_timeout': None,\n",
       "    'logit_bias': {},\n",
       "    'max_retries': 6,\n",
       "    'streaming': False,\n",
       "    'allowed_special': [],\n",
       "    'disallowed_special': 'all'},\n",
       "   'prompt': {'input_variables': ['summary', 'new_lines'],\n",
       "    'output_parser': None,\n",
       "    'partial_variables': {},\n",
       "    'template': 'Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n{summary}\\n\\nNew lines of conversation:\\n{new_lines}\\n\\nNew summary:',\n",
       "    'template_format': 'f-string',\n",
       "    'validate_template': True},\n",
       "   'summary_message_cls': {'_NON_SERIALIZED_OBJECT': {'class': 'ModelMetaclass',\n",
       "     'module': 'pydantic.main',\n",
       "     'bases': ['ABCMeta']}},\n",
       "   'chat_memory': {'messages': [{'content': 'Who is Shayak?',\n",
       "      'additional_kwargs': {},\n",
       "      'example': False},\n",
       "     {'content': ' Shayak is a computer scientist who obtained his PhD in Computer Science from Carnegie Mellon University and BTech in Computer Science from the Indian Institute of Technology, Delhi. He has been building systems and leading research to make machine learning and big data systems more explainable, privacy compliant, and fair for the past 10 years.',\n",
       "      'additional_kwargs': {},\n",
       "      'example': False}]},\n",
       "   'output_key': 'answer',\n",
       "   'input_key': None,\n",
       "   'return_messages': False,\n",
       "   'max_token_limit': 650,\n",
       "   'moving_summary_buffer': '',\n",
       "   'memory_key': 'chat_history'},\n",
       "  'callbacks': None,\n",
       "  'callback_manager': None,\n",
       "  'verbose': False,\n",
       "  'combine_docs_chain': {'memory': None,\n",
       "   'callbacks': None,\n",
       "   'callback_manager': None,\n",
       "   'verbose': False,\n",
       "   'input_key': 'input_documents',\n",
       "   'output_key': 'output_text',\n",
       "   'llm_chain': {'memory': None,\n",
       "    'callbacks': None,\n",
       "    'callback_manager': None,\n",
       "    'verbose': False,\n",
       "    'prompt': {'input_variables': ['context', 'question'],\n",
       "     'output_parser': None,\n",
       "     'partial_variables': {},\n",
       "     'template': \"Use only the relevant contexts to answer the question at the end . Some pieces of context may not be relevant. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nContexts: \\n{context}\\n\\nQuestion: {question}\\nHelpful Answer: \",\n",
       "     'template_format': 'f-string',\n",
       "     'validate_template': True},\n",
       "    'llm': {'cache': None,\n",
       "     'verbose': False,\n",
       "     'callbacks': None,\n",
       "     'callback_manager': None,\n",
       "     'client': {'_NON_SERIALIZED_OBJECT': {'class': 'type',\n",
       "       'module': 'builtins',\n",
       "       'bases': ['object']}},\n",
       "     'model_name': 'text-davinci-003',\n",
       "     'temperature': 0.0,\n",
       "     'max_tokens': 128,\n",
       "     'top_p': 1,\n",
       "     'frequency_penalty': 0,\n",
       "     'presence_penalty': 0,\n",
       "     'n': 1,\n",
       "     'best_of': 1,\n",
       "     'model_kwargs': {},\n",
       "     'openai_api_key': None,\n",
       "     'openai_api_base': None,\n",
       "     'openai_organization': None,\n",
       "     'batch_size': 20,\n",
       "     'request_timeout': None,\n",
       "     'logit_bias': {},\n",
       "     'max_retries': 6,\n",
       "     'streaming': False,\n",
       "     'allowed_special': [],\n",
       "     'disallowed_special': 'all'},\n",
       "    'output_key': 'text'},\n",
       "   'document_prompt': {'input_variables': ['page_content'],\n",
       "    'output_parser': None,\n",
       "    'partial_variables': {},\n",
       "    'template': '\\tContext: {page_content}',\n",
       "    'template_format': 'f-string',\n",
       "    'validate_template': True},\n",
       "   'document_variable_name': 'context',\n",
       "   'document_separator': '\\n\\n',\n",
       "   '_call': {'args': {'inputs': {'input_documents': [{'page_content': 'When Shayak started building production grade machine learning models for algorithmic trading 10 years ago, he realized the need for putting the ‘science’ back in ‘data science’. Since then, he has been building systems and leading research to make machine learning and big data systems more explainable, privacy compliant, and fair. Shayak’s research at Carnegie Mellon University introduced a number of pioneering breakthroughs to the field of explainable AI. Shayak obtained his PhD in Computer Science from Carnegie Mellon University and BTech in Computer Science from the Indian Institute of Technology, Delhi.',\n",
       "        'metadata': {'source': 'https://truera.com/ai-quality-research'}},\n",
       "       {'page_content': 'When Shayak started building production grade machine learning models for algorithmic trading 10 years ago, he realized the need for putting the ‘science’ back in ‘data science’. Since then, he has been building systems and leading research to make machine learning and big data systems more explainable, privacy compliant, and fair. Shayak’s research at Carnegie Mellon University introduced a number of pioneering breakthroughs to the field of explainable AI. Shayak obtained his PhD in Computer Science from Carnegie Mellon University and BTech in Computer Science from the Indian Institute of Technology, Delhi.',\n",
       "        'metadata': {'source': 'https://truera.com/ai-quality-leader'}},\n",
       "       {'page_content': 'Most recently, Shameek was Group Chief Data Officer at Standard Chartered Bank, where he helped the bank explore and adopt AI in multiple areas (e.g., credit, financial crime compliance, customer analytics, surveillance), and shaped the bank’s internal approach to responsible AI',\n",
       "        'metadata': {'source': 'https://marketing.truera.com/webinar-eu-ai-law'}},\n",
       "       {'page_content': 'Shameek has spent most of his career in driving responsible adoption of data analytics/ AI in the financial services industry. Most recently, Shameek was Group Chief Data Officer at Standard Chartered Bank, where he helped the bank explore and adopt AI in multiple areas and shaped the bank’s internal approach to responsible AI. He plays an active role in the future of AI as a member of the Bank of England’s AI Public-Private Forum and the OECD Global Partnership on AI.',\n",
       "        'metadata': {'source': 'https://marketing.truera.com/trustworthy-ai-podcast'}}],\n",
       "      'question': 'Who is Shayak?',\n",
       "      'chat_history': ''}},\n",
       "    'start_time': '2023-05-17 19:25:15.740791',\n",
       "    'end_time': '2023-05-17 19:25:22.565872',\n",
       "    'pid': 5142,\n",
       "    'tid': 6275196,\n",
       "    'chain_stack': [['chain'], ['chain', 'combine_docs_chain']],\n",
       "    'rets': {'output_text': ' Shayak is a computer scientist who obtained his PhD in Computer Science from Carnegie Mellon University and BTech in Computer Science from the Indian Institute of Technology, Delhi. He has been building systems and leading research to make machine learning and big data systems more explainable, privacy compliant, and fair for the past 10 years.'}}},\n",
       "  'question_generator': {'memory': None,\n",
       "   'callbacks': None,\n",
       "   'callback_manager': None,\n",
       "   'verbose': False,\n",
       "   'prompt': {'input_variables': ['chat_history', 'question'],\n",
       "    'output_parser': None,\n",
       "    'partial_variables': {},\n",
       "    'template': 'Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:',\n",
       "    'template_format': 'f-string',\n",
       "    'validate_template': True},\n",
       "   'llm': {'cache': None,\n",
       "    'verbose': False,\n",
       "    'callbacks': None,\n",
       "    'callback_manager': None,\n",
       "    'client': {'_NON_SERIALIZED_OBJECT': {'class': 'type',\n",
       "      'module': 'builtins',\n",
       "      'bases': ['object']}},\n",
       "    'model_name': 'text-davinci-003',\n",
       "    'temperature': 0.0,\n",
       "    'max_tokens': 128,\n",
       "    'top_p': 1,\n",
       "    'frequency_penalty': 0,\n",
       "    'presence_penalty': 0,\n",
       "    'n': 1,\n",
       "    'best_of': 1,\n",
       "    'model_kwargs': {},\n",
       "    'openai_api_key': None,\n",
       "    'openai_api_base': None,\n",
       "    'openai_organization': None,\n",
       "    'batch_size': 20,\n",
       "    'request_timeout': None,\n",
       "    'logit_bias': {},\n",
       "    'max_retries': 6,\n",
       "    'streaming': False,\n",
       "    'allowed_special': [],\n",
       "    'disallowed_special': 'all'},\n",
       "   'output_key': 'text',\n",
       "   '_call': {'args': {'inputs': {'question': 'Who is Shayak?',\n",
       "      'context': '\\tContext: When Shayak started building production grade machine learning models for algorithmic trading 10 years ago, he realized the need for putting the ‘science’ back in ‘data science’. Since then, he has been building systems and leading research to make machine learning and big data systems more explainable, privacy compliant, and fair. Shayak’s research at Carnegie Mellon University introduced a number of pioneering breakthroughs to the field of explainable AI. Shayak obtained his PhD in Computer Science from Carnegie Mellon University and BTech in Computer Science from the Indian Institute of Technology, Delhi.\\n\\n\\tContext: When Shayak started building production grade machine learning models for algorithmic trading 10 years ago, he realized the need for putting the ‘science’ back in ‘data science’. Since then, he has been building systems and leading research to make machine learning and big data systems more explainable, privacy compliant, and fair. Shayak’s research at Carnegie Mellon University introduced a number of pioneering breakthroughs to the field of explainable AI. Shayak obtained his PhD in Computer Science from Carnegie Mellon University and BTech in Computer Science from the Indian Institute of Technology, Delhi.\\n\\n\\tContext: Most recently, Shameek was Group Chief Data Officer at Standard Chartered Bank, where he helped the bank explore and adopt AI in multiple areas (e.g., credit, financial crime compliance, customer analytics, surveillance), and shaped the bank’s internal approach to responsible AI\\n\\n\\tContext: Shameek has spent most of his career in driving responsible adoption of data analytics/ AI in the financial services industry. Most recently, Shameek was Group Chief Data Officer at Standard Chartered Bank, where he helped the bank explore and adopt AI in multiple areas and shaped the bank’s internal approach to responsible AI. He plays an active role in the future of AI as a member of the Bank of England’s AI Public-Private Forum and the OECD Global Partnership on AI.'}},\n",
       "    'start_time': '2023-05-17 19:25:15.748207',\n",
       "    'end_time': '2023-05-17 19:25:22.565669',\n",
       "    'pid': 5142,\n",
       "    'tid': 6275196,\n",
       "    'chain_stack': [['chain'],\n",
       "     ['chain', 'combine_docs_chain'],\n",
       "     ['chain', 'question_generator']],\n",
       "    'rets': {'text': ' Shayak is a computer scientist who obtained his PhD in Computer Science from Carnegie Mellon University and BTech in Computer Science from the Indian Institute of Technology, Delhi. He has been building systems and leading research to make machine learning and big data systems more explainable, privacy compliant, and fair for the past 10 years.'}}},\n",
       "  'output_key': 'answer',\n",
       "  'return_source_documents': True,\n",
       "  'get_chat_history': {'_NON_SERIALIZED_OBJECT': {'class': 'function',\n",
       "    'module': 'builtins',\n",
       "    'bases': ['object']}},\n",
       "  'retriever': {'vectorstore': {'_NON_SERIALIZED_OBJECT': {'class': 'Pinecone',\n",
       "     'module': 'langchain.vectorstores.pinecone',\n",
       "     'bases': ['VectorStore']}},\n",
       "   'search_type': 'similarity',\n",
       "   'search_kwargs': {}},\n",
       "  'max_tokens_limit': 4096,\n",
       "  '_call': {'args': {'inputs': {'question': 'Who is Shayak?',\n",
       "     'chat_history': ''}},\n",
       "   'start_time': '2023-05-17 19:25:15.310489',\n",
       "   'end_time': '2023-05-17 19:25:22.566261',\n",
       "   'pid': 5142,\n",
       "   'tid': 6275196,\n",
       "   'chain_stack': [['chain']],\n",
       "   'rets': {'answer': ' Shayak is a computer scientist who obtained his PhD in Computer Science from Carnegie Mellon University and BTech in Computer Science from the Indian Institute of Technology, Delhi. He has been building systems and leading research to make machine learning and big data systems more explainable, privacy compliant, and fair for the past 10 years.',\n",
       "    'source_documents': [{'page_content': 'When Shayak started building production grade machine learning models for algorithmic trading 10 years ago, he realized the need for putting the ‘science’ back in ‘data science’. Since then, he has been building systems and leading research to make machine learning and big data systems more explainable, privacy compliant, and fair. Shayak’s research at Carnegie Mellon University introduced a number of pioneering breakthroughs to the field of explainable AI. Shayak obtained his PhD in Computer Science from Carnegie Mellon University and BTech in Computer Science from the Indian Institute of Technology, Delhi.',\n",
       "      'metadata': {'source': 'https://truera.com/ai-quality-research'}},\n",
       "     {'page_content': 'When Shayak started building production grade machine learning models for algorithmic trading 10 years ago, he realized the need for putting the ‘science’ back in ‘data science’. Since then, he has been building systems and leading research to make machine learning and big data systems more explainable, privacy compliant, and fair. Shayak’s research at Carnegie Mellon University introduced a number of pioneering breakthroughs to the field of explainable AI. Shayak obtained his PhD in Computer Science from Carnegie Mellon University and BTech in Computer Science from the Indian Institute of Technology, Delhi.',\n",
       "      'metadata': {'source': 'https://truera.com/ai-quality-leader'}},\n",
       "     {'page_content': 'Most recently, Shameek was Group Chief Data Officer at Standard Chartered Bank, where he helped the bank explore and adopt AI in multiple areas (e.g., credit, financial crime compliance, customer analytics, surveillance), and shaped the bank’s internal approach to responsible AI',\n",
       "      'metadata': {'source': 'https://marketing.truera.com/webinar-eu-ai-law'}},\n",
       "     {'page_content': 'Shameek has spent most of his career in driving responsible adoption of data analytics/ AI in the financial services industry. Most recently, Shameek was Group Chief Data Officer at Standard Chartered Bank, where he helped the bank explore and adopt AI in multiple areas and shaped the bank’s internal approach to responsible AI. He plays an active role in the future of AI as a member of the Bank of England’s AI Public-Private Forum and the OECD Global Partnership on AI.',\n",
       "      'metadata': {'source': 'https://marketing.truera.com/trustworthy-ai-podcast'}}]}}},\n",
       " 'chain_id': 'TruBot_langprompt',\n",
       " 'recording': False}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TruDB.dictify(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hugs = tru_feedback.Huggingface()\n",
    "openai = tru_feedback.OpenAI()\n",
    "\n",
    "f_toxic = tru_feedback.Feedback(hugs.not_toxic).on_response()\n",
    "f_lang_match = tru_feedback.Feedback(hugs.language_match).on(text1=\"prompt\", text2=\"response\")\n",
    "f_relevance = tru_feedback.Feedback(openai.qs_relevance).on(question=\"input\", statement=\"output\")\n",
    "\n",
    "\n",
    "f_lang_match = tru_feedback.Feedback(hugs.language_match).on(text1=\"prompt\", text2=\"response\")\n",
    "f_qs_relevance = tru_feedback.Feedback(openai.qs_relevance) \\\n",
    "    .on(question=\"input\", statement=Record.chain.combine_docs_chain._call.args.inputs.input_documents) \\\n",
    "    .on_multiple(multiarg=\"statement\", each_query=Record.page_content)\n",
    "\n",
    "feedbacks = tru.run_feedback_functions(chain=tc, record=record, feedback_functions=[f_qs_relevance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in TruDB.project(query=Record.chain.combine_docs_chain._call.args.inputs.input_documents, obj=record):\n",
    "    print(doc)\n",
    "    content = TruDB.project(query=Record.page_content, obj=doc)\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e = Endpoint(name=\"openai\", rpm=120)\n",
    "# print(e.pace.qsize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.endpoint_openai.tqdm.display()\n",
    "i = 0\n",
    "while True:\n",
    "    # print(e.pace.qsize())\n",
    "    tru.endpoint_openai.pace_me()\n",
    "    # print(i)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tru_feedback.huggingface_language_match(prompt=\"Hello there?\", response=\"How are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db = LocalTinyDB(\"slackbot.json\")\n",
    "#tru.init_db(\"slackbot.sql\")\n",
    "#db = LocalSQLite(\"slackbot.sql.db\")\n",
    "db = LocalSQLite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, dff = db.get_records_and_feedback(chain_ids=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import PrettyPrinter\n",
    "pp = PrettyPrinter(compact=True)\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    \n",
    "    display(widgets.HTML(f\"<b>Question:</b> {row.input}\"))\n",
    "    \n",
    "    display(widgets.HTML(f\"<b>Answer:</b> {row.output}\"))\n",
    "    \n",
    "    details = json.loads(eval(row.details))\n",
    "\n",
    "    display(widgets.HTML(str(details['chain']['combine_docs_chain']['llm_chain']['prompt']['template'])))\n",
    "    \n",
    "    for doc in details['chain']['combine_docs_chain']['_call']['args']['inputs']['input_documents']:\n",
    "        display(widgets.HTML(f\"\"\"\n",
    "        <div style=\"border: 1px solid black; padding: 5px;\">\n",
    "        <b>Context chunk</b>: {doc['page_content']}\n",
    "        \"\"\"))\n",
    "\n",
    "        \"\"\"<br/>\n",
    "\n",
    "        <b>source</b>: {doc['metadata']['source']}\n",
    "        </div>\"\"\"\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db.select(\n",
    "    Record,\n",
    "    Record.record_id,\n",
    "    Record.chain_id,\n",
    "    Record.chain._call.args.inputs.question,\n",
    "    Record.chain._call.rets.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row_id, row in df.iterrows():\n",
    "    record_id = row.record_id\n",
    "    chain_id = row.chain_id\n",
    "\n",
    "    main_question = row['Record.chain._call.args.inputs.question']\n",
    "    main_answer = row['Record.chain._call.rets.answer']\n",
    "\n",
    "    print(chain_id, record_id, main_question)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(question, answer)\n",
    "\n",
    "    # Run feedback function and get value\n",
    "\n",
    "    feedback = tru.run_feedback_function(\n",
    "        main_question, main_answer, [\n",
    "            tru_feedback.get_not_hate_function(\n",
    "                evaluation_choice='prompt',\n",
    "                provider='openai',\n",
    "                model_engine='moderation'\n",
    "            ),\n",
    "            tru_feedback.get_sentimentpositive_function(\n",
    "                evaluation_choice='response',\n",
    "                provider='openai',\n",
    "                model_engine='gpt-3.5-turbo'\n",
    "            ),\n",
    "            tru_feedback.get_relevance_function(\n",
    "                evaluation_choice='both',\n",
    "                provider='openai',\n",
    "                model_engine='gpt-3.5-turbo'\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    print(f\"will insert overall feedback for chain {chain_id}, record {record_id}\")\n",
    "    db.insert_feedback(record_id=record_id, chain_id=chain_id, feedback=feedback)\n",
    "    \"\"\"\n",
    "    \n",
    "    # display(JSON(row.Record))\n",
    "    # print(row.Record['chain'])\n",
    "\n",
    "    model_name = \"gpt-3.5-turbo\"\n",
    "\n",
    "    \"\"\"\n",
    "    for page in TruDB.project(query=Record.chain.combine_docs_chain._call.args.inputs.input_documents, obj=row.Record):\n",
    "        answer = page['page_content']\n",
    "        feedback = tru.run_feedback_function(\n",
    "            main_question,\n",
    "            answer,\n",
    "\t        [\n",
    "            tru_feedback.get_qs_relevance_function(\n",
    "                evaluation_choice='prompt',\n",
    "                provider='openai',\n",
    "                model_engine=model_name\n",
    "            )]\n",
    "        )\n",
    "        db.insert_feedback(record_id=record_id, chain_id=chain_id, feedback=feedback)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    feedback = tru.run_feedback_function(\n",
    "        main_question, main_answer, [\n",
    "            tru_feedback.get_language_match_function(\n",
    "                provider='huggingface'\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    print(f\"will insert language match feedback for chain {chain_id}, record {record_id}\")\n",
    "    db.insert_feedback(record_id=record_id, chain_id=chain_id, feedback=feedback)\n",
    "\n",
    "    # feedback = tru.run_feedback_function(\n",
    "\n",
    "    #for leaf in TruDB.leafs(row.Record):\n",
    "    #    print(leaf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row.record_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback = {'openai_hate_function': 1.849137515819166e-05,\n",
    " 'openai_sentimentpositive_feedback_function': 1,\n",
    " 'openai_relevance_function': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.insert_feedback(2, feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.select(Record, table=db.feedbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-3.5-turbo\"\n",
    "feedback = tru.run_feedback_function(\n",
    "    \"Who is Piotr?\",\n",
    "    \"Piotr Mardziel works on transparency and accountability in machine learning with applications to security, privacy, and fairness. He holds Bachelor’s and Master’s degrees from the Worcester Polytechnic Institute and a PhD in computer science from University of Maryland, College Park. He has conducted post-doctoral research at Carnegie Mellon University, as well as taught classes in trustworthy machine learning at Stanford University and machine learning privacy and security at Carnegie Mellon University.\",\n",
    "\t [\n",
    "        tru_feedback.get_qs_relevance_function(\n",
    "            evaluation_choice='prompt',\n",
    "            provider='openai',\n",
    "            model_engine=model_name\n",
    "        )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
