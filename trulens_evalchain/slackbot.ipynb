{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slackbot-related work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got OPENAI_API_KEY\n",
      "got PINECONE_API_KEY\n",
      "got PINECONE_ENV\n",
      "got HUGGINGFACE_API_KEY\n",
      "got SLACK_TOKEN\n",
      "got SLACK_SIGNING_SECRET\n",
      "got COHERE_API_KEY\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ecf015d286042a6858f12bea3f6af70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openai: 0request [00:00, ?request/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08133e05a454daaaf178d3b8ddd3b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "huggingface: 0request [00:00, ?request/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4f69f26b5d41399d46336ce071ea2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cohere: 0request [00:00, ?request/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trulens_evalchain.tru_db import Record, LocalTinyDB, TruDB, LocalSQLite\n",
    "from trulens_evalchain import tru\n",
    "from trulens_evalchain import tru_feedback\n",
    "from IPython.display import JSON\n",
    "from ipywidgets import widgets\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: will not be able to serialize object of type <class 'langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain'> because it has memory.\n",
      "WARNING: will not be able to serialize object of type <class 'langchain.chains.conversational_retrieval.base.BaseConversationalRetrievalChain'> because it has memory.\n",
      "WARNING: will not be able to serialize object of type <class 'langchain.chains.base.Chain'> because it has memory.\n",
      "WARNING: do not know how to instrument <langchain.vectorstores.pinecone.Pinecone object at 0x1666f6af0>\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms.base import BaseLLM\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "\n",
    "from trulens_evalchain import tru\n",
    "from trulens_evalchain import tru_chain\n",
    "from trulens_evalchain.keys import *\n",
    "from trulens_evalchain.keys import PINECONE_API_KEY\n",
    "from trulens_evalchain.keys import PINECONE_ENV\n",
    "\n",
    "# Set up GPT-3 model\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "\n",
    "chain_id = \"TruBot_langprompt\"\n",
    "\n",
    "# Pinecone configuration.\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "    environment=PINECONE_ENV  # next to api key in console\n",
    ")\n",
    "\n",
    "identity = lambda h: h\n",
    "\n",
    "# Embedding needed for Pinecone vector db.\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-ada-002')  # 1536 dims\n",
    "docsearch = Pinecone.from_existing_index(\n",
    "    index_name=\"llmdemo\", embedding=embedding\n",
    ")\n",
    "retriever = docsearch.as_retriever()\n",
    "\n",
    "# LLM for completing prompts, and other tasks.\n",
    "llm = OpenAI(temperature=0, max_tokens=128)\n",
    "\n",
    "# Conversation memory.\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    max_token_limit=650,\n",
    "    llm=llm,\n",
    "    memory_key=\"chat_history\",\n",
    "    output_key='answer'\n",
    ")\n",
    "\n",
    "# Conversational chain puts it all together.\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    memory=memory,\n",
    "    get_chat_history=identity,\n",
    "    max_tokens_limit=4096\n",
    ")\n",
    "\n",
    "# Language mismatch fix:\n",
    "chain.combine_docs_chain.llm_chain.prompt.template = \\\n",
    "    \"Use the following pieces of context to answer the question at the end \" \\\n",
    "    \"in the same language as the question. If you don't know the answer, \" \\\n",
    "    \"just say that you don't know, don't try to make up an answer.\\n\\n\" \\\n",
    "    \"{context}\\n\\n\" \\\n",
    "    \"Question: {question}\\n\" \\\n",
    "    \"Helpful Answer: \"\n",
    "\n",
    "# Trulens instrumentation.\n",
    "tc = tru_chain.TruChain(chain, chain_id=chain_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM ('chain', 'memory', 'llm')\n",
      "LLM ('chain', 'combine_docs_chain', 'llm_chain', 'llm')\n",
      "LLM ('chain', 'question_generator', 'llm')\n"
     ]
    }
   ],
   "source": [
    "for query, obj in TruDB.matching_objects(tc, match=lambda q, o: len(q._path) > 0 and \"llm\" == q._path[-1]):\n",
    "    print(\"LLM\", query._path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, record = tc(dict(question=\"Who is Shayak?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hugs = tru_feedback.Huggingface()\n",
    "openai = tru_feedback.OpenAI()\n",
    "\n",
    "f_toxic = tru_feedback.Feedback(hugs.not_toxic).on_response()\n",
    "f_lang_match = tru_feedback.Feedback(hugs.language_match).on(text1=\"prompt\", text2=\"response\")\n",
    "f_relevance = tru_feedback.Feedback(openai.qs_relevance).on(question=\"input\", statement=\"output\")\n",
    "\n",
    "\n",
    "f_lang_match = tru_feedback.Feedback(hugs.language_match).on(text1=\"prompt\", text2=\"response\")\n",
    "f_qs_relevance = tru_feedback.Feedback(openai.qs_relevance) \\\n",
    "    .on(question=\"input\", statement=Record.chain.combine_docs_chain._call.args.inputs.input_documents) \\\n",
    "    .on_multiple(multiarg=\"statement\", each_query=Record.page_content)\n",
    "\n",
    "feedbacks = tru.run_feedback_functions(chain=tc, record=record, feedback_functions=[f_qs_relevance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in TruDB.project(query=Record.chain.combine_docs_chain._call.args.inputs.input_documents, obj=record):\n",
    "    print(doc)\n",
    "    content = TruDB.project(query=Record.page_content, obj=doc)\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e = Endpoint(name=\"openai\", rpm=120)\n",
    "# print(e.pace.qsize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.endpoint_openai.tqdm.display()\n",
    "i = 0\n",
    "while True:\n",
    "    # print(e.pace.qsize())\n",
    "    tru.endpoint_openai.pace_me()\n",
    "    # print(i)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tru_feedback.huggingface_language_match(prompt=\"Hello there?\", response=\"How are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db = LocalTinyDB(\"slackbot.json\")\n",
    "#tru.init_db(\"slackbot.sql\")\n",
    "#db = LocalSQLite(\"slackbot.sql.db\")\n",
    "db = LocalSQLite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, dff = db.get_records_and_feedback(chain_ids=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import PrettyPrinter\n",
    "pp = PrettyPrinter(compact=True)\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    \n",
    "    display(widgets.HTML(f\"<b>Question:</b> {row.input}\"))\n",
    "    \n",
    "    display(widgets.HTML(f\"<b>Answer:</b> {row.output}\"))\n",
    "    \n",
    "    details = json.loads(eval(row.details))\n",
    "\n",
    "    display(widgets.HTML(str(details['chain']['combine_docs_chain']['llm_chain']['prompt']['template'])))\n",
    "    \n",
    "    for doc in details['chain']['combine_docs_chain']['_call']['args']['inputs']['input_documents']:\n",
    "        display(widgets.HTML(f\"\"\"\n",
    "        <div style=\"border: 1px solid black; padding: 5px;\">\n",
    "        <b>Context chunk</b>: {doc['page_content']}\n",
    "        \"\"\"))\n",
    "\n",
    "        \"\"\"<br/>\n",
    "\n",
    "        <b>source</b>: {doc['metadata']['source']}\n",
    "        </div>\"\"\"\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db.select(\n",
    "    Record,\n",
    "    Record.record_id,\n",
    "    Record.chain_id,\n",
    "    Record.chain._call.args.inputs.question,\n",
    "    Record.chain._call.rets.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row_id, row in df.iterrows():\n",
    "    record_id = row.record_id\n",
    "    chain_id = row.chain_id\n",
    "\n",
    "    main_question = row['Record.chain._call.args.inputs.question']\n",
    "    main_answer = row['Record.chain._call.rets.answer']\n",
    "\n",
    "    print(chain_id, record_id, main_question)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(question, answer)\n",
    "\n",
    "    # Run feedback function and get value\n",
    "\n",
    "    feedback = tru.run_feedback_function(\n",
    "        main_question, main_answer, [\n",
    "            tru_feedback.get_not_hate_function(\n",
    "                evaluation_choice='prompt',\n",
    "                provider='openai',\n",
    "                model_engine='moderation'\n",
    "            ),\n",
    "            tru_feedback.get_sentimentpositive_function(\n",
    "                evaluation_choice='response',\n",
    "                provider='openai',\n",
    "                model_engine='gpt-3.5-turbo'\n",
    "            ),\n",
    "            tru_feedback.get_relevance_function(\n",
    "                evaluation_choice='both',\n",
    "                provider='openai',\n",
    "                model_engine='gpt-3.5-turbo'\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    print(f\"will insert overall feedback for chain {chain_id}, record {record_id}\")\n",
    "    db.insert_feedback(record_id=record_id, chain_id=chain_id, feedback=feedback)\n",
    "    \"\"\"\n",
    "    \n",
    "    # display(JSON(row.Record))\n",
    "    # print(row.Record['chain'])\n",
    "\n",
    "    model_name = \"gpt-3.5-turbo\"\n",
    "\n",
    "    \"\"\"\n",
    "    for page in TruDB.project(query=Record.chain.combine_docs_chain._call.args.inputs.input_documents, obj=row.Record):\n",
    "        answer = page['page_content']\n",
    "        feedback = tru.run_feedback_function(\n",
    "            main_question,\n",
    "            answer,\n",
    "\t        [\n",
    "            tru_feedback.get_qs_relevance_function(\n",
    "                evaluation_choice='prompt',\n",
    "                provider='openai',\n",
    "                model_engine=model_name\n",
    "            )]\n",
    "        )\n",
    "        db.insert_feedback(record_id=record_id, chain_id=chain_id, feedback=feedback)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    feedback = tru.run_feedback_function(\n",
    "        main_question, main_answer, [\n",
    "            tru_feedback.get_language_match_function(\n",
    "                provider='huggingface'\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    print(f\"will insert language match feedback for chain {chain_id}, record {record_id}\")\n",
    "    db.insert_feedback(record_id=record_id, chain_id=chain_id, feedback=feedback)\n",
    "\n",
    "    # feedback = tru.run_feedback_function(\n",
    "\n",
    "    #for leaf in TruDB.leafs(row.Record):\n",
    "    #    print(leaf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row.record_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback = {'openai_hate_function': 1.849137515819166e-05,\n",
    " 'openai_sentimentpositive_feedback_function': 1,\n",
    " 'openai_relevance_function': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.insert_feedback(2, feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.select(Record, table=db.feedbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-3.5-turbo\"\n",
    "feedback = tru.run_feedback_function(\n",
    "    \"Who is Piotr?\",\n",
    "    \"Piotr Mardziel works on transparency and accountability in machine learning with applications to security, privacy, and fairness. He holds Bachelor’s and Master’s degrees from the Worcester Polytechnic Institute and a PhD in computer science from University of Maryland, College Park. He has conducted post-doctoral research at Carnegie Mellon University, as well as taught classes in trustworthy machine learning at Stanford University and machine learning privacy and security at Carnegie Mellon University.\",\n",
    "\t [\n",
    "        tru_feedback.get_qs_relevance_function(\n",
    "            evaluation_choice='prompt',\n",
    "            provider='openai',\n",
    "            model_engine=model_name\n",
    "        )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
