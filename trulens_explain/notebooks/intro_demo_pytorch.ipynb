{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install trulens\n",
    "!{sys.executable} -m pip install torchvision\n",
    "!{sys.executable} -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Wrappers\n",
    "\n",
    "In order to support a wide variety of backends with different interfaces for their respective models, TruLens uses its own `ModelWrapper` class which provides a general model interface to simplify the implementation of the API functions.\n",
    "To get the model wrapper, use the `get_model_wrapper` method in `trulens.nn.models`. A model wrapper class exists for each backend that converts a model in the respective backend's format to the general TruLens `ModelWrapper` interface. The wrappers are found in the `models` module, and any model defined using Keras, Pytorch, or Tensorflow should be wrapped before being used with the other API functions that require a model -- all other TruLens functionalities expect models to be an instance of `trulens.nn.models.ModelWrapper`.\n",
    "\n",
    "For more details on allowed parameters, see the [get_model_wrapper](https://truera.github.io/trulens/api/model_wrappers/) documentation.\n",
    "\n",
    "For this demo, we will be using a Pytorch model pre-trained on Imagenet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "from trulens.nn.models import get_model_wrapper\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model = models.vgg16(pretrained=True)\n",
    "device = \"cpu\"\n",
    "# Produce a wrapped model from the pytorch model.\n",
    "model = get_model_wrapper(pytorch_model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_layer_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loaded our model, lets take a quick look at its predictions on the following record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "# Load the example record.\n",
    "url = \"https://raw.githubusercontent.com/truera/trulens/main/trulens_explain/notebooks/resources/dog_bike.jpg\"\n",
    "with PIL.Image.open(requests.get(url, stream=True).raw) as img:\n",
    "    # View the image.\n",
    "    plt.imshow(img)\n",
    "\n",
    "    x = np.array(img.resize((224, 224), PIL.Image.LANCZOS))[np.newaxis]\n",
    "\n",
    "    # Normalize with https://pytorch.org/docs/stable/torchvision/models.html\n",
    "    normalize = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),  # convert to [0, 1]\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    x_pp = np.array(normalize(x[0])).transpose(1, 2, 0)[np.newaxis]\n",
    "\n",
    "    # Transpose to [*, C, H, W] for PyTorch convention.\n",
    "    x = x.transpose(0, 3, 1, 2)\n",
    "    x_pp = x_pp.transpose(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "# Pretty-print the model's top 5 predictions on this record.\n",
    "with torch.no_grad():\n",
    "    output = pytorch_model(torch.from_numpy(x_pp).to(device)).cpu().numpy()\n",
    "\n",
    "with open(\"imagenet_class_index.json\") as file:\n",
    "    class_idx = json.load(file)\n",
    "\n",
    "idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]\n",
    "top_labels = [\n",
    "    (idx, idx2label[idx], output[0][idx]) for idx in np.argsort(output[0])[::-1][:5]\n",
    "]\n",
    "\n",
    "print(\"\\n\".join([str(label) for label in top_labels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we'll show how to use the TruLens API to further investigate the model's predictions on this record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribution Methods\n",
    "\n",
    "*Attribution methods*, in the most general sense, allow us to quantify the contribution of particular variables in a model towards a particular behavior of the model. \n",
    "In many cases, for example, this may simply measure the effect each input variable has on the output of the network.\n",
    "\n",
    "Attribution methods extend the `AttributionMethod` class, and many concrete instances are found in the `attribution` module.\n",
    "\n",
    "Once an attribution method has been instantiated, its main function is its `attributions` method, which takes an `np.Array` of batched records, where each record matches the shape of the input to the model the attribution method was instantiated with.\n",
    "In this demo, we will mostly make use of the `InternalInfluence` attribution method, as it is the most general, though we will compare it briefly with the other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started: Saliency Maps and Integrated Gradients\n",
    "\n",
    "The first methods that we will examine directly measure the importance of input features on the classification of the model. These are the most straightforward type of attributions to compute, as there are relatively few choices to make.\n",
    "\n",
    "* _Saliency maps_ take the gradient of the network's predicted class on a given input. See the reference by Simonyan et al. given below for more details and discussion of this method.\n",
    "* _Integrated gradients_ addresses some issues with saliency maps described by Sundararajan et al., and proposes a way of addressing them by aggregating the gradient on a linear subspace of the model's input features. Because this aggregation cannot be computed exactly in general, the implementation evaluates the gradient on a specified number of samples.\n",
    "\n",
    "We will see how to use each of these methods using the API, and how to visualize the results.\n",
    "\n",
    "---\n",
    "Karen Simonyan, Andrea Vedaldi, Andrew Zisserman. *Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps*. ICLR 2014. [ArXiv](https://arxiv.org/pdf/1312.6034.pdf)\n",
    "\n",
    "Mukund Sundararajan, Ankur Taly, Qiqi Yan. *Axiomatic Attribution for Deep Networks* [PMLR](http://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.nn.attribution import InputAttribution\n",
    "from trulens.nn.attribution import IntegratedGradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saliency maps are implemented by the `InputAttribution` class. This takes several optional arguments, the meaning of which we will discuss later in this notebook. The provided defaults instantiate an `AttributionMethod` that is consistent with the method described in the reference above.\n",
    "\n",
    "The required argument to the constructor is a `ModelWrapper`. After constructing the attribution method, we call it on our data point, and receive an array containing the attributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infl = InputAttribution(model)\n",
    "attrs_input = infl.attributions(x_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the attributions, we can use `MaskVisualizer` from the `visualizations` module. This class takes a `blur` and `threshold` argument, and allows us to overlay a partially-opaque mask over a given image that reveals the top-threshold percentage of pixels by attribution, after applying a Gaussian blur of the given radius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.visualizations import MaskVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_image = MaskVisualizer(blur=5, threshold=0.95)(attrs_input, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning to Integrated Gradients, the workflow for obtaining attributions is nearly identical. The only difference is that the `AttributionMethod` instance we construct is one of `IntegratedGradients` rather than `InputGradients`.\n",
    "\n",
    "The `resolution` argument, which is optional and defaults to 50, specifies the number of samples to take. Larger values approximate the true aggregate quantity more closely, and in practical terms, tend to be more stable.\n",
    "\n",
    "Another optional argument is the `baseline`, which specifies the linear subspace over which the quantity is aggregated. By default this is `None`, which is interpreted as an appropriately-sized zero tensor. In this case the linear subspace is the line between this zero tensor, and the point for which the attributions are computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infl = IntegratedGradients(model, resolution=10)\n",
    "attrs_input = infl.attributions(x_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the results, it is apparent that Integrated Gradients in this case is better able to focus on the pixels corresponding to the beagle, which is consistent with the model's top predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_image = MaskVisualizer(blur=5, threshold=0.95)(attrs_input, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discovering Important Internal Neurons \n",
    "\n",
    "Now we'll examine is *Internal Influence* (Leino et al.), a powerful and general attribution method that can calculate attributions for internal neurons in a network as well as for the inputs to the network. Internal Influence is implemented by the `InternalInfluence` class in the `attribution` module.\n",
    "\n",
    "The `InternalInfluence` constructor takes a TruLens `ModelWrapper` and three special parameters: a *slice*, a *quantity of interest* (QoI), and a *distribution of interest* (DoI), which are instances of the `Slice` (in the `slices` module), `QoI` (in the `quantities` module), and `DoI` (in the `distributions` module) classes, respectively.\n",
    "\n",
    "The slice essentially defines a layer to use for internal attributions. A `Slice` object specifies two `Cut`s corresponding to two layers: (1) the layer of the variables that we are calculating attribution *for* (e.g., the input layer), and (2) the layer whose output defines our quantity of interest (e.g., the output layer; see below for more on quantities of interest).\n",
    "\n",
    "The shape of the attributions will always match the shape of the first cut. In the case of `InputAttribution`, it is the shape of the input. For neuron explanations, the attributions can take the shape of the output or input of a specific network layer. The default behavior is to create attributions for the output of a layer, but this can be specified via the `anchor` in the `Cut` class. See the [Slice](https://truera.github.io/trulens/api/slices/) documentation for more detail.\n",
    "\n",
    "The quantity of interest (QoI) essentially defines the model behavior we would like to explain using attributions. The QoI is a function of the model's output at some layer. For example, it may select the confidence score for a particular class. In its most general form, the QoI can be pecified by an implementation of the `QoI` class in the `quantities` module. Several common default implementations are provided in this module as well.\n",
    "\n",
    "The distribution of interest (DoI) essentially specifies points surrounding each record for which the calculated attribution should be faithful. The distribution can be specified via an implementation of the `DoI` class in the `distributions` module, which is a function taking an input record and producing a list of input points to aggregate attribution over. A few common default distributions implementing the `DoI` class can be found in the `distributions` module.\n",
    "\n",
    "---\n",
    "\n",
    "* Klas Leino, Shayak Sen, Anupam Datta, Matt Fredrikson, and Linyi Li. *Influence-Directed Explanations for Deep Convolutional Networks*. IEEE ITC 2018. [ArXiv](https://arxiv.org/pdf/1802.03788.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.nn.attribution import InternalInfluence\n",
    "from trulens.nn.distributions import PointDoi\n",
    "from trulens.nn.quantities import InternalChannelQoI, MaxClassQoI\n",
    "from trulens.nn.slices import Cut, InputCut, OutputCut, Slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be calculating attributions for the feature maps in the layer labeled `'features_28'` (specified via the slice below). In our first example, we are interested in explaining the model's *predicted class* for our record. We specify this by using a `MaxClassQoI`, which sets the attributions to explain the model's output for its highest-confidence class. We will initially use the `PointDoI` which specifies that we are only concerned with the model's behavior on one particular point, i.e., we want a very *local* explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the influence measure.\n",
    "infl = InternalInfluence(\n",
    "    model, Slice(Cut(\"features_28\"), OutputCut()), MaxClassQoI(), PointDoi()\n",
    ")\n",
    "\n",
    "# Get the attributions for the internal neurons at layer -10. Because layer -10\n",
    "# contains 2D feature maps, we take the sum over the width and height of the\n",
    "# feature maps to obtain a single attribution for each feature map.\n",
    "attrs_internal = infl.attributions(x_pp).sum(axis=(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that above we used the `Slice`, `MaxClassQoI`, and `PointDoI` classes to define the slice, QoI, and DoI. The TruLens API also offers several simple shorthands for defining these parameters more simply. For example, the above code could be more succinctly written as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the influence measure.\n",
    "infl = InternalInfluence(model, \"features_28\", \"max\", \"point\")\n",
    "\n",
    "# Get the attributions for the internal neurons at layer 'features_28'. Because\n",
    "# layer 'features_28' contains 2D feature maps, we take the sum over the width\n",
    "# and height of the feature maps to obtain a single attribution for each feature\n",
    "# map.\n",
    "attrs_internal = infl.attributions(x_pp).sum(axis=(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the most important feature map towards the model's top prediction, by taking the argmax over the internal attributions for this record. The most important feature map represents some type of *learned feature* that was the *most important* in the network's decision to label this point as `'beagle'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feature_map = int(attrs_internal[0].argmax())\n",
    "\n",
    "print(\"Top feature map:\", top_feature_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Important Internal Neurons\n",
    "\n",
    "We would now like to visualize our identified feature map in a meaningful way. Since the feature map represents a learned feature, which is not readily interpretable, we will use a second set of attributions to identify the input features that are most important in defining this particular feature map. We will then use a *visualizer*, found in the `visualizations` module, to visualize these input features that relate to our identified important feature map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.visualizations import MaskVisualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create another attributer, again using `InternalInfluence`. This time, we specify our slice to begin at the input of the model and end at layer `'features_28`, the layer of our identified feature map. We select our quantity of interest to be an `InternalChannelQoI` - this allows us to specify a particular channel that we want to calculate attributions towards (in our case we specify this channel to be our identified feature map). We will again use the `Point` DoI.\n",
    "\n",
    "Note that if we simply give the top feature map to `InternalInfluence`, it will automatically wrap it in an instance of `InternalChannelQoI` for us; additionally, the `Slice` object is inferred from the tuple of cuts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infl_input = InternalInfluence(\n",
    "    model,\n",
    "    Slice(InputCut(), Cut(\"features_28\")),\n",
    "    InternalChannelQoI(top_feature_map),\n",
    "    PointDoi(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the above code can be simplified to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infl_input = InternalInfluence(\n",
    "    model, (InputCut(), Cut(\"features_28\")), top_feature_map, \"point\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the input attributions and visualize the top feature map by using the input attributions as a mask over the original image, using the `MaskVisualizer` (found in the `visualizations` module).\n",
    "\n",
    "The `MaskVisualizer` takes two fine-tuning parameters, `blur` and `threshold`, that affect the quality of the visualization. The attributions are first blurred using a Gaussian blur with radius `blur`, and then only the pixels whose blurred attribution value are at or above the percentile given by `threshold` are highlighted. Depending on the particular record and application, different `blur` and `threshold` parameters may be appropriate.\n",
    "\n",
    "Increasing `blur` gives a more abstract, region-focused explanation, while a smaller blur gives a noisier, but more precise explanation.\n",
    "\n",
    "Increasing `threshold` selects a smaller portion of the image to highlight, showing only the most important regions, while a smaller threshold will highlight a larger portion of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs_input = infl_input.attributions(x_pp)\n",
    "\n",
    "masked_image = MaskVisualizer(blur=10, threshold=0.95)(attrs_input, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above procedure &mdash; using a second set of attributions to identify the input features that are most important in defining a particular feature map, then using a visualizer on the resulting input attributions &mdash; is a common use case when dealing with internal attributions. This procedure can instead be done via a single step, using a `ChannelMaskVisualizer` also found in the `visualizations` module, demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.visualizations import ChannelMaskVisualizer\n",
    "\n",
    "masked_image = ChannelMaskVisualizer(\n",
    "    model, \"features_28\", top_feature_map, blur=10, threshold=0.95\n",
    ")(x, x_pp)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(masked_image[0].transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Quantities of Interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also change the quantity that we want the attributions to explain. For example, our example image contains both a bike and a dog. Recall that while the top class predicted by our model was `'beagle'`, imagenet also contains bike-related classes, e.g., `'mountain bike, all-terrain bike, off-roader'`. We will use the `ClassQoI` to view the attributions towards the class `'mountain bike, all-terrain bike, off-roader'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the influence measure.\n",
    "infl_bike = InternalInfluence(model, \"features_28\", 671, \"point\")\n",
    "\n",
    "# The above is shorthand for\n",
    "#\n",
    "# infl_bike = InternalInfluence(\n",
    "#     model,\n",
    "#     Slice(Cut('features_28', OutputCut()),\n",
    "#     ClassQoI(671),\n",
    "#     PointDoi())\n",
    "\n",
    "# Get the attributions for each feature map.\n",
    "attrs_bike_internal = infl_bike.attributions(x_pp).sum(axis=(2, 3))\n",
    "\n",
    "# Find the index of the top feature map.\n",
    "top_feature_map_bike = int(attrs_bike_internal[0].argmax())\n",
    "\n",
    "print(\"Top feature map:\", top_feature_map_bike)\n",
    "\n",
    "# Visualize the top feature map in the input space.\n",
    "masked_image = ChannelMaskVisualizer(\n",
    "    model, \"features_28\", top_feature_map_bike, blur=10, threshold=0.95\n",
    ")(x, x_pp)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(masked_image[0].transpose((1, 2, 0)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "test-fresh-11-29",
   "language": "python",
   "name": "fresh-11-29"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
