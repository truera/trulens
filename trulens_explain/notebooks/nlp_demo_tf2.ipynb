{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Trulens\n",
    "!{sys.executable} -m pip install \"git+https://github.com/truera/trulens.git#subdirectory=trulens_explain\"\n",
    "!{sys.executable} -m pip install 'tensorflow-text~=2.11.0'\n",
    "!{sys.executable} -m pip install 'tf-models-official~=2.11.0'\n",
    "!{sys.executable} -m pip install 'protobuf==3.20.0'\n",
    "!{sys.executable} -m pip install -U tensorflow-hub gdown opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from IPython.display import HTML as html_print\n",
    "import plotly.express as px\n",
    "\n",
    "from trulens.nn.models import get_model_wrapper\n",
    "from trulens.nn.attribution import InternalInfluence\n",
    "from trulens.nn.slices import OutputCut, Slice, Cut\n",
    "from trulens.nn.quantities import MaxClassQoI\n",
    "from trulens.nn.distributions import LinearDoi\n",
    "import gdown\n",
    "\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdown.download(\n",
    "    id=\"1-bVFx-qU_kD7gGqV2E8ucRrV0LKFxHzB\", output=\"resources.zip\", quiet=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download notebook resources.\n",
    "!mkdir -p resources\n",
    "!unzip -o -d resources resources.zip\n",
    "!rm resources.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model\n",
    "The notebook resources include a model checkpoint. The model uses the Tensorflow Hub [Text Preprocessing layer](https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3) and [Small Bert layer](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/2) followed by several convolutional and fully connected layers.\n",
    "\n",
    "The model has already been trained on a sentiment analysis task with the Covid-19 Tweets dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"classifierbert-cnn\"\n",
    "\n",
    "model = tf.keras.models.load_model(\"./resources/\" + model_name, compile=False)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Vocabulary\n",
    "We also load the vocabulary behind the model. This helps us translate our token IDs back into tokenized words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = f\"./resources/{model_name}/assets/vocab.txt\"\n",
    "with open(vocab_file) as f:\n",
    "    vocab = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model describes the sentiment of tweets into 5 classes: positive, extremely positive, negative, extremely negative, or neutral. Lets try it out on some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Fill up the fridge with enough food, ready medical supplies, water, avoid crowd, be updated with news, dont panic, work from home if feasible, boost immune system by drinking vitamins and always wash hands. Stay safe and healthy! #COVID2019 #metroManilaCovid\",\n",
    "    \"Big thanks to all the retail, supermarket workers &amp; nurses out there. This is mental and the subsequent panic buying and rise in cases shows just how important they are #Covid_19\",\n",
    "    \"I understand food being out of stock, but why toilet paper? what's up with that? #covid_19 #coronavirus\",\n",
    "    \"This Friday the 13th is a nightmare for supermarket employees. People are panic buying a day after Duterte announced an NCR lockdown. Carts are filled w/ all sorts of noodles. I guess these Metro Manila residents will be on pancit canton/bihon diet for a month #Covid_19 https://t.co/33Bw2ZKnds\",\n",
    "    \"Food, emergency supply stores struggle to meet demand #coronavirus #yzf https://t.co/wZ6yLBU2rl https://t.co/2Ef6Fy9u8y\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    \"Extremely Negative\",\n",
    "    \"Negative\",\n",
    "    \"Neutral\",\n",
    "    \"Positive\",\n",
    "    \"Extremely Positive\",\n",
    "]\n",
    "\n",
    "predictions = model(tf.constant(sentences)).numpy()\n",
    "for sentence, pred in zip(sentences, predictions):\n",
    "    print(f\"Predicted {classes[np.argmax(pred)]}: '{sentence}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Wrapper\n",
    "\n",
    "As in the prior notebooks, we need to wrap the model with the appropriate Trulens functionality. As we are using a tf.keras model, it should be specified in the backend parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_model = get_model_wrapper(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attributions\n",
    "\n",
    "The model takes in text as input, which get tokenized in the `preprocessing` layer and translated into embeddings in the `BERT_encoder` layer. Since we cannot take the gradient with respect to the raw input text or tokenized text directly, we must use the embedding representation of our inputs.\n",
    "\n",
    "Below, we can inspect the available layers in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[layer_name for layer_name in k_model._layers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Above, `BERT_encoder/bert_encoder/word_embeddings` is the layer that produces a continuous representation of each input token so we will use that layer as the one defining the **distribution of interest**. While most neural NLP models contain a token embedding, the layer name will differ.\n",
    "\n",
    "The second thing to note is the form of model outputs. Specifically, outputs are structures which contain a 'logits' attribute that stores the model scores.\n",
    "\n",
    "Putting these things together, we instantiate `InternalInfluence` to attribute each embedding dimension to the maximum class (i.e. the predicted class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer_name = \"BERT_encoder/bert_encoder/word_embeddings\"\n",
    "\n",
    "infl = InternalInfluence(\n",
    "    model=k_model,\n",
    "    cuts=Slice(Cut(embedding_layer_name, anchor=\"out\"), OutputCut()),\n",
    "    qoi=MaxClassQoI(),\n",
    "    doi=LinearDoi(resolution=10, cut=Cut(embedding_layer_name, anchor=\"in\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the preprocessing step to tokenize our input text. Using the model vocabulary, the token IDs (`sentence_encodings`) can be translated back into tokenized words (`tokens`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessor\n",
    "inp = model.input\n",
    "preprocessing_layer = model.get_layer(\"preprocessing\").get_output_at(-1)\n",
    "pp_func = tf.keras.backend.function(inp, preprocessing_layer)\n",
    "\n",
    "sentence_encodings = pp_func(tf.constant(sentences))[\"input_word_ids\"]\n",
    "tokens = [[vocab[i] for i in sentence] for sentence in sentence_encodings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting attributions uses the same call as model evaluation and returns a tensor. We can aggregate the attributions across the embedding dimension to get an approximate look at the influence of each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs_internal = infl.attributions(np.array(sentences))\n",
    "total_attrs = attrs_internal.sum(axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Influences\n",
    "Here we display visualizations that describe the influence of each token on the final prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_str(r, g, b):\n",
    "    return \"rgb(%d,%d,%d)\" % (r, g, b)\n",
    "\n",
    "\n",
    "def cstr(s, color=\"black\", background=\"white\"):\n",
    "    return \"<text style=color:{};background-color:{}>{}</text>\".format(\n",
    "        color, background, s\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"\"\n",
    "for sentence_idx in range(len(sentences)):\n",
    "    html += classes[np.argmax(predictions[sentence_idx])] + \": \"\n",
    "\n",
    "    # Define the coloring for each token. Green=positive, Red=negative.\n",
    "    # Color intensity describes the magnitude of the influence in either direction.\n",
    "    max_imp = max(abs(total_attrs[sentence_idx]))\n",
    "    rgbs = []\n",
    "    for imp in total_attrs[sentence_idx]:\n",
    "        normed_imp = int(imp / max_imp * 256)\n",
    "        intensity = abs(normed_imp)\n",
    "        if normed_imp > 0:  # green\n",
    "            rgbs.append(rgb_str(256 - intensity, 256, 256 - intensity))\n",
    "        else:  # red\n",
    "            rgbs.append(rgb_str(256, 256 - intensity, 256 - intensity))\n",
    "\n",
    "    for i, token in enumerate(tokens[sentence_idx]):\n",
    "        if token != \"[PAD]\":\n",
    "            html += cstr(token, \"black\", rgbs[i]) + \" \"\n",
    "    html += \"</br></br>\"\n",
    "html_print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence_idx in range(len(sentences)):\n",
    "    df = pd.DataFrame(\n",
    "        {\"Tokens\": tokens[sentence_idx], \"Importance\": total_attrs[sentence_idx]}\n",
    "    )\n",
    "    fig = px.bar(df, x=\"Tokens\", y=\"Importance\")\n",
    "    fig.update_layout(\n",
    "        width=1200,\n",
    "        height=300,\n",
    "    )\n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "nlp_demo_tf2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "trulens_deps",
   "language": "python",
   "name": "trulens_deps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce4e35a76a569399d57219f9877d3cff9bc99a439b1c8dd709c903be401418f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
