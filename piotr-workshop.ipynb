{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got OPENAI_API_KEY\n",
      "got PINECONE_API_KEY\n",
      "got PINECONE_ENV\n",
      "got HUGGINGFACE_API_KEY\n",
      "got SLACK_TOKEN\n",
      "got SLACK_SIGNING_SECRET\n"
     ]
    }
   ],
   "source": [
    "from keys import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import pinecone\n",
    "import requests\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.chains import (ConversationalRetrievalChain,\n",
    "                              SimpleSequentialChain)\n",
    "from langchain.document_loaders import (PagedPDFSplitter, TextLoader,\n",
    "                                        UnstructuredHTMLLoader,\n",
    "                                        UnstructuredMarkdownLoader,\n",
    "                                        UnstructuredPDFLoader)\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline, OpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Pinecone\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "from keys import HUGGINGFACE_HEADERS\n",
    "from slackbot import chain\n",
    "\n",
    "# https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/pinecone.html?highlight=pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTIMENT_API_URL = \"https://api-inference.huggingface.co/models/cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "LANGUAGE_API_URL = \"https://api-inference.huggingface.co/models/papluca/xlm-roberta-base-language-detection\"\n",
    "TOXIC_API_URL = \"https://api-inference.huggingface.co/models/martin-ha/toxic-comment-model\"\n",
    "GIBBERISH_API_URL = \"https://api-inference.huggingface.co/models/madhurjindal/autonlp-Gibberish-Detector-492513457\"\n",
    "Q_OR_S_API_URL = \"https://api-inference.huggingface.co/models/shahrukhx01/question-vs-statement-classifier\"\n",
    "EMOTION_API_URL = \"https://api-inference.huggingface.co/models/joeddav/distilbert-base-uncased-go-emotions-student\"\n",
    "OFFENSIVE_API_URL = \"https://api-inference.huggingface.co/models/cardiffnlp/twitter-roberta-base-offensive\"\n",
    "GPT2_DETECTOR_API_URL = \"https://api-inference.huggingface.co/models/roberta-large-openai-detector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(payload, url):\n",
    "    response = requests.post(url, headers=HUGGINGFACE_HEADERS, json=payload)\n",
    "    return response.json()\n",
    "\t\n",
    "# These queries via huggingface are free but may need to wait for models to load.\n",
    "\n",
    "sentiment_query = lambda p: query(p, SENTIMENT_API_URL)\n",
    "language_query = lambda p: query(p, LANGUAGE_API_URL)\n",
    "toxic_query = lambda p: query(p, TOXIC_API_URL)\n",
    "gibberish_query = lambda p: query(p, GIBBERISH_API_URL)\n",
    "q_or_s_query = lambda p: query(p, Q_OR_S_API_URL)\n",
    "emotion_query = lambda p: query(p, EMOTION_API_URL)\n",
    "offsensive_query = lambda p: query(p, OFFENSIVE_API_URL)\n",
    "gpt2_detector_query = lambda p: query(p, GPT2_DETECTOR_API_URL)\n",
    "\n",
    "#output = sentiment_query({\n",
    "#\t\"inputs\": \"I like you. I love you\",\n",
    "#})\n",
    "\n",
    "\t\n",
    "#language_query({\n",
    "#\t\"inputs\": \"I like you. I love you\",\n",
    "#})\n",
    "\n",
    "# emotion_query(\"Wie gehts?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "llm = OpenAI()\n",
    "template = \"\"\"Q: {question} A:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain, ConstitutionalChain\n",
    "from langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\n",
    "\n",
    "qa_prompt = PromptTemplate(\n",
    "    template=\"Q: {question} A:\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "qa_chain = LLMChain(llm=llm, prompt=qa_prompt)\n",
    "\n",
    "constitutional_chain = ConstitutionalChain.from_llm(\n",
    "    llm=None,\n",
    "    chain=qa_chain,\n",
    "    constitutional_principles=[\n",
    "        ConstitutionalPrinciple(\n",
    "            critique_request=\"Tell if this answer is good.\",\n",
    "            revision_request=\"Give a better answer.\",\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "constitutional_chain.run(question=\"What is the meaning of life?\")\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "1. Langchain does not have support for classification models: https://python.langchain.com/en/latest/modules/models.html\n",
    "\n",
    "    - Will have to figure out out-of-band retrieval and execution of feedback models that are not LLM's.\n",
    "\n",
    "2. Can add steps to chain to capture text at various points in a chain: https://python.langchain.com/en/latest/reference/modules/chains.html#langchain.chains.SequentialChain .\n",
    "\n",
    "\n",
    "# Links\n",
    "\n",
    "- https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/llama#transformers.LlamaForCausalLM\n",
    "\n",
    "- https://huggingface.co/docs/transformers/main_classes/text_generation\n",
    "\n",
    "\n",
    "# Pinecone\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict\n",
    "from langchain.input import get_color_mapping\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "@dataclass\n",
    "class TruFeedback():\n",
    "    typ: str\n",
    "    input: LLMChain = None\n",
    "    output: LLMChain = None\n",
    "\n",
    "    def similar_to(aval: 'TruSentiment'):\n",
    "        ...\n",
    "\n",
    "@dataclass\n",
    "class TruCheck():\n",
    "    chain: LLMChain\n",
    "    feedback: TruFeedback\n",
    "\n",
    "    test: Callable[[Any]]\n",
    "\n",
    "    pass\n",
    "\"\"\"\n",
    "\n",
    "# TruMetric(\n",
    "#   TruFeedback(\"sentiment\", output=llm_chain)\n",
    "# )\n",
    "# TruTest(\n",
    "#   TruFeedback(\"sentiment\", input=llm_chain).similar_to(TruFeedback(\"sentiment\", output=llm_chain))\n",
    "# )\n",
    "\n",
    "class TruFeedbackChain(SimpleSequentialChain):\n",
    "\n",
    "    feedback_pipeline: Any = None\n",
    "    feedback_name: str = None\n",
    "    feedback_records: pd.DataFrame = None\n",
    "\n",
    "    def __init__(self, feedback=\"sentiment\", *args, **kwargs):\n",
    "        SimpleSequentialChain.__init__(self, *args, **kwargs)\n",
    "\n",
    "        # others:\n",
    "        # - relevance\n",
    "        # - \n",
    "        if feedback == \"sentiment\":\n",
    "            self.feedback_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "            self.feedback_name = \"sentiment\"\n",
    "        elif feedback == \"language\":\n",
    "            self.feedback_pipeline = pipeline(\"text-classification\", model=\"papluca/xlm-roberta-base-language-detection\")\n",
    "            self.feedback_name = \"language\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported feedback type {feedback}\")\n",
    "\n",
    "        chains = kwargs['chains']\n",
    "        record_cols = dict()\n",
    "        for i, k in enumerate(chains):\n",
    "            record_cols[f\"{i}: {k.__class__.__name__} input\"] = []\n",
    "            record_cols[f\"{i}: {k.__class__.__name__} output\"] = []\n",
    "            record_cols[f\"{i}: {k.__class__.__name__} input {self.feedback_name}\"] = []\n",
    "            record_cols[f\"{i}: {k.__class__.__name__} output {self.feedback_name}\"] = []\n",
    "\n",
    "        self.feedback_records = pd.DataFrame(record_cols)\n",
    "\n",
    "        assert self.feedback_pipeline is not None\n",
    "\n",
    "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
    "        _input = inputs[self.input_key]\n",
    "        color_mapping = get_color_mapping([str(i) for i in range(len(self.chains))])\n",
    "\n",
    "        row = dict()\n",
    "\n",
    "        for i, chain in enumerate(self.chains):\n",
    "            print(f\"{i} input:\", _input)\n",
    "            feedback = self.feedback_pipeline(_input)\n",
    "\n",
    "            row[f\"{i}: {chain.__class__.__name__} input\"] = _input\n",
    "            row[f\"{i}: {chain.__class__.__name__} input {self.feedback_name}\"] = feedback[0]['label']\n",
    "\n",
    "            _input = chain.run(_input)\n",
    "\n",
    "            print(f\"{i} output:\", _input, self.feedback_pipeline(_input))\n",
    "            feedback = self.feedback_pipeline(_input)\n",
    "            row[f\"{i}: {chain.__class__.__name__} output\"] = _input\n",
    "            row[f\"{i}: {chain.__class__.__name__} output {self.feedback_name}\"] = feedback[0]['label']\n",
    "\n",
    "            if self.strip_outputs:\n",
    "                _input = _input.strip()\n",
    "            self.callback_manager.on_text(\n",
    "                _input, color=color_mapping[str(i)], end=\"\\n\", verbose=self.verbose\n",
    "            )\n",
    "\n",
    "        self.feedback_records.loc[len(self.feedback_records)] = row\n",
    "\n",
    "        return {self.output_key: _input}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_chain = TruFeedbackChain(\n",
    "    feedback=\"language\",\n",
    "    chains=[llm_chain, llm_chain_2],\n",
    "    input_key=\"question\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_chain.run(question=\"Wie gehts?\")\n",
    "feedback_chain.run(question=\"How are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_chain.feedback_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install accelerate transformers bitsandbytes\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "import torch\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-ul2\", device_map=\"auto\", load_in_8bit=True)#, torch_dtype=torch.int8)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\")\n",
    "\n",
    "input_string = \"Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?\"\n",
    "\n",
    "inputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n",
    "outputs = model.generate(inputs, max_length=200)\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "# <pad> They have 23 - 20 = 3 apples left. They have 3 + 6 = 9 apples. Therefore, the answer is 9.</s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xxl\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-xxl\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"conversational\",\n",
    "    #\"conditional-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=16,\n",
    "    device_map=\"auto\",\n",
    "    early_stopping=True\n",
    "    # generate_kwargs=dict()\n",
    " #   device=\"auto\"\n",
    ")\n",
    "# llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Conversation\n",
    "\n",
    "pipe(conversations=[Conversation(\"hello there\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from slackbot import chain\n",
    "import langchain\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama.hf import LLaMATokenizer\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "import torch\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "# model_id = \"decapoda-research/llama-7b-hf\"\n",
    "#model_id = \"decapoda-research/llama-13b-hf\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map='auto', torch_dtype=torch.float16, local_files_only=True)#.to(\"cuda:0\")\n",
    "print(\"model loaded\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, local_files_only=True)\n",
    "print(\"tokenizer loaded\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=16,\n",
    "    device_map=\"auto\",\n",
    "    early_stopping=True\n",
    "    # generate_kwargs=dict()\n",
    " #   device=\"auto\"\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tru import TruChain\n",
    "\n",
    "# llm = OpenAI()\n",
    "template = \"\"\"Q: {question} A:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "tru_chain = TruChain(chain=llm_chain)\n",
    "\n",
    "tru_chain.run(dict(question=\"How are you?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_chain.run(dict(question=\"How are you today?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_chain.records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_chain.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import (ConversationalRetrievalChain,\n",
    "                              SimpleSequentialChain)\n",
    "\n",
    "verb = False\n",
    "\n",
    "template = \"\"\"Q: {question} A:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=verb)\n",
    "\n",
    "template_2 = \"\"\"Reverse this sentence: {sentence}.\"\"\"\n",
    "prompt_2 = PromptTemplate(template=template_2, input_variables=[\"sentence\"])\n",
    "llm_chain_2 = LLMChain(prompt=prompt_2, llm=llm, verbose=verb)\n",
    "\n",
    "# print(llm_chain.run(question=\"What is the average air speed velocity of a laden swallow?\"))\n",
    "\n",
    "print(llm_chain_2.run(sentence=\"How are you doing?\"))\n",
    "\n",
    "seq_chain = SimpleSequentialChain(chains=[llm_chain, llm_chain_2], input_key=\"question\", output_key=\"answer\")\n",
    "seq_chain.run(question=\"What is the average air speed velocity of a laden swallow?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_chain_2 = TruChain(seq_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_chain.run(question=\"What is the average air speed velocity of a laden swallow? again\")\n",
    "tru_chain_2.run(question=\"What is the average air speed velocity of a laden swallow?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_chain_2.records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_chain_2.run(question=\"What is the average air speed velocity of a laden swallow?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_chain_2.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
