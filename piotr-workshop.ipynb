{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got OPENAI_API_KEY\n",
      "got PINECONE_API_KEY\n",
      "got PINECONE_ENV\n",
      "got HUGGINGFACE_API_KEY\n",
      "got SLACK_TOKEN\n",
      "got SLACK_SIGNING_SECRET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotrm/anaconda3/envs/demo3/lib/python3.10/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from keys import *\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import pinecone\n",
    "import requests\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.chains import (ConversationalRetrievalChain,\n",
    "                              SimpleSequentialChain)\n",
    "from langchain.document_loaders import (PagedPDFSplitter, TextLoader,\n",
    "                                        UnstructuredHTMLLoader,\n",
    "                                        UnstructuredMarkdownLoader,\n",
    "                                        UnstructuredPDFLoader)\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline, OpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Pinecone\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "from keys import HUGGINGFACE_HEADERS\n",
    "from slackbot import chain\n",
    "\n",
    "from pprint import PrettyPrinter\n",
    "pp = PrettyPrinter()\n",
    "\n",
    "# https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/pinecone.html?highlight=pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel():\n",
    "    def __init__(self):\n",
    "        # llm = OpenAI()\n",
    "       \n",
    "        self.llm_model_id = \"gpt2\"\n",
    "        # This model is pretty bad but using it for tests because it is free and\n",
    "        # relatively small.\n",
    "\n",
    "        # model_id = \"decapoda-research/llama-7b-hf\"\n",
    "        # model_id = \"decapoda-research/llama-13b-hf\"\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.llm_model_id,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            local_files_only=True)\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.llm_model_id,\n",
    "                                                       local_files_only=True)\n",
    "\n",
    "        self.pipe = pipeline(\"text-generation\",\n",
    "                             model=self.model,\n",
    "                             tokenizer=self.tokenizer,\n",
    "                             max_new_tokens=16,\n",
    "                             device_map=\"auto\",\n",
    "                             early_stopping=True)\n",
    "\n",
    "        self.llm = HuggingFacePipeline(pipeline=self.pipe)\n",
    "\n",
    "        template = \"\"\"Q: {question} A:\"\"\"\n",
    "        self.prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "        self.llm_chain = LLMChain(prompt=self.prompt, llm=self.llm, verbose=True)\n",
    "\n",
    "t = TestModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tru_chain import TruChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotrm/anaconda3/envs/demo3/lib/python3.10/site-packages/transformers/generation/utils.py:1253: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mQ: hello there A:\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'hello there',\n",
       " 'text': ' so what are i doing now? Are you there, and if so where is'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc = TruChain(t.llm_chain)\n",
    "tc(\"hello there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n@jsonpickle.handlers.register(pydantic.BaseModel, base=True)\\nclass FooHandler(jsonpickle.handlers.BaseHandler):\\n    def flatten(self, obj, data):\\n        print(\"pydantic\", obj, data)\\n        h = jsonpickle.handlers.get(dict)#(self.context)\\n        print(h)\\n        h = h(self.context)\\n        h.flatten(obj.dict(), data)\\n        #if isinstance(obj, pydantic.BaseModel):\\n        #    # h = jsonpickle.handlers.BaseHandler(self.context)\\n        #    \\n        #    print(h)\\n        #    h.flatten(obj, data)\\n        #else:\\n        #    pass\\n\\n\\n@jsonpickle.handlers.register(object, base=True)\\nclass FooHandler(jsonpickle.handlers.BaseHandler):\\n    def flatten(self, obj, data):\\n        print(\"default\", obj, data)\\n        pass    \\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from types import NoneType\n",
    "from typing import Dict, Sequence\n",
    "import jsonpickle\n",
    "import pydantic\n",
    "\n",
    "\n",
    "class MaybeJSONDecoder(json.JSONDecoder):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.decoded = dict()\n",
    "\n",
    "    def _build(self, struct):\n",
    "        print(f\"building {struct}\")\n",
    "        if isinstance(struct, Dict) and \"_pydantic\" in struct:\n",
    "            metadata = struct['_pydantic']\n",
    "            print(f\"metadata={metadata}\")\n",
    "\n",
    "            data = {k: self._build(v) for k, v in struct.items() if k != \"_pydantic\"}\n",
    "\n",
    "            print(data)\n",
    "\n",
    "            mod = __import__(metadata['module'])\n",
    "            cls = getattr(mod, metadata['class'])\n",
    "            return cls.parse_obj(data)\n",
    "\n",
    "        elif isinstance(struct, Dict):\n",
    "            data = {k: self._build(v) for k, v in struct.items()}\n",
    "            return data\n",
    "\n",
    "        return struct\n",
    "\n",
    "    def decode(self, s):\n",
    "        struct = super().decode(s)\n",
    "\n",
    "        return self._build(struct)\n",
    "\n",
    "\n",
    "class MaybeJSONEncoder(json.JSONEncoder):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.encoded = dict()\n",
    "\n",
    "    # @staticmethod\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (str, int, float, bool, NoneType)):\n",
    "            return obj\n",
    "        \n",
    "        obj_id = id(obj)\n",
    "        # print(obj_id)\n",
    "\n",
    "        if obj_id in self.encoded:\n",
    "            return self.encoded[obj_id]#{'_ref': obj_id}\n",
    "\n",
    "        self.encoded[obj_id] = {'_ref': obj_id}\n",
    "\n",
    "        metadata = {\n",
    "                'class': obj.__class__.__name__,\n",
    "                'module': obj.__class__.__module__,\n",
    "                'id': id(obj)\n",
    "            }\n",
    "\n",
    "        src = dict()\n",
    "\n",
    "        print(f\"defaulting {type(obj)}\")\n",
    "\n",
    "        if isinstance(obj, pydantic.BaseModel):\n",
    "            try:\n",
    "                src = obj.dict(models_as_dict=False)\n",
    "            except BaseException as e:\n",
    "                metadata['error'] = str(e)\n",
    "                \n",
    "            src['_pydantic'] = metadata\n",
    "            self.encoded[obj_id] = src\n",
    "\n",
    "            return src\n",
    "\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: self.default(v) for k, v in obj.items()}\n",
    "        \n",
    "        elif isinstance(obj, list):\n",
    "            return [self.default(v) for v in obj]\n",
    "\n",
    "        else:\n",
    "            return {\n",
    "                '_nonserial': metadata\n",
    "            }\n",
    "    \n",
    "    # @staticmethod\n",
    "    def encode(self, obj):\n",
    "        print(f\"encoding {obj}\")\n",
    "        obj = self.default(obj)\n",
    "        return super().encode(obj)\n",
    "\n",
    "    \"\"\"\n",
    "        obj_id = id(obj)\n",
    "        print(obj_id)\n",
    "\n",
    "        if obj_id in self.encoded:\n",
    "            return super().encode({'_ref': obj_id})\n",
    "        \n",
    "        # temporary reference\n",
    "        self.encoded[obj_id] = {'_ref': obj_id}\n",
    "\n",
    "        if isinstance(obj, Dict):\n",
    "            ret = {}\n",
    "            for k, v in obj.items():\n",
    "                ret[k] = self.encode(v)\n",
    "\n",
    "        elif isinstance(obj, Sequence):\n",
    "            ret = []\n",
    "            for v in obj:\n",
    "                ret.append(self.encode(v))\n",
    "        else:\n",
    "            print(type(obj))\n",
    "            ret = self.default(obj)\n",
    "            # ret = super().encode(obj)\n",
    "\n",
    "        self.encoded[obj_id] = ret\n",
    "\n",
    "        return super().encode(ret)\n",
    "    \"\"\"\n",
    "\n",
    "import langchain\n",
    "\n",
    "class TestModel(pydantic.BaseModel):\n",
    "    \n",
    "    chain: langchain.chains.base.Chain\n",
    "\n",
    "    class Config:\n",
    "        encoder = MaybeJSONEncoder()\n",
    "        decoder = MaybeJSONDecoder()\n",
    "\n",
    "        def json_dumps(o, default):\n",
    "            return TestModel.Config.encoder.encode(o)\n",
    "        \n",
    "        def json_loads(s):\n",
    "            return TestModel.Config.decoder.decode(s)\n",
    "        \n",
    "        #json_encoders = {\n",
    "        #    TestModel: encoder.encode\n",
    "        #}\n",
    "        #json_dumps = lambda o, default: Config.encoder.encode(o)\n",
    "\n",
    "LLMChain.Config = TestModel.Config\n",
    "tm = TestModel(chain=t.llm_chain)\n",
    "\n",
    "# json.dumps(tc.model, default=default)\n",
    "\"\"\"\n",
    "@jsonpickle.handlers.register(pydantic.BaseModel, base=True)\n",
    "class FooHandler(jsonpickle.handlers.BaseHandler):\n",
    "    def flatten(self, obj, data):\n",
    "        print(\"pydantic\", obj, data)\n",
    "        h = jsonpickle.handlers.get(dict)#(self.context)\n",
    "        print(h)\n",
    "        h = h(self.context)\n",
    "        h.flatten(obj.dict(), data)\n",
    "        #if isinstance(obj, pydantic.BaseModel):\n",
    "        #    # h = jsonpickle.handlers.BaseHandler(self.context)\n",
    "        #    \n",
    "        #    print(h)\n",
    "        #    h.flatten(obj, data)\n",
    "        #else:\n",
    "        #    pass\n",
    "\n",
    "\n",
    "@jsonpickle.handlers.register(object, base=True)\n",
    "class FooHandler(jsonpickle.handlers.BaseHandler):\n",
    "    def flatten(self, obj, data):\n",
    "        print(\"default\", obj, data)\n",
    "        pass    \n",
    "\"\"\"\n",
    "        \n",
    "# jsonpickle.handlers.register(object, )\n",
    "\n",
    "#temp = jsonpickle.dumps(tc.chain)\n",
    "\n",
    "# temp = jsonpickle.encode(tc.model)\n",
    "\n",
    "# enc = MaybeJSONEncoder(check_circular=False)\n",
    "# dec = MaybeJSONDecoder()\n",
    "\n",
    "# dump = json.dumps(enc.encode(tc.chain))\n",
    "\n",
    "# dec.decode(dump)\n",
    "# print(dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.llm_chain.save(\"temp.json\")\n",
    "llm_chain2 = langchain.chains.loading.load_chain_from_config(t.llm_chain.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory=None callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7fa48a49f490> verbose=True prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='Q: {question} A:', template_format='f-string', validate_template=True) llm=HuggingFacePipeline(cache=None, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7fa48a49f490>, pipeline=None, model_id='gpt2', model_kwargs=None) output_key='text'\n"
     ]
    }
   ],
   "source": [
    "print(llm_chain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.TestModel'>\n",
      "encoding {'chain': LLMChain(memory=None, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7fa48a49f490>, verbose=True, prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='Q: {question} A:', template_format='f-string', validate_template=True), llm=HuggingFacePipeline(cache=None, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7fa48a49f490>, pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7fa3a4cafeb0>, model_id='gpt2', model_kwargs=None), output_key='text')}\n",
      "defaulting <class 'dict'>\n",
      "defaulting <class 'langchain.chains.llm.LLMChain'>\n",
      "{\"chain\": {\"memory\": null, \"verbose\": true, \"prompt\": {\"input_variables\": [\"question\"], \"output_parser\": null, \"partial_variables\": {}, \"template\": \"Q: {question} A:\", \"template_format\": \"f-string\", \"validate_template\": true, \"_type\": \"prompt\"}, \"llm\": {\"model_id\": \"gpt2\", \"model_kwargs\": null, \"_type\": \"huggingface_pipeline\"}, \"output_key\": \"text\", \"_type\": \"llm_chain\", \"_pydantic\": {\"class\": \"LLMChain\", \"module\": \"langchain.chains.llm\", \"id\": 140346287236784}}}\n",
      "building {'chain': {'memory': None, 'verbose': True, 'prompt': {'input_variables': ['question'], 'output_parser': None, 'partial_variables': {}, 'template': 'Q: {question} A:', 'template_format': 'f-string', 'validate_template': True, '_type': 'prompt'}, 'llm': {'model_id': 'gpt2', 'model_kwargs': None, '_type': 'huggingface_pipeline'}, 'output_key': 'text', '_type': 'llm_chain', '_pydantic': {'class': 'LLMChain', 'module': 'langchain.chains.llm', 'id': 140346287236784}}}\n",
      "building {'memory': None, 'verbose': True, 'prompt': {'input_variables': ['question'], 'output_parser': None, 'partial_variables': {}, 'template': 'Q: {question} A:', 'template_format': 'f-string', 'validate_template': True, '_type': 'prompt'}, 'llm': {'model_id': 'gpt2', 'model_kwargs': None, '_type': 'huggingface_pipeline'}, 'output_key': 'text', '_type': 'llm_chain', '_pydantic': {'class': 'LLMChain', 'module': 'langchain.chains.llm', 'id': 140346287236784}}\n",
      "metadata={'class': 'LLMChain', 'module': 'langchain.chains.llm', 'id': 140346287236784}\n",
      "building None\n",
      "building True\n",
      "building {'input_variables': ['question'], 'output_parser': None, 'partial_variables': {}, 'template': 'Q: {question} A:', 'template_format': 'f-string', 'validate_template': True, '_type': 'prompt'}\n",
      "building ['question']\n",
      "building None\n",
      "building {}\n",
      "building Q: {question} A:\n",
      "building f-string\n",
      "building True\n",
      "building prompt\n",
      "building {'model_id': 'gpt2', 'model_kwargs': None, '_type': 'huggingface_pipeline'}\n",
      "building gpt2\n",
      "building None\n",
      "building huggingface_pipeline\n",
      "building text\n",
      "building llm_chain\n",
      "{'memory': None, 'verbose': True, 'prompt': {'input_variables': ['question'], 'output_parser': None, 'partial_variables': {}, 'template': 'Q: {question} A:', 'template_format': 'f-string', 'validate_template': True, '_type': 'prompt'}, 'llm': {'model_id': 'gpt2', 'model_kwargs': None, '_type': 'huggingface_pipeline'}, 'output_key': 'text', '_type': 'llm_chain'}\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"pydantic/main.py\", line 539, in pydantic.main.BaseModel.parse_raw\n",
      "  File \"pydantic/parse.py\", line 37, in pydantic.parse.load_str_bytes\n",
      "  File \"/tmp/ipykernel_2260677/2695286400.py\", line 139, in json_loads\n",
      "    return TestModel.Config.decoder.decode(s)\n",
      "  File \"/tmp/ipykernel_2260677/2695286400.py\", line 36, in decode\n",
      "    return self._build(struct)\n",
      "  File \"/tmp/ipykernel_2260677/2695286400.py\", line 28, in _build\n",
      "    data = {k: self._build(v) for k, v in struct.items()}\n",
      "  File \"/tmp/ipykernel_2260677/2695286400.py\", line 28, in <dictcomp>\n",
      "    data = {k: self._build(v) for k, v in struct.items()}\n",
      "  File \"/tmp/ipykernel_2260677/2695286400.py\", line 25, in _build\n",
      "    return cls.parse_obj(data)\n",
      "  File \"pydantic/main.py\", line 526, in pydantic.main.BaseModel.parse_obj\n",
      "  File \"pydantic/main.py\", line 341, in pydantic.main.BaseModel.__init__\n",
      "pydantic.error_wrappers.ValidationError: 3 validation errors for LLMChain\n",
      "prompt\n",
      "  Can't instantiate abstract class BasePromptTemplate with abstract methods format, format_prompt (type=type_error)\n",
      "llm\n",
      "  Can't instantiate abstract class BaseLanguageModel with abstract methods agenerate_prompt, generate_prompt (type=type_error)\n",
      "_type\n",
      "  extra fields not permitted (type=value_error.extra)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/piotrm/anaconda3/envs/demo3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_2260677/1430293909.py\", line 4, in <module>\n",
      "    TestModel.parse_raw(encoded)\n",
      "  File \"pydantic/main.py\", line 548, in pydantic.main.BaseModel.parse_raw\n",
      "pydantic.error_wrappers.ValidationError: 3 validation errors for TestModel\n",
      "__root__ -> prompt\n",
      "  Can't instantiate abstract class BasePromptTemplate with abstract methods format, format_prompt (type=type_error)\n",
      "__root__ -> llm\n",
      "  Can't instantiate abstract class BaseLanguageModel with abstract methods agenerate_prompt, generate_prompt (type=type_error)\n",
      "__root__ -> _type\n",
      "  extra fields not permitted (type=value_error.extra)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/piotrm/anaconda3/envs/demo3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2057, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/piotrm/anaconda3/envs/demo3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1288, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/piotrm/anaconda3/envs/demo3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1177, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/piotrm/anaconda3/envs/demo3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1049, in structured_traceback\n",
      "    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n",
      "  File \"/home/piotrm/anaconda3/envs/demo3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 935, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"/home/piotrm/anaconda3/envs/demo3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1003, in get_records\n",
      "    lines, first = inspect.getsourcelines(etb.tb_frame)\n",
      "  File \"/home/piotrm/anaconda3/envs/demo3/lib/python3.10/inspect.py\", line 1129, in getsourcelines\n",
      "    lines, lnum = findsource(object)\n",
      "  File \"/home/piotrm/anaconda3/envs/demo3/lib/python3.10/inspect.py\", line 958, in findsource\n",
      "    raise OSError('could not get source code')\n",
      "OSError: could not get source code\n"
     ]
    }
   ],
   "source": [
    "print(type(tm))\n",
    "encoded = tm.json(models_as_dict=False)\n",
    "print(encoded)\n",
    "TestModel.parse_raw(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tru_chain import Selection\n",
    "\n",
    "tc._select(select=[\n",
    "    Selection(param=(\"chain\", \"prompt\", \"template\")),\n",
    "    Selection(param=(\"chain\", \"llm\", \"model_id\")),\n",
    "    Selection(record=(\"input\", \"inputs\", \"question\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Q: {question} A:\"\"\"\n",
    "prompt = PromptTemplate(template=template,\n",
    "                        input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=t.llm)\n",
    "\n",
    "template_2 = \"\"\"Reverse this sentence: {sentence}.\"\"\"\n",
    "prompt_2 = PromptTemplate(template=template_2,\n",
    "                            input_variables=[\"sentence\"])\n",
    "llm_chain_2 = LLMChain(prompt=prompt_2, llm=t.llm)\n",
    "\n",
    "seq_chain = SimpleSequentialChain(chains=[llm_chain, llm_chain_2],\n",
    "                                    input_key=\"question\",\n",
    "                                    output_key=\"answer\")\n",
    "\n",
    "tc = TruChain(seq_chain)\n",
    "tc.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc(\"hello there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc._get_obj_at_address(address=(\"chains\", 0, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "1. Langchain does not have support for classification models: https://python.langchain.com/en/latest/modules/models.html\n",
    "\n",
    "    - Will have to figure out out-of-band retrieval and execution of feedback models that are not LLM's.\n",
    "\n",
    "2. Can add steps to chain to capture text at various points in a chain: https://python.langchain.com/en/latest/reference/modules/chains.html#langchain.chains.SequentialChain .\n",
    "\n",
    "\n",
    "# Links\n",
    "\n",
    "- https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/llama#transformers.LlamaForCausalLM\n",
    "\n",
    "- https://huggingface.co/docs/transformers/main_classes/text_generation\n",
    "\n",
    "\n",
    "# Pinecone\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from slackbot import chain\n",
    "import langchain\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import (ConversationalRetrievalChain,\n",
    "                              SimpleSequentialChain)\n",
    "\n",
    "verb = False\n",
    "\n",
    "template = \"\"\"Q: {question} A:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=t.llm, verbose=verb)\n",
    "\n",
    "template_2 = \"\"\"Reverse this sentence: {sentence}.\"\"\"\n",
    "prompt_2 = PromptTemplate(template=template_2, input_variables=[\"sentence\"])\n",
    "llm_chain_2 = LLMChain(prompt=prompt_2, llm=t.llm, verbose=verb)\n",
    "\n",
    "# print(llm_chain.run(question=\"What is the average air speed velocity of a laden swallow?\"))\n",
    "\n",
    "print(llm_chain_2.run(sentence=\"How are you doing?\"))\n",
    "\n",
    "seq_chain = SimpleSequentialChain(chains=[llm_chain, llm_chain_2], input_key=\"question\", output_key=\"answer\")\n",
    "seq_chain.run(question=\"What is the average air speed velocity of a laden swallow?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_chain_2 = TruChain(seq_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_chain.run(question=\"What is the average air speed velocity of a laden swallow? again\")\n",
    "tru_chain_2.run(question=\"What is the average air speed velocity of a laden swallow?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in tru_chain_2.records:\n",
    "    print(pp.pformat(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_chain_2.run(question=\"What is the average air speed velocity of a laden swallow?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_chain_2.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "for frame_info in inspect.stack():\n",
    "    frame = frame_info.frame\n",
    "    print(frame_info.function)\n",
    "    # print(frame.f_code)\n",
    "    print(frame.f_locals.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
