{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from keys import *\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "from tinydb import TinyDB\n",
    "\n",
    "import pinecone\n",
    "import requests\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import (ConversationalRetrievalChain,\n",
    "                              SimpleSequentialChain)\n",
    "from langchain.document_loaders import (PagedPDFSplitter, TextLoader,\n",
    "                                        UnstructuredHTMLLoader,\n",
    "                                        UnstructuredMarkdownLoader,\n",
    "                                        UnstructuredPDFLoader)\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline, OpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Pinecone\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "from keys import HUGGINGFACE_HEADERS\n",
    "from slackbot import obj\n",
    "\n",
    "from pprint import PrettyPrinter\n",
    "pp = PrettyPrinter()\n",
    "\n",
    "# https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/pinecone.html?highlight=pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel():\n",
    "    def __init__(self):\n",
    "        # llm = OpenAI()\n",
    "       \n",
    "        self.llm_model_id = \"gpt2\"\n",
    "        # This model is pretty bad but using it for tests because it is free and\n",
    "        # relatively small.\n",
    "\n",
    "        # model_id = \"decapoda-research/llama-7b-hf\"\n",
    "        # model_id = \"decapoda-research/llama-13b-hf\"\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.llm_model_id,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            local_files_only=True)\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.llm_model_id,\n",
    "                                                       local_files_only=True)\n",
    "\n",
    "        self.pipe = pipeline(\"text-generation\",\n",
    "                             model=self.model,\n",
    "                             tokenizer=self.tokenizer,\n",
    "                             max_new_tokens=16,\n",
    "                             device_map=\"auto\",\n",
    "                             early_stopping=True)\n",
    "\n",
    "        self.llm = HuggingFacePipeline(pipeline=self.pipe)\n",
    "\n",
    "        self.memory=ConversationBufferWindowMemory(k=2)\n",
    "\n",
    "        template = \"\"\"Q: {question} A:\"\"\"\n",
    "        self.prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "        self.llm_chain = LLMChain(prompt=self.prompt, llm=self.llm, verbose=True, memory=self.memory)\n",
    "\n",
    "\n",
    "t = TestModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tru_chain import TruChain, Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = TinyDB(\"db.json\")\n",
    "db.drop_table(\"records\")\n",
    "tc = TruChain(t.llm_chain, db=db)\n",
    "\n",
    "tc.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc(\"hello there\")\n",
    "tc(\"hello there general kanobi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.select(\n",
    "    Record.chain.prompt.template,\n",
    "    Record.chain.llm._call.input.prompt,\n",
    "    Record.chain._call.input.inputs.question,\n",
    "    Record.chain._call.output.text,\n",
    "    where=Record.chain._call.output.text != None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Q: {question} A:\"\"\"\n",
    "prompt = PromptTemplate(template=template,\n",
    "                        input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=t.llm)\n",
    "\n",
    "template_2 = \"\"\"Reverse this sentence: {sentence}.\"\"\"\n",
    "prompt_2 = PromptTemplate(template=template_2,\n",
    "                            input_variables=[\"sentence\"])\n",
    "llm_chain_2 = LLMChain(prompt=prompt_2, llm=t.llm)\n",
    "\n",
    "seq_chain = SimpleSequentialChain(chains=[llm_chain, llm_chain_2],\n",
    "                                    input_key=\"question\",\n",
    "                                    output_key=\"answer\")\n",
    "\n",
    "tc = TruChain(seq_chain, db=db)\n",
    "tc.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc(\"hello there\")\n",
    "tc(\"hello there mister bond\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.select(Record.chain.chains[1]._call[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "1. Langchain does not have support for classification models: https://python.langchain.com/en/latest/modules/models.html\n",
    "\n",
    "    - Will have to figure out out-of-band retrieval and execution of feedback models that are not LLM's.\n",
    "\n",
    "2. Can add steps to chain to capture text at various points in a chain: https://python.langchain.com/en/latest/reference/modules/chains.html#langchain.chains.SequentialChain .\n",
    "\n",
    "\n",
    "# Links\n",
    "\n",
    "- https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/llama#transformers.LlamaForCausalLM\n",
    "\n",
    "- https://huggingface.co/docs/transformers/main_classes/text_generation\n",
    "\n",
    "\n",
    "# Pinecone\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from slackbot import obj\n",
    "import langchain\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import (ConversationalRetrievalChain,\n",
    "                              SimpleSequentialChain)\n",
    "\n",
    "verb = True\n",
    "\n",
    "template = \"\"\"Q: {question} A:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=t.llm, verbose=verb)\n",
    "\n",
    "template_2 = \"\"\"Reverse this sentence: {sentence}.\"\"\"\n",
    "prompt_2 = PromptTemplate(template=template_2, input_variables=[\"sentence\"])\n",
    "llm_chain_2 = LLMChain(prompt=prompt_2, llm=t.llm, verbose=verb)\n",
    "\n",
    "# print(llm_chain.run(question=\"What is the average air speed velocity of a laden swallow?\"))\n",
    "\n",
    "print(llm_chain_2.run(sentence=\"How are you doing?\"))\n",
    "\n",
    "seq_chain = SimpleSequentialChain(chains=[llm_chain, llm_chain_2], input_key=\"question\", output_key=\"answer\")\n",
    "seq_chain.run(question=\"What is the average air speed velocity of a laden swallow?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_chain_2 = TruChain(seq_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_chain.run(question=\"What is the average air speed velocity of a laden swallow? again\")\n",
    "tru_chain_2.run(question=\"What is the average air speed velocity of a laden swallow?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in tru_chain_2.records:\n",
    "    print(pp.pformat(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_chain_2.run(question=\"What is the average air speed velocity of a laden swallow?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_chain_2.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruBot testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: will not be able to serialize object of type <class 'langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain'> because it has memory.\n",
      "WARNING: will not be able to serialize object of type <class 'langchain.chains.conversational_retrieval.base.BaseConversationalRetrievalChain'> because it has memory.\n",
      "WARNING: will not be able to serialize object of type <class 'langchain.chains.base.Chain'> because it has memory.\n",
      "WARNING: do not know how to instrument <langchain.callbacks.shared.SharedCallbackManager object at 0x7f60a39b3220>\n",
      "WARNING: do not know how to instrument <langchain.callbacks.shared.SharedCallbackManager object at 0x7f60a39b3220>\n",
      "WARNING: do not know how to instrument <langchain.callbacks.shared.SharedCallbackManager object at 0x7f60a39b3220>\n",
      "WARNING: do not know how to instrument <langchain.callbacks.shared.SharedCallbackManager object at 0x7f60a39b3220>\n",
      "WARNING: do not know how to instrument <langchain.callbacks.shared.SharedCallbackManager object at 0x7f60a39b3220>\n",
      "WARNING: do not know how to instrument <langchain.callbacks.shared.SharedCallbackManager object at 0x7f60a39b3220>\n",
      "WARNING: do not know how to instrument <langchain.callbacks.shared.SharedCallbackManager object at 0x7f60a39b3220>\n",
      "WARNING: do not know how to instrument <langchain.vectorstores.pinecone.Pinecone object at 0x7f60d41516c0>\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory, ConversationSummaryBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "    environment=PINECONE_ENV  # next to api key in console\n",
    ")\n",
    "\n",
    "index_name = \"llmdemo\"\n",
    "\n",
    "verb = True\n",
    "\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-ada-002')  # 1536 dims\n",
    "\n",
    "docsearch = Pinecone.from_existing_index(\n",
    "    index_name=index_name, embedding=embedding\n",
    ")\n",
    "\n",
    "llm = OpenAI(temperature=0, max_tokens=128, verbose=verb)\n",
    "retriever = docsearch.as_retriever()\n",
    "\n",
    "convos = dict()\n",
    "\n",
    "db = TinyDB(\"test.records.json\", default=lambda o: f\"NON-SERIALIZED OBJECT: {o}\")\n",
    "\n",
    "def get_convo(cid):\n",
    "    if cid in convos:\n",
    "        return convos[cid]\n",
    "    \n",
    "    memory = ConversationSummaryBufferMemory(max_token_limit = 650, llm=llm, memory_key=\"chat_history\", output_key='answer')\n",
    "    chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm, retriever=retriever, verbose=verb, return_source_documents=True, memory=memory, get_chat_history=lambda h : h,\n",
    "        max_tokens_limit=4096\n",
    "    )\n",
    "    return TruChain(chain, db=db)\n",
    "\n",
    "c1 = get_convo(\"piotrm\")\n",
    "#chain = ConversationChain(\n",
    "#    llm=llm, memory=memory, verbose=verb\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1.chain.max_tokens_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1.dict(json=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling wrapped chain.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "uence (QII)ative Input In\n",
      "As stressed in the previous section, at the heart of our framework developed in this paper is an\n",
      "uence (QII) approach. Weon approach - the Quantitative Input In\n",
      "outline it in this section, before moving to combining it with other approaches. Traditionally,\n",
      "uence measures have been studied for feature selection, i.e. informing the choice of which\n",
      "uence measures have been used as explain-cently, in\n",
      "uence measures explain the behavior ofex models. In\n",
      "models by indicating the relative importance of inputs and their direction. While the space\n",
      "uence measures is quite large, we point out two requirements that they need\n",
      "to satisfy: (i) taking into account variable correlations, and (ii) capturing feature interactions.\n",
      "When inputs to a model tend to move together (e.g. income and loan size), simple measures\n",
      "of association (such as the correlation between income and defaults) do not distinguish the di-\n",
      "rection in which each a\u000bects outcomes. In complex non-linear classi\fers e\u000bects arise out of the\n",
      "interaction between inputs (e.g. if only individuals with high age andhigh income are deemed\n",
      "uence measures should account for these.\n",
      "QII [1] controls for correlations by employing randomising interventions on inputs, and\n",
      "accounts for input interactions by measuring the average marginal contributions of inputs over\n",
      "all sets of features they may interact with. In other words, with this technique, we attempt\n",
      "various changes of input variables and analyse what changes to output variables they produce.\n",
      "To see how this works in practice, we focus on the example highlighted in this paper,\n",
      "uence of a particular input|say, theterested in the in\n",
      "borrower's income|on the probability of default estimated by the model, which becomes our\n",
      "quantity of interest . (Any property of the system conditional on a particular distribution of\n",
      "inputs can be a quantity of interest for measuring QII.) In what follows, we start by concen-\n",
      "trating on individual outcomes, i.e. on questions such as \\why was the loan application of this\n",
      "individual rejected?\" or \\what would it take for that particular individual to lower their default\n",
      "probability?\". (These are type-1 explanations according to Table 1.) We will then build up\n",
      "from these individual-level questions to the the overall functioning of the model, and focus on\n",
      "7\n",
      "\n",
      "B. QII for Group Outcomes\n",
      "As in the running example, the quantity of interest may\n",
      "be the classiﬁcation outcome for a set of individuals. Given a\n",
      "group of individuals Y\u0012X , we deﬁneQY\n",
      "grp(\u0001)to beE(c(\u0001) =\n",
      "1jX2Y). The inﬂuence measure is therefore:\n",
      "\u0013Y\n",
      "grp(i) =E(c(X) = 1jX2Y)\u0000E(c(X\u0000iUi) = 1jX2Y)\n",
      "(6)\n",
      "C. QII for Group Disparity\n",
      "Instead of simply classiﬁcation outcomes, an analyst may\n",
      "be interested in more nuanced properties of data analytics\n",
      "systems. Recently, disparate impact has come to the fore as a\n",
      "measure of unfairness, which compares the rates of positive\n",
      "classiﬁcation within protected groups deﬁned by gender or\n",
      "race. The ‘80% rule’ in employment which states that the\n",
      "rate of selection within a protected demographic should be\n",
      "at least 80% of the rate of selection within the unprotected\n",
      "demographic. The quantity of interest in such a scenario is\n",
      "the ratio in positive classiﬁcation outcomes for a protected\n",
      "groupYfrom the rest of the population XnY .\n",
      "E(c(X) = 1jX2Y)\n",
      "E(c(X) = 1jX62Y)\n",
      "However, the ratio of classiﬁcation rates is unstable at at\n",
      "low values of positive classiﬁcation. Therefore, for the com-\n",
      "putations in this paper we use the difference in classiﬁcation\n",
      "rates as our measure of group disparity.\n",
      "QY\n",
      "disp(\u0001) =jE(c(\u0001) = 1jX2Y)\u0000E(c(\u0001) = 1jX62Y)j\n",
      "(7)\n",
      "The QII measure of an input group disparity, as a result is:\n",
      "\u0013Y\n",
      "disp(i) =QY\n",
      "disp(X)\u0000QY\n",
      "disp(X\u0000iUi): (8)\n",
      "More generally, group disparity can be viewed as an as-\n",
      "sociation between classiﬁcation outcomes and membership\n",
      "in a group. QII on a measure of such association (e.g.,\n",
      "group disparity) identiﬁes the variable that causes the associ-\n",
      "ation in the classiﬁer. Proxy variables are variables that are\n",
      "associated with protected attributes. However, for concerns\n",
      "of discrimination such as digital redlining , it is important\n",
      "to identify which proxy variables actually introduce group\n",
      "disparity. It is straightforward to observe that features with\n",
      "high QII for group disparity are proxy variables, and also cause\n",
      "group disparity. Therefore, QII on group disparity is a useful\n",
      "diagnostic tool for determining discriminiation. The use of QII\n",
      "in identifying proxy variables is explored experimentally in\n",
      "Section VII-B. Note that because of such proxy variables,\n",
      "simply ensuring that protected attributes are not input to\n",
      "the classiﬁer is not sufﬁcient to avoid discrimination (see\n",
      "also [12]).III. S ET AND MARGINAL QII\n",
      "In many situations, intervention on a single input variable\n",
      "has no inﬂuence on the outcome of a system. Consider, for\n",
      "example, a two-feature setting where features are age ( A) and\n",
      "income (I), and the classiﬁer is c(A;I) = (A=old)^(I=\n",
      "high). In other words, the only datapoints that are labeled 1\n",
      "are those of elderly persons with high income. Now, given\n",
      "a datapoint where A=young;I=low, an intervention on\n",
      "either age or income would result in the same classiﬁcation.\n",
      "However, it would be misleading to say that neither age nor\n",
      "income have an inﬂuence over the outcome: changing both the\n",
      "states of income and age would result in a change in outcome.\n",
      "Equating inﬂuence with the individual ability to affect the\n",
      "outcome is uninformative in real datasets as well: Figure 1 is a\n",
      "histogram of inﬂuences of features on outcomes of individuals\n",
      "for a classiﬁer learnt from the adult dataset [13]2. For most\n",
      "individuals, all features have zero inﬂuence: changing the state\n",
      "of one feature alone is not likely to change the outcome of\n",
      "a classiﬁer. Of the 19537 datapoints we evaluate, more than\n",
      "half have\u0013x(i) = 0 for alli2N, Indeed, changes to outcome\n",
      "are more likely to occur if we intervene on sets of features .\n",
      "In order to get a better understanding of the inﬂuence of a\n",
      "featurei2N, we should measure its effect when coupled\n",
      "with interventions on other features. We deﬁne the inﬂuence\n",
      "of a set of inputs as a straightforward extension of the inﬂuence\n",
      "of individual inputs. Essentially, we wish the inﬂuence of a set\n",
      "of inputsS\u0012Nto be the same as when the set of inputs is\n",
      "considered to be a single input; when intervening on S, we\n",
      "draw the states of i2Sbased on the joint distribution of the\n",
      "states of features in S,\u0019S(uS), as deﬁned in Equation (1).\n",
      "We can naturally deﬁne a distribution over X\u0002Q\n",
      "i2SXi,\n",
      "naturally extending (2) as:\n",
      "~\u0019(x;uS) =\u0019(x)\u0019S(uS): (9)\n",
      "We also deﬁne the random variable X\u0000SUS(x;uS) =\n",
      "xjNnSuS;X\u0000S(x;uS)has the states of features in NnS\n",
      "ﬁxed to their original values in x, but features in Stake on\n",
      "new values according to uS.\n",
      "Deﬁnition 2 (Set QII) .For a quantity of interest Q, and an\n",
      "inputi, the Quantitative Input Inﬂuence of set S\u0012NonQ\n",
      "is deﬁned to be\n",
      "\u0013Q(S) =Q(X)\u0000Q(X\u0000SUS):\n",
      "Considering the inﬂuence of a set of inputs opens up a\n",
      "number of interesting questions due to the interaction between\n",
      "inputs. First among these is how does one measure the\n",
      "individual effect of a feature, given the measured effects of\n",
      "interventions on sets of features. One natural way of doing so\n",
      "is by measuring the marginal effect of a feature on a set.\n",
      "2The adult dataset contains approximately 31k datapoints of users’ personal\n",
      "attributes, and whether their income is more than $50k per annum; see\n",
      "Section VII for more details.\n",
      "\n",
      "Name Notation Quantity of Interest Sensitivity\n",
      "QII on Individual Outcomes (3) \u0013ind(S) Positive Classiﬁcation of an Individual 1=jDj\n",
      "QII on Actual Individual Outcomes (4) \u0013ind-act(S) Actual Classiﬁcation of an Individual 1=jDj\n",
      "Average QII (5) \u0013ind-avg (S) Average Actual Classiﬁcation 2=jDj\n",
      "QII on Group Outcomes (6) \u0013Y\n",
      "grp(S) Positive Classiﬁcation for a Group 2=jD \\ Yj\n",
      "QII on Group Disparity (8) \u0013Y\n",
      "disp(S) Difference in classiﬁcation rates among groups 2 max(1=jD n Yj;1=jD \\ Yj )\n",
      "TABLE I: A summary of the QII measures deﬁned in the paper\n",
      "classiﬁers: (A) where gender is provided as an actual input,\n",
      "and (B) where gender is not provided as an input. For classiﬁer\n",
      "(B), clearly the input Gender has no effect and any correlation\n",
      "between the outcome and gender is caused via inference from\n",
      "other inputs. In Table II, for both the adult and the arrests\n",
      "dataset, we compute the following observational measures:\n",
      "Mutual Information (MI), Jaccard Index (Jaccard), Pearson\n",
      "Correlation (corr), and the Disparate Impact Ratio (disp) to\n",
      "measure the similarity between Gender and the classiﬁers\n",
      "outcome. We also measure the QII of Gender on outcome.\n",
      "We observe that in many scenarios the observational quantities\n",
      "do not change, or sometimes increase, from classiﬁer A to\n",
      "classiﬁer B, when gender is removed as an actual input\n",
      "to the classiﬁer. On the other hand, if the outcome of the\n",
      "classiﬁer does not depend on the input Gender , then the QII\n",
      "is guaranteed to be zero.\n",
      "B. Unary QII Measures\n",
      "In Figure 2, we illustrate the use of different Unary QII\n",
      "measures. Figures 2a, and 2b, show the Average QII measure\n",
      "(Equation 5) computed for features of a decision forest classi-\n",
      "ﬁer. For the income classiﬁer trained on the adult dataset, the\n",
      "feature with highest inﬂuence is Marital Status , followed by\n",
      "Occupation ,Relationship andCapital Gain . Sensitive features\n",
      "such as Gender and Race have relatively lower inﬂuence.\n",
      "For the predictive policing classiﬁer trained on the arrests\n",
      "dataset, the most inﬂuential input is Drug History , followed by\n",
      "Gender , and Smoking History . We observe that inﬂuence on\n",
      "outcomes may be different from inﬂuence on group disparity.\n",
      "QII on group disparity: Figures 2c, 2d show inﬂuences\n",
      "of features on group disparity for two different settings. The\n",
      "ﬁgure on the left shows the inﬂuence of features on group\n",
      "disparity by Gender in the adult dataset; the ﬁgure on the\n",
      "right shows the inﬂuence of group disparity by Race in the\n",
      "arrests dataset. For the income classiﬁer trained on the\n",
      "adult dataset, we observe that most inputs have negative\n",
      "inﬂuence on group disparity; randomly intervening on most\n",
      "inputs would lead to a reduction in group disparity. In other\n",
      "words, a classiﬁer that did not use these inputs would be fairer.\n",
      "Interestingly, in this classiﬁer, marital status and not sex has\n",
      "the highest inﬂuence on group disparity by sex.\n",
      "For the arrests dataset, most inputs have the effect of\n",
      "increasing group disparity if randomly intervened on. In\n",
      "particular, Drug history has the highest positive inﬂuence on\n",
      "disparity in arrests . Although Drug history is correlated with\n",
      "race, using it reduces disparate impact by race, i.e. makes fairer\n",
      "decisions.In both examples, features correlated with the sensitive\n",
      "attribute are the most inﬂuential for group disparity according\n",
      "to the sensitive attribute instead of the sensitive attribute\n",
      "itself. It is in this sense that QII measures can identify\n",
      "proxy variables that cause associations between outcomes and\n",
      "sensitive attributes.\n",
      "QII with artiﬁcial discrimination: We simulate discrimi-\n",
      "nation using an artiﬁcial experiment. We ﬁrst randomly assign\n",
      "ZIP codes to individuals in our dataset. Then to simulate\n",
      "systematic bias, we make an ffraction of the ZIP codes\n",
      "discriminatory in the following sense: All individuals in the\n",
      "protected set are automatically assigned a negative classiﬁ-\n",
      "cation outcome. We then study the change in the inﬂuence\n",
      "of features as we increase f. Figure 3a, shows that the\n",
      "inﬂuence of Gender increases almost linearly with f. Recall\n",
      "that Marital Status was the most inﬂuential feature for this\n",
      "classiﬁer without any added discrimination. As fincreases,\n",
      "the importance of Marital Status decreases as expected, since\n",
      "the number of individuals for whom Marital Status is pivotal\n",
      "decreases.\n",
      "C. Personalized Transparency Reports\n",
      "To illustrate the utility of personalized transparency reports,\n",
      "we study the classiﬁcation of individuals who received poten-\n",
      "tially unexpected outcomes. For the personalized transparency\n",
      "reports, we use decision forests.\n",
      "The inﬂuence measure that we employ is the Shapley value,\n",
      "with the underlying cooperative game deﬁned over the local\n",
      "inﬂuenceQ. In more detail, v(S) =\u0013QA(S), withQAbeing\n",
      "E[c(\u0001) = 1jX=x]; that is, the marginal contribution of\n",
      "i2NtoSis given by mi(S) =E[c(X\u0000S) = 1jX=\n",
      "x]\u0000E[c(X\u0000S[fig) = 1jX=x].\n",
      "We emphasize that some features may have a negative\n",
      "Shapley value; this should be interpreted as follows: a feature\n",
      "with a high positive Shapley value often increases the certainty\n",
      "that the classiﬁcation outcome is 1, whereas a feature whose\n",
      "Shapley value is negative is one that increases the certainty\n",
      "that the classiﬁcation outcome would be zero.\n",
      "Mr. X: The ﬁrst example is of an individual from the\n",
      "adult dataset, who we refer to as Mr. X, and is described in\n",
      "Figure 4a. He is is deemed to be a low income individual, by\n",
      "an income classiﬁer learned from the data. This result may be\n",
      "surprising to him: he reports high capital gains ($14k), and\n",
      "only 2.1% of people with capital gains higher than $10k are\n",
      "reported as low income. In fact, he might be led to believe that\n",
      "his classiﬁcation may be a result of his ethnicity or country\n",
      "of origin. Examining his transparency report in Figure 4b,\n",
      "however, we ﬁnd that the the most inﬂuential features that led\n",
      "\n",
      "Question: What is QII?\n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Writing 1 record(s) to <TinyDB tables=['records'], tables_count=1, default_table_documents_count=0, all_tables_documents_count=['records=1']> table <Table name='records', total=1, storage=<tinydb.storages.JSONStorage object at 0x7f60d4151600>>.\n"
     ]
    }
   ],
   "source": [
    "ret = c1(dict(question=\"What is QII?\"))\n",
    "# chain.predict(input=\"Who is Piotr?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is QII?',\n",
       " 'chat_history': '',\n",
       " 'answer': ' QII stands for Quantitative Input Influence and is a measure of the influence of a set of inputs on a quantity of interest. It is used to explain the behavior of models by indicating the relative importance of inputs and their direction.',\n",
       " 'source_documents': [Document(page_content='2.2 Quantitative Input In\\ruence (QII)\\nAs stressed in the previous section, at the heart of our framework developed in this paper is an\\ninstance-based explanation approach - the Quantitative Input In\\ruence (QII) approach. We\\noutline it in this section, before moving to combining it with other approaches. Traditionally,\\nin\\ruence measures have been studied for feature selection, i.e. informing the choice of which\\nvariables to include in the model [8]. Recently, in\\ruence measures have been used as explain-\\nability mechanisms [1, 7, 9] for complex models. In\\ruence measures explain the behavior of\\nmodels by indicating the relative importance of inputs and their direction. While the space\\nof potential in\\ruence measures is quite large, we point out two requirements that they need\\nto satisfy: (i) taking into account variable correlations, and (ii) capturing feature interactions.\\nWhen inputs to a model tend to move together (e.g. income and loan size), simple measures\\nof association (such as the correlation between income and defaults) do not distinguish the di-\\nrection in which each a\\x0bects outcomes. In complex non-linear classi\\x0cers e\\x0bects arise out of the\\ninteraction between inputs (e.g. if only individuals with high age andhigh income are deemed\\ncreditworthy), and therefore in\\ruence measures should account for these.\\nQII [1] controls for correlations by employing randomising interventions on inputs, and\\naccounts for input interactions by measuring the average marginal contributions of inputs over\\nall sets of features they may interact with. In other words, with this technique, we attempt\\nvarious changes of input variables and analyse what changes to output variables they produce.\\nTo see how this works in practice, we focus on the example highlighted in this paper,\\nmortgage defaults. Suppose we are interested in the in\\ruence of a particular input|say, the\\nborrower\\'s income|on the probability of default estimated by the model, which becomes our\\nquantity of interest . (Any property of the system conditional on a particular distribution of\\ninputs can be a quantity of interest for measuring QII.) In what follows, we start by concen-\\ntrating on individual outcomes, i.e. on questions such as \\\\why was the loan application of this\\nindividual rejected?\" or \\\\what would it take for that particular individual to lower their default\\nprobability?\". (These are type-1 explanations according to Table 1.) We will then build up\\nfrom these individual-level questions to the the overall functioning of the model, and focus on\\n7', metadata={'page': 7.0, 'source': 'https://truera.com/wp-content/uploads/2020/08/machine-learning-explainability-in-finance-an-application-to-default-risk-analysis.pdf'}),\n",
       "  Document(page_content='B. QII for Group Outcomes\\nAs in the running example, the quantity of interest may\\nbe the classiﬁcation outcome for a set of individuals. Given a\\ngroup of individuals Y\\x12X , we deﬁneQY\\ngrp(\\x01)to beE(c(\\x01) =\\n1jX2Y). The inﬂuence measure is therefore:\\n\\x13Y\\ngrp(i) =E(c(X) = 1jX2Y)\\x00E(c(X\\x00iUi) = 1jX2Y)\\n(6)\\nC. QII for Group Disparity\\nInstead of simply classiﬁcation outcomes, an analyst may\\nbe interested in more nuanced properties of data analytics\\nsystems. Recently, disparate impact has come to the fore as a\\nmeasure of unfairness, which compares the rates of positive\\nclassiﬁcation within protected groups deﬁned by gender or\\nrace. The ‘80% rule’ in employment which states that the\\nrate of selection within a protected demographic should be\\nat least 80% of the rate of selection within the unprotected\\ndemographic. The quantity of interest in such a scenario is\\nthe ratio in positive classiﬁcation outcomes for a protected\\ngroupYfrom the rest of the population XnY .\\nE(c(X) = 1jX2Y)\\nE(c(X) = 1jX62Y)\\nHowever, the ratio of classiﬁcation rates is unstable at at\\nlow values of positive classiﬁcation. Therefore, for the com-\\nputations in this paper we use the difference in classiﬁcation\\nrates as our measure of group disparity.\\nQY\\ndisp(\\x01) =jE(c(\\x01) = 1jX2Y)\\x00E(c(\\x01) = 1jX62Y)j\\n(7)\\nThe QII measure of an input group disparity, as a result is:\\n\\x13Y\\ndisp(i) =QY\\ndisp(X)\\x00QY\\ndisp(X\\x00iUi): (8)\\nMore generally, group disparity can be viewed as an as-\\nsociation between classiﬁcation outcomes and membership\\nin a group. QII on a measure of such association (e.g.,\\ngroup disparity) identiﬁes the variable that causes the associ-\\nation in the classiﬁer. Proxy variables are variables that are\\nassociated with protected attributes. However, for concerns\\nof discrimination such as digital redlining , it is important\\nto identify which proxy variables actually introduce group\\ndisparity. It is straightforward to observe that features with\\nhigh QII for group disparity are proxy variables, and also cause\\ngroup disparity. Therefore, QII on group disparity is a useful\\ndiagnostic tool for determining discriminiation. The use of QII\\nin identifying proxy variables is explored experimentally in\\nSection VII-B. Note that because of such proxy variables,\\nsimply ensuring that protected attributes are not input to\\nthe classiﬁer is not sufﬁcient to avoid discrimination (see\\nalso [12]).III. S ET AND MARGINAL QII\\nIn many situations, intervention on a single input variable\\nhas no inﬂuence on the outcome of a system. Consider, for\\nexample, a two-feature setting where features are age ( A) and\\nincome (I), and the classiﬁer is c(A;I) = (A=old)^(I=\\nhigh). In other words, the only datapoints that are labeled 1\\nare those of elderly persons with high income. Now, given\\na datapoint where A=young;I=low, an intervention on\\neither age or income would result in the same classiﬁcation.\\nHowever, it would be misleading to say that neither age nor\\nincome have an inﬂuence over the outcome: changing both the\\nstates of income and age would result in a change in outcome.\\nEquating inﬂuence with the individual ability to affect the\\noutcome is uninformative in real datasets as well: Figure 1 is a\\nhistogram of inﬂuences of features on outcomes of individuals\\nfor a classiﬁer learnt from the adult dataset [13]2. For most\\nindividuals, all features have zero inﬂuence: changing the state\\nof one feature alone is not likely to change the outcome of\\na classiﬁer. Of the 19537 datapoints we evaluate, more than\\nhalf have\\x13x(i) = 0 for alli2N, Indeed, changes to outcome\\nare more likely to occur if we intervene on sets of features .\\nIn order to get a better understanding of the inﬂuence of a\\nfeaturei2N, we should measure its effect when coupled\\nwith interventions on other features. We deﬁne the inﬂuence\\nof a set of inputs as a straightforward extension of the inﬂuence\\nof individual inputs. Essentially, we wish the inﬂuence of a set\\nof inputsS\\x12Nto be the same as when the set of inputs is\\nconsidered to be a single input; when intervening on S, we\\ndraw the states of i2Sbased on the joint distribution of the\\nstates of features in S,\\x19S(uS), as deﬁned in Equation (1).\\nWe can naturally deﬁne a distribution over X\\x02Q\\ni2SXi,\\nnaturally extending (2) as:\\n~\\x19(x;uS) =\\x19(x)\\x19S(uS): (9)\\nWe also deﬁne the random variable X\\x00SUS(x;uS) =\\nxjNnSuS;X\\x00S(x;uS)has the states of features in NnS\\nﬁxed to their original values in x, but features in Stake on\\nnew values according to uS.\\nDeﬁnition 2 (Set QII) .For a quantity of interest Q, and an\\ninputi, the Quantitative Input Inﬂuence of set S\\x12NonQ\\nis deﬁned to be\\n\\x13Q(S) =Q(X)\\x00Q(X\\x00SUS):\\nConsidering the inﬂuence of a set of inputs opens up a\\nnumber of interesting questions due to the interaction between\\ninputs. First among these is how does one measure the\\nindividual effect of a feature, given the measured effects of\\ninterventions on sets of features. One natural way of doing so\\nis by measuring the marginal effect of a feature on a set.\\n2The adult dataset contains approximately 31k datapoints of users’ personal\\nattributes, and whether their income is more than $50k per annum; see\\nSection VII for more details.', metadata={'page': 4.0, 'source': 'https://truera.com/wp-content/uploads/2020/08/Quantitative-Input-Influence-2016.pdf'}),\n",
       "  Document(page_content='Name Notation Quantity of Interest Sensitivity\\nQII on Individual Outcomes (3) \\x13ind(S) Positive Classiﬁcation of an Individual 1=jDj\\nQII on Actual Individual Outcomes (4) \\x13ind-act(S) Actual Classiﬁcation of an Individual 1=jDj\\nAverage QII (5) \\x13ind-avg (S) Average Actual Classiﬁcation 2=jDj\\nQII on Group Outcomes (6) \\x13Y\\ngrp(S) Positive Classiﬁcation for a Group 2=jD \\\\ Yj\\nQII on Group Disparity (8) \\x13Y\\ndisp(S) Difference in classiﬁcation rates among groups 2 max(1=jD n Yj;1=jD \\\\ Yj )\\nTABLE I: A summary of the QII measures deﬁned in the paper\\nclassiﬁers: (A) where gender is provided as an actual input,\\nand (B) where gender is not provided as an input. For classiﬁer\\n(B), clearly the input Gender has no effect and any correlation\\nbetween the outcome and gender is caused via inference from\\nother inputs. In Table II, for both the adult and the arrests\\ndataset, we compute the following observational measures:\\nMutual Information (MI), Jaccard Index (Jaccard), Pearson\\nCorrelation (corr), and the Disparate Impact Ratio (disp) to\\nmeasure the similarity between Gender and the classiﬁers\\noutcome. We also measure the QII of Gender on outcome.\\nWe observe that in many scenarios the observational quantities\\ndo not change, or sometimes increase, from classiﬁer A to\\nclassiﬁer B, when gender is removed as an actual input\\nto the classiﬁer. On the other hand, if the outcome of the\\nclassiﬁer does not depend on the input Gender , then the QII\\nis guaranteed to be zero.\\nB. Unary QII Measures\\nIn Figure 2, we illustrate the use of different Unary QII\\nmeasures. Figures 2a, and 2b, show the Average QII measure\\n(Equation 5) computed for features of a decision forest classi-\\nﬁer. For the income classiﬁer trained on the adult dataset, the\\nfeature with highest inﬂuence is Marital Status , followed by\\nOccupation ,Relationship andCapital Gain . Sensitive features\\nsuch as Gender and Race have relatively lower inﬂuence.\\nFor the predictive policing classiﬁer trained on the arrests\\ndataset, the most inﬂuential input is Drug History , followed by\\nGender , and Smoking History . We observe that inﬂuence on\\noutcomes may be different from inﬂuence on group disparity.\\nQII on group disparity: Figures 2c, 2d show inﬂuences\\nof features on group disparity for two different settings. The\\nﬁgure on the left shows the inﬂuence of features on group\\ndisparity by Gender in the adult dataset; the ﬁgure on the\\nright shows the inﬂuence of group disparity by Race in the\\narrests dataset. For the income classiﬁer trained on the\\nadult dataset, we observe that most inputs have negative\\ninﬂuence on group disparity; randomly intervening on most\\ninputs would lead to a reduction in group disparity. In other\\nwords, a classiﬁer that did not use these inputs would be fairer.\\nInterestingly, in this classiﬁer, marital status and not sex has\\nthe highest inﬂuence on group disparity by sex.\\nFor the arrests dataset, most inputs have the effect of\\nincreasing group disparity if randomly intervened on. In\\nparticular, Drug history has the highest positive inﬂuence on\\ndisparity in arrests . Although Drug history is correlated with\\nrace, using it reduces disparate impact by race, i.e. makes fairer\\ndecisions.In both examples, features correlated with the sensitive\\nattribute are the most inﬂuential for group disparity according\\nto the sensitive attribute instead of the sensitive attribute\\nitself. It is in this sense that QII measures can identify\\nproxy variables that cause associations between outcomes and\\nsensitive attributes.\\nQII with artiﬁcial discrimination: We simulate discrimi-\\nnation using an artiﬁcial experiment. We ﬁrst randomly assign\\nZIP codes to individuals in our dataset. Then to simulate\\nsystematic bias, we make an ffraction of the ZIP codes\\ndiscriminatory in the following sense: All individuals in the\\nprotected set are automatically assigned a negative classiﬁ-\\ncation outcome. We then study the change in the inﬂuence\\nof features as we increase f. Figure 3a, shows that the\\ninﬂuence of Gender increases almost linearly with f. Recall\\nthat Marital Status was the most inﬂuential feature for this\\nclassiﬁer without any added discrimination. As fincreases,\\nthe importance of Marital Status decreases as expected, since\\nthe number of individuals for whom Marital Status is pivotal\\ndecreases.\\nC. Personalized Transparency Reports\\nTo illustrate the utility of personalized transparency reports,\\nwe study the classiﬁcation of individuals who received poten-\\ntially unexpected outcomes. For the personalized transparency\\nreports, we use decision forests.\\nThe inﬂuence measure that we employ is the Shapley value,\\nwith the underlying cooperative game deﬁned over the local\\ninﬂuenceQ. In more detail, v(S) =\\x13QA(S), withQAbeing\\nE[c(\\x01) = 1jX=x]; that is, the marginal contribution of\\ni2NtoSis given by mi(S) =E[c(X\\x00S) = 1jX=\\nx]\\x00E[c(X\\x00S[fig) = 1jX=x].\\nWe emphasize that some features may have a negative\\nShapley value; this should be interpreted as follows: a feature\\nwith a high positive Shapley value often increases the certainty\\nthat the classiﬁcation outcome is 1, whereas a feature whose\\nShapley value is negative is one that increases the certainty\\nthat the classiﬁcation outcome would be zero.\\nMr. X: The ﬁrst example is of an individual from the\\nadult dataset, who we refer to as Mr. X, and is described in\\nFigure 4a. He is is deemed to be a low income individual, by\\nan income classiﬁer learned from the data. This result may be\\nsurprising to him: he reports high capital gains ($14k), and\\nonly 2.1% of people with capital gains higher than $10k are\\nreported as low income. In fact, he might be led to believe that\\nhis classiﬁcation may be a result of his ethnicity or country\\nof origin. Examining his transparency report in Figure 4b,\\nhowever, we ﬁnd that the the most inﬂuential features that led', metadata={'page': 10.0, 'source': 'https://truera.com/wp-content/uploads/2020/08/Quantitative-Input-Influence-2016.pdf'})]}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1.records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.chat_memory.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.predict(input=\"What does he work on?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
