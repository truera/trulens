{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got OPENAI_API_KEY\n",
      "got PINECONE_API_KEY\n",
      "got PINECONE_ENV\n",
      "got HUGGINGFACE_API_KEY\n",
      "got SLACK_TOKEN\n",
      "got SLACK_SIGNING_SECRET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotrm/anaconda3/envs/demo3/lib/python3.10/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from keys import *\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import pinecone\n",
    "import requests\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.chains import (ConversationalRetrievalChain,\n",
    "                              SimpleSequentialChain)\n",
    "from langchain.document_loaders import (PagedPDFSplitter, TextLoader,\n",
    "                                        UnstructuredHTMLLoader,\n",
    "                                        UnstructuredMarkdownLoader,\n",
    "                                        UnstructuredPDFLoader)\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline, OpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Pinecone\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "from keys import HUGGINGFACE_HEADERS\n",
    "from slackbot import chain\n",
    "\n",
    "from pprint import PrettyPrinter\n",
    "pp = PrettyPrinter()\n",
    "\n",
    "# https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/pinecone.html?highlight=pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel():\n",
    "    def __init__(self):\n",
    "        # llm = OpenAI()\n",
    "       \n",
    "        self.llm_model_id = \"gpt2\"\n",
    "        # This model is pretty bad but using it for tests because it is free and\n",
    "        # relatively small.\n",
    "\n",
    "        # model_id = \"decapoda-research/llama-7b-hf\"\n",
    "        # model_id = \"decapoda-research/llama-13b-hf\"\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.llm_model_id,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            local_files_only=True)\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.llm_model_id,\n",
    "                                                       local_files_only=True)\n",
    "\n",
    "        self.pipe = pipeline(\"text-generation\",\n",
    "                             model=self.model,\n",
    "                             tokenizer=self.tokenizer,\n",
    "                             max_new_tokens=16,\n",
    "                             device_map=\"auto\",\n",
    "                             early_stopping=True)\n",
    "\n",
    "        self.llm = HuggingFacePipeline(pipeline=self.pipe)\n",
    "\n",
    "        template = \"\"\"Q: {question} A:\"\"\"\n",
    "        self.prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "        self.llm_chain = LLMChain(prompt=self.prompt, llm=self.llm, verbose=True)\n",
    "\n",
    "t = TestModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tru_chain import TruChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mQ: hello there A:\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'hello there',\n",
       " 'text': ' hello there\\n\\n\\nPTSD: hello there\\n\\n[16:39'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc = TruChain(t.llm_chain)\n",
    "tc(\"hello there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cls': 'LLMChain',\n",
       " 'module': 'langchain.chains.llm',\n",
       " 'value': None,\n",
       " 'dump': None,\n",
       " 'ident': 140367896061136,\n",
       " 'address': (),\n",
       " 'fields': {'memory': {'cls': 'NoneType',\n",
       "   'module': 'builtins',\n",
       "   'value': None,\n",
       "   'dump': None,\n",
       "   'ident': 7653056,\n",
       "   'address': ('memory',),\n",
       "   'fields': None},\n",
       "  'callback_manager': {'cls': 'SharedCallbackManager',\n",
       "   'module': 'langchain.callbacks.shared',\n",
       "   'value': <langchain.callbacks.shared.SharedCallbackManager at 0x7fa9ba4233d0>,\n",
       "   'dump': None,\n",
       "   'ident': 140366951101392,\n",
       "   'address': ('callback_manager',),\n",
       "   'fields': None},\n",
       "  'verbose': True,\n",
       "  'prompt': {'cls': 'PromptTemplate',\n",
       "   'module': 'langchain.prompts.prompt',\n",
       "   'value': None,\n",
       "   'dump': None,\n",
       "   'ident': 140367896058832,\n",
       "   'address': ('prompt',),\n",
       "   'fields': {'input_variables': ['question'],\n",
       "    'partial_variables': {'cls': 'dict',\n",
       "     'module': 'builtins',\n",
       "     'value': {},\n",
       "     'dump': None,\n",
       "     'ident': 140367853876608,\n",
       "     'address': ('prompt', 'partial_variables'),\n",
       "     'fields': None},\n",
       "    'template': 'Q: {question} A:',\n",
       "    'template_format': 'f-string',\n",
       "    'validate_template': True}},\n",
       "  'llm': {'cls': 'HuggingFacePipeline',\n",
       "   'module': 'langchain.llms.huggingface_pipeline',\n",
       "   'value': None,\n",
       "   'dump': None,\n",
       "   'ident': 140367896059936,\n",
       "   'address': ('llm',),\n",
       "   'fields': {'verbose': False,\n",
       "    'pipeline': {'cls': 'TextGenerationPipeline',\n",
       "     'module': 'transformers.pipelines.text_generation',\n",
       "     'value': <transformers.pipelines.text_generation.TextGenerationPipeline at 0x7fa9eb3a66e0>,\n",
       "     'dump': None,\n",
       "     'ident': 140367937543888,\n",
       "     'address': ('llm', 'pipeline'),\n",
       "     'fields': None},\n",
       "    'model_id': 'gpt2'}},\n",
       "  'output_key': 'text'}}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc.model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[defaultdict(list,\n",
       "             {(): [2176693/2176693 2023-05-06 20:48:51.621800 - 2023-05-06 20:48:51.756299\n",
       "               input:\n",
       "                 {'inputs': {'question': 'hello there'}}\n",
       "               output:\n",
       "                 {'text': ' hello there\\n\\n\\nPTSD: hello there\\n\\n[16:39'}\n",
       "               chain call stack:\n",
       "                 [()]]})]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc.records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(): [{'input': {'inputs': {'question': 'hello there'}}, 'output': {'text': ' hello there\\n\\n\\nPTSD: hello there\\n\\n[16:39'}, 'error': None, 'stack': [()], 'start_time': datetime.datetime(2023, 5, 6, 20, 48, 51, 621800), 'end_time': datetime.datetime(2023, 5, 6, 20, 48, 51, 756299), 'pid': 2176693, 'tid': 2176693}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_2176693/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">1728294382.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_2176693/1728294382.py'</span>                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/piotrm/repos/llm-experiments/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">tru_chain.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">276</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_select</span>                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">273 │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> s.model <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> record:                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">274 │   │   │   │   │   │   </span>temp = record[s.model]                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">275 │   │   │   │   │   │   </span>                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>276 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>temp = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._get_obj_at_address(s.record, obj = record)            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">277 │   │   │   │   │   </span>                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">278 │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">279 │   │   │   │   │   │   </span>temp = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/piotrm/repos/llm-experiments/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">tru_chain.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">304</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_get_obj_at_address</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">301 │   │   │   </span>rest = ()                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">302 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">303 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(first, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>):                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>304 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">assert</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">hasattr</span>(                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">305 │   │   │   │   </span>obj, <span style=\"color: #808000; text-decoration-color: #808000\">\"__fields__\"</span>                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">306 │   │   │   </span>), <span style=\"color: #808000; text-decoration-color: #808000\">f\"pydantic.BaseModel expected but was {</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(obj).mro()<span style=\"color: #808000; text-decoration-color: #808000\">}\"</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">307 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">assert</span> first <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> obj.__fields__, <span style=\"color: #808000; text-decoration-color: #808000\">f\"Object has no field '{</span>first<span style=\"color: #808000; text-decoration-color: #808000\">}', it has {</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">lis</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AssertionError: </span>pydantic.BaseModel expected but was <span style=\"font-weight: bold\">[&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'dict'</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;, &lt;class </span><span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"font-weight: bold\">&gt;]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_2176693/\u001b[0m\u001b[1;33m1728294382.py\u001b[0m:\u001b[94m3\u001b[0m in \u001b[92m<module>\u001b[0m                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_2176693/1728294382.py'\u001b[0m                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/piotrm/repos/llm-experiments/\u001b[0m\u001b[1;33mtru_chain.py\u001b[0m:\u001b[94m276\u001b[0m in \u001b[92m_select\u001b[0m                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m273 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mif\u001b[0m s.model \u001b[95min\u001b[0m record:                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m274 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mtemp = record[s.model]                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m275 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m276 \u001b[2m│   │   │   │   │   │   \u001b[0mtemp = \u001b[96mself\u001b[0m._get_obj_at_address(s.record, obj = record)            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m277 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m278 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m279 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mtemp = \u001b[94mNone\u001b[0m                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/piotrm/repos/llm-experiments/\u001b[0m\u001b[1;33mtru_chain.py\u001b[0m:\u001b[94m304\u001b[0m in \u001b[92m_get_obj_at_address\u001b[0m                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m301 \u001b[0m\u001b[2m│   │   │   \u001b[0mrest = ()                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m302 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m303 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(first, \u001b[96mstr\u001b[0m):                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m304 \u001b[2m│   │   │   \u001b[0m\u001b[94massert\u001b[0m \u001b[96mhasattr\u001b[0m(                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m305 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mobj, \u001b[33m\"\u001b[0m\u001b[33m__fields__\u001b[0m\u001b[33m\"\u001b[0m                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m306 \u001b[0m\u001b[2m│   │   │   \u001b[0m), \u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mpydantic.BaseModel expected but was \u001b[0m\u001b[33m{\u001b[0m\u001b[96mtype\u001b[0m(obj).mro()\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m307 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94massert\u001b[0m first \u001b[95min\u001b[0m obj.__fields__, \u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mObject has no field \u001b[0m\u001b[33m'\u001b[0m\u001b[33m{\u001b[0mfirst\u001b[33m}\u001b[0m\u001b[33m'\u001b[0m\u001b[33m, it has \u001b[0m\u001b[33m{\u001b[0m\u001b[96mlis\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mAssertionError: \u001b[0mpydantic.BaseModel expected but was \u001b[1m[\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'dict'\u001b[0m\u001b[39m>, <class \u001b[0m\u001b[32m'object'\u001b[0m\u001b[1m>\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tru_chain import Selection\n",
    "\n",
    "tc._select(select=[\n",
    "    Selection(param=(\"chain\", \"prompt\", \"template\")),\n",
    "    Selection(param=(\"chain\", \"llm\", \"model_id\")),\n",
    "    Selection(record=(\"input\", \"inputs\", \"question\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.chains.llm.LLMChain (at ('chain',), loaded at 140367896061136)\n",
       "  memory: builtins.NoneType (at ('chain', 'memory'), loaded at 7653056)\n",
       "  callback_manager: langchain.callbacks.shared.SharedCallbackManager (at ('chain', 'callback_manager'), loaded at 140366951101392) = <langchain.callbacks.shared.SharedCallbackManager object at 0x7fa9ba4233d0>\n",
       "  verbose = True\n",
       "  prompt: langchain.prompts.prompt.PromptTemplate (at ('chain', 'prompt'), loaded at 140367896058832)\n",
       "    input_variables = ['question']\n",
       "    partial_variables: builtins.dict (at ('chain', 'prompt', 'partial_variables'), loaded at 140367853876608) = {}\n",
       "    template = 'Q: {question} A:'\n",
       "    template_format = 'f-string'\n",
       "    validate_template = True\n",
       "  llm: langchain.llms.huggingface_pipeline.HuggingFacePipeline (at ('chain', 'llm'), loaded at 140367896059936)\n",
       "    verbose = False\n",
       "    pipeline: transformers.pipelines.text_generation.TextGenerationPipeline (at ('chain', 'llm', 'pipeline'), loaded at 140367937543888) = <transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7fa9f50e1ed0>\n",
       "    model_id = 'gpt2'\n",
       "  output_key = 'text'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Q: {question} A:\"\"\"\n",
    "prompt = PromptTemplate(template=template,\n",
    "                        input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=t.llm)\n",
    "\n",
    "template_2 = \"\"\"Reverse this sentence: {sentence}.\"\"\"\n",
    "prompt_2 = PromptTemplate(template=template_2,\n",
    "                            input_variables=[\"sentence\"])\n",
    "llm_chain_2 = LLMChain(prompt=prompt_2, llm=t.llm)\n",
    "\n",
    "seq_chain = SimpleSequentialChain(chains=[llm_chain, llm_chain_2],\n",
    "                                    input_key=\"question\",\n",
    "                                    output_key=\"answer\")\n",
    "\n",
    "tc = TruChain(seq_chain)\n",
    "tc.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc(\"hello there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc._get_obj_at_address(address=(\"chains\", 0, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "1. Langchain does not have support for classification models: https://python.langchain.com/en/latest/modules/models.html\n",
    "\n",
    "    - Will have to figure out out-of-band retrieval and execution of feedback models that are not LLM's.\n",
    "\n",
    "2. Can add steps to chain to capture text at various points in a chain: https://python.langchain.com/en/latest/reference/modules/chains.html#langchain.chains.SequentialChain .\n",
    "\n",
    "\n",
    "# Links\n",
    "\n",
    "- https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/llama#transformers.LlamaForCausalLM\n",
    "\n",
    "- https://huggingface.co/docs/transformers/main_classes/text_generation\n",
    "\n",
    "\n",
    "# Pinecone\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from slackbot import chain\n",
    "import langchain\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import (ConversationalRetrievalChain,\n",
    "                              SimpleSequentialChain)\n",
    "\n",
    "verb = False\n",
    "\n",
    "template = \"\"\"Q: {question} A:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=t.llm, verbose=verb)\n",
    "\n",
    "template_2 = \"\"\"Reverse this sentence: {sentence}.\"\"\"\n",
    "prompt_2 = PromptTemplate(template=template_2, input_variables=[\"sentence\"])\n",
    "llm_chain_2 = LLMChain(prompt=prompt_2, llm=t.llm, verbose=verb)\n",
    "\n",
    "# print(llm_chain.run(question=\"What is the average air speed velocity of a laden swallow?\"))\n",
    "\n",
    "print(llm_chain_2.run(sentence=\"How are you doing?\"))\n",
    "\n",
    "seq_chain = SimpleSequentialChain(chains=[llm_chain, llm_chain_2], input_key=\"question\", output_key=\"answer\")\n",
    "seq_chain.run(question=\"What is the average air speed velocity of a laden swallow?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_chain_2 = TruChain(seq_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_chain.run(question=\"What is the average air speed velocity of a laden swallow? again\")\n",
    "tru_chain_2.run(question=\"What is the average air speed velocity of a laden swallow?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in tru_chain_2.records:\n",
    "    print(pp.pformat(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_chain_2.run(question=\"What is the average air speed velocity of a laden swallow?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_chain_2.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "for frame_info in inspect.stack():\n",
    "    frame = frame_info.frame\n",
    "    print(frame_info.function)\n",
    "    # print(frame.f_code)\n",
    "    print(frame.f_locals.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
