{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from keys import *\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "from tinydb import TinyDB\n",
    "\n",
    "import pinecone\n",
    "import requests\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import (ConversationalRetrievalChain,\n",
    "                              SimpleSequentialChain)\n",
    "from langchain.document_loaders import (PagedPDFSplitter, TextLoader,\n",
    "                                        UnstructuredHTMLLoader,\n",
    "                                        UnstructuredMarkdownLoader,\n",
    "                                        UnstructuredPDFLoader)\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline, OpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Pinecone\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "from keys import HUGGINGFACE_HEADERS\n",
    "# from slackbot import obj\n",
    "\n",
    "from pprint import PrettyPrinter\n",
    "pp = PrettyPrinter()\n",
    "\n",
    "# https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/pinecone.html?highlight=pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TestModel():\n",
    "    def __init__(self):\n",
    "        # llm = OpenAI()\n",
    "       \n",
    "        self.llm_model_id = \"gpt2\"\n",
    "        # This model is pretty bad but using it for tests because it is free and\n",
    "        # relatively small.\n",
    "\n",
    "        # model_id = \"decapoda-research/llama-7b-hf\"\n",
    "        # model_id = \"decapoda-research/llama-13b-hf\"\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.llm_model_id,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            local_files_only=True)\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.llm_model_id,\n",
    "                                                       local_files_only=True)\n",
    "\n",
    "        self.pipe = pipeline(\"text-generation\",\n",
    "                             model=self.model,\n",
    "                             tokenizer=self.tokenizer,\n",
    "                             max_new_tokens=16,\n",
    "                             device_map=\"auto\",\n",
    "                             early_stopping=True)\n",
    "\n",
    "        self.llm = HuggingFacePipeline(pipeline=self.pipe)\n",
    "\n",
    "        self.memory=ConversationBufferWindowMemory(k=2)\n",
    "\n",
    "        template = \"\"\"Q: {question} A:\"\"\"\n",
    "        self.prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "        self.llm_chain = LLMChain(prompt=self.prompt, llm=self.llm, verbose=True, memory=self.memory)\n",
    "\n",
    "\n",
    "t = TestModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tru_chain import TruChain\n",
    "from tru_db import Record, TruTinyDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db = TruTinyDB(\"temp.json\")\n",
    "tc = TruChain(t.llm_chain)\n",
    "\n",
    "tc.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"something\" in db.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.insert_model(model_name=tc.model_name, model=tc.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.models.contains(doc_id=tc.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tc(\"hello there\")\n",
    "tc(\"hello there general kanobi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tc.db.select(Record.chain._call.args.inputs, Record.chain._call.rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "json.dumps(TinyDB, default=lambda o: \"lol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tc.db.select(\n",
    "    Record.chain.prompt.template,\n",
    "    Record.chain.llm._call.args.prompt,\n",
    "    Record.chain._call.args.inputs.question,\n",
    "    Record.chain._call.rets.text,\n",
    "    where=Record.chain._call.rets.text != None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Q: {question} A:\"\"\"\n",
    "prompt = PromptTemplate(template=template,\n",
    "                        input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=t.llm)\n",
    "\n",
    "template_2 = \"\"\"Reverse this sentence: {sentence}.\"\"\"\n",
    "prompt_2 = PromptTemplate(template=template_2,\n",
    "                            input_variables=[\"sentence\"])\n",
    "llm_chain_2 = LLMChain(prompt=prompt_2, llm=t.llm)\n",
    "\n",
    "seq_chain = SimpleSequentialChain(chains=[llm_chain, llm_chain_2],\n",
    "                                    input_key=\"question\",\n",
    "                                    output_key=\"answer\")\n",
    "\n",
    "tc = TruChain(seq_chain)\n",
    "tc.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc(\"hello there\")\n",
    "tc(\"hello there mister bond\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.db.select(Record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "1. Langchain does not have support for classification models: https://python.langchain.com/en/latest/modules/models.html\n",
    "\n",
    "    - Will have to figure out out-of-band retrieval and execution of feedback models that are not LLM's.\n",
    "\n",
    "2. Can add steps to chain to capture text at various points in a chain: https://python.langchain.com/en/latest/reference/modules/chains.html#langchain.chains.SequentialChain .\n",
    "\n",
    "\n",
    "# Links\n",
    "\n",
    "- https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/llama#transformers.LlamaForCausalLM\n",
    "\n",
    "- https://huggingface.co/docs/transformers/main_classes/text_generation\n",
    "\n",
    "\n",
    "# Pinecone\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from slackbot\n",
    "import langchain\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import (ConversationalRetrievalChain,\n",
    "                              SimpleSequentialChain)\n",
    "\n",
    "verb = True\n",
    "\n",
    "template = \"\"\"Q: {question} A:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=t.llm, verbose=verb)\n",
    "\n",
    "template_2 = \"\"\"Reverse this sentence: {sentence}.\"\"\"\n",
    "prompt_2 = PromptTemplate(template=template_2, input_variables=[\"sentence\"])\n",
    "llm_chain_2 = LLMChain(prompt=prompt_2, llm=t.llm, verbose=verb)\n",
    "\n",
    "# print(llm_chain.run(question=\"What is the average air speed velocity of a laden swallow?\"))\n",
    "\n",
    "print(llm_chain_2.run(sentence=\"How are you doing?\"))\n",
    "\n",
    "seq_chain = SimpleSequentialChain(chains=[llm_chain, llm_chain_2], input_key=\"question\", output_key=\"answer\")\n",
    "seq_chain.run(question=\"What is the average air speed velocity of a laden swallow?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_chain_2 = TruChain(seq_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_chain.run(question=\"What is the average air speed velocity of a laden swallow? again\")\n",
    "tru_chain_2.run(question=\"What is the average air speed velocity of a laden swallow?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in tru_chain_2.db.select(Record).iterrows():\n",
    "    print(pp.pformat(r[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_chain_2.run(question=\"What is the average air speed velocity of a laden swallow?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruBot testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory, ConversationSummaryBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "    environment=PINECONE_ENV  # next to api key in console\n",
    ")\n",
    "\n",
    "index_name = \"llmdemo\"\n",
    "\n",
    "verb = True\n",
    "\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-ada-002')  # 1536 dims\n",
    "\n",
    "docsearch = Pinecone.from_existing_index(\n",
    "    index_name=index_name, embedding=embedding\n",
    ")\n",
    "\n",
    "llm = OpenAI(temperature=0, max_tokens=128, verbose=verb)\n",
    "retriever = docsearch.as_retriever()\n",
    "\n",
    "convos = dict()\n",
    "\n",
    "# db = TinyDB(\"test.records.json\", default=lambda o: f\"NON-SERIALIZED OBJECT: {o}\")\n",
    "db = TruTinyDB()\n",
    "\n",
    "def get_convo(cid):\n",
    "    if cid in convos:\n",
    "        return convos[cid]\n",
    "    \n",
    "    memory = ConversationSummaryBufferMemory(max_token_limit = 650, llm=llm, memory_key=\"chat_history\", output_key='answer')\n",
    "    chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm, retriever=retriever, verbose=verb, return_source_documents=True, memory=memory, get_chat_history=lambda h : h,\n",
    "        max_tokens_limit=4096\n",
    "    )\n",
    "    return TruChain(chain)\n",
    "\n",
    "c1 = get_convo(\"piotrm\")\n",
    "#chain = ConversationChain(\n",
    "#    llm=llm, memory=memory, verbose=verb\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1(\"How can I measure performance?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1(\"What metrics can I use to measure it?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(c1.db.select())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = c1(dict(question=\"What is QII?\"))\n",
    "# chain.predict(input=\"Who is Piotr?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = c1.db.select(Record).iloc[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tru_db import TruDB\n",
    "\n",
    "for p, v in TruDB.leafs(model):\n",
    "    print(f\"{p}:{type(v)} = {pp.pformat(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tru_db import TruTinyDB, Record\n",
    "from IPython.display import JSON\n",
    "db = TruTinyDB(\"slackbot.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx, record in list(db.select(\n",
    "    Record.chain._call.args.inputs,\n",
    "    Record.chain.combine_docs_chain._call.args.inputs.input_documents,\n",
    "    Record.chain._call.rets\n",
    "    ).iterrows())[-6:]:\n",
    "    ins = record[0]\n",
    "    docs = record[1]\n",
    "    ans = record[2]\n",
    "    #print(\"\\nINPUTS\")#\n",
    "    display(JSON(ins, expanded=False, root=\"inputs\"))#\", ins, \"\\n\")\n",
    "    #print(\"\\nOUTPUTS\")#, ans, \"\\n\")\n",
    "    display(JSON(ans, expanded=False, root=\"outputs\", hide=['']))\n",
    "    for doc in docs:\n",
    "        display(JSON(doc, expanded=False, root=\"chunk\"))\n",
    "        #display(JSON(doc['metadata']))\n",
    "        #display(JSON(doc['page_content']))\n",
    "        # print(\"\\n\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"\\n\")\n",
    "    # display(JSON(record, expanded=False))\n",
    "    # display(JSON(sources))\n",
    "    #for k, v in TruDB.leafs(record):\n",
    "    #    print(f\"{k}:{type(v)} = \")\n",
    "    #    pp.pprint(v)\n",
    "    # print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "JSON(data=record, expanded=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question: str = \"How can I measure performance?\"\n",
    "response = chain(question)['answer']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm(\"Who is Reimi?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are a RELEVANCE classifier, providing the relevance of the given statement to the given question.\n",
    "Provide all responses only as a number from 1 to 10 where 1 is the least relevant and 10 is the most relevant.\n",
    "Never elaborate. \n",
    "\n",
    "STATEMENT: Most recently, Shameek was Group Chief Data Officer at Standard Chartered Bank, where he helped the bank explore and adopt AI in multiple areas (e.g., credit, financial crime compliance, customer analytics, surveillance), and shaped the bank’s internal approach to responsible AI.\n",
    "\n",
    "QUESTION: Who is Shayak?\n",
    "\n",
    "RELEVANCE:\"\"\"\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are a RELEVANCE classifier, providing the relevance of the given statement to the given question.\n",
    "Provide all responses only as a number from 1 to 10 where 1 is the least relevant and 10 is the most relevant.\n",
    "Never elaborate. \n",
    "\n",
    "STATEMENT: When Shayak started building production grade machine learning models for algorithmic trading 10 years ago, he realized the need for putting the ‘science’ back in ‘data science’. Since then, he has been building systems and leading research to make machine learning and big data systems more explainable, privacy compliant, and fair. Shayak’s research at Carnegie Mellon University introduced a number of pioneering breakthroughs to the field of explainable AI. Shayak obtained his PhD in Computer Science from Carnegie Mellon University and BTech in Computer Science from the Indian Institute of Technology, Delhi.\n",
    "\n",
    "QUESTION: Who is Shayak?\n",
    "\n",
    "RELEVANCE:\"\"\"\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are a RELEVANCE classifier, providing the relevance of the given statement to the given question.\n",
    "Provide all responses only as a number from 1 to 10 where 1 is the least relevant and 10 is the most relevant.\n",
    "Never elaborate. \n",
    "\n",
    "STATEMENT: Shameek has spent most of his career in driving responsible adoption of data analytics/ AI in the financial services industry. Most recently, Shameek was Group Chief Data Officer at Standard Chartered Bank, where he helped the bank explore and adopt AI in multiple areas and shaped the bank’s internal approach to responsible AI. He plays an active role in the future of AI as a member of the Bank of England’s AI Public-Private Forum and the OECD Global Partnership on AI.\n",
    "\n",
    "QUESTION: Who is Shayak?\n",
    "\n",
    "RELEVANCE:\"\"\"\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are a RELEVANCE classifier, providing the relevance of the given statement to the given question.\n",
    "Provide all responses only as a number from 1 to 10 where 1 is the least relevant and 10 is the most relevant.\n",
    "Never elaborate. \n",
    "\n",
    "STATEMENT: {statement}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "RELEVANCE:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Translate the following statement to the same language as the question.\n",
    "\n",
    "Statement: 2.2 Quantitative Input In\\ruence (QII)\\nAs stressed in the previous section, at the heart of our framework developed in this paper is an\\ninstance-based explanation approach - the Quantitative Input In\\ruence (QII) approach. We\\noutline it in this section, before moving to combining it with other approaches. Traditionally,\\nin\\ruence measures have been studied for feature selection, i.e. informing the choice of which\\nvariables to include in the model [8]. Recently, in\\ruence measures have been used as explain-\\nability mechanisms [1, 7, 9] for complex models. In\\ruence measures explain the behavior of\\nmodels by indicating the relative importance of inputs and their direction. While the space\\nof potential in\\ruence measures is quite large, we point out two requirements that they need\\nto satisfy: (i) taking into account variable correlations, and (ii) capturing feature interactions.\\nWhen inputs to a model tend to move together (e.g. income and loan size), simple measures\\nof association (such as the correlation between income and defaults) do not distinguish the di-\\nrection in which each a\\u000bects outcomes. In complex non-linear classi\\fers e\\u000bects arise out of the\\ninteraction between inputs (e.g. if only individuals with high age andhigh income are deemed\\ncreditworthy), and therefore in\\ruence measures should account for these.\\nQII [1] controls for correlations by employing randomising interventions on inputs, and\\naccounts for input interactions by measuring the average marginal contributions of inputs over\\nall sets of features they may interact with. In other words, with this technique, we attempt\\nvarious changes of input variables and analyse what changes to output variables they produce.\\nTo see how this works in practice, we focus on the example highlighted in this paper,\\nmortgage defaults. Suppose we are interested in the in\\ruence of a particular input|say, the\\nborrower's income|on the probability of default estimated by the model, which becomes our\\nquantity of interest . (Any property of the system conditional on a particular distribution of\\ninputs can be a quantity of interest for measuring QII.) In what follows, we start by concen-\\ntrating on individual outcomes, i.e. on questions such as \\\\why was the loan application of this\\nindividual rejected?\\\" or \\\\what would it take for that particular individual to lower their default\\nprobability?\\\". (These are type-1 explanations according to Table 1.) We will then build up\\nfrom these individual-level questions to the the overall functioning of the model, and focus on\\n7\n",
    "\n",
    "Question: Por favor expliqueme, que es QII?\n",
    "\n",
    "Translated Statement:\"\"\"\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keys import *\n",
    "import tru_chain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import HumanMessagePromptTemplate, ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "def run_chain(prompt):\n",
    "    full_prompt = HumanMessagePromptTemplate(\n",
    "            prompt=PromptTemplate(\n",
    "                template=\n",
    "                \"Provide a helpful response with relevant background information for the following: {prompt}\",\n",
    "                input_variables=[\"prompt\"],\n",
    "            )\n",
    "        )\n",
    "    chat_prompt_template = ChatPromptTemplate.from_messages([full_prompt])\n",
    "\n",
    "    chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.9)\n",
    "\n",
    "    chain = LLMChain(llm=chat, prompt=chat_prompt_template)\n",
    "    tc = tru_chain.TruChain(chain)\n",
    "\n",
    "    return tc(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_chain(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
