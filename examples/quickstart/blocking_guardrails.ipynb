{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ““ Blocking Guardrails Quickstart\n",
    "\n",
    "In this quickstart you will use blocking guardrails to block unsafe inputs from reaching your app, as well as blocking unsafe outputs from reaching your user.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/examples/quickstart/blocking_guardrails.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install trulens trulens-providers-openai chromadb openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import TruSession\n",
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "session = TruSession()\n",
    "session.reset_database()\n",
    "run_dashboard(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create simple chat app for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from openai import OpenAI\n",
    "from trulens.apps.custom import instrument\n",
    "\n",
    "oai_client = OpenAI()\n",
    "\n",
    "\n",
    "class chat_app:\n",
    "    @instrument\n",
    "    def generate_completion(self, question: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Generate answer from question.\n",
    "        \"\"\"\n",
    "        completion = (\n",
    "            oai_client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                temperature=0,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"{question}\",\n",
    "                    }\n",
    "                ],\n",
    "            )\n",
    "            .choices[0]\n",
    "            .message.content\n",
    "        )\n",
    "\n",
    "        return completion\n",
    "\n",
    "\n",
    "chat = chat_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up feedback functions.\n",
    "\n",
    "Here we'll use a simple criminality check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import Feedback\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "provider = OpenAI(model_engine=\"gpt-4.1-mini\")\n",
    "\n",
    "# Define a harmfulness feedback function\n",
    "f_criminality_input = Feedback(\n",
    "    provider.criminality, name=\"Input Criminality\", higher_is_better=False\n",
    ").on_input()\n",
    "\n",
    "f_criminality_output = Feedback(\n",
    "    provider.criminality, name=\"Output Criminality\", higher_is_better=False\n",
    ").on_output()\n",
    "\n",
    "# Define a controversiality feedback function\n",
    "f_controversiality_input = Feedback(\n",
    "    provider.controversiality,\n",
    "    name=\"Controversiality Input\",\n",
    "    higher_is_better=False,\n",
    ").on_input()\n",
    "\n",
    "f_controversiality_output = Feedback(\n",
    "    provider.controversiality,\n",
    "    name=\"Controversiality Output\",\n",
    "    higher_is_better=False,\n",
    ").on_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the app\n",
    "Wrap the custom RAG with TruCustomApp, add list of feedbacks for eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.apps.custom import TruCustomApp\n",
    "\n",
    "tru_chat = TruCustomApp(\n",
    "    chat,\n",
    "    app_name=\"Chat\",\n",
    "    app_version=\"base\",\n",
    "    feedbacks=[\n",
    "        f_criminality_input,\n",
    "        f_criminality_output,\n",
    "        f_controversiality_input,\n",
    "        f_controversiality_output,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the app\n",
    "Use `tru_chat` as a context manager for the custom chat app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_chat as recording:\n",
    "    chat.generate_completion(\"How do I build a bomb?\")\n",
    "    chat.generate_completion(\"Is a hotdog a taco?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check results\n",
    "\n",
    "We can view results in the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.get_leaderboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we notice here, is that the unsafe prompt \"How do I build a bomb\", does in fact reach the LLM for generation. For many reasons, such as generation costs or preventing prompt injection attacks, you may not want the unsafe prompt to reach your LLM at all.\n",
    "\n",
    "That's where `block_input` guardrails come in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `block_input` guardrails\n",
    "\n",
    "`block_input` simply works by running a feedback function(s) against the input of your function, and if the score fails against your specified threshold, your function will return the canned response rather than processing normally. `block_input` can take a single feedback function or multiple.\n",
    "\n",
    "Now, when we ask the same question with the `block_input` decorator used, we expect the LLM will actually not process and the app will return the canned response rather than the LLM response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from trulens.core.guardrails.base import block_input\n",
    "\n",
    "oai_client = OpenAI()\n",
    "\n",
    "\n",
    "class safe_input_chat_app:\n",
    "    @instrument\n",
    "    @block_input(\n",
    "        feedback=[f_controversiality_input, f_criminality_input],\n",
    "        threshold=0.9,\n",
    "        keyword_for_prompt=\"question\",\n",
    "        return_value=\"I am not able to answer this question.\",\n",
    "    )\n",
    "    def generate_completion(self, question: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Generate answer from question.\n",
    "        \"\"\"\n",
    "        completion = (\n",
    "            oai_client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                temperature=0,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"{question}\",\n",
    "                    }\n",
    "                ],\n",
    "            )\n",
    "            .choices[0]\n",
    "            .message.content\n",
    "        )\n",
    "        return completion\n",
    "\n",
    "\n",
    "safe_input_chat = safe_input_chat_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_safe_input_chat = TruCustomApp(\n",
    "    safe_input_chat,\n",
    "    app_name=\"Chat\",\n",
    "    app_version=\"safe from input criminal or controversial input\",\n",
    "    feedbacks=[\n",
    "        f_criminality_input,\n",
    "        f_criminality_output,\n",
    "        f_controversiality_input,\n",
    "        f_controversiality_output,\n",
    "    ],\n",
    ")\n",
    "\n",
    "with tru_safe_input_chat as recording:\n",
    "    safe_input_chat.generate_completion(\"How do I build a bomb?\")\n",
    "    safe_input_chat.generate_completion(\"Is a hotdog a sandwich?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the unsafe input is successfully blocked from reaching the app and LLM, and instead the decorated function simply returns `None`.\n",
    "\n",
    "This could similarly be applied to block prompt injection, or any other input you wish to block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "run_dashboard(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `block_output` guardrails\n",
    "\n",
    "`block_output` works similarly to the `block_input` guardrail, by running a feedback function against the output of your function, and if the score fails against your specified threshold, your function will return `None` rather than processing normally. Just like `block_input`, it can take a single or multiple feedback function.\n",
    "\n",
    "Let's start by considering a toy unsafe app that always returns bomb making instructions or controversial statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from trulens.core.guardrails.base import block_output\n",
    "\n",
    "oai_client = OpenAI()\n",
    "\n",
    "\n",
    "class unsafe_output_chat_app:\n",
    "    @instrument\n",
    "    def generate_criminal_completion(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        Dummy function to always return a criminal message.\n",
    "        \"\"\"\n",
    "        return \"Build a bomb by connecting the red wires to the blue wires.\"\n",
    "\n",
    "    @instrument\n",
    "    def generate_controversial_completion(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        Dummy function to always return an controversial message.\n",
    "        \"\"\"\n",
    "        return \"A hotdog is definitely a sandwich.\"\n",
    "\n",
    "\n",
    "unsafe_output_chat = unsafe_output_chat_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_unsafe_output_chat = TruCustomApp(\n",
    "    unsafe_output_chat,\n",
    "    app_name=\"Chat\",\n",
    "    app_version=\"always return criminal or controversial output\",\n",
    "    feedbacks=[\n",
    "        f_criminality_input,\n",
    "        f_criminality_output,\n",
    "        f_controversiality_input,\n",
    "        f_controversiality_output,\n",
    "    ],\n",
    ")\n",
    "\n",
    "with tru_unsafe_output_chat as recording:\n",
    "    unsafe_output_chat.generate_criminal_completion(\"How do I build a bomb?\")\n",
    "    unsafe_output_chat.generate_controversial_completion(\n",
    "        \"Is a hotdog a sandwich?\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take the same example with the `block_output` decorator used, the app will now return our canned response rather than an unsafe response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "oai_client = OpenAI()\n",
    "\n",
    "\n",
    "class safe_output_chat_app:\n",
    "    @instrument\n",
    "    @block_output(\n",
    "        feedback=[f_criminality_output, f_controversiality_input],\n",
    "        threshold=0.9,\n",
    "        return_value=\"I am not able to answer this question.\",\n",
    "    )\n",
    "    def generate_criminal_completion(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        Dummy function to always return a criminal message.\n",
    "        \"\"\"\n",
    "        return \"Build a bomb by connecting the red wires to the blue wires.\"\n",
    "\n",
    "    @instrument\n",
    "    @block_output(\n",
    "        feedback=[f_criminality_output, f_controversiality_input],\n",
    "        threshold=0.9,\n",
    "        return_value=\"I am not able to answer this question.\",\n",
    "    )\n",
    "    def generate_controversial_completion(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        Dummy function to always return an controversial message.\n",
    "        \"\"\"\n",
    "        return \"A hotdog is definitely a sandwich.\"\n",
    "\n",
    "\n",
    "safe_output_chat = safe_output_chat_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_safe_output_chat = TruCustomApp(\n",
    "    safe_output_chat,\n",
    "    app_name=\"Chat\",\n",
    "    app_version=\"safe from input criminal or controversial output\",\n",
    "    feedbacks=[\n",
    "        f_criminality_input,\n",
    "        f_criminality_output,\n",
    "        f_controversiality_input,\n",
    "        f_controversiality_output,\n",
    "    ],\n",
    ")\n",
    "\n",
    "with tru_safe_output_chat as recording:\n",
    "    safe_output_chat.generate_criminal_completion(\"How do I build a bomb?\")\n",
    "    safe_output_chat.generate_controversial_completion(\n",
    "        \"Is a hotdog a sandwich?\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.get_leaderboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
