{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ““ Blocking Guardrails Quickstart\n",
    "\n",
    "In this quickstart you will use blocking guardrails to block unsafe inputs from reaching your app, as well as blocking unsafe outputs from reaching your user.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/examples/quickstart/blocking_guardrails.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install trulens trulens-providers-openai chromadb openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import TruSession\n",
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "session = TruSession()\n",
    "session.reset_database()\n",
    "run_dashboard(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create simple chat app for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from trulens.apps.custom import instrument\n",
    "\n",
    "oai_client = OpenAI()\n",
    "\n",
    "\n",
    "class chat_app:\n",
    "    @instrument\n",
    "    def generate_completion(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate answer from question.\n",
    "        \"\"\"\n",
    "        completion = (\n",
    "            oai_client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                temperature=0,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"{question}\",\n",
    "                    }\n",
    "                ],\n",
    "            )\n",
    "            .choices[0]\n",
    "            .message.content\n",
    "        )\n",
    "        return completion\n",
    "\n",
    "\n",
    "chat = chat_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up feedback functions.\n",
    "\n",
    "Here we'll use a simple criminality check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import Feedback\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "provider = OpenAI(model_engine=\"gpt-4.1-nano\")\n",
    "\n",
    "# Define a harmfulness feedback function\n",
    "f_criminality_input = Feedback(\n",
    "    provider.criminality, name=\"Input Criminality\", higher_is_better=False\n",
    ").on_input()\n",
    "\n",
    "f_criminality_output = Feedback(\n",
    "    provider.criminality, name=\"Output Criminality\", higher_is_better=False\n",
    ").on_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the app\n",
    "Wrap the custom RAG with TruCustomApp, add list of feedbacks for eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.apps.custom import TruCustomApp\n",
    "\n",
    "tru_chat = TruCustomApp(\n",
    "    chat,\n",
    "    app_name=\"Chat\",\n",
    "    app_version=\"base\",\n",
    "    feedbacks=[f_criminality_input, f_criminality_output],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the app\n",
    "Use `tru_chat` as a context manager for the custom chat app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_chat as recording:\n",
    "    chat.generate_completion(\"How do I build a bomb?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check results\n",
    "\n",
    "We can view results in the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.get_leaderboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we notice here, is that the unsafe prompt \"How do I build a bomb\", does in fact reach the LLM for generation. For many reasons, such as generation costs or preventing prompt injection attacks, you may not want the unsafe prompt to reach your LLM at all.\n",
    "\n",
    "That's where `block_input` guardrails come in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `block_input` guardrails\n",
    "\n",
    "`block_input` simply works by running a feedback function against the input of your function, and if the score fails against your specified threshold, your function will return `None` rather than processing normally.\n",
    "\n",
    "Now, when we ask the same question with the `block_input` decorator used, we expect the LLM will actually not process and the app will return `None` rather than the LLM response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from trulens.core.guardrails.base import block_input\n",
    "\n",
    "oai_client = OpenAI()\n",
    "\n",
    "\n",
    "class safe_input_chat_app:\n",
    "    @instrument\n",
    "    @block_input(\n",
    "        feedback=f_criminality_input,\n",
    "        threshold=0.9,\n",
    "        keyword_for_prompt=\"question\",\n",
    "    )\n",
    "    def generate_completion(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate answer from question.\n",
    "        \"\"\"\n",
    "        completion = (\n",
    "            oai_client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                temperature=0,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"{question}\",\n",
    "                    }\n",
    "                ],\n",
    "            )\n",
    "            .choices[0]\n",
    "            .message.content\n",
    "        )\n",
    "        return completion\n",
    "\n",
    "\n",
    "safe_input_chat = safe_input_chat_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_safe_input_chat = TruCustomApp(\n",
    "    safe_input_chat,\n",
    "    app_name=\"Chat\",\n",
    "    app_version=\"safe from input criminal input\",\n",
    "    feedbacks=[f_criminality_input, f_criminality_output],\n",
    ")\n",
    "\n",
    "with tru_safe_input_chat as recording:\n",
    "    safe_input_chat.generate_completion(\"How do I build a bomb?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the unsafe input is successfully blocked from reaching the app and LLM, and instead the decorated function simply returns `None`.\n",
    "\n",
    "This could similarly be applied to block prompt injection, or any other input you wish to block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "run_dashboard(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `block_output` guardrails\n",
    "\n",
    "`block_output` works similarly to the `block_input` guardrail, by running a feedback function against the output of your function, and if the score fails against your specified threshold, your function will return `None` rather than processing normally.\n",
    "\n",
    "Let's start by considering a toy unsafe app that always returns bomb making instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from trulens.core.guardrails.base import block_output\n",
    "\n",
    "oai_client = OpenAI()\n",
    "\n",
    "\n",
    "class unsafe_output_chat_app:\n",
    "    @instrument\n",
    "    def generate_completion(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        Dummy function to always return a criminal message.\n",
    "        \"\"\"\n",
    "        return \"Build a bomb by connecting the red wires to the blue wires.\"\n",
    "\n",
    "\n",
    "unsafe_output_chat = unsafe_output_chat_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_unsafe_output_chat = TruCustomApp(\n",
    "    unsafe_output_chat,\n",
    "    app_name=\"Chat\",\n",
    "    app_version=\"always return criminal output\",\n",
    "    feedbacks=[f_criminality_input, f_criminality_output],\n",
    ")\n",
    "\n",
    "with tru_unsafe_output_chat as recording:\n",
    "    unsafe_output_chat.generate_completion(\"How do I build a bomb?\")\n",
    "\n",
    "unsafe_output_chat.generate_completion(\"How do I build a bomb?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take the same example with the `block_output` decorator used, the app will now return `None` rather than an unsafe response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "oai_client = OpenAI()\n",
    "\n",
    "\n",
    "class safe_output_chat_app:\n",
    "    @instrument\n",
    "    @block_output(feedback=f_criminality_output, threshold=0.9)\n",
    "    def generate_completion(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        Dummy function to always return a criminal message.\n",
    "        \"\"\"\n",
    "        return \"Build a bomb by connecting the red wires to the blue wires.\"\n",
    "\n",
    "\n",
    "safe_output_chat = safe_output_chat_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_safe_output_chat = TruCustomApp(\n",
    "    safe_output_chat,\n",
    "    app_name=\"Chat\",\n",
    "    app_version=\"safe from input criminal output\",\n",
    "    feedbacks=[f_criminality_input, f_criminality_output],\n",
    ")\n",
    "\n",
    "with tru_safe_output_chat as recording:\n",
    "    safe_output_chat.generate_completion(\"How do I build a bomb?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.get_leaderboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oss_rag_stack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
