{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground-truth based evaluation on information retrieval componenet \n",
    "When developing a RAG application, the retrieval component plays a critical role in the entire system. Thus, we need to be able to quickly measure the search quality, where directly affects an end-to-end LLM powered application's ability to accurately answer queries based on contextualized knowledge. In this notebook, we walkthrough how you can leverage your curated ground truth datasets containing golden contexts that are relevant to a query to perform evalaution using well established information retrieval (IR) metrics of your app. The key different from this ground-truth-based workflow than RAG triad is that RAG triad is reference free, and is mostly suitable for cases when ground truth data are not available.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install trulens trulens-provider-openai openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import TruSession\n",
    "\n",
    "session = TruSession()\n",
    "session.reset_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add and create your custom ground-truth dataset to TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"query\": [\"what is AI?\"],\n",
    "    \"query_id\": [\"1\"],\n",
    "    \"expected_response\": [\"Artificial Intelligence\"],\n",
    "    \"expected_chunks\": [\n",
    "        [\n",
    "            {\n",
    "                \"text\": \"AI is the simulation of human intelligence processes by machines, especially computer systems.\",\n",
    "                \"title\": \"AI is not a bubble :(\",\n",
    "                \"expected_score\": 0.9,\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"AI is the evil overlod that's going to rule over all human beings.\",\n",
    "                \"title\": \"AI should be feared\",\n",
    "                \"expected_score\": 0.4,\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"AI is the future of humanity.\",\n",
    "                \"title\": \"AI is the future\",\n",
    "                \"expected_score\": 0.5,\n",
    "            },\n",
    "        ],\n",
    "    ],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.add_ground_truth_to_dataset(\n",
    "    dataset_name=\"test_dataset_ir\",\n",
    "    ground_truth_df=df,\n",
    "    dataset_metadata={\"domain\": \"Random IR dataset\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_df = session.get_ground_truth(\"test_dataset_ir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import Feedback\n",
    "from trulens.core.schema.select import Select\n",
    "from trulens.feedback import GroundTruthAgreement\n",
    "from trulens.providers.openai import OpenAI as fOpenAI\n",
    "\n",
    "# define argument selectors (Lens) based on the setup of the application so that the feedback can be applied to the correct function calls\n",
    "arg_query_selector = (\n",
    "    Select.RecordCalls.retrieve_and_generate.args.query\n",
    ")  # 1st argument of retrieve_and_generate function\n",
    "arg_retrieval_k_selector = (\n",
    "    Select.RecordCalls.retrieve_and_generate.args.k\n",
    ")  # 2nd argument of retrieve_and_generate function\n",
    "\n",
    "arg_completion_str_selector = Select.RecordCalls.retrieve_and_generate.rets[\n",
    "    0\n",
    "]  # 1st returned value from retrieve_and_generate function\n",
    "arg_retrieved_context_selector = Select.RecordCalls.retrieve_and_generate.rets[\n",
    "    1\n",
    "]  # 2nd returned value from retrieve_and_generate function\n",
    "arg_relevance_scores_selector = Select.RecordCalls.retrieve_and_generate.rets[\n",
    "    2\n",
    "]  # last returned value from retrieve_and_generate function\n",
    "\n",
    "f_ir_hit_rate = (\n",
    "    Feedback(\n",
    "        GroundTruthAgreement(ground_truth_df, provider=fOpenAI()).ir_hit_rate,\n",
    "        name=\"IR hit rate\",\n",
    "    )\n",
    "    .on(arg_query_selector)\n",
    "    .on(arg_retrieved_context_selector)\n",
    "    .on(arg_retrieval_k_selector)\n",
    ")\n",
    "\n",
    "f_ndcg_at_k = (\n",
    "    Feedback(\n",
    "        GroundTruthAgreement(ground_truth_df, provider=fOpenAI()).ndcg_at_k,\n",
    "        name=\"NDCG@k\",\n",
    "    )\n",
    "    .on(arg_query_selector)\n",
    "    .on(arg_retrieved_context_selector)\n",
    "    .on(arg_relevance_scores_selector)\n",
    "    .on(arg_retrieval_k_selector)\n",
    ")\n",
    "\n",
    "\n",
    "f_recall_at_k = (\n",
    "    Feedback(\n",
    "        GroundTruthAgreement(ground_truth_df, provider=fOpenAI()).recall_at_k,\n",
    "        name=\"Recall@k\",\n",
    "    )\n",
    "    .on(arg_query_selector)\n",
    "    .on(arg_retrieved_context_selector)\n",
    "    .on(arg_relevance_scores_selector)\n",
    "    .on(arg_retrieval_k_selector)\n",
    ")\n",
    "f_groundtruth_answer = (\n",
    "    Feedback(\n",
    "        GroundTruthAgreement(ground_truth_df).agreement_measure,\n",
    "        name=\"Ground Truth answer (semantic similarity)\",\n",
    "    )\n",
    "    .on(arg_query_selector)\n",
    "    .on(arg_completion_str_selector)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "from openai import OpenAI\n",
    "from trulens.apps.custom import TruCustomApp\n",
    "from trulens.apps.custom import instrument\n",
    "\n",
    "oai_client = OpenAI()\n",
    "\n",
    "\n",
    "class APP:\n",
    "    @instrument\n",
    "    def retrieve_and_generate(\n",
    "        self, query: str, k: int\n",
    "    ) -> Tuple[str | None, List[str], List[float]]:\n",
    "        # k is optional used to specify metrics computation like NDCG@k\n",
    "        completion_str = (\n",
    "            oai_client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                temperature=0,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"Please answer the question: {query}\",\n",
    "                    }\n",
    "                ],\n",
    "            )\n",
    "            .choices[0]\n",
    "            .message.content\n",
    "        )\n",
    "        retrieved_chunks = [\n",
    "            \"AI is the future of humanity.\",\n",
    "            \"AI is going to replace all human labor.\",\n",
    "        ]\n",
    "        retrieval_scores = [\n",
    "            1.0,\n",
    "            0.85,\n",
    "        ]  # optional scores typically come from a retrieval model\n",
    "        return completion_str, retrieved_chunks, retrieval_scores\n",
    "\n",
    "\n",
    "retrieval_app = APP()\n",
    "# add trulens as a context manager for llm_app\n",
    "\n",
    "\n",
    "tru_app = TruCustomApp(\n",
    "    retrieval_app,\n",
    "    app_name=\"Retrieval App v1\",\n",
    "    feedbacks=[f_ir_hit_rate, f_ndcg_at_k, f_recall_at_k, f_groundtruth_answer],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_app as recording:\n",
    "    resp = retrieval_app.retrieve_and_generate(\"what is AI?\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.get_leaderboard(app_ids=[tru_app.app_id])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
