{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype Evals with Streaming App\n",
    "This notebook shows the use of the dummy feedback function provider which\n",
    "behaves like the huggingface provider except it does not actually perform any\n",
    "network calls and just produces constant results. It can be used to prototype\n",
    "feedback function wiring for your apps before invoking potentially slow (to\n",
    "run/to load) feedback functions.\n",
    "\n",
    "It also demonstrates the wrapping of a custom streaming method.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/examples/quickstart/custom_stream.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install trulens trulens-providers-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import Feedback\n",
    "from trulens.core import TruSession\n",
    "\n",
    "session = TruSession()\n",
    "session.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "run_dashboard(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from trulens.apps.custom import instrument\n",
    "\n",
    "oai_client = OpenAI()\n",
    "\n",
    "\n",
    "class APP:\n",
    "    @instrument\n",
    "    def stream_completion(self, prompt):\n",
    "        completion = oai_client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            stream=True,\n",
    "            stream_options={\n",
    "                \"include_usage\": True\n",
    "            },  # not yet tracked by trulens\n",
    "            temperature=0,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Please answer the question: {prompt}\",\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "        for chunk in completion:\n",
    "            if (\n",
    "                len(choices := chunk.choices) > 0\n",
    "                and (content := choices[0].delta.content) is not None\n",
    "            ):\n",
    "                yield content\n",
    "\n",
    "\n",
    "llm_app = APP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dummy feedback\n",
    "\n",
    "By setting the provider as `Dummy()`, you can erect your evaluation suite and then easily substitute in a real model provider (e.g. OpenAI) later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.providers.huggingface.provider import Dummy\n",
    "\n",
    "hugs = Dummy()\n",
    "\n",
    "f_positive_sentiment = Feedback(hugs.positive_sentiment).on_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add trulens as a context manager for llm_app with dummy feedback\n",
    "from trulens.apps.custom import TruCustomApp\n",
    "\n",
    "tru_app = TruCustomApp(\n",
    "    llm_app,\n",
    "    app_name=\"LLM App\",\n",
    "    app_version=\"v1\",\n",
    "    feedbacks=[f_positive_sentiment],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_app as recording:\n",
    "    for chunk in llm_app.stream_completion(\n",
    "        \"give me a good name for a colorful sock company and the store behind its founding\"\n",
    "    ):\n",
    "        print(chunk, end=\"\")\n",
    "\n",
    "record = recording.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check full output:\n",
    "\n",
    "record.main_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check costs, not that only the number of chunks is presently tracked for streaming apps.\n",
    "\n",
    "record.cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.get_leaderboard(app_ids=[tru_app.app_id])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
