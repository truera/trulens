{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth dataset persistence and evaluation in TruLens\n",
    "\n",
    "In this notebook, we give a quick walkthrough of how you can prepare your own ground truth dataset, as well as utilize our utility function to load preprocessed BEIR (Benchmarking IR) datasets to take advantage of its unified format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install trulens trulens-provider-openai openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import TruSession\n",
    "\n",
    "session = TruSession()\n",
    "session.reset_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add custom ground truth dataset to TruLens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"query\": [\"hello world\", \"who is the president?\", \"what is AI?\"],\n",
    "    \"query_id\": [\"1\", \"2\", \"3\"],\n",
    "    \"expected_response\": [\"greeting\", \"Joe Biden\", \"Artificial Intelligence\"],\n",
    "    \"expected_chunks\": [\n",
    "        [\n",
    "            {\n",
    "                \"text\": \"All CS major students must know the term 'Hello World'\",\n",
    "                \"title\": \"CS 101\",\n",
    "            }\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"text\": \"Barack Obama was the president of the US (POTUS) from 2008 to 2016.'\",\n",
    "                \"title\": \"US Presidents\",\n",
    "            }\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"text\": \"AI is the simulation of human intelligence processes by machines, especially computer systems.\",\n",
    "                \"title\": \"AI is not a bubble :(\",\n",
    "            }\n",
    "        ],\n",
    "    ],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idempotency in TruLens dataset:\n",
    " IDs for both datasets and ground truth data entries are based on their content and metadata, so `add_ground_truth_to_dataset` is idempotent and should not create duplicate rows in the DB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.add_ground_truth_to_dataset(\n",
    "    dataset_name=\"test_dataset_new\",\n",
    "    ground_truth_df=df,\n",
    "    dataset_metadata={\"domain\": \"Random QA\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving groundtruth dataset from the DB for Ground truth evaluation (semantic similarity)\n",
    "\n",
    "Below we will introduce how to retrieve the ground truth dataset (or a subset of it) that we just persisted, and use it as the golden set in `GroundTruthAgreement` feedback function to perform ground truth lookup and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_df = session.get_ground_truth(\"test_dataset_new\")\n",
    "ground_truth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from trulens.core import Feedback\n",
    "from trulens.feedback import GroundTruthAgreement\n",
    "from trulens.providers.openai import OpenAI as fOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "f_groundtruth = Feedback(\n",
    "    GroundTruthAgreement(ground_truth_df, provider=fOpenAI()).agreement_measure,\n",
    "    name=\"Ground Truth (semantic similarity measurement)\",\n",
    ").on_input_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Simple LLM Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from trulens.apps.custom import instrument\n",
    "\n",
    "oai_client = OpenAI()\n",
    "\n",
    "\n",
    "class APP:\n",
    "    @instrument\n",
    "    def completion(self, prompt):\n",
    "        completion = (\n",
    "            oai_client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                temperature=0,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"Please answer the question: {prompt}\",\n",
    "                    }\n",
    "                ],\n",
    "            )\n",
    "            .choices[0]\n",
    "            .message.content\n",
    "        )\n",
    "        return completion\n",
    "\n",
    "\n",
    "llm_app = APP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrument chain for logging with TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add trulens as a context manager for llm_app\n",
    "from trulens.apps.custom import TruCustomApp\n",
    "\n",
    "tru_app = TruCustomApp(\n",
    "    llm_app, app_name=\"LLM App v1\", feedbacks=[f_groundtruth]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instrumented query engine can operate as a context manager:\n",
    "with tru_app as recording:\n",
    "    llm_app.completion(\"what is AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.get_leaderboard(app_ids=[tru_app.app_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.reset_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset to a dataframe:\n",
    "This is helpful when we'd want to inspect the groundtruth dataset after transformation. The below example \n",
    "loads a preprocessed dataset from BEIR (Benchmarking Information Retrieval) collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.benchmark.benchmark_frameworks.dataset.beir_loader import (\n",
    "    TruBEIRDataLoader,\n",
    ")\n",
    "\n",
    "beir_data_loader = TruBEIRDataLoader(data_folder=\"./\", dataset_name=\"scifact\")\n",
    "\n",
    "gt_df = beir_data_loader.load_dataset_to_df(download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df.expected_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we can save the ground truth to the dataset\n",
    "session.add_ground_truth_to_dataset(\n",
    "    dataset_name=\"my_beir_scifact\",\n",
    "    ground_truth_df=gt_df,\n",
    "    dataset_metadata={\"domain\": \"Information Retrieval\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single method to save to the databse \n",
    "We also make directly persisting to DB easy. This is particular useful for larger datasets such as MSMARCO, where there are over 8 million documents in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beir_data_loader.persist_dataset(\n",
    "    session=session,\n",
    "    dataset_name=\"my_beir_scifact\",\n",
    "    dataset_metadata={\"domain\": \"Information Retrieval\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking feedback functions / evaluators as a special case of groundtruth evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "provider = OpenAI(model_engine=\"gpt-4o\")\n",
    "\n",
    "\n",
    "def context_relevance_ff(\n",
    "    input, output, benchmark_params\n",
    ") -> Tuple[float, float]:\n",
    "    return provider.context_relevance(\n",
    "        question=input,\n",
    "        context=output,\n",
    "        temperature=benchmark_params[\"temperature\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df = gt_df.head(10)\n",
    "gt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.feedback import GroundTruthAggregator\n",
    "\n",
    "true_labels = []\n",
    "\n",
    "for chunks in gt_df.expected_chunks:\n",
    "    for chunk in chunks:\n",
    "        true_labels.append(chunk[\"expected_score\"])\n",
    "ndcg_agg_func = GroundTruthAggregator(true_labels=true_labels, k=10).ndcg_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.benchmark.benchmark_frameworks.tru_benchmark_experiment import (\n",
    "    BenchmarkParams,\n",
    ")\n",
    "from trulens.benchmark.benchmark_frameworks.tru_benchmark_experiment import (\n",
    "    TruBenchmarkExperiment,\n",
    ")\n",
    "from trulens.benchmark.benchmark_frameworks.tru_benchmark_experiment import (\n",
    "    create_benchmark_experiment_app,\n",
    ")\n",
    "\n",
    "benchmark_experiment = TruBenchmarkExperiment(\n",
    "    feedback_fn=context_relevance_ff,\n",
    "    agg_funcs=[ndcg_agg_func],\n",
    "    benchmark_params=BenchmarkParams(temperature=0.5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_benchmark = create_benchmark_experiment_app(\n",
    "    app_id=\"Cortex benchmark demo\", benchmark_experiment=benchmark_experiment\n",
    ")\n",
    "\n",
    "with tru_benchmark as recording:\n",
    "    feedback_res = tru_benchmark.app(gt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.get_leaderboard(app_ids=[])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
