{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth dataset persistence and evaluation in TruLens\n",
    "\n",
    "In this notebook, we give a quick walkthrough of how you can prepare your own ground truth dataset, as well as utilize our utility function to load preprocessed BEIR (Benchmarking IR) datasets to take advantage of its unified format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install trulens trulens-provider-openai openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from trulens.core import TruSession\n",
    "\n",
    "tru = TruSession()\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add custom ground truth dataset to TruLens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"query\": [\"hello world\", \"who is the president?\", \"what is AI?\"],\n",
    "    \"query_id\": [\"1\", \"2\", \"3\"],\n",
    "    \"expected_response\": [\"greeting\", \"Joe Biden\", \"Artificial Intelligence\"],\n",
    "    \"expected_chunks\": [\n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "    ],  # This can also be lists if applicable\n",
    "    \"meta\": [None, None, None],  # Metadata can also be a list of dictionaries\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idempotency in TruLens dataset:\n",
    " IDs for both datasets and ground truth data entries are based on their content and metadata, so `add_ground_truth_to_dataset` is idempotent and should not create duplicate rows in the DB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.add_ground_truth_to_dataset(\n",
    "    dataset_name=\"test_dataset_new\",\n",
    "    ground_truth_df=df,\n",
    "    dataset_metadata={\"domain\": \"Random QA\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset to a dataframe:\n",
    "So that we can inspect the groundtruth dataset after transformation:\n",
    "Load a preprocessed dataset from BEIR (Benchmarking Information Retrieval) collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core.dataset.beir_loader import TruBEIRDataLoader\n",
    "\n",
    "beir_data_loader = TruBEIRDataLoader(\n",
    "    data_folder=\"/Users/dhuang/Documents\", dataset_name=\"quora\"\n",
    ")\n",
    "\n",
    "df = beir_data_loader.load_dataset_to_df(download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we can save the ground truth to the dataset\n",
    "tru.add_ground_truth_to_dataset(\n",
    "    dataset_name=\"my_beir_quora\",\n",
    "    ground_truth_df=df,\n",
    "    dataset_metadata={\"domain\": \"Information Retrieval\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single method to save to the databse \n",
    "We also make directly persisting to DB easy. This is particular useful for larger datasets such as MSMARCO, where there are over 8 million documents in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beir_data_loader.persist_dataset(\n",
    "    tru=tru,\n",
    "    dataset_name=\"my_beir_quora\",\n",
    "    dataset_metadata={\"domain\": \"Information Retrieval\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving groundtruth dataset from the DB for evaluation\n",
    "\n",
    "Below we will introduce how to retrieve the ground truth dataset (or a subset of it) that we just persisted, and use it as the golden set in `GroundTruthAgreement` feedback function to perform ground truth lookup and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_df = tru.get_ground_truth(\"test_dataset_new\")\n",
    "ground_truth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from trulens.core import Feedback\n",
    "from trulens.feedback import GroundTruthAgreement\n",
    "from trulens.providers.openai import OpenAI as fOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "f_groundtruth = Feedback(\n",
    "    GroundTruthAgreement(ground_truth_df, provider=fOpenAI()).agreement_measure,\n",
    "    name=\"Ground Truth (semantic similarity measurement)\",\n",
    ").on_input_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Simple LLM Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from trulens.core.app.custom import instrument\n",
    "\n",
    "oai_client = OpenAI()\n",
    "\n",
    "\n",
    "class APP:\n",
    "    @instrument\n",
    "    def completion(self, prompt):\n",
    "        completion = (\n",
    "            oai_client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                temperature=0,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"Please answer the question: {prompt}\",\n",
    "                    }\n",
    "                ],\n",
    "            )\n",
    "            .choices[0]\n",
    "            .message.content\n",
    "        )\n",
    "        return completion\n",
    "\n",
    "\n",
    "llm_app = APP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrument chain for logging with TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add trulens as a context manager for llm_app\n",
    "from trulens.core import TruCustomApp\n",
    "\n",
    "tru_app = TruCustomApp(llm_app, app_id=\"LLM App v1\", feedbacks=[f_groundtruth])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instrumented query engine can operate as a context manager:\n",
    "with tru_app as recording:\n",
    "    llm_app.completion(\"what is AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_leaderboard(app_ids=[tru_app.app_id])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
