{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ““ TruLens with Outside Logs\n",
    "\n",
    "If your application was run (and logged) outside of TruLens, TruVirtual can be used to ingest and evaluate the logs.\n",
    "\n",
    "The first step to loading your app logs into TruLens is creating a virtual app. This virtual app can be a plain dictionary or use our VirtualApp class to store any information you would like. You can refer to these values for evaluating feedback.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/quickstart/existing_data_quickstart.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import Select\n",
    "from trulens.core.app import VirtualApp\n",
    "\n",
    "virtual_app = dict(\n",
    "    llm=dict(modelname=\"some llm component model name\"),\n",
    "    template=\"information about the template I used in my app\",\n",
    "    debug=\"all of these fields are completely optional\",\n",
    ")\n",
    "\n",
    "virtual_app = VirtualApp(virtual_app)  # can start with the prior dictionary\n",
    "virtual_app[Select.RecordCalls.llm.maxtokens] = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When setting up the virtual app, you should also include any components that you would like to evaluate in the virtual app. This can be done using the Select class. Using selectors here lets use reuse the setup you use to define feedback functions. Below you can see how to set up a virtual app with a retriever component, which will be used later in the example for feedback evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Select.RecordCalls.retriever\n",
    "synthesizer = Select.RecordCalls.synthesizer\n",
    "\n",
    "virtual_app[retriever] = \"retriever\"\n",
    "virtual_app[synthesizer] = \"synthesizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from trulens.core import VirtualRecord\n",
    "\n",
    "# The selector for a presumed context retrieval component's call to\n",
    "# `get_context`. The names are arbitrary but may be useful for readability on\n",
    "# your end.\n",
    "context_call = retriever.get_context\n",
    "generation = synthesizer.generate\n",
    "\n",
    "rec1 = VirtualRecord(\n",
    "    main_input=\"Where is Germany?\",\n",
    "    main_output=\"Germany is in Europe\",\n",
    "    calls={\n",
    "        context_call: dict(\n",
    "            args=[\"Where is Germany?\"],\n",
    "            rets=[\"Germany is a country located in Europe.\"],\n",
    "        ),\n",
    "        generation: dict(\n",
    "            args=[\n",
    "                \"\"\"\n",
    "                    We have provided the below context: \\n\n",
    "                    ---------------------\\n\n",
    "                    Germany is a country located in Europe.\n",
    "                    ---------------------\\n\n",
    "                    Given this information, please answer the question: \n",
    "                    Where is Germany?\n",
    "                      \"\"\"\n",
    "            ],\n",
    "            rets=[\"Germany is a country located in Europe.\"],\n",
    "        ),\n",
    "    },\n",
    ")\n",
    "\n",
    "# set usage and cost information for a record with the cost attribute\n",
    "rec1.cost.n_tokens = 234\n",
    "rec1.cost.cost = 0.05\n",
    "\n",
    "# set start and end times with the perf attribute\n",
    "\n",
    "start_time = datetime.datetime(\n",
    "    2024, 6, 12, 10, 30, 0\n",
    ")  # June 12th, 2024 at 10:30:00 AM\n",
    "end_time = datetime.datetime(\n",
    "    2024, 6, 12, 10, 31, 30\n",
    ")  # June 12th, 2024 at 12:31:30 PM\n",
    "rec1.perf.start_time = start_time\n",
    "rec1.perf.end_time = end_time\n",
    "\n",
    "rec2 = VirtualRecord(\n",
    "    main_input=\"Where is Germany?\",\n",
    "    main_output=\"Poland is in Europe\",\n",
    "    calls={\n",
    "        context_call: dict(\n",
    "            args=[\"Where is Germany?\"],\n",
    "            rets=[\"Poland is a country located in Europe.\"],\n",
    "        ),\n",
    "        generation: dict(\n",
    "            args=[\n",
    "                \"\"\"\n",
    "                    We have provided the below context: \\n\n",
    "                    ---------------------\\n\n",
    "                    Germany is a country located in Europe.\n",
    "                    ---------------------\\n\n",
    "                    Given this information, please answer the question: \n",
    "                    Where is Germany?\n",
    "                      \"\"\"\n",
    "            ],\n",
    "            rets=[\"Poland is a country located in Europe.\"],\n",
    "        ),\n",
    "    },\n",
    ")\n",
    "\n",
    "data = [rec1, rec2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've ingested constructed the virtual records, we can build our feedback functions. This is done just the same as normal, except the context selector will instead refer to the new context_call we added to the virtual record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import Feedback\n",
    "from trulens.ext.provider.openai import OpenAI\n",
    "\n",
    "# Initialize provider class\n",
    "provider = OpenAI()\n",
    "\n",
    "# Select context to be used in feedback. We select the return values of the\n",
    "# virtual `get_context` call in the virtual `retriever` component. Names are\n",
    "# arbitrary except for `rets`.\n",
    "context = context_call.rets[:]\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(provider.context_relevance_with_cot_reasons).on_input().on(context)\n",
    ")\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(\n",
    "        provider.groundedness_measure_with_cot_reasons, name=\"Groundedness\"\n",
    "    )\n",
    "    .on(context.collect())\n",
    "    .on_output()\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = Feedback(\n",
    "    provider.relevance_with_cot_reasons, name=\"Answer Relevance\"\n",
    ").on_input_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the virtual recorder\n",
    "\n",
    "Here, we'll use deferred mode. This way you can see the records in the dashboard before we've run evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import TruVirtual\n",
    "\n",
    "virtual_recorder = TruVirtual(\n",
    "    app_id=\"a virtual app\",\n",
    "    app=virtual_app,\n",
    "    feedbacks=[f_context_relevance, f_groundedness, f_qa_relevance],\n",
    "    feedback_mode=\"deferred\",  # optional\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in data:\n",
    "    virtual_recorder.add_record(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import Tru\n",
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "tru = Tru()\n",
    "run_dashboard(tru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you can start the evaluator at a time of your choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.start_evaluator()\n",
    "\n",
    "# tru.stop_evaluator() # stop if needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trucanopy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
