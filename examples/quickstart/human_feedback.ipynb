{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìì Logging Human Feedback\n",
    "\n",
    "In many situations, it can be useful to log human feedback from your users about your LLM app's performance. Combining human feedback along with automated feedback can help you drill down on subsets of your app that underperform, and uncover new failure modes. This example will walk you through a simple example of recording human feedback with TruLens.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/examples/quickstart/human_feedback.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install trulens openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from trulens.apps.custom import TruCustomApp\n",
    "from trulens.core import TruSession\n",
    "\n",
    "session = TruSession()\n",
    "session.start_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Keys\n",
    "\n",
    "For this example, you need an OpenAI key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up your app\n",
    "\n",
    "Here we set up a custom application using just an OpenAI chat completion. The process for logging human feedback is the same however you choose to set up your app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from trulens.apps.custom import instrument\n",
    "\n",
    "oai_client = OpenAI()\n",
    "\n",
    "\n",
    "class APP:\n",
    "    @instrument\n",
    "    def completion(self, prompt):\n",
    "        completion = (\n",
    "            oai_client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                temperature=0,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"Please answer the question: {prompt}\",\n",
    "                    }\n",
    "                ],\n",
    "            )\n",
    "            .choices[0]\n",
    "            .message.content\n",
    "        )\n",
    "        return completion\n",
    "\n",
    "\n",
    "llm_app = APP()\n",
    "\n",
    "# add trulens as a context manager for llm_app\n",
    "tru_app = TruCustomApp(llm_app, app_name=\"LLM App\", app_version=\"v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_app as recording:\n",
    "    llm_app.completion(\"Give me 10 names for a colorful sock company\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the record to add the feedback to.\n",
    "record = recording.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a mechanism for recording human feedback.\n",
    "\n",
    "Be sure to click an emoji in the record to record `human_feedback` to log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Button\n",
    "from ipywidgets import HBox\n",
    "from ipywidgets import Label\n",
    "from ipywidgets import Textarea\n",
    "from ipywidgets import VBox\n",
    "from trulens.core.schema.feedback import FeedbackCall\n",
    "\n",
    "thumbs_up_button = Button(description=\"üëç\")\n",
    "thumbs_down_button = Button(description=\"üëé\")\n",
    "\n",
    "\n",
    "def update_feedback(human_feedback):\n",
    "    # add the human feedback to a particular app and record\n",
    "    session.add_feedback(\n",
    "        name=\"Human Feedack\",\n",
    "        record_id=record.record_id,\n",
    "        app_id=tru_app.app_id,\n",
    "        result=human_feedback,\n",
    "    )\n",
    "\n",
    "\n",
    "def on_thumbs_up_button_clicked(b):\n",
    "    update_feedback(human_feedback=1)\n",
    "    print(\"üëç\")\n",
    "\n",
    "\n",
    "def on_thumbs_down_button_clicked(b):\n",
    "    update_feedback(human_feedback=0)\n",
    "    print(\"üëé\")\n",
    "\n",
    "\n",
    "thumbs_up_button.on_click(on_thumbs_up_button_clicked)\n",
    "thumbs_down_button.on_click(on_thumbs_down_button_clicked)\n",
    "\n",
    "VBox([\n",
    "    Label(record.main_input),\n",
    "    Label(record.main_output),\n",
    "    HBox([thumbs_up_button, thumbs_down_button]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Feedback call to attach more than one human feedback and optionally\n",
    "# metadata. Here we allow the user to press the feedback buttons multiple times\n",
    "# and give a reason for their feedback. The aggregate feedback result is\n",
    "# computed in the code below as the mean of the human feedback results.\n",
    "\n",
    "calls = []\n",
    "\n",
    "thumbs_up_button = Button(description=\"üëç\")\n",
    "thumbs_down_button = Button(description=\"üëé\")\n",
    "reason_area = Textarea(description=\"Reason\")\n",
    "\n",
    "\n",
    "def add_human_feedback(human_feedback, reason):\n",
    "    if not reason:\n",
    "        reason = \"No reason provided\"\n",
    "\n",
    "    calls.append(\n",
    "        FeedbackCall(args={}, ret=human_feedback, meta={\"reason\": reason})\n",
    "    )\n",
    "\n",
    "    session.add_feedback(\n",
    "        name=\"Human Feedack with Metadata\",\n",
    "        record_id=record.record_id,\n",
    "        app_id=tru_app.app_id,\n",
    "        result=sum([call.ret for call in calls]) / len(calls),\n",
    "        calls=calls,\n",
    "    )\n",
    "\n",
    "    if human_feedback == 1:\n",
    "        print(\"üëç\", reason)\n",
    "    else:\n",
    "        print(\"üëé\", reason)\n",
    "\n",
    "\n",
    "def on_thumbs_up_button_clicked(b):\n",
    "    add_human_feedback(1.0, reason_area.value)\n",
    "    reason_area.value = \"\"\n",
    "\n",
    "\n",
    "def on_thumbs_down_button_clicked(b):\n",
    "    add_human_feedback(0.0, reason_area.value)\n",
    "    reason_area.value = \"\"\n",
    "\n",
    "\n",
    "thumbs_up_button.on_click(on_thumbs_up_button_clicked)\n",
    "thumbs_down_button.on_click(on_thumbs_down_button_clicked)\n",
    "\n",
    "VBox([\n",
    "    Label(record.main_input),\n",
    "    Label(record.main_output),\n",
    "    HBox([thumbs_up_button, thumbs_down_button, reason_area]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See the result logged with your app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that individual FeedbackCall are not shown in leaderboard and nor is\n",
    "# their metadata.\n",
    "\n",
    "session.get_leaderboard(app_ids=[tru_app.app_id])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens-9bG3yHQd-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
