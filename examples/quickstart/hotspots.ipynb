{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ““ Hotspots: detecting suspicious features in your evals\n",
    "\n",
    "TruLens Hotspots is a tool for detecting suspicious _features_ in your evaluation results. For instance, it can detect that a specific word in the input lowers the evaluation score much.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/examples/quickstart/hotspots.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install trulens-hotspots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Hotspots on a CSV file\n",
    "\n",
    "Hotspots can be run in a simple way, without using any other TruLens features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you need for TruLens Hotspots is a **data frame with per-sample evaluation scores** (but it does matter what evaluation metric you're specific).\n",
    "\n",
    "So let's read a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "df = read_csv(\"../../tests/files/sample.csv.gz\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to define a TruLens Hotspots configuration; you're required to specify the column with the evaluation score. You can also list irrelevant columns to be skipped. If your evaluation metric is the-lower-the-better, like Mean Absolute/Square Error, you need to state that explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.hotspots import HotspotsConfig\n",
    "from trulens.hotspots import hotspots_as_df\n",
    "\n",
    "hotspots_config = HotspotsConfig(\n",
    "    score_column=\"score\", skip_columns=[\"id\"], higher_is_better=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Hotspots is simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspots_df = hotspots_as_df(hotspots_config, df)\n",
    "\n",
    "hotspots_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task being evaluated is predicting the publication year of a short historical text. Older texts are the hardest, also some specific words make the overall score worse."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
