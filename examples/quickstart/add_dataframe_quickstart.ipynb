{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ““ TruLens with Outside Logs in a Dataframe\n",
    "\n",
    "If your application was run (and logged) outside of TruLens, TruVirtual can be used to ingest and evaluate the logs.\n",
    "\n",
    "This notebook walks through how to quickly log a dataframe of prompts, responses and contexts (optional) to TruLens as traces, and how to run evaluations with the trace data.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/examples/quickstart/add_dataframe_quickstart.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install trulens trulens-providers-openai openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or load a dataframe\n",
    "\n",
    "The dataframe should include minimally columns named `query` and `response`. You can also include a column named `contexts` if you wish to evaluate retrieval systems or RAGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"query\": [\"Where is Germany?\", \"What is the capital of France?\"],\n",
    "    \"response\": [\"Germany is in Europe\", \"The capital of France is Paris\"],\n",
    "    \"contexts\": [\n",
    "        [\"Germany is a country located in Europe.\"],\n",
    "        [\n",
    "            \"France is a country in Europe and its capital is Paris.\",\n",
    "            \"Germany is a country located in Europe\",\n",
    "        ],\n",
    "    ],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a virtual app for tracking purposes.\n",
    "\n",
    "This can be initialized simply, or you can track application metadata by passing a `dict` to `VirtualApp()`. For simplicity, we'll leave it empty here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.apps.virtual import VirtualApp\n",
    "\n",
    "virtual_app = VirtualApp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define feedback functions.\n",
    "\n",
    "The `add_dataframe` method we plan to use will load the prompt, context and response into virtual records. We should define our feedback functions to access this data in the structure it will be stored. We can do so as follows:\n",
    "\n",
    "- prompt: selected using `.on_input()`\n",
    "- response: selected using `on_output()`\n",
    "- context: selected using `VirtualApp.select_context()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import Feedback\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "# Initialize provider class\n",
    "provider = OpenAI()\n",
    "\n",
    "# Select context to be used in feedback.\n",
    "context = VirtualApp.select_context()\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(\n",
    "        provider.context_relevance_with_cot_reasons, name=\"Context Relevance\"\n",
    "    )\n",
    "    .on_input()\n",
    "    .on(context)\n",
    ")\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(\n",
    "        provider.groundedness_measure_with_cot_reasons, name=\"Groundedness\"\n",
    "    )\n",
    "    .on(context.collect())\n",
    "    .on_output()\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = Feedback(\n",
    "    provider.relevance_with_cot_reasons, name=\"Answer Relevance\"\n",
    ").on_input_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a TruLens logging session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import TruSession\n",
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "session = TruSession()\n",
    "run_dashboard(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the virtual app\n",
    "\n",
    "We can now register our virtual app, including any feedback functions we'd like to use for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.apps.virtual import TruVirtual\n",
    "\n",
    "virtual_recorder = TruVirtual(\n",
    "    app_name=\"RAG\",\n",
    "    app_version=\"simple\",\n",
    "    app=virtual_app,\n",
    "    feedbacks=[f_context_relevance, f_groundedness, f_qa_relevance],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the dataframe to TruLens\n",
    "\n",
    "We can then add the dataframe to TruLens using the virual recorder method `add_dataframe`. Doing so will immediately log the traces, and kick off the computation of evaluations. After some time, the evaluation results will be accessible both from the sdk (e.g. `session.get_leaderboard`) and in the TruLens dashboard.\n",
    "\n",
    "If you wish to skip evaluations and only log traces, you can simply skip the sections of this notebook where feedback functions are defined, and exclude them from the construction of the `virtual_recorder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virtual_records = virtual_recorder.add_dataframe(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trucanopy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
