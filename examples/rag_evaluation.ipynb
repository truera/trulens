{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "770cdd1f-39b7-4df9-bef4-aabf71a2af1e",
   "metadata": {},
   "source": [
    "# Evaluate gen AI apps with Snowflake Cortex AI and TruLens\n",
    "This notebook demonstrates how AI Observability in Snowflake Cortex AI helps quantitatively measure the performance of a RAG applications using  different LLMs, providing insights into application behavior and helping the user select the best model for their use case.\n",
    "\n",
    "### Required Packages\n",
    "* trulens-core (1.4.5 or above)\n",
    "* trulens-connectors-snowflake (1.4.5 or above)\n",
    "* trulens-providers-cortex (1.4.5 or above)\n",
    "* snowflake.core (1.0.5 or above)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2a5cc2-066d-4701-8ca4-4ef10ad37367",
   "metadata": {},
   "source": [
    "## Session Information\n",
    "Fetches the current session information and the connection details for the Snowflake account. This connection details will be used to ingest application traces and trigger metric computation jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "session = get_active_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cb43dc-a352-4aa6-a472-50b15cbaa78a",
   "metadata": {},
   "source": [
    "## Cortex Search Retriever\n",
    "Initializes a retriever using Cortex Search Service for the RAG application. The Cortex Search service is based on the tutorial : [Build a PDF chatbot](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-search/tutorials/cortex-search-tutorial-3-chat-advanced)\n",
    "\n",
    "Complete Steps 1 to Spet 4 in the above tutorial, and continue to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a2f3fb-8a46-4e43-8410-644e8107e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from snowflake.core import Root\n",
    "from snowflake.snowpark.session import Session\n",
    "\n",
    "\n",
    "class CortexSearchRetriever:\n",
    "    def __init__(self, snowpark_session: Session, limit_to_retrieve: int = 4):\n",
    "        self._snowpark_session = snowpark_session\n",
    "        self._limit_to_retrieve = limit_to_retrieve\n",
    "\n",
    "    def retrieve(self, query: str) -> List[str]:\n",
    "        root = Root(session)\n",
    "\n",
    "        search_service = (\n",
    "            root.databases[\"cortex_search_tutorial_db\"]\n",
    "            .schemas[\"PUBLIC\"]\n",
    "            .cortex_search_services[\"fomc_meeting\"]\n",
    "        )\n",
    "        resp = search_service.search(\n",
    "            query=query, columns=[\"chunk\"], limit=self._limit_to_retrieve\n",
    "        )\n",
    "\n",
    "        if resp.results:\n",
    "            return [curr[\"chunk\"] for curr in resp.results]\n",
    "        else:\n",
    "            return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d850455-dbe2-49ea-8bcd-fb60f8cc7b86",
   "metadata": {},
   "source": [
    "## Environment Variables\n",
    "\n",
    "Sets the environment variables to use OpenTelemetry for generated traces. This step is mandatory to trace and evaluate the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf43fcdd-69b1-494c-849f-1f5a712fb285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TRULENS_OTEL_TRACING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c623a9e-42c2-4dfd-9bca-362b40198f9a",
   "metadata": {},
   "source": [
    "## RAG Application\n",
    "Defines the RAG application with retrieval and generation steps. The generation function contains the prompt to the LLM and uses Cortex [COMPLETE](https://docs.snowflake.com/en/sql-reference/functions/complete-snowflake-cortex) function for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45784ef-2ba3-453b-b12b-97e6267dabe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.cortex import complete\n",
    "from trulens.core.otel.instrument import instrument\n",
    "from trulens.otel.semconv.trace import SpanAttributes\n",
    "\n",
    "\n",
    "class RAG:\n",
    "    def __init__(self, llm_model):\n",
    "        self.retriever = CortexSearchRetriever(\n",
    "            snowpark_session=session, limit_to_retrieve=4\n",
    "        )\n",
    "        self.llm_model = llm_model\n",
    "\n",
    "    @instrument(\n",
    "        span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
    "        attributes={\n",
    "            SpanAttributes.RETRIEVAL.QUERY_TEXT: \"query\",\n",
    "            SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS: \"return\",\n",
    "        },\n",
    "    )\n",
    "    def retrieve_context(self, query: str) -> list:\n",
    "        \"\"\"\n",
    "        Retrieve relevant text from vector store.\n",
    "        \"\"\"\n",
    "        return self.retriever.retrieve(query)\n",
    "\n",
    "    @instrument(span_type=SpanAttributes.SpanType.GENERATION)\n",
    "    def generate_completion(self, query: str, context_str: list) -> str:\n",
    "        \"\"\"\n",
    "        Generate answer from context.\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "          You are an expert assistant extracting information from context provided.\n",
    "          Answer the question in long-form, fully and completely, based on the context. Do not hallucinate.\n",
    "          If you don´t have the information just say so.\n",
    "          Context: {context_str}\n",
    "          Question:\n",
    "          {query}\n",
    "          Answer:\n",
    "        \"\"\"\n",
    "        response = \"\"\n",
    "        stream = complete(self.llm_model, prompt, stream=True)\n",
    "        for update in stream:\n",
    "            response += update\n",
    "            print(update, end=\"\")\n",
    "        return response\n",
    "\n",
    "    @instrument(\n",
    "        span_type=SpanAttributes.SpanType.RECORD_ROOT,\n",
    "        attributes={\n",
    "            SpanAttributes.RECORD_ROOT.INPUT: \"query\",\n",
    "            SpanAttributes.RECORD_ROOT.OUTPUT: \"return\",\n",
    "        },\n",
    "    )\n",
    "    def query(self, query: str) -> str:\n",
    "        context_str = self.retrieve_context(query)\n",
    "        return self.generate_completion(query, context_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8b8391-ae06-49f3-a85f-6c71e02be1ef",
   "metadata": {},
   "source": [
    "## RAG App Initialization\n",
    "Initializes two instances of the RAG application with llama3.1-70b and mistral-large2 for LLM inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7a6cab-fc81-4959-a45a-594b892d9681",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_llama = RAG(\"llama3.1-70b\")\n",
    "rag_mistral = RAG(\"mistral-large2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78dfb4d-989b-4640-afec-be184e0543c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===========================================\")\n",
    "print(\"RAG App response with llama3.1-70b\")\n",
    "print(\"===========================================\")\n",
    "response = rag_llama.query(\n",
    "    \"What were the strongest components to gdp growth in q4?\"\n",
    ")\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"===========================================\")\n",
    "print(\"RAG App response with mistral-large2\")\n",
    "print(\"===========================================\")\n",
    "response = rag_mistral.query(\n",
    "    \"What were the strongest components to gdp growth in q4?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc886d20-5bed-4ee0-9d98-6370275e6fb4",
   "metadata": {},
   "source": [
    "## App Registration\n",
    "Registers the two app instances in Snowflake, creating EXTERNAL AGENT objects to represent the app instances in the Snowflake account and registers both the app instances as different versions of the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b91a63-1354-4186-972c-af99a032dc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.apps.app import TruApp\n",
    "from trulens.connectors.snowflake import SnowflakeConnector\n",
    "\n",
    "snowflake_connector = SnowflakeConnector(snowpark_session=session)\n",
    "\n",
    "FOMC_Chatbot_llama = TruApp(\n",
    "    rag_llama,\n",
    "    app_name=\"FOMC RAG Chatbot\",\n",
    "    app_version=\"version 1\",\n",
    "    connector=snowflake_connector,\n",
    ")\n",
    "\n",
    "FOMC_Chatbot_mistral = TruApp(\n",
    "    rag_mistral,\n",
    "    app_name=\"FOMC RAG Chatbot\",\n",
    "    app_version=\"version 2\",\n",
    "    connector=snowflake_connector,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc603b08-7ca1-4767-88c1-b97b70a8be36",
   "metadata": {},
   "source": [
    "## Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce0ed52-4aef-4172-910e-c387c9ce039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "evaluation_dataset = {\n",
    "    \"query\": [\n",
    "        \"What were the key points discussed in the FOMC meeting in January 2023?\",\n",
    "        \"How did the FOMC view the economic outlook in mid-2023?\",\n",
    "        \"What were the inflation expectations for the end of 2023?\",\n",
    "        \"What were the main topics in the FOMC meeting in February 2024?\",\n",
    "        \"How did the FOMC assess the labor market in mid-2024?\",\n",
    "        \"What were the GDP growth projections for the end of 2024?\",\n",
    "        \"What were the primary concerns in the FOMC meeting in March 2025?\",\n",
    "        \"How did the FOMC evaluate the financial stability in mid-2025?\",\n",
    "        \"What were the interest rate expectations for the end of 2025?\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "evaluation_df = pd.DataFrame(evaluation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c4e169-d1d6-4630-819c-d049d2dc748c",
   "metadata": {},
   "source": [
    "## Run Configurations\n",
    "Defines the run configurations for evaluating both instances of the RAG app. The run configs contains the run name, description, dataset details, and an optional label to tag the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4201f8f-7469-41ab-afc4-3b9bc274118d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core.run import RunConfig\n",
    "\n",
    "run_config_llama = RunConfig(\n",
    "    run_name=\"Experiment_llama3.1-70b\",\n",
    "    description=\"Q&A evaluation with llama3.1-70b\",\n",
    "    dataset_name=\"FOMC_Queries\",\n",
    "    source_type=\"DATAFRAME\",\n",
    "    label=\"LLM_Test\",\n",
    "    dataset_spec={\n",
    "        \"RECORD_ROOT.INPUT\": \"query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "run_config_mistral = RunConfig(\n",
    "    run_name=\"Experiment_mistral-large2\",\n",
    "    description=\"Q&A evaluation with mistral-large2\",\n",
    "    dataset_name=\"FOMC_Queries\",\n",
    "    source_type=\"DATAFRAME\",\n",
    "    label=\"LLM_Test\",\n",
    "    dataset_spec={\n",
    "        \"RECORD_ROOT.INPUT\": \"query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "run_llama = FOMC_Chatbot_llama.add_run(run_config=run_config_llama)\n",
    "run_mistral = FOMC_Chatbot_mistral.add_run(run_config=run_config_mistral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af86fb94-6680-4b0e-9778-3326c9a6b55e",
   "metadata": {},
   "source": [
    "## Run Invocation\n",
    "Starts two evaluation runs (one each for llama3.1-70b and mistral-large2) by executing the application and generating the traces. This process iterates over the application corresponding to input queries in the dataset and generates the responses, traces and ingests them in Snowflake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ca89e3-5e41-4c76-8ac1-07d17982672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==================================================\")\n",
    "print(\"RAG App Invocation with llama3.1-70b\")\n",
    "print(\"==================================================\")\n",
    "run_llama.start(input_df=evaluation_df)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"==================================================\")\n",
    "print(\"RAG App Invocation with mistral-large2\")\n",
    "print(\"==================================================\")\n",
    "run_mistral.start(input_df=evaluation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd78985-f8e7-4bc5-b4de-ad8f774d461a",
   "metadata": {},
   "source": [
    "## Run Status Check\n",
    "Checks the status of the runs for \"INVOCATION_IN_PROGRESS\". \n",
    "\n",
    "Note: Metric computation cannot be started until the invocation is in progress. Once the runs' status is changed to \"INVOCATION_COMPLETED\", metric computation can be triggered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7428e570-3451-4ec6-b58a-20384da91a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while (run_llama.get_status() == \"INVOCATION_IN_PROGRESS\") or (\n",
    "    run_mistral.get_status() == \"INVOCATION_IN_PROGRESS\"\n",
    "):\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f43331-a337-43fe-abb8-cf006738e6a4",
   "metadata": {},
   "source": [
    "## Compute Metrics\n",
    "\n",
    "Computes the RAG triad metrics for both runs to measure the quality of response in the RAG application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8266642e-d28f-4701-bcc4-e49dda668e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_llama.compute_metrics([\n",
    "    \"answer_relevance\",\n",
    "    \"context_relevance\",\n",
    "    \"groundedness\",\n",
    "])\n",
    "\n",
    "run_mistral.compute_metrics([\n",
    "    \"answer_relevance\",\n",
    "    \"context_relevance\",\n",
    "    \"groundedness\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c402dc53-573e-4ba6-a7ed-469d145d270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Run status for llama3.1-70b - \", run_llama.get_status())\n",
    "print(\"Run status for mistral-large2 - \", run_mistral.get_status())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36b50cb-4f1d-4a87-822e-afc86098a537",
   "metadata": {},
   "source": [
    "## Evaluation Results\n",
    "\n",
    "To view evaluation results:\n",
    "* Login to [Snowsight](https://app.snowflake.com/).\n",
    "* Navigate to **AI & ML** -> **Evaluations** from the left navigation menu.\n",
    "* Select “FOMC RAG CHATBOT” to view the runs, see detailed traces and compare runs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "anubhav.mishra@snowflake.com",
   "authorId": "478848201371",
   "authorName": "ANMISHRA",
   "lastEditTime": 1742318737681,
   "notebookId": "mnfqp4yscsowgm6uq655",
   "sessionId": "076ced64-9984-4d30-bbfd-ea826c1e4937"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
