{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# This is needed due to zipkin issues related to protobuf.\n",
    "# os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import dotenv\n",
    "\n",
    "# Add base dir to path to be able to access test folder.\n",
    "base_dir = Path().cwd().parent.parent.resolve()\n",
    "if str(base_dir) not in sys.path:\n",
    "    print(f\"Adding {base_dir} to sys.path\")\n",
    "    sys.path.append(str(base_dir))\n",
    "\n",
    "os.environ[\"OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED\"] = \"true\"\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Deps\n",
    "# ! pip install opentelemetry-api opentelemetry-sdk opentelemetry-exporter-otlp\n",
    "# ! pip install opentelemetry-instrumentation-llamaindex\n",
    "\n",
    "# Install zipkin python package:\n",
    "# ! pip install opentelemetry-exporter-zipkin-proto-http\n",
    "\n",
    "# Start the zipkin docker container:\n",
    "# ! docker run --rm -d -p 9411:9411 --name zipkin openzipkin/zipkin\n",
    "\n",
    "# Stop the zipkin docker container:\n",
    "# ! docker stop $(docker ps -a -q --filter ancestor=openzipkin/zipkin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry import trace\n",
    "from trulens.experimental.otel_tracing.core.trace import TracerProvider\n",
    "\n",
    "# from opentelemetry.sdk.trace import TracerProvider\n",
    "\n",
    "provider = TracerProvider()\n",
    "# processor = BatchSpanProcessor(ConsoleSpanExporter())\n",
    "# provider.add_span_processor(processor)\n",
    "\n",
    "# Sets the global default tracer provider\n",
    "trace.set_tracer_provider(provider)\n",
    "\n",
    "# Creates a tracer from the global tracer provider\n",
    "tracer = trace.get_tracer(\"my.tracer.name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\"\n",
    "file_path = \"data/paul_graham_essay.txt\"\n",
    "\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\")\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry.exporter.zipkin.json import ZipkinExporter\n",
    "from trulens.core.session import TruSession\n",
    "\n",
    "tru = TruSession(\n",
    "    _experimental_otel_exporter=ZipkinExporter(\n",
    "        endpoint=\"http://localhost:9411/api/v2/spans\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "TruSession().experimental_enable_feature(\"otel_tracing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from opentelemetry.instrumentation.llamaindex import LlamaIndexInstrumentor\n",
    "\n",
    "LlamaIndexInstrumentor().instrument()\n",
    "\n",
    "Settings.chunk_size = 128\n",
    "Settings.chunk_overlap = 16\n",
    "Settings.llm = OpenAI()\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.apps.llamaindex import TruLlama\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "tru_query_engine_recorder = TruLlama(\n",
    "    query_engine,\n",
    "    app_name=\"LlamaIndex_App\",\n",
    "    app_version=\"base\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or as context manager\n",
    "with tru_query_engine_recorder as recording:\n",
    "    with tracer.start_as_current_span(\"Querying LlamaIndex\") as span:\n",
    "        span.set_attribute(\"query\", \"What did the author do growing up?\")\n",
    "\n",
    "        query_engine.query(\"What did the author do growing up?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens-9bG3yHQd-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
