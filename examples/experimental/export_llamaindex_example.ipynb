{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracing LlamaIndex with OTEL Spans using _TruLens_\n",
    "\n",
    "This notebook demonstrates the \"otel-tracing\" experimental feature in _TruLens_.\n",
    "This enables the collection of _OpenTelemetry_ spans during app execution. Data\n",
    "that is collected by _TruLens_ is recorded as spans. Spans created by other tools\n",
    "can also be made available alongside those created by TruLens. Spans can be\n",
    "exported via an OTEL exporter to other tools in the ecosystem.\n",
    "\n",
    "- Spans demonstrated in this notebook are:\n",
    "\n",
    "  - OTEL `sqlalchemy` module instrumentation. Note that `sqlalchemy` is used\n",
    "    internally by _TruLens_ for storage.\n",
    "\n",
    "  - OTEL `requests` module instrumentation. `requests` is used by TruLens to\n",
    "    make requests in the _HuggingFace_ provider.\n",
    "\n",
    "  - _Traceloop_ LlamaIndex and OpenAI instrumentation. See\n",
    "    [OpenLLMetry](https://github.com/traceloop/openllmetry) for other\n",
    "    instrumentation supported by _Traceloop_.\n",
    "\n",
    "  - Arize _OpenInference_ LlamaIndex instrumentation. See\n",
    "    [OpenInference](https://github.com/Arize-ai/openinference) for other\n",
    "    instrumentation supported by _OpenInference_.\n",
    "\n",
    "- OTEL exporters demonstrated in this notebook are:\n",
    "\n",
    "  - Console exporter (prints exported spans in the console or stream).\n",
    "\n",
    "  - In-memory exporter. This stores spans in python list you can access in this\n",
    "    notebook.\n",
    "\n",
    "  - _Zipkin_ exporter. Setup below includes `docker` commands to download and\n",
    "    start a _Zipkin_ collector for demonstration purposes. To open the UI for\n",
    "    this exporter, open _Docker Desktop_, click on the triple dots under\n",
    "    \"Actions\" for the zipkin container and select \"Open with browser\".\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python deps, OTEL:\n",
    "# ! pip install opentelemetry-api opentelemetry-sdk opentelemetry-exporter-otlp\n",
    "\n",
    "# OTEL contrib instrumentors\n",
    "#  ! pip install opentelemetry-instrumentation-sqlalchemy opentelemetry-instrumentation-requests\n",
    "\n",
    "# Traceloop instrumentors\n",
    "# ! pip install opentelemetry-instrumentation-llamaindex opentelemetry-instrumentation-openai\n",
    "\n",
    "# Arize openinference instrumentors\n",
    "# ! pip install \"openinference-instrumentation-llama-index>=2\"\n",
    "\n",
    "# OTEL zipkin exporter\n",
    "# ! pip install opentelemetry-exporter-zipkin-proto-http\n",
    "\n",
    "# Start the zipkin docker container:\n",
    "# ! docker run --rm -d -p 9411:9411 --name zipkin openzipkin/zipkin\n",
    "\n",
    "# Stop the zipkin docker container:\n",
    "# ! docker stop $(docker ps -a -q --filter ancestor=openzipkin/zipkin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: F401\n",
    "from io import StringIO\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import urllib.request\n",
    "\n",
    "import dotenv\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from openinference.instrumentation.llama_index import (\n",
    "    LlamaIndexInstrumentor as oi_LlamaIndexInstrumentor,\n",
    ")\n",
    "\n",
    "# arize openinference instrumentor\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.exporter.zipkin.json import ZipkinExporter  # zipkin exporter\n",
    "from opentelemetry.instrumentation.llamaindex import (\n",
    "    LlamaIndexInstrumentor,  # traceloop instrumentors\n",
    ")\n",
    "from opentelemetry.instrumentation.openai import (\n",
    "    OpenAIInstrumentor,  # traceloop instrumentors\n",
    ")\n",
    "from opentelemetry.instrumentation.requests import (\n",
    "    RequestsInstrumentor,  # otel contrib instrumentors\n",
    ")\n",
    "from opentelemetry.instrumentation.sqlalchemy import (\n",
    "    SQLAlchemyInstrumentor,  # otel contrib instrumentors:\n",
    ")\n",
    "from opentelemetry.sdk.trace.export import (\n",
    "    ConsoleSpanExporter,  # console exporter\n",
    ")\n",
    "from opentelemetry.sdk.trace.export.in_memory_span_exporter import (\n",
    "    InMemorySpanExporter,  # in-memory exporter\n",
    ")\n",
    "from trulens.apps.llamaindex import TruLlama\n",
    "from trulens.core import Feedback\n",
    "from trulens.core import Select\n",
    "from trulens.core.session import TruSession\n",
    "from trulens.experimental.otel_tracing.core.trace import TracerProvider\n",
    "from trulens.providers.huggingface import Huggingface\n",
    "\n",
    "# This is needed due to zipkin issues related to protobuf.\n",
    "os.environ[\"OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED\"] = \"true\"\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the global default tracer provider to be the trulens one.\n",
    "trace.set_tracer_provider(TracerProvider())\n",
    "\n",
    "# Creates a tracer for custom spans below.\n",
    "tracer = trace.get_tracer(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download some base data for query engine.\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\"\n",
    "file_path = \"data/paul_graham_essay.txt\"\n",
    "\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\")\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup in-memory span exporter.\n",
    "exporter = InMemorySpanExporter()\n",
    "\n",
    "# Setup console/file/string exporter\n",
    "# stream = StringIO()\n",
    "\n",
    "# Will print A LOT to stdout unless we set a different stream.\n",
    "# exporter = ConsoleSpanExporter(out=stream)\n",
    "\n",
    "# Setup zipkin exporter\n",
    "# exporter = ZipkinExporter(endpoint=\"http://localhost:9411/api/v2/spans\")\n",
    "\n",
    "# Create a TruLens session.\n",
    "session = TruSession()\n",
    "\n",
    "# To export spans to an external OTEL SpanExporter tool, set it here:\n",
    "session.experimental_otel_exporter = exporter\n",
    "\n",
    "# (Optional) Enable otel_tracing. Note that this is not required if you set the\n",
    "# exporter above. If you would like to trace using spans without an exporter,\n",
    "# this step is required.\n",
    "session.experimental_enable_feature(\"otel_tracing\")\n",
    "\n",
    "session.reset_database()\n",
    "session.start_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable otel contrib instrumentation\n",
    "SQLAlchemyInstrumentor().instrument()\n",
    "RequestsInstrumentor().instrument()\n",
    "\n",
    "# enable traceloop instrumentation\n",
    "LlamaIndexInstrumentor().instrument()\n",
    "OpenAIInstrumentor().instrument()\n",
    "\n",
    "# enable arize open inference instrumentation\n",
    "oi_LlamaIndexInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query engine\n",
    "\n",
    "Settings.llm = OpenAI()\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feedback function and wrap app with trulens recorder.\n",
    "\n",
    "provider = Huggingface()\n",
    "\n",
    "f_lang_match = (\n",
    "    Feedback(provider.language_match)\n",
    "    .on(\n",
    "        Select.RecordSpans.trulens.call.query.attributes[\n",
    "            \"trulens.bindings\"\n",
    "        ].str_or_query_bundle\n",
    "    )\n",
    "    .on(\n",
    "        Select.RecordSpans.trulens.call.query.attributes[\"trulens.ret\"].response\n",
    "    )\n",
    ")\n",
    "# The parts of the selector are:\n",
    "#\n",
    "# - Select.RecordSpans - The spans organized by span name.\n",
    "#\n",
    "# - trulens.call.query - The span name we are interested in. TruLens names all\n",
    "#   call spans with the name \"trulens.call.<methodname>\".\n",
    "#\n",
    "# - attributes - the attributes of the span.\n",
    "#\n",
    "# - \"trulens.bindings\" - The attribute we are interested in. TruLens puts the\n",
    "#   call arguments in the attribute called \"trulens.bindings\".\n",
    "#\n",
    "#    - str_or_query_bundle - The call argument.\n",
    "#\n",
    "# - \"trulens.ret\" - The return value of the method call.\n",
    "#\n",
    "#    - response - The response key assuming the return value is a dictionary.\n",
    "#\n",
    "# - (not shown) \"trulens.error\" - For calls that do not return and raise an\n",
    "#   exception instead, that exception is stored in this attribute.\n",
    "\n",
    "tru_query_engine_recorder = TruLlama(\n",
    "    query_engine,\n",
    "    app_name=\"LlamaIndex_App\",\n",
    "    app_version=\"base\",\n",
    "    feedbacks=[f_lang_match],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal trulens recording usage\n",
    "\n",
    "with tru_query_engine_recorder as recording:\n",
    "    # Custom spans can be included:\n",
    "    with tracer.start_as_current_span(\"Querying LlamaIndex\") as span:\n",
    "        # With custom attributes.\n",
    "        span.set_attribute(\"custom_attribute\", \"This can by anything.\")\n",
    "\n",
    "        # Query the engine as normal.\n",
    "        res = query_engine.query(\"What did the author do growing up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the record from the recording.\n",
    "\n",
    "rec = recording.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the feedback result.\n",
    "\n",
    "rec.feedback_results[0].result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all spans in the record. Here we are using a selector to retrieve the\n",
    "# spans from within the record.\n",
    "\n",
    "rec.get(Select.RecordSpans)\n",
    "\n",
    "# Alternatively, spans can be accessed directly in the record as a list. The\n",
    "# above indexes them by name instead.\n",
    "\n",
    "# rec.experimental_otel_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the attributes we used to define the feedback functions.\n",
    "\n",
    "print(\n",
    "    rec.get(\n",
    "        Select.RecordSpans.trulens.call.query.attributes[\n",
    "            \"trulens.bindings\"\n",
    "        ].str_or_query_bundle\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    rec.get(\n",
    "        Select.RecordSpans.trulens.call.query.attributes[\"trulens.ret\"].response\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the spans listed above should be visible in the chosen exporter.\n",
    "\n",
    "# The InMemorySpanExporter stores the spans in memory. Lets read them back here\n",
    "# to inspect them:\n",
    "\n",
    "if \"exporter\" in locals():\n",
    "    print(f\"Spans exported to {exporter}:\")\n",
    "\n",
    "    if isinstance(exporter, InMemorySpanExporter):\n",
    "        spans = exporter.get_finished_spans()\n",
    "\n",
    "        for span in spans:\n",
    "            print(span.name)\n",
    "\n",
    "    # The ConsoleSpanExporter writes json dumps of each span. Lets read those back\n",
    "    # here to inspect them:\n",
    "\n",
    "    if isinstance(exporter, ConsoleSpanExporter):\n",
    "        match_root_json = re.compile(r\"(?:(^|\\n))\\{.+?\\n\\}\", re.DOTALL)\n",
    "\n",
    "        if \"stream\" in locals():\n",
    "            dumps = match_root_json.finditer(stream.getvalue())  # noqa: F821\n",
    "\n",
    "            for dump in dumps:\n",
    "                span = json.loads(dump.group())\n",
    "                print(span[\"name\"])\n",
    "\n",
    "    elif isinstance(exporter, ZipkinExporter):\n",
    "        print(\n",
    "            \"The spans should be visible in the zipkin dashboard at http://localhost:9411/zipkin/\"\n",
    "        )\n",
    "\n",
    "# This should include:\n",
    "#\n",
    "# - 0: a special span made by TruLens that indicates a recording context. This\n",
    "#   is named \"trulens.recording\".\n",
    "#\n",
    "# - 1: the custom span entitled \"Querying LlamaIndex\" made above.\n",
    "#\n",
    "# - 2: the span made by TruLens that corresponds to the call to\n",
    "#  `query_engine.query`.\n",
    "#\n",
    "# - 3,4: two of the spans produced by the two LlamaIndex instrumentors that\n",
    "#   represents that same call.\n",
    "#\n",
    "# - A bunch more spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a spans produced by TruLens. Note that span instances created by TruLens\n",
    "# are represented as:\n",
    "#\n",
    "#  <class name>(<name>, <trace_id>/<span_id> -> <parent trace_id>/<parent span_id>)\n",
    "#\n",
    "# where trace_id and span_id are only the last byte of each for easier readability.\n",
    "\n",
    "rec.get(Select.RecordSpans.trulens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check details of one the main span (representing the call to `query`).\n",
    "\n",
    "rec.get(Select.RecordSpans.trulens.call.query.attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check attributes of the same information as instrumented by OpenInference:\n",
    "\n",
    "rec.get(Select.RecordSpans.OpenAI.chat.attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check attributes of the same information as instrumented by TraceLoop:\n",
    "\n",
    "rec.get(Select.RecordSpans.RetrieverQueryEngine.workflow.attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for spans that were produced outside of the recording. Here we print all\n",
    "# of the root spans (those that do not have a parent). This should include the\n",
    "# special TruLens span that corresponds to a recording but also other spans\n",
    "# produced before and after the recording.\n",
    "\n",
    "for span in tracer.spans.values():\n",
    "    if span.parent is None:\n",
    "        print(span, span.status, span.attributes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check some of the specific spans.\n",
    "\n",
    "# SQLAlchmey spans:\n",
    "\n",
    "for span in tracer.spans.values():\n",
    "    if span.name == \"connect\":\n",
    "        print(span, span.status, span.attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requests spans:\n",
    "\n",
    "for span in tracer.spans.values():\n",
    "    if span.name in [\"POST\", \"GET\"]:\n",
    "        print(span, span.status, span.attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = index.as_chat_engine(similarity_top_k=3)\n",
    "tru_chat_engine_recorder = TruLlama(\n",
    "    chat_engine, app_name=\"LlamaIndex_App\", app_version=\"chat\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_chat_engine_recorder as recording:\n",
    "    response = chat_engine.stream_chat(\"What did the author do growing up?\")\n",
    "    for chunk in response.response_gen:\n",
    "        print(chunk, end=\"\")\n",
    "\n",
    "record = recording.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the main span:\n",
    "record.get(Select.RecordSpans.trulens.call.stream_chat.attributes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens-9bG3yHQd-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
