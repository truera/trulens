{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# This is needed due to zipkin issues related to protobuf.\n",
    "# os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import dotenv\n",
    "\n",
    "# Add base dir to path to be able to access test folder.\n",
    "base_dir = Path().cwd().parent.parent.resolve()\n",
    "if str(base_dir) not in sys.path:\n",
    "    print(f\"Adding {base_dir} to sys.path\")\n",
    "    sys.path.append(str(base_dir))\n",
    "\n",
    "os.environ[\"OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED\"] = \"true\"\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Deps, OTEL\n",
    "# ! pip install opentelemetry-api opentelemetry-sdk opentelemetry-exporter-otlp\n",
    "\n",
    "# OTEL contrib instrumentors\n",
    "#  opentelemetry-instrumentation-sqlalchemy opentelemetry-instrumentation-requests\n",
    "\n",
    "# Traceloop instrumentors\n",
    "# ! pip install opentelemetry-instrumentation-llamaindex opentelemetry-instrumentation-openai\n",
    "\n",
    "# Arize openinference instrumentors\n",
    "# ! pip install \"openinference-instrumentation-llama-index>=2\"\n",
    "\n",
    "# Install zipkin python package:\n",
    "# ! pip install opentelemetry-exporter-zipkin-proto-http\n",
    "\n",
    "# Start the zipkin docker container:\n",
    "# ! docker run --rm -d -p 9411:9411 --name zipkin openzipkin/zipkin\n",
    "\n",
    "# Stop the zipkin docker container:\n",
    "# ! docker stop $(docker ps -a -q --filter ancestor=openzipkin/zipkin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from openinference.instrumentation.llama_index import (\n",
    "    LlamaIndexInstrumentor as oi_LlamaIndexInstrumentor,\n",
    ")\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.exporter.zipkin.json import ZipkinExporter\n",
    "from opentelemetry.instrumentation.llamaindex import LlamaIndexInstrumentor\n",
    "from opentelemetry.instrumentation.openai import OpenAIInstrumentor\n",
    "from opentelemetry.instrumentation.requests import RequestsInstrumentor\n",
    "from opentelemetry.instrumentation.sqlalchemy import SQLAlchemyInstrumentor\n",
    "from trulens.apps.llamaindex import TruLlama\n",
    "from trulens.core import Select\n",
    "from trulens.core.session import TruSession\n",
    "from trulens.experimental.otel_tracing.core.trace import TracerProvider\n",
    "\n",
    "# from opentelemetry.sdk.trace import TracerProvider\n",
    "\n",
    "provider = TracerProvider()\n",
    "# processor = BatchSpanProcessor(ConsoleSpanExporter())\n",
    "# provider.add_span_processor(processor)\n",
    "\n",
    "# Sets the global default tracer provider\n",
    "trace.set_tracer_provider(provider)\n",
    "\n",
    "# Creates a tracer from the global tracer provider\n",
    "tracer = trace.get_tracer(\"example-tracer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\"\n",
    "file_path = \"data/paul_graham_essay.txt\"\n",
    "\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\")\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru = TruSession(\n",
    "    _experimental_otel_exporter=ZipkinExporter(\n",
    "        endpoint=\"http://localhost:9411/api/v2/spans\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "TruSession().experimental_enable_feature(\"otel_tracing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oi_LlamaIndexInstrumentor().instrument()\n",
    "\n",
    "LlamaIndexInstrumentor().instrument()\n",
    "SQLAlchemyInstrumentor().instrument()\n",
    "RequestsInstrumentor().instrument()\n",
    "OpenAIInstrumentor().instrument()\n",
    "\n",
    "Settings.chunk_size = 128\n",
    "Settings.chunk_overlap = 16\n",
    "Settings.llm = OpenAI()\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_query_engine_recorder = TruLlama(\n",
    "    query_engine,\n",
    "    app_name=\"LlamaIndex_App\",\n",
    "    app_version=\"base\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or as context manager\n",
    "with tru_query_engine_recorder as recording:\n",
    "    with tracer.start_as_current_span(\"Querying LlamaIndex\") as span:\n",
    "        span.set_attribute(\"custom_attribute\", \"This can by anything.\")\n",
    "\n",
    "        query_engine.query(\"What did the author do growing up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = recording.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the span produced by openinference:\n",
    "rec.get(Select.RecordSpans.OpenAI.chat.attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the span produced by traceloop:\n",
    "rec.get(Select.RecordSpans.OpenAI.workflow.attributes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens-9bG3yHQd-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
