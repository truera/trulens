{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate gen AI apps with Snowflake Cortex AI and TruLens\n",
    "This notebook demonstrates how AI Observability in Snowflake Cortex AI helps quantitatively measure the performance of a RAG applications using  different LLMs, providing insights into application behavior and helping the user select the best model for their use case.\n",
    "\n",
    "### Required Packages\n",
    "* trulens-core (1.4.5 or above)\n",
    "* trulens-connectors-snowflake (1.4.5 or above)\n",
    "* trulens-providers-cortex (1.4.5 or above)\n",
    "* snowflake.core (1.0.5 or above)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Information\n",
    "Fetches the current session information and the connection details for the Snowflake account. This connection details will be used to ingest application traces and trigger metric computation jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"SNOWFLAKE_ACCOUNT\"] = \"...\"\n",
    "os.environ[\"SNOWFLAKE_USER\"] = \"...\"\n",
    "os.environ[\"SNOWFLAKE_USER_PASSWORD\"] = \"...\"\n",
    "os.environ[\"SNOWFLAKE_DATABASE\"] = \"...\"\n",
    "os.environ[\"SNOWFLAKE_SCHEMA\"] = \"...\"\n",
    "os.environ[\"SNOWFLAKE_WAREHOUSE\"] = \"...\"\n",
    "os.environ[\"SNOWFLAKE_ROLE\"] = \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Session\n",
    "from trulens.connectors.snowflake import SnowflakeConnector\n",
    "\n",
    "snowflake_connection_parameters = {\n",
    "    \"account\": os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "    \"user\": os.environ[\"SNOWFLAKE_USER\"],\n",
    "    \"password\": os.environ[\"SNOWFLAKE_USER_PASSWORD\"],\n",
    "    \"database\": os.environ[\"SNOWFLAKE_DATABASE\"],\n",
    "    \"schema\": os.environ[\"SNOWFLAKE_SCHEMA\"],\n",
    "    \"role\": os.environ[\"SNOWFLAKE_ROLE\"],\n",
    "    \"warehouse\": os.environ[\"SNOWFLAKE_WAREHOUSE\"],\n",
    "}\n",
    "snowpark_session = Session.builder.configs(\n",
    "    snowflake_connection_parameters\n",
    ").create()\n",
    "\n",
    "# TruSession is no longer required as long as snowflake connector exists\n",
    "sf_connector = SnowflakeConnector(snowpark_session=snowpark_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual Run - New Feature!\n",
    "With the new virtual run feature, you can now ingest existing data directly into the Event Table without creating a dummy app. This approach is much cleaner and avoids the awkward pattern of creating fake app methods.\n",
    "\n",
    "The example schema used is shown below:\n",
    "```sql\n",
    "create table YOUR_TABLE_NAME (\n",
    "    query_string VARCHAR,\n",
    "    output_string VARCHAR, \n",
    "    contexts VARCHAR\n",
    ");\n",
    "```\n",
    "\n",
    "### Two approaches available:\n",
    "\n",
    "**1. New Virtual Run Approach (Recommended):**\n",
    "- No need to create a dummy app\n",
    "- Directly ingest existing data using `run.start(virtual=True)`\n",
    "- Much cleaner and more intuitive\n",
    "\n",
    "**2. Legacy Approach (shown below for comparison):**\n",
    "- Requires creating a TestApp with dummy methods\n",
    "- More verbose and awkward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Virtual Run (Recommended)\n",
    "\n",
    "This approach uses a minimal placeholder app class instead of the complex TestApp with actual data fetching logic. The key benefits:\n",
    "\n",
    "- **Minimal boilerplate**: Just a simple placeholder class with one method\n",
    "- **No data fetching logic**: The method is never actually called\n",
    "- **Same TruApp flow**: Uses the familiar `tru_app.add_run()` pattern\n",
    "- **Virtual execution**: `run.start(virtual=True)` creates spans from existing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Virtual Run approach - much cleaner!\n",
    "import uuid\n",
    "\n",
    "from trulens.apps.app import TruApp\n",
    "from trulens.core.run import RunConfig\n",
    "\n",
    "APP_NAME = \"RAG evaluation run on existing data\"\n",
    "APP_VERSION = \"V1\"\n",
    "\n",
    "\n",
    "# Create a minimal placeholder app for virtual runs\n",
    "class VirtualApp:\n",
    "    \"\"\"Minimal placeholder app for virtual runs - no real methods needed\"\"\"\n",
    "\n",
    "    def virtual_query(self, query: str) -> str:\n",
    "        \"\"\"Placeholder method - not actually called in virtual runs\"\"\"\n",
    "        return \"virtual_result\"\n",
    "\n",
    "\n",
    "# Create TruApp with placeholder - preserves existing add_run() flow\n",
    "virtual_app = VirtualApp()\n",
    "tru_app = TruApp(\n",
    "    virtual_app,\n",
    "    app_name=APP_NAME,\n",
    "    app_version=APP_VERSION,\n",
    "    connector=sf_connector,\n",
    "    # main_method=virtual_app.virtual_query,  # Specify the main method explicitly\n",
    ")\n",
    "\n",
    "# Create run config with dataset specification\n",
    "run_name = f\"virtual_run_{uuid.uuid4()}\"\n",
    "\n",
    "run_config = RunConfig(\n",
    "    run_name=run_name,\n",
    "    dataset_name=\"YOUR_TABLE_NAME\",  # Your Snowflake table name\n",
    "    source_type=\"TABLE\",\n",
    "    dataset_spec={\n",
    "        \"record_root.input\": \"QUERY_STRING\",  # Maps to input field\n",
    "        \"record_root.output\": \"OUTPUT_STRING\",  # Maps to output field\n",
    "        \"retrieved_contexts\": \"CONTEXTS\",  # Maps to contexts field (optional)\n",
    "        # Add other fields as needed\n",
    "    },\n",
    ")\n",
    "\n",
    "# Use the existing add_run() flow\n",
    "virtual_run = tru_app.add_run(run_config=run_config)\n",
    "\n",
    "print(f\"Created virtual run: {run_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the virtual run - this will create OTEL spans from existing data\n",
    "# The virtual=True flag tells the run to create spans from existing data\n",
    "# instead of actually invoking the VirtualApp.virtual_query method\n",
    "virtual_run.start(virtual=True)\n",
    "\n",
    "print(\"Virtual run completed! Data has been ingested into Event Table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check virtual run status\n",
    "import time\n",
    "\n",
    "while virtual_run.get_status() == \"INVOCATION_IN_PROGRESS\":\n",
    "    print(\"Waiting for ingestion to complete...\")\n",
    "    time.sleep(2)\n",
    "\n",
    "print(f\"Virtual run status: {virtual_run.get_status()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics for the virtual run\n",
    "virtual_run.compute_metrics([\n",
    "    \"answer_relevance\",\n",
    "    \"context_relevance\",\n",
    "    \"groundedness\",\n",
    "])\n",
    "\n",
    "print(\"Metrics computation started for virtual run!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Legacy Approach (for comparison)\n",
    "\n",
    "This shows the old way of doing virtual runs, which required creating a complex TestApp with actual data fetching methods. Compare the complexity below with the simple VirtualApp above:\n",
    "\n",
    "**Key differences:**\n",
    "- **More complex**: TestApp has actual data fetching logic in each method\n",
    "- **Awkward pattern**: Methods fetch data that already exists in the table  \n",
    "- **More code**: Requires implementing retrieval, generation, and query methods\n",
    "- **Still works**: This approach is still supported for backward compatibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core.otel.instrument import instrument\n",
    "from trulens.otel.semconv.trace import SpanAttributes\n",
    "\n",
    "\n",
    "class TestApp:\n",
    "    def __init__(self, snowflake_table_name: str):\n",
    "        self.snowflake_table_name = snowflake_table_name\n",
    "\n",
    "    @instrument(\n",
    "        span_type=SpanAttributes.SpanType.RECORD_ROOT,\n",
    "        attributes={\n",
    "            SpanAttributes.RECORD_ROOT.INPUT: \"query\",\n",
    "            SpanAttributes.RECORD_ROOT.OUTPUT: \"return\",\n",
    "        },\n",
    "    )\n",
    "    def query(self, query: str) -> str:\n",
    "        retrieved_contexts = self.get_contexts(query)\n",
    "        return self.generate_answer(query, retrieved_contexts)\n",
    "\n",
    "    @instrument(\n",
    "        span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
    "        attributes={\n",
    "            SpanAttributes.RETRIEVAL.QUERY_TEXT: \"query\",\n",
    "            SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS: \"return\",\n",
    "        },\n",
    "    )\n",
    "    def get_contexts(self, query: str) -> list[str]:\n",
    "        # query the snowflake table with query and find the relevant retrieved contexts. the contexts column is a string in comma separated format. parse them into a list of strings.\n",
    "        query_result = snowpark_session.sql(\n",
    "            f\"SELECT CONTEXTS FROM {self.snowflake_table_name} WHERE QUERY_STRING = '{query}'\"\n",
    "        ).collect()\n",
    "\n",
    "        if not query_result:\n",
    "            return []\n",
    "\n",
    "        # Get contexts string from first row\n",
    "        contexts_str = query_result[0][\"CONTEXTS\"]\n",
    "\n",
    "        # Parse comma-separated string into list\n",
    "        if contexts_str:\n",
    "            contexts = [context.strip() for context in contexts_str.split(\",\")]\n",
    "            return contexts\n",
    "\n",
    "        return []\n",
    "\n",
    "    @instrument(\n",
    "        span_type=SpanAttributes.SpanType.GENERATION,\n",
    "    )\n",
    "    def generate_answer(self, query: str, contexts: list[str]) -> str:\n",
    "        # Query snowflake table to get output string for the given query\n",
    "\n",
    "        if len(contexts) == 0:\n",
    "            return \"Sorry, I couldn't find an answer to your question.\"\n",
    "        query_result = snowpark_session.sql(\n",
    "            f\"SELECT OUTPUT_STRING FROM {self.snowflake_table_name} WHERE QUERY_STRING = '{query}'\"\n",
    "        ).collect()\n",
    "        answer = query_result[0][\"OUTPUT_STRING\"] if query_result else None\n",
    "        if answer:\n",
    "            return answer\n",
    "        else:\n",
    "            return \"Did not find an answer.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## App Registration\n",
    "Registers the two app instances in Snowflake, creating EXTERNAL AGENT objects to represent the app instances in the Snowflake account and registers both the app instances as different versions of the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TruLens instrumented app from custom app.\n",
    "\n",
    "import uuid\n",
    "\n",
    "from trulens.apps.app import TruApp\n",
    "\n",
    "APP_NAME = \"RAG evaluation run on existing data\"\n",
    "APP_VERSION = \"V1\"\n",
    "\n",
    "test_app = TestApp(snowflake_table_name=\"YOUR_TABLE_NAME\")\n",
    "\n",
    "tru_app = TruApp(\n",
    "    test_app,\n",
    "    app_name=APP_NAME,\n",
    "    app_version=APP_VERSION,\n",
    "    connector=sf_connector,\n",
    "    main_method=test_app.query,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add runs to agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core.run import Run\n",
    "from trulens.core.run import RunConfig\n",
    "\n",
    "run_name = f\"test_virtual_run_{uuid.uuid4()}\"\n",
    "\n",
    "run_config = RunConfig(\n",
    "    run_name=run_name,\n",
    "    dataset_name=\"VIRTUAL_RUN_TEST\",\n",
    "    source_type=\"TABLE\",\n",
    "    dataset_spec={\n",
    "        \"RECORD_ROOT.INPUT\": \"QUERY_STRING\",  # column name \"QUERY_STRING\" is case sensitive\n",
    "    },\n",
    ")\n",
    "\n",
    "run: Run = tru_app.add_run(run_config=run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Status Check\n",
    "Checks the status of the runs for \"INVOCATION_IN_PROGRESS\". \n",
    "\n",
    "Note: Metric computation cannot be started until the invocation is in progress. Once the runs' status is changed to \"INVOCATION_COMPLETED\", metric computation can be triggered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while run.get_status() == \"INVOCATION_IN_PROGRESS\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.compute_metrics([\n",
    "    \"answer_relevance\",\n",
    "    \"context_relevance\",\n",
    "    \"groundedness\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Metrics\n",
    "\n",
    "Computes the RAG triad metrics for both runs to measure the quality of response in the RAG application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Results\n",
    "\n",
    "To view evaluation results:\n",
    "* Login to [Snowsight](https://app.snowflake.com/).\n",
    "* Navigate to **AI & ML** -> **Evaluations** from the left navigation menu.\n",
    "* Select “RAG evaluation run on existing data” to view the runs, see detailed traces and compare runs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
