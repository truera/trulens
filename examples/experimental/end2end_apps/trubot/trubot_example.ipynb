{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruBot\n",
    "\n",
    "Example setup and monitoring of a conversational bot with context made up of the\n",
    "TruEra website. This example requires either a pinecone vector db set up with\n",
    "some contexts to answer questions with or alternatively can use the local\n",
    "database for use with hnswlib provided here. To use hnswlib, some additional\n",
    "requirements need to be installed with pip. Regardless of the vector db\n",
    "provider, the example feedback functions here use openai and huggingface free\n",
    "inference APIs and need their respective keys to be provided in a .env file.\n",
    "\n",
    "## HNSWLIB additional requirements\n",
    "\n",
    "Run the following in your shell or the equivalent in the following cell to\n",
    "install additional requirements for use with HNSWLIB. This is not required if\n",
    "you are running this example with a pinecone db.\n",
    "\n",
    "```bash\n",
    "pip install docarray hnswlib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install docarray hnswlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# If running from github repo, can use this:\n",
    "sys.path.append(str(Path().cwd().parent.parent.parent.parent.resolve()))\n",
    "\n",
    "# Uncomment for more debugging printouts.\n",
    "\"\"\"\n",
    "import logging\n",
    "root = logging.getLogger()\n",
    "root.setLevel(logging.DEBUG)\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "root.addHandler(handler)\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API keys setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core.utils.keys import check_keys\n",
    "\n",
    "check_keys(\"OPENAI_API_KEY\", \"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import PrettyPrinter\n",
    "\n",
    "# Imports from LangChain to build app:\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "import numpy as np\n",
    "from trulens.apps.langchain import WithFeedbackFilterDocuments\n",
    "\n",
    "# Imports main tools:\n",
    "from trulens.core import Feedback\n",
    "from trulens.core import FeedbackMode\n",
    "from trulens.core import Select\n",
    "from trulens.core import TruSession\n",
    "from trulens.core.utils.threading import TP\n",
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "pp = PrettyPrinter()\n",
    "\n",
    "# Tru object manages the database of apps, records, and feedbacks; and the\n",
    "# dashboard to display these.\n",
    "tru = TruSession()\n",
    "\n",
    "# Start the dashboard. If you running from github repo, you will need to adjust\n",
    "# the path the dashboard streamlit app starts in by providing the _dev argument.\n",
    "run_dashboard(\n",
    "    tru, force=True, _dev=Path().cwd().parent.parent.parent.parent.resolve()\n",
    ")\n",
    "\n",
    "# If needed, you can reset the trulens dashboard database by running the\n",
    "# below line:\n",
    "\n",
    "# tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.providers.huggingface import Huggingface\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "# Select vector db provider. Pinecone requires setting up a pinecone database\n",
    "# first while the hnsw database is included with trulens.\n",
    "# db_host = \"pinecone\"\n",
    "db_host = \"pinecone\"\n",
    "\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "app_name = \"TruBot\"\n",
    "\n",
    "# Embedding for vector db.\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-ada-002\")  # 1536 dims\n",
    "\n",
    "if db_host == \"pinecone\":\n",
    "    check_keys(\"PINECONE_API_KEY\", \"PINECONE_ENV\")\n",
    "\n",
    "    # Pinecone configuration if using pinecone.\n",
    "\n",
    "    from langchain_community.vectorstores import Pinecone\n",
    "    import pinecone\n",
    "\n",
    "    pinecone.init(\n",
    "        api_key=os.environ.get(\"PINECONE_API_KEY\"),  # find at app.pinecone.io\n",
    "        environment=os.environ.get(\n",
    "            \"PINECONE_ENV\"\n",
    "        ),  # next to api key in console\n",
    "    )\n",
    "\n",
    "    # If using pinecone, make sure you create your index under name 'llmdemo' or\n",
    "    # change the below.\n",
    "\n",
    "    def get_doc_search():\n",
    "        docsearch = Pinecone.from_existing_index(\n",
    "            index_name=\"llmdemo\", embedding=embedding\n",
    "        )\n",
    "\n",
    "        return docsearch\n",
    "\n",
    "elif db_host == \"hnsw\":\n",
    "    # Local pinecone alternative. Requires precomputed 'hnswlib_truera' folder.\n",
    "\n",
    "    from langchain.vectorstores import DocArrayHnswSearch\n",
    "\n",
    "    def get_doc_search():\n",
    "        # We need to create this object in the thread in which it is used so we\n",
    "        # wrap it in this function for later usage.\n",
    "\n",
    "        docsearch = DocArrayHnswSearch.from_params(\n",
    "            embedding=embedding,\n",
    "            work_dir=\"hnswlib_trubot\",\n",
    "            n_dim=1536,\n",
    "            max_elements=1024,\n",
    "        )\n",
    "\n",
    "        return docsearch\n",
    "\n",
    "else:\n",
    "    raise RuntimeError(\"Unhandled db_host, select either 'pinecone' or 'hnsw'.\")\n",
    "\n",
    "# LLM for completing prompts, and other tasks.\n",
    "llm = OpenAI(temperature=0, max_tokens=256)\n",
    "\n",
    "# Construct feedback functionfs.\n",
    "\n",
    "# API endpoints for models used in feedback functions:\n",
    "\n",
    "hugs = Huggingface()\n",
    "openai = OpenAI()\n",
    "\n",
    "# Language match between question/answer.\n",
    "f_lang_match = Feedback(hugs.language_match).on_input_output()\n",
    "# By default this will evaluate feedback on main app input and main app output.\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = Feedback(openai.relevance).on_input_output()\n",
    "# By default this will evaluate feedback on main app input and main app output.\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(openai.context_relevance)\n",
    "    .on_input()\n",
    "    .on(\n",
    "        Select.Record.app.combine_docs_chain._call.args.inputs.input_documents[\n",
    "            :\n",
    "        ].page_content\n",
    "    )\n",
    "    .aggregate(np.min)\n",
    ")\n",
    "# First feedback argument is set to main app input, and the second is taken from\n",
    "# the context sources as passed to an internal `combine_docs_chain._call`.\n",
    "\n",
    "all_feedbacks = [f_lang_match, f_qa_relevance, f_context_relevance]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruBot Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.apps.langchain import TruChain\n",
    "\n",
    "\n",
    "def v1_new_conversation(feedback_mode=FeedbackMode.WITH_APP):\n",
    "    \"\"\"\n",
    "    Create a _LangChain_ app for a new conversation with a question-answering bot.\n",
    "\n",
    "    Feedback_mode controls when feedback is evaluated:\n",
    "\n",
    "    - FeedbackMode.WITH_APP -- app will wait until feedback is evaluated before\n",
    "      returning from calls.\n",
    "\n",
    "    - FeedbackMode.WITH_APP_THREAD -- app will return from calls and evaluate\n",
    "      feedback in a new thread.\n",
    "\n",
    "    - FeedbackMode.DEFERRED -- app will return and a separate runner thread (see\n",
    "      usage later in this notebook) will evaluate feedback.\n",
    "    \"\"\"\n",
    "\n",
    "    # Blank conversation memory.\n",
    "    memory = ConversationSummaryBufferMemory(\n",
    "        max_token_limit=650,\n",
    "        llm=llm,\n",
    "        memory_key=\"chat_history\",\n",
    "        output_key=\"answer\",\n",
    "    )\n",
    "\n",
    "    docsearch = get_doc_search()\n",
    "\n",
    "    # Context retriever.\n",
    "    retriever = docsearch.as_retriever()\n",
    "\n",
    "    # Conversational app puts it all together.\n",
    "    app = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        memory=memory,\n",
    "        get_chat_history=lambda a: a,\n",
    "        max_tokens_limit=4096,\n",
    "    )\n",
    "\n",
    "    # Trulens instrumentation.\n",
    "\n",
    "    tc = TruChain(\n",
    "        app_name=app_name,\n",
    "        app_version=\"v1\",\n",
    "        app=app,\n",
    "        feedbacks=all_feedbacks,\n",
    "        feedback_mode=feedback_mode,\n",
    "    )\n",
    "\n",
    "    return app, tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the app with fresh memory:\n",
    "\n",
    "import traceback\n",
    "\n",
    "try:\n",
    "    app1, tc1 = v1_new_conversation()\n",
    "except Exception:\n",
    "    print(traceback.format_exc())\n",
    "\n",
    "# Call the app:\n",
    "\n",
    "res, record = tc1.with_record(app1, \"Who is Shayak?\")\n",
    "res\n",
    "\n",
    "# Notice the `source_documents` returned include chunks about Shameek and the\n",
    "# answer includes bits about Shameek as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The feedback should already be present in the dashboard, but we can check the\n",
    "# context_relevance here manually as well:\n",
    "feedback = f_context_relevance.run(record=record, app=tc1)\n",
    "feedback.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now a question about QII (quantitative input influence is a base technology\n",
    "# employed in TruEra's products) question but in a non-English language:\n",
    "\n",
    "# Start a new conversation as the app keeps prior questions in its memory which\n",
    "# may cause you some testing woes.\n",
    "app1, tc1 = v1_new_conversation()\n",
    "\n",
    "# res, record = tc1.with_record(app1, \"Co jest QII?\") # Polish\n",
    "res, record = tc1.with_record(app1, \"Was ist QII?\")  # German\n",
    "res\n",
    "\n",
    "# Note here the response is in English. This example sometimes matches language\n",
    "# so other variants may need to be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language match failure can be seen using the f_lang_match (and is visible in\n",
    "# dashboard):\n",
    "feedback = f_lang_match.run(record=record, app=tc1)\n",
    "feedback.model_dump()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruBot Version 2 - Language match fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v2_new_conversation(feedback_mode=FeedbackMode.WITH_APP):\n",
    "    \"\"\"\n",
    "    Create a _LangChain_ app for a new conversation with a question-answering bot.\n",
    "    \"\"\"\n",
    "\n",
    "    # Blank conversation memory.\n",
    "    memory = ConversationSummaryBufferMemory(\n",
    "        max_token_limit=650,\n",
    "        llm=llm,\n",
    "        memory_key=\"chat_history\",\n",
    "        output_key=\"answer\",\n",
    "    )\n",
    "\n",
    "    docsearch = get_doc_search()\n",
    "\n",
    "    # Context retriever.\n",
    "    retriever = docsearch.as_retriever()\n",
    "\n",
    "    # Conversational app puts it all together.\n",
    "    app = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        memory=memory,\n",
    "        get_chat_history=lambda a: a,\n",
    "        max_tokens_limit=4096,\n",
    "    )\n",
    "\n",
    "    ### DIFFERENCES START HERE\n",
    "\n",
    "    # Need to copy these otherwise various apps will feature templates that\n",
    "    # point to the same objects.\n",
    "    app.combine_docs_chain.llm_chain.prompt = (\n",
    "        app.combine_docs_chain.llm_chain.prompt.copy()\n",
    "    )\n",
    "    app.combine_docs_chain.document_prompt = (\n",
    "        app.combine_docs_chain.document_prompt.copy()\n",
    "    )\n",
    "\n",
    "    # Language mismatch fix via a prompt adjustment:\n",
    "    app.combine_docs_chain.llm_chain.prompt.template = (\n",
    "        \"Use the following pieces of context to answer the question at the end \"\n",
    "        \"in the same language as the question. If you don't know the answer, \"\n",
    "        \"just say that you don't know, don't try to make up an answer.\\n\\n\"\n",
    "        \"{context}\\n\\n\"\n",
    "        \"Question: {question}\\n\"\n",
    "        \"Helpful Answer: \"\n",
    "    )\n",
    "\n",
    "    ### END OF DIFFERENCES\n",
    "\n",
    "    # Trulens instrumentation.\n",
    "    tc = TruChain(\n",
    "        app_name=app_name,\n",
    "        app_version=\"v2\",\n",
    "        app=app,\n",
    "        feedbacks=all_feedbacks,\n",
    "        feedback_mode=feedback_mode,\n",
    "    )\n",
    "\n",
    "    return app, tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the version 2 app:\n",
    "\n",
    "app2, tc2 = v2_new_conversation()\n",
    "\n",
    "# Now the non-English question again:\n",
    "\n",
    "res, record = tc2.with_record(app2, \"Was ist QII?\")\n",
    "res\n",
    "\n",
    "# Note that the response is now the appropriate language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the language match feedback is happy:\n",
    "\n",
    "feedback = f_lang_match.run(record=record, app=tc2)\n",
    "feedback.model_dump()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruBot Version 3: Context Filtering with Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v3_new_conversation(feedback_mode=FeedbackMode.WITH_APP):\n",
    "    \"\"\"\n",
    "    Create a _LangChain_ app for a new conversation with a question-answering bot.\n",
    "    \"\"\"\n",
    "\n",
    "    # Blank conversation memory.\n",
    "    memory = ConversationSummaryBufferMemory(\n",
    "        max_token_limit=650,\n",
    "        llm=llm,\n",
    "        memory_key=\"chat_history\",\n",
    "        output_key=\"answer\",\n",
    "    )\n",
    "\n",
    "    docsearch = get_doc_search()\n",
    "\n",
    "    # Context retriever.\n",
    "    retriever = docsearch.as_retriever()\n",
    "\n",
    "    ### DIFFERENCES START HERE\n",
    "\n",
    "    # Modified retriever that first filters returned contexts using\n",
    "    # f_context_relevance with a minimum relevance threshold (of 0.5):\n",
    "    retriever_filtered = WithFeedbackFilterDocuments.of_retriever(\n",
    "        retriever=retriever, feedback=f_context_relevance, threshold=0.5\n",
    "    )\n",
    "\n",
    "    ### END OF DIFFERENCES\n",
    "\n",
    "    # Conversational app puts it all together.\n",
    "    app = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever_filtered,\n",
    "        return_source_documents=True,\n",
    "        memory=memory,\n",
    "        get_chat_history=lambda a: a,\n",
    "        max_tokens_limit=4096,\n",
    "    )\n",
    "\n",
    "    # Trulens instrumentation.\n",
    "    tc = TruChain(\n",
    "        app_name=app_name,\n",
    "        app_version=\"v3\",\n",
    "        app=app,\n",
    "        feedbacks=all_feedbacks,\n",
    "        feedback_mode=feedback_mode,\n",
    "    )\n",
    "\n",
    "    return app, tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the version 3 app:\n",
    "\n",
    "app3, tc3 = v3_new_conversation()\n",
    "\n",
    "# Call the app:\n",
    "\n",
    "res, record = tc3.with_record(app3, \"Who is Shayak?\")\n",
    "res\n",
    "\n",
    "# Notice the `source_documents` returned now does not include the low-relevance\n",
    "# chunks and the answer likewise does not reference them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruBot Version 4: Lang match fix and context filter\n",
    "\n",
    "This is left as an exercise to the reader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v4_new_conversation(feedback_mode=FeedbackMode.WITH_APP):\n",
    "    \"\"\"\n",
    "    Create a _LangChain_ app for a new conversation with a question-answering bot.\n",
    "    \"\"\"\n",
    "\n",
    "    ### TO FILL IN HERE ###\n",
    "    app = ...\n",
    "    ### END OF TO FILL IN ###\n",
    "\n",
    "    # Trulens instrumentation.\n",
    "    tc = TruChain(\n",
    "        app_name=app_name,\n",
    "        app_version=\"v4\",\n",
    "        app=app,\n",
    "        feedbacks=all_feedbacks,\n",
    "        feedback_mode=feedback_mode,\n",
    "    )\n",
    "\n",
    "    return app, tc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test conversations\n",
    "\n",
    "Lets try out the 3 (or 4) trubot versions on a collection of test instances\n",
    "about Shayak and some technical terms in several languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps = [\n",
    "    v1_new_conversation,\n",
    "    v2_new_conversation,\n",
    "    v3_new_conversation,\n",
    "    # v4_new_conversation # include this if you completed the exercise\n",
    "]\n",
    "\n",
    "questions = [\n",
    "    \"Who is Shayak?\",\n",
    "    \"Wer ist Shayak?\",\n",
    "    \"Kim jest Shayak?\",\n",
    "    \"¿Quién es Shayak?\",\n",
    "    \"What is QII?\",\n",
    "    \"Was ist QII?\",\n",
    "    \"Co jest QII?\",\n",
    "    \"¿Que es QII?\",\n",
    "]\n",
    "\n",
    "# Comment out the next two lines to try all of the version and question\n",
    "# combinations. Otherwise we select here only 2 questions and 2 models to start with.\n",
    "apps = apps[0:2]\n",
    "questions = questions[0:2]\n",
    "\n",
    "\n",
    "def test_app_on_question(new_convo, question):\n",
    "    print(new_convo.__name__, question)\n",
    "    app, tc = new_convo(feedback_mode=FeedbackMode.DEFERRED)\n",
    "    answer = tc.with_(app, question)\n",
    "    return answer\n",
    "\n",
    "\n",
    "# This asks all of the questions in parallel:\n",
    "for new_convo in apps:\n",
    "    for question in questions:\n",
    "        TP().submit(\n",
    "            test_app_on_question, new_convo=new_convo, question=question\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For deferred feedback evaluation. Start this:\n",
    "\n",
    "TruSession().start_evaluator(restart=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
