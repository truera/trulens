{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Client-Side Custom Metrics with TruLens\n",
    "\n",
    "This notebook demonstrates how to create and use client-side custom metrics with TruLens. Client-side custom metrics allow you to define your own evaluation functions that run locally on the client instead of on the server (Snowflake).\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Custom Metric Decorator**: Use `@custom_metric` to convert any function into a metric\n",
    "- **EvaluationConfig**: Explicit configuration for mapping metric parameters to span attributes\n",
    "- **Flexible Selectors**: Map metric parameters to span attributes using selectors\n",
    "- **Client-Side Computation**: Metrics are computed locally and results uploaded as OTEL spans\n",
    "- **Seamless Integration**: Works with existing TruLens apps and runs\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- OTEL tracing enabled\n",
    "- TruLens feedback package installed\n",
    "- Access to a TruLens app with instrumented methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from trulens.apps.app import TruApp\n",
    "from trulens.core.feedback.custom_metric import EvaluationConfig\n",
    "from trulens.core.feedback.custom_metric import custom_metric\n",
    "from trulens.core.feedback.selector import Selector\n",
    "from trulens.core.otel.instrument import instrument\n",
    "from trulens.otel.semconv.trace import SpanAttributes\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Enable OTEL tracing - MUST be set before importing TruLens\n",
    "os.environ[\"TRULENS_OTEL_TRACING\"] = \"1\"\n",
    "\n",
    "\n",
    "# Define a mock RAG app.\n",
    "\n",
    "\n",
    "class TestApp:\n",
    "    @instrument(\n",
    "        span_type=SpanAttributes.SpanType.RECORD_ROOT,\n",
    "        attributes={\n",
    "            SpanAttributes.RECORD_ROOT.INPUT: \"query\",\n",
    "            SpanAttributes.RECORD_ROOT.OUTPUT: \"return\",\n",
    "        },\n",
    "    )\n",
    "    def query(self, query: str) -> str:\n",
    "        retrieved_contexts = self.get_contexts(query)\n",
    "        return self.generation(query, retrieved_contexts)\n",
    "\n",
    "    @instrument(\n",
    "        span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
    "        attributes={\n",
    "            SpanAttributes.RETRIEVAL.QUERY_TEXT: \"query\",\n",
    "            SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS: \"return\",\n",
    "        },\n",
    "    )\n",
    "    def get_contexts(self, query: str) -> list[str]:\n",
    "        return [\"context 1\", \"context 2\", \"context 3\", \"context 4\"]\n",
    "\n",
    "    @instrument(\n",
    "        span_type=SpanAttributes.SpanType.GENERATION,\n",
    "    )\n",
    "    def generation(self, query: str, contexts: list[str]) -> str:\n",
    "        if len(contexts) == 0:\n",
    "            return \"Sorry, I couldn't find an answer to your question.\"\n",
    "        return \"Answer to your question.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define Custom Metrics\n",
    "\n",
    "Let's create some custom metrics using the `@custom_metric` decorator. These metrics will evaluate the quality of text-to-SQL generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom metrics using the decorator\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "@custom_metric(metric_type=\"custom_accuracy\", higher_is_better=True)\n",
    "def custom_accuracy(query: str) -> float:\n",
    "    \"\"\"\n",
    "    A custom implementation of some arbitrary accuracy metric. Here we just check if the length of the query is greater than 50.\n",
    "    This is a simplified example - in practice you'd have more sophisticated logic.\n",
    "    \"\"\"\n",
    "    if len(query) > 50:\n",
    "        return 1.0\n",
    "\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "@custom_metric(metric_type=\"random_metric\", higher_is_better=True)\n",
    "def custom_random_metric(some_str: str) -> Tuple[float, str]:\n",
    "    \"\"\"\n",
    "    A custom implementation of another arbitrary accuracy metric.\n",
    "    \"\"\"\n",
    "    import random\n",
    "\n",
    "    score = random.random()\n",
    "    return score, some_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create EvaluationConfig Objects\n",
    "\n",
    "Now let's create explicit evaluation configurations that define how to map OTEL span attributes to metric function parameters. This is where the **span-to-argument mapping** happens!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Create EvaluationConfig using fluent interface\n",
    "eval_config_1 = EvaluationConfig(\n",
    "    name=\"evaluation_config_1\",\n",
    "    metric_type=\"custom_accuracy\",\n",
    "    computation_type=\"client\",\n",
    "    description=\"Evaluates some custom accuracy\",\n",
    ").add_selector(\n",
    "    \"query\",  # Parameter name in the metric function\n",
    "    Selector(\n",
    "        function_attribute=\"query\",  # Extract from 'query' parameter\n",
    "        function_name=\"TestApp.query\",  # From this specific function\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Method 2: Create EvaluationConfig from dictionary (matches original specification)\n",
    "eval_config_dict = {\n",
    "    \"name\": \"evaluation_config_2\",\n",
    "    \"metric_type\": \"custom_random_metric\",\n",
    "    \"computation_type\": \"client\",\n",
    "    \"description\": \"Evaluate with a random number generating metric\",\n",
    "    \"selectors\": {\n",
    "        \"some_str\": Selector(\n",
    "            function_attribute=\"return\", function_name=\"TestApp.generation\"\n",
    "        )\n",
    "    },\n",
    "}\n",
    "\n",
    "eval_config_2 = EvaluationConfig.from_dict(eval_config_dict)\n",
    "\n",
    "print(\"EvaluationConfig objects created:\")\n",
    "print(f\"1. {eval_config_1}\")\n",
    "print(f\"2. {eval_config_2}\")\n",
    "\n",
    "# Show the span-to-argument mapping details\n",
    "print(\"\\\\n=== Span-to-Argument Mapping ===\")\n",
    "print(\"eval_config_1 mapping:\")\n",
    "for param_name, selector in eval_config_1.selectors.items():\n",
    "    print(\n",
    "        f\"  Parameter '{param_name}' ‚Üê {selector.function_name}.{selector.function_attribute}\"\n",
    "    )\n",
    "\n",
    "print(\"\\\\eval_config_2 mapping:\")\n",
    "for param_name, selector in eval_config_2.selectors.items():\n",
    "    print(\n",
    "        f\"  Parameter '{param_name}' ‚Üê {selector.function_name}.{selector.function_attribute}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create snowpark session.\n",
    "import os\n",
    "\n",
    "from snowflake.snowpark import Session\n",
    "from trulens.connectors.snowflake import SnowflakeConnector\n",
    "\n",
    "snowflake_connection_parameters = {\n",
    "    \"account\": os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "    \"user\": os.environ[\"SNOWFLAKE_USER\"],\n",
    "    \"password\": os.environ[\"SNOWFLAKE_USER_PASSWORD\"],\n",
    "    \"database\": os.environ[\"SNOWFLAKE_DATABASE\"],\n",
    "    \"schema\": os.environ[\"SNOWFLAKE_SCHEMA\"],\n",
    "    \"role\": os.environ[\"SNOWFLAKE_ROLE\"],\n",
    "    \"warehouse\": os.environ[\"SNOWFLAKE_WAREHOUSE\"],\n",
    "}\n",
    "snowpark_session = Session.builder.configs(\n",
    "    snowflake_connection_parameters\n",
    ").create()\n",
    "\n",
    "# TruSession is no longer required as long as snowflake connector exists\n",
    "sf_connector = SnowflakeConnector(snowpark_session=snowpark_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TruLens instrumented app from custom app.\n",
    "\n",
    "\n",
    "# APP_NAME = f\"{os.getlogin()} custom metrics client-side flow {uuid.uuid4()}\"\n",
    "APP_NAME = \"dhuang custom metrics client-side flow b055dcbe-d0ae-491d-9694-200c7622f1ae\"\n",
    "APP_VERSION = \"V1\"\n",
    "\n",
    "test_app = TestApp()\n",
    "tru_app = TruApp(\n",
    "    test_app, app_name=APP_NAME, app_version=APP_VERSION, connector=sf_connector\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_entries = [\n",
    "    {\n",
    "        \"query\": \"What wave of coffee culture is Starbucks seen to represent in the United States?\"\n",
    "    },\n",
    "    {\"query\": \"What is the largest city in New Zealand?\"},\n",
    "    {\n",
    "        \"query\": \"What is the main campus of the University of Washington located?\"\n",
    "    },\n",
    "    {\"query\": \"What is the capital city of New Zealand?\"},\n",
    "    {\n",
    "        \"query\": \"What is the largest institution of higher education in Washington state?\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What wave of coffee culture is Starbucks seen to represent in the New Zealand?\"\n",
    "    },\n",
    "    {\"query\": \"What year was Washington State University founded?\"},\n",
    "    {\n",
    "        \"query\": \"Which university has a strong focus on veterinary medicine and agriculture?\"\n",
    "    },\n",
    "    {\"query\": \"Which landmark in Seattle was built for the 1962 World‚Äôs Fair?\"},\n",
    "    {\"query\": \"How many campuses does the University of Washington have?\"},\n",
    "    {\"query\": \"Where is Starbucks headquartered?\"},\n",
    "]\n",
    "\n",
    "\n",
    "user_input_data_df = pd.DataFrame(test_data_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Add metric using EvaluationConfig (recommended approach)\n",
    "print(\"\\\\n=== Registering Metrics with EvaluationConfig ===\")\n",
    "\n",
    "tru_app.add_metric_with_evaluation_config(\n",
    "    metric=custom_accuracy, evaluation_config=eval_config_1\n",
    ")\n",
    "print(f\"‚úÖ Registered: {eval_config_1.name}\")\n",
    "\n",
    "tru_app.add_metric_with_evaluation_config(\n",
    "    metric=custom_random_metric, evaluation_config=eval_config_2\n",
    ")\n",
    "print(f\"‚úÖ Registered: {eval_config_2.name}\")\n",
    "\n",
    "print(\"\\\\nMetrics registered using EvaluationConfig approach!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Understanding the Evaluation Config Mapping\n",
    "\n",
    "Let's examine how the evaluation configs map OTEL span attributes to metric function parameters. This shows exactly how the **span-to-argument mapping** works!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from trulens.core.run import Run\n",
    "# from trulens.core.run import RunConfig\n",
    "\n",
    "# run_name = f\"test_run_0623_{uuid.uuid4()}\"\n",
    "\n",
    "# run_config = RunConfig(\n",
    "#     run_name=run_name,\n",
    "#     dataset_name=\"dummy_test_rag_set\",\n",
    "#     source_type=\"DATAFRAME\",\n",
    "#     dataset_spec={\"RECORD_ROOT.INPUT\": \"query\"},\n",
    "# )  # type: ignore\n",
    "\n",
    "# run: Run = tru_app.add_run(run_config=run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"test_run_0623_dee81f62-1b55-4c9e-9eb9-f3d1fe984b14\"\n",
    "run = tru_app.get_run(run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.start(input_df=user_input_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.compute_metrics([\n",
    "    # \"answer_relevance\",\n",
    "    \"custom_accuracy\",\n",
    "    \"custom_random_metric\",\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
