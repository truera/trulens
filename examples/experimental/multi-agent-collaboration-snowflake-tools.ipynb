{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39fd1948-b5c3-48c4-b10e-2ae7e8c83334",
   "metadata": {},
   "source": [
    "# Multi-agent network with Snowflake tools for querying unstructured and structured data\n",
    "\n",
    "Adapted from the original [Langgraph multi-agent notebook example](https://github.com/langchain-ai/langgraph/blob/main/docs/docs/tutorials/multi_agent/multi-agent-collaboration.ipynb)\n",
    "\n",
    "A single agent can usually operate effectively using a handful of tools within a single domain, but even using powerful models like `gpt-4`, it can be less effective at using many tools. \n",
    "\n",
    "One way to approach complicated tasks is through a \"divide-and-conquer\" approach: create a specialized agent for each task or domain and route tasks to the correct \"expert\". This is an example of a [multi-agent network](https://langchain-ai.github.io/langgraph/concepts/multi_agent/#network) architecture.\n",
    "\n",
    "This notebook (inspired by the paper [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation](https://arxiv.org/abs/2308.08155), by Wu, et. al.) shows one way to do this using LangGraph.\n",
    "\n",
    "This notebook is an extension of the multi-agent-collaboration notebook, showing how access to more tools - particularly with private data can enhance the ability of a data agent.\n",
    "\n",
    "We will slowly build up the agent with more tools, starting with web search, then adding document search via Cortex Search, and lastly replacing document search with a Cortex Agent that can both document search and query snowflake tables in sql via Cortex Analyst.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7b6dcc-c985-46e2-8457-7e6b0298b950",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "# pip install -U langchain_community langchain_openai langchain_experimental matplotlib langgraph pygraphviz google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c0e4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"Finance Data Agent 40\"  # set this app name for your use case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e48d5f",
   "metadata": {},
   "source": [
    "## Set keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bdc791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# need both API keys\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\"\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"...\"\n",
    "\n",
    "os.environ[\"SNOWFLAKE_ACCOUNT\"] = \"SFDEVREL_ENTERPRISE\"\n",
    "os.environ[\"SNOWFLAKE_USER\"] = \"JREINI\"\n",
    "os.environ[\"SNOWFLAKE_USER_PASSWORD\"] = \"...\"\n",
    "os.environ[\"SNOWFLAKE_DATABASE\"] = \"AGENTS_DB\"\n",
    "os.environ[\"SNOWFLAKE_SCHEMA\"] = \"NOTEBOOKS\"\n",
    "os.environ[\"SNOWFLAKE_ROLE\"] = \"CORTEX_USER_ROLE\"\n",
    "os.environ[\"SNOWFLAKE_WAREHOUSE\"] = \"CONTAINER_RUNTIME_WH\"\n",
    "os.environ[\"SNOWFLAKE_PAT\"] = \"...\"\n",
    "\n",
    "os.environ[\"TRULENS_OTEL_TRACING\"] = (\n",
    "    \"1\"  # to enable OTEL tracing -> note the Snowsight UI experience for now is limited to PuPr customers, not yet supported for OSS.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58f2f08",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e874860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from typing import List, Literal\n",
    "import uuid\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "from langchain.load.dump import dumps\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import StructuredTool\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langgraph.graph import END\n",
    "from langgraph.graph import START\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.types import Command\n",
    "from pydantic import BaseModel\n",
    "from snowflake.snowpark import Session\n",
    "from trulens.apps.app import TruApp\n",
    "from trulens.connectors.snowflake import SnowflakeConnector\n",
    "from trulens.core.otel.instrument import instrument\n",
    "from trulens.core.run import Run\n",
    "from trulens.core.run import RunConfig\n",
    "from trulens.otel.semconv.trace import BASE_SCOPE\n",
    "from trulens.otel.semconv.trace import SpanAttributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bb1f53",
   "metadata": {},
   "source": [
    "## Create TruLens/Snowflake Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3c39f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowflake account for trulens\n",
    "snowflake_connection_parameters = {\n",
    "    \"account\": os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "    \"user\": os.environ[\"SNOWFLAKE_USER\"],\n",
    "    \"password\": os.environ[\"SNOWFLAKE_USER_PASSWORD\"],\n",
    "    \"database\": os.environ[\"SNOWFLAKE_DATABASE\"],\n",
    "    \"schema\": os.environ[\"SNOWFLAKE_SCHEMA\"],\n",
    "    \"role\": os.environ[\"SNOWFLAKE_ROLE\"],\n",
    "    \"warehouse\": os.environ[\"SNOWFLAKE_WAREHOUSE\"],\n",
    "}\n",
    "snowpark_session_trulens = Session.builder.configs(\n",
    "    snowflake_connection_parameters\n",
    ").create()\n",
    "\n",
    "\n",
    "trulens_sf_connector = SnowflakeConnector(\n",
    "    snowpark_session=snowpark_session_trulens\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a283e-ea04-40c1-b792-f9e5f7d81203",
   "metadata": {},
   "source": [
    "### Define the agent with web search and charting tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cab43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolState(MessagesState):\n",
    "    selected_tools: List[str] = []\n",
    "\n",
    "\n",
    "def build_graph(search_max_results: int = 5):\n",
    "    def make_system_prompt(suffix: str) -> str:\n",
    "        return (\n",
    "            \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "            \" Use the provided tools to progress towards answering the question.\"\n",
    "            \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "            \" will help where you left off. Execute what you can to make progress.\"\n",
    "            \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "            \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "            f\"\\n{suffix}\"\n",
    "        )\n",
    "\n",
    "    search = SerpAPIWrapper()\n",
    "\n",
    "    search_tool = Tool(\n",
    "        name=\"web_search\",\n",
    "        description=\"Search the web for current information, such as weather or news\",\n",
    "        func=search.run,\n",
    "    )\n",
    "\n",
    "    # 4) Register it under a UUID and turn that into a Document\n",
    "    tool_id = str(uuid.uuid4())\n",
    "    tool_registry = {tool_id: search_tool}\n",
    "\n",
    "    tool_documents = [\n",
    "        Document(\n",
    "            page_content=search_tool.name,  # your human-readable blurb\n",
    "            id=tool_id,  # must match the registry key\n",
    "            metadata={\n",
    "                \"tool_name\": tool.name,\n",
    "                \"tool_description\": tool.description,\n",
    "            },\n",
    "        )\n",
    "        for tool_id, tool in tool_registry.items()\n",
    "    ]\n",
    "\n",
    "    # search_tool = TavilySearchResults(max_results=search_max_results)\n",
    "\n",
    "    # tool_registry = {str(uuid.uuid4()): tavily_tool}\n",
    "\n",
    "    # # Index tool descriptions in a vector store for semantic tool retrieval\n",
    "    # tool_documents = [\n",
    "    #     Document(\n",
    "    #         page_content=tavily_tool.description,\n",
    "    #         id=tid,\n",
    "    #         metadata={\"tool_name\": tavily_tool.name},\n",
    "    #     )\n",
    "    #     for tid, tavily_tool in tool_registry.items()\n",
    "    # ]\n",
    "    vector_store = InMemoryVectorStore(embedding=OpenAIEmbeddings())\n",
    "    vector_store.add_documents(tool_documents)\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"SELECT_TOOLS\",\n",
    "        attributes=lambda ret, exc, *args, **kw: {\n",
    "            # ---- state as JSON-text (OTLP needs a scalar) -----------------\n",
    "            f\"{BASE_SCOPE}.select_tools_input_state\": json.dumps(  # ← turns dict → str\n",
    "                {\n",
    "                    **{k: v for k, v in args[0].items() if k != \"messages\"},\n",
    "                    \"messages\": [\n",
    "                        {\"type\": m.__class__.__name__, \"content\": m.content}\n",
    "                        if hasattr(m, \"content\")  # BaseMessage subclasses\n",
    "                        else m  # already JSON-friendly\n",
    "                        for m in args[0].get(\"messages\", [])\n",
    "                    ],\n",
    "                }\n",
    "            ),\n",
    "            # ---- selected tool IDs as a simple comma-separated string -----\n",
    "            f\"{BASE_SCOPE}.selected_tool_ids\": \", \".join(\n",
    "                ret.get(\"selected_tools\", [])\n",
    "            )\n",
    "            if isinstance(ret, dict)\n",
    "            else \"\",\n",
    "        },\n",
    "    )\n",
    "    def select_tools(\n",
    "        state: ToolState,\n",
    "    ) -> Command[Literal[\"research_agent\", END]]:\n",
    "        print(\">>> in select_tools, incoming state keys:\", list(state.keys()))\n",
    "\n",
    "        messages = state[\"messages\"]\n",
    "        last = messages[-1]\n",
    "        query = last[\"content\"] if isinstance(last, dict) else last.content\n",
    "        print(\"selecting tools based on\", query)\n",
    "\n",
    "        # 1. pull top-k with their scores\n",
    "        results: list[tuple[Document, float]] = (\n",
    "            vector_store.similarity_search_with_score(\n",
    "                query,\n",
    "                k=5,  # look at top-5 candidates\n",
    "            )\n",
    "        )\n",
    "        print(\"tool search results\", results)\n",
    "\n",
    "        # 2. filter by minimum cosine-similarity\n",
    "        MIN_SIMILARITY = 0.7\n",
    "        filtered = [doc for doc, score in results if score >= MIN_SIMILARITY]\n",
    "\n",
    "        # 3a. no sufficiently similar tool → end\n",
    "        if not filtered:\n",
    "            print(\"no tool selected\")\n",
    "            msg = HumanMessage(\n",
    "                content=\"Sorry, I don’t have a tool that’s relevant enough to answer that.\",\n",
    "                name=\"assistant\",\n",
    "            )\n",
    "            return Command(\n",
    "                update={\"messages\": messages + [msg]},\n",
    "                goto=END,\n",
    "            )\n",
    "\n",
    "        # 3b. otherwise select those tools and move on\n",
    "        selected_ids = [doc.id for doc in filtered]\n",
    "        print(\"tools selected\", selected_ids)\n",
    "        return Command(\n",
    "            update={\n",
    "                \"selected_tools\": selected_ids,\n",
    "            },\n",
    "            goto=\"research_agent\",\n",
    "        )\n",
    "\n",
    "    # Warning: This executes code locally, which can be unsafe when not sandboxed\n",
    "\n",
    "    repl = PythonREPL()\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "    @tool\n",
    "    @instrument(\n",
    "        span_type=\"PYTHON_REPL_TOOL\",\n",
    "        attributes={\n",
    "            f\"{BASE_SCOPE}.python_tool_input_code\": \"code\",\n",
    "        },\n",
    "    )\n",
    "    def python_repl_tool(code: str):\n",
    "        \"\"\"\n",
    "        Run arbitrary Python, grab the CURRENT matplotlib figure (if any),\n",
    "        save it to ./langgraph_saved_images_snowflaketools/v1/chart_<uuid>.png,\n",
    "        and return a first-line `CHART_PATH=…`.\n",
    "        \"\"\"\n",
    "        import matplotlib\n",
    "\n",
    "        matplotlib.use(\"Agg\")  # headless safety\n",
    "        import os\n",
    "        import uuid\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        # ------------------ run user code & capture stdout ------------------\n",
    "        repl.run(code)\n",
    "\n",
    "        # ------------------ locate a figure (if generated) ------------------\n",
    "        fig = plt.gcf()\n",
    "        has_axes = bool(fig.axes)  # True if something was plotted\n",
    "\n",
    "        # ------------------ always save if we have a figure -----------------\n",
    "        chart_path = \"\"\n",
    "        if has_axes:\n",
    "            target_dir = \"./langgraph_saved_images_snowflaketools/v1\"\n",
    "            os.makedirs(target_dir, exist_ok=True)\n",
    "            chart_path = os.path.join(\n",
    "                target_dir, f\"chart_{uuid.uuid4().hex}.png\"\n",
    "            )\n",
    "            fig.savefig(chart_path, format=\"png\")\n",
    "            plt.close(fig)\n",
    "\n",
    "        # ------------------ tool result (1st line = CHART_PATH) -------------\n",
    "        return f\"CHART_PATH={chart_path if chart_path else 'NONE'}\\n\"\n",
    "\n",
    "    def get_next_node(last_message: BaseMessage, goto: str):\n",
    "        if \"FINAL ANSWER\" in last_message.content:\n",
    "            # Any agent decided the work is done\n",
    "            return END\n",
    "        return goto\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"RESEARCH_NODE\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            f\"{BASE_SCOPE}.research_node_input\": args[0][\"messages\"][\n",
    "                -1\n",
    "            ].content,\n",
    "            f\"{BASE_SCOPE}.research_node_response\": ret.update[\"messages\"][\n",
    "                -1\n",
    "            ].content\n",
    "            if hasattr(ret, \"update\")\n",
    "            else json.dumps(ret, indent=4, sort_keys=True),\n",
    "            f\"{BASE_SCOPE}.tool_messages\": [\n",
    "                dumps(message)\n",
    "                for message in ret.update[\"messages\"]\n",
    "                if isinstance(message, ToolMessage)\n",
    "            ]\n",
    "            if hasattr(ret, \"update\")\n",
    "            else \"No tool call\",\n",
    "        },\n",
    "    )\n",
    "    @instrument(\n",
    "        span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            SpanAttributes.RETRIEVAL.QUERY_TEXT: args[0][\"messages\"][\n",
    "                -1\n",
    "            ].content,\n",
    "            SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS: [\n",
    "                msg.content\n",
    "                for msg in ret.update[\"messages\"]\n",
    "                if isinstance(msg, ToolMessage) and msg.content\n",
    "            ]\n",
    "            if hasattr(ret, \"update\") and \"messages\" in ret.update\n",
    "            else [],\n",
    "        },\n",
    "    )\n",
    "    def research_agent_node(\n",
    "        state: ToolState,\n",
    "    ) -> Command[Literal[\"chart_generator\"]]:\n",
    "        \"\"\"\n",
    "        Always binds the selected tools and invokes the bound agent.\n",
    "        Stops on FINAL ANSWER or moves to chart_generator.\n",
    "        \"\"\"\n",
    "        # grab (non-empty) list of selected tool IDs\n",
    "        selected_ids = state[\"selected_tools\"]\n",
    "\n",
    "        # bind only those tools\n",
    "        selected_tools = [tool_registry[tid] for tid in selected_ids]\n",
    "        bound_llm = llm.bind_tools(selected_tools)\n",
    "        bound_agent = create_react_agent(\n",
    "            bound_llm,\n",
    "            tools=selected_tools,  # already bound\n",
    "            prompt=make_system_prompt(\n",
    "                \"You can only do research. You are working with both a chart generator and a chart summarizer colleagues.\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # run it\n",
    "        result = bound_agent.invoke(state)\n",
    "\n",
    "        # decide if we’re done\n",
    "        last = result[\"messages\"][-1]\n",
    "        goto = get_next_node(last, \"chart_generator\")\n",
    "\n",
    "        # tag the origin of the final message\n",
    "        result[\"messages\"][-1] = HumanMessage(\n",
    "            content=last.content,\n",
    "            name=\"research_agent\",\n",
    "        )\n",
    "\n",
    "        return Command(\n",
    "            update={\"messages\": result[\"messages\"]},\n",
    "            goto=goto,\n",
    "        )\n",
    "\n",
    "    # Chart generator agent and node\n",
    "    # NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION, WHICH CAN BE UNSAFE WHEN NOT SANDBOXED\n",
    "    chart_agent = create_react_agent(\n",
    "        llm,\n",
    "        [python_repl_tool],\n",
    "        prompt=make_system_prompt(\n",
    "            \"You can only generate charts. The generated chart should be save at a local directory at current directory PATH './langgraph_saved_images_snowflaketools/v1' , and this PATH should be sent to your colleague. You are working with a chart summarizer colleague.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    def extract_chart_path(text: str) -> str | None:\n",
    "        \"\"\"\n",
    "        Returns the first CHART_PATH=… found in `text`, else None.\n",
    "        \"\"\"\n",
    "        m = re.search(r\"^CHART_PATH=(.+)$\", text, flags=re.MULTILINE)\n",
    "        return m.group(1).strip() if m else None\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"CHART_GENERATOR_NODE\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            f\"{BASE_SCOPE}.chart_node_input\": args[0][\"messages\"][-1].content,\n",
    "            f\"{BASE_SCOPE}.chart_node_response\": (\n",
    "                ret.update[\"messages\"][-1].content\n",
    "                if ret and hasattr(ret, \"update\") and ret.update\n",
    "                else \"No update response\"\n",
    "            ),\n",
    "        },\n",
    "    )\n",
    "    def chart_node(\n",
    "        state: ToolState,\n",
    "    ) -> Command[Literal[\"chart_summarizer\"]]:\n",
    "        \"\"\"\n",
    "        This function represents the chart generation node in the workflow.\n",
    "        It invokes the chart generation agent to create a chart based on the provided state.\n",
    "        The generated chart is saved to a specified directory, and its path is extracted from the tool messages.\n",
    "        A summary prompt is then prepared to send to the chart summarizer agent.\n",
    "        If the chart path is not found, the workflow ends; otherwise, it transitions to the chart summarizer node.\n",
    "        \"\"\"\n",
    "        print(\"generating chart\")\n",
    "        # 1. let the agent run\n",
    "        result = chart_agent.invoke(state)\n",
    "\n",
    "        # 2. try to grab a path from any ToolMessage\n",
    "        chart_path = None\n",
    "        for msg in result[\"messages\"]:\n",
    "            if isinstance(msg, ToolMessage):\n",
    "                chart_path = extract_chart_path(msg.content)\n",
    "                if chart_path and chart_path.upper() != \"NONE\":\n",
    "                    break\n",
    "\n",
    "        print(f\"Chart Path: {chart_path!r}\")\n",
    "\n",
    "        # Count how many times we've already said “Failed to generate chart”\n",
    "        failures = sum(\n",
    "            1\n",
    "            for msg in state[\"messages\"]\n",
    "            if isinstance(msg, HumanMessage)\n",
    "            and msg.name == \"chart_generator\"\n",
    "            and msg.content.startswith(\"Failed to generate chart\")\n",
    "        )\n",
    "        attempt = failures + 1\n",
    "\n",
    "        if not chart_path or chart_path.upper() == \"NONE\":\n",
    "            # If this is the 3rd failure, give up\n",
    "            if attempt >= 3:\n",
    "                result[\"messages\"].append(\n",
    "                    HumanMessage(\n",
    "                        content=\"Sorry, I couldn’t generate a chart after multiple tries. Let’s move on.\",\n",
    "                        name=\"chart_generator\",\n",
    "                    )\n",
    "                )\n",
    "                return Command(\n",
    "                    update={\"messages\": result[\"messages\"]}, goto=END\n",
    "                )\n",
    "\n",
    "            # Otherwise retry via the research agent\n",
    "            result[\"messages\"].append(\n",
    "                HumanMessage(\n",
    "                    content=f\"Failed to generate chart. Retrying… (attempt {attempt}/3)\",\n",
    "                    name=\"chart_generator\",\n",
    "                )\n",
    "            )\n",
    "            return Command(\n",
    "                update={\"messages\": result[\"messages\"]}, goto=\"research_agent\"\n",
    "            )\n",
    "\n",
    "        # 4. Add chart path to messages for downstream nodes\n",
    "        result[\"messages\"].append(\n",
    "            HumanMessage(\n",
    "                content=f\"CHART_PATH={chart_path}\", name=\"chart_generator\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # 5. Add summary prompt\n",
    "        summary_prompt = \"Please summarise the chart in ≤ 3 sentences.\"\n",
    "        result[\"messages\"].append(\n",
    "            HumanMessage(name=\"chart_generator\", content=summary_prompt)\n",
    "        )\n",
    "\n",
    "        return Command(\n",
    "            update={\"messages\": result[\"messages\"]}, goto=\"chart_summarizer\"\n",
    "        )\n",
    "\n",
    "    # Build the image captioning agent.\n",
    "    # If you have any specific image processing tools (e.g., for extracting chart images),\n",
    "    # you can add them in the tools list. For now, we leave it empty.\n",
    "    chart_summary_agent = create_react_agent(\n",
    "        llm,\n",
    "        tools=[],  # Add image processing tools if available/needed.\n",
    "        prompt=make_system_prompt(\n",
    "            \"\"\"You can only generate charts with Python.\n",
    "        ALWAYS:\n",
    "        1. Save the figure as PNG to './langgraph_saved_images_snowflaketools/v1'.\n",
    "        2. Do NOT display the image inline.\n",
    "        3. End your reply with `CHART_PATH=<absolute-or-relative-path>`.\n",
    "        You are working with a summariser colleague.\"\"\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    reflection_prompt_template = PromptTemplate(\n",
    "        input_variables=[\"user_query\", \"chart_summary\"],\n",
    "        template=\"\"\"\\\n",
    "        You are an AI assistant tasked with reflecting on the quality of a chart summary. The user has asked the following question:\n",
    "        \"{user_query}\"\n",
    "\n",
    "        You are given the following chart summary:\n",
    "        \"{chart_summary}\"\n",
    "\n",
    "        Your task is to evaluate how well the chart summary answers the user's question. Consider the following:\n",
    "        - Does the summary capture the **key insights** and trends from the chart, even if in a more general form?\n",
    "        - Does it provide **adequate context** to address the user's query, even if it's not exhaustive?\n",
    "        - If the summary provides some context but could benefit from more details, consider it sufficient for now unless significant details are missing.\n",
    "\n",
    "        If the summary **generally** addresses the question, respond with 'Task complete'. If the summary **lacks significant** details or clarity, then respond with specific details on how the answer should be improved and what information is needed. Avoid being overly critical unless the summary completely misses key elements necessary to answer the query.\n",
    "\n",
    "        Please provide your answer in a **concise and encouraging** manner.\n",
    "        \"\"\",\n",
    "    )\n",
    "\n",
    "    # Create the chain using the prompt template and the LLM (ChatOpenAI)\n",
    "    reflection_chain = reflection_prompt_template | llm\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"CHART_SUMMARY_REFLECTION\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            f\"{BASE_SCOPE}.chart_summary_reflection_input_user_query\": args[0],\n",
    "            f\"{BASE_SCOPE}.chart_summary_reflection_input_chart_summary\": args[\n",
    "                1\n",
    "            ],\n",
    "            f\"{BASE_SCOPE}.chart_summary_reflection_response\": ret,\n",
    "        },\n",
    "    )\n",
    "    def perform_reflection(user_query: str, chart_summary: str) -> str:\n",
    "        \"\"\"\n",
    "        This function uses an LLM to reflect on the quality of a chart summary\n",
    "        and determine if the task is complete or requires further refinement.\n",
    "        \"\"\"\n",
    "        print(\"doing reflection\")\n",
    "        # Call the chain with the user query and chart summary\n",
    "        reflection_result = reflection_chain.invoke({\n",
    "            \"user_query\": user_query,\n",
    "            \"chart_summary\": chart_summary,\n",
    "        })\n",
    "        return reflection_result.content\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"CHART_SUMMARY_NODE\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            f\"{BASE_SCOPE}.summary_node_input\": args[0][\"messages\"][-1].content,\n",
    "            f\"{BASE_SCOPE}.summary_node_output\": (\n",
    "                ret.update[\"messages\"][-1].content\n",
    "                if hasattr(ret, \"update\")\n",
    "                else \"NO SUMMARY GENERATED\"\n",
    "            ),\n",
    "        },\n",
    "    )\n",
    "    def chart_summary_node(state: ToolState) -> Command[Literal[END]]:\n",
    "        \"\"\"\n",
    "        This function represents the chart summarizer node in the workflow graph.\n",
    "        It uses the chart summary agent to generate a concise summary for the chart image\n",
    "        provided by the chart generator node. The summary is limited to three sentences\n",
    "        and is based on the chart image saved at the specified local path.\n",
    "        \"\"\"\n",
    "        print(\"calling chart summary\")\n",
    "        # 1. Extract chart path from messages\n",
    "        chart_path = None\n",
    "        for msg in state[\"messages\"]:\n",
    "            if isinstance(msg, ToolMessage) and \"CHART_PATH=\" in msg.content:\n",
    "                chart_path = extract_chart_path(msg.content)\n",
    "                if chart_path and chart_path.upper() != \"NONE\":\n",
    "                    break\n",
    "\n",
    "        print(f\"Chart Path in Chart Summary Node: {chart_path!r}\")\n",
    "\n",
    "        # 2. If no valid chart path, return to researcher with error message\n",
    "        if not chart_path or chart_path.upper() == \"NONE\":\n",
    "            return Command(\n",
    "                update={\n",
    "                    \"messages\": state[\"messages\"]\n",
    "                    + [\n",
    "                        HumanMessage(\n",
    "                            content=\"No valid chart was generated. Please try again.\",\n",
    "                            name=\"chart_summarizer\",\n",
    "                        )\n",
    "                    ]\n",
    "                },\n",
    "                goto=\"select_tools\",\n",
    "            )\n",
    "\n",
    "        # 3. Run the summarizer\n",
    "        result = chart_summary_agent.invoke(state)\n",
    "        if not result or \"messages\" not in result:\n",
    "            return Command(\n",
    "                update={\n",
    "                    \"messages\": state[\"messages\"]\n",
    "                    + [\n",
    "                        HumanMessage(\n",
    "                            content=\"Failed to generate chart summary. Please try again.\",\n",
    "                            name=\"chart_summarizer\",\n",
    "                        )\n",
    "                    ]\n",
    "                },\n",
    "                goto=\"select_tools\",\n",
    "            )\n",
    "\n",
    "        # 4. Add reflection\n",
    "        user_query = state[\"messages\"][-2].content\n",
    "        chart_summary = result[\"messages\"][-1].content\n",
    "        print(chart_summary)\n",
    "        reflection = perform_reflection(user_query, chart_summary)\n",
    "        reflection_clean = reflection.strip().rstrip(\".\").lower()\n",
    "        print(f\"REFLECTION raw repr: {reflection!r}\")\n",
    "        print(f\"REFLECTION clean repr: {reflection_clean!r}\")\n",
    "\n",
    "        # 5. Determine next node\n",
    "        if \"task complete\" in reflection_clean:\n",
    "            goto = END\n",
    "        else:\n",
    "            goto = \"select_tools\"\n",
    "\n",
    "        print(goto)\n",
    "\n",
    "        result[\"messages\"][-1] = HumanMessage(\n",
    "            name=\"chart_summarizer\", content=chart_summary\n",
    "        )\n",
    "\n",
    "        return Command(update={\"messages\": result[\"messages\"]}, goto=goto)\n",
    "\n",
    "    workflow = StateGraph(ToolState)\n",
    "    workflow.add_node(\"select_tools\", select_tools)\n",
    "    workflow.add_node(\"research_agent\", research_agent_node)\n",
    "    workflow.add_node(\"chart_generator\", chart_node)\n",
    "    workflow.add_node(\"chart_summarizer\", chart_summary_node)\n",
    "\n",
    "    # Update transitions: begin with tool selection then go to research agent.\n",
    "    workflow.add_edge(START, \"select_tools\")\n",
    "    # workflow.add_edge(\"select_tools\", END)\n",
    "    workflow.add_edge(\"select_tools\", \"research_agent\")\n",
    "    workflow.add_edge(\"research_agent\", \"chart_generator\")\n",
    "    workflow.add_edge(\"chart_generator\", \"chart_summarizer\")\n",
    "    workflow.add_edge(\"chart_summarizer\", END)\n",
    "    compiled_graph = workflow.compile()\n",
    "\n",
    "    return compiled_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca89763c",
   "metadata": {},
   "source": [
    "## Register the agent and create a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58ffc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TruAgent:\n",
    "    def __init__(self):\n",
    "        self.graph = build_graph()\n",
    "\n",
    "    @instrument(\n",
    "        span_type=SpanAttributes.SpanType.RECORD_ROOT,\n",
    "        attributes={\n",
    "            SpanAttributes.RECORD_ROOT.INPUT: \"query\",\n",
    "            SpanAttributes.RECORD_ROOT.OUTPUT: \"return\",\n",
    "        },\n",
    "    )\n",
    "    def invoke_agent_graph(self, query: str) -> str:\n",
    "        # Initialize state with proper message format\n",
    "        state = {\"messages\": [HumanMessage(content=query)]}\n",
    "\n",
    "        # Stream events with recursion limit\n",
    "        events = self.graph.stream(\n",
    "            state,\n",
    "            {\"recursion_limit\": 30},\n",
    "        )\n",
    "\n",
    "        # Track all messages through the conversation\n",
    "        all_messages = []\n",
    "        for event in events:\n",
    "            # Get the payload from the event\n",
    "            _, payload = next(iter(event.items()))\n",
    "            if not payload:  # Skip empty payloads\n",
    "                continue\n",
    "\n",
    "            messages = payload.get(\"messages\")\n",
    "            if not messages:\n",
    "                continue\n",
    "            all_messages.extend(messages)\n",
    "\n",
    "        # Return the last message's content if available\n",
    "        return (\n",
    "            all_messages[-1].content\n",
    "            if all_messages and hasattr(all_messages[-1], \"content\")\n",
    "            else \"\"\n",
    "        )\n",
    "\n",
    "\n",
    "tru_agent = TruAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b361154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_agent_app = TruApp(\n",
    "    tru_agent,\n",
    "    app_name=APP_NAME,\n",
    "    app_version=\"web search\",\n",
    "    connector=trulens_sf_connector,\n",
    "    main_method=tru_agent.invoke_agent_graph,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe95b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_1 = datetime.datetime.fromtimestamp(time.time()).strftime(\n",
    "    \"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    run_name=\"Multi-agent demo run - small\",\n",
    "    description=\"this is a run with access to web search and charting capabilities\",\n",
    "    dataset_name=\"Research test dataset\",\n",
    "    source_type=\"DATAFRAME\",\n",
    "    label=\"langgraph demo\",\n",
    "    dataset_spec={\n",
    "        \"RECORD_ROOT.INPUT\": \"query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "run: Run = tru_agent_app.add_run(run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156adb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tru_app = TruApp(tru_agent, app_name = APP_NAME, app_version = \"web search\", connector = trulens_sf_connector, main_method = tru_agent.invoke_agent_graph)\n",
    "# tru_app.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed35594",
   "metadata": {},
   "source": [
    "## Display the agent's graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c325b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(tru_agent.graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad18dba0",
   "metadata": {},
   "source": [
    "## Start the run\n",
    "\n",
    "This runs the agent in batch using the queries in the `input_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba1cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "user_queries = [\n",
    "    \"What has been the temperature in London so far in 2025? Create a line chart that best illustrates this data?\",\n",
    "    \"In 2023, how did the fed funds rate fluctuate? What were the key drivers? Create a line chart that best illustrates this data, including a caption with the key drivers.\",\n",
    "    \"What is the trend of Cash and Cash Equivalents values across the last 4 quarters? Create a line chart that best illustrates this data\",\n",
    "]\n",
    "\n",
    "user_queries_df = pd.DataFrame(user_queries, columns=[\"query\"])\n",
    "\n",
    "run.start(input_df=user_queries_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1a8c8f",
   "metadata": {},
   "source": [
    "## Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3d1614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while run.get_status() == \"INVOCATION_IN_PROGRESS\":\n",
    "    time.sleep(3)\n",
    "\n",
    "run.compute_metrics([\"groundedness\", \"context_relevance\", \"answer_relevance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e580b028",
   "metadata": {},
   "source": [
    "Web is not as precise as it could be if it had access to private minutes data. Let's supplement web search with a document search.\n",
    "\n",
    "## Add Cortex Search to the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765e30a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.core import Root\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "\n",
    "class CortexSearchArgs(BaseModel):\n",
    "    query: str\n",
    "\n",
    "\n",
    "# --- Define a new Cortex Search Tool to perform document search via Cortex ---\n",
    "class CortexSearchTool(StructuredTool):\n",
    "    name: str = \"CortexSearch\"\n",
    "    description: str = \"Searches documents using Cortex Search via Snowflake.\"\n",
    "    args_schema: type[BaseModel] = CortexSearchArgs\n",
    "    session: Session\n",
    "\n",
    "    def run(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Executes a search query using the Cortex Search service in Snowflake.\n",
    "\n",
    "        Args:\n",
    "            query (str): The search query string.\n",
    "\n",
    "        Returns:\n",
    "            str: A JSON string containing the search results, limited to 10 entries.\n",
    "        \"\"\"\n",
    "        root = Root(self.session)\n",
    "        search_service = (\n",
    "            root.databases[\"CORTEX_SEARCH_TUTORIAL_DB\"]\n",
    "            .schemas[\"PUBLIC\"]\n",
    "            .cortex_search_services[\"FOMC_SEARCH_SERVICE\"]\n",
    "        )\n",
    "        resp = search_service.search(query=query, columns=[\"chunk\"], limit=10)\n",
    "        return resp.to_json()\n",
    "\n",
    "\n",
    "def build_graph_with_search(search_max_results: int = 5):\n",
    "    def make_system_prompt(suffix: str) -> str:\n",
    "        return (\n",
    "            \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "            \" Use the provided tools to progress towards answering the question.\"\n",
    "            \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "            \" will help where you left off. Execute what you can to make progress.\"\n",
    "            \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "            \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "            f\"\\n{suffix}\"\n",
    "        )\n",
    "\n",
    "    search = SerpAPIWrapper()\n",
    "\n",
    "    # search_tool = TavilySearchResults(max_results=search_max_results)\n",
    "\n",
    "    search_tool = Tool(\n",
    "        name=\"web_search\",\n",
    "        description=\"Search the web for current information, such as weather or news\",\n",
    "        func=search.run,\n",
    "    )\n",
    "\n",
    "    # # Create document search tool using Cortex Search (uses your Snowflake session)\n",
    "    cortex_search_tool = CortexSearchTool(session=snowpark_session_trulens)\n",
    "\n",
    "    # wrap so sync-compatible\n",
    "    wrapped_cortex_search_tool = Tool(\n",
    "        name=cortex_search_tool.name,\n",
    "        description=cortex_search_tool.description,\n",
    "        func=cortex_search_tool.run,\n",
    "        return_direct=False,  # set to True only if you want the agent to stop after using it\n",
    "    )\n",
    "    # The tool registry now includes both the web and document search tools.\n",
    "    tool_registry = {\n",
    "        str(uuid.uuid4()): search_tool,\n",
    "        str(uuid.uuid4()): wrapped_cortex_search_tool,\n",
    "    }\n",
    "\n",
    "    # Index tool descriptions in a vector store for semantic tool retrieval\n",
    "    tool_documents = [\n",
    "        Document(\n",
    "            page_content=search_tool.name,  # your human-readable blurb\n",
    "            id=tool_id,  # must match the registry key\n",
    "            metadata={\n",
    "                \"tool_name\": tool.name,\n",
    "                \"tool_description\": tool.description,\n",
    "            },\n",
    "        )\n",
    "        for tool_id, tool in tool_registry.items()\n",
    "    ]\n",
    "    vector_store = InMemoryVectorStore(embedding=OpenAIEmbeddings())\n",
    "    vector_store.add_documents(tool_documents)\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"SELECT_TOOLS\",\n",
    "        attributes=lambda ret, exc, *args, **kw: {\n",
    "            # ---- state as JSON-text (OTLP needs a scalar) -----------------\n",
    "            f\"{BASE_SCOPE}.select_tools_input_state\": json.dumps(  # ← turns dict → str\n",
    "                {\n",
    "                    **{k: v for k, v in args[0].items() if k != \"messages\"},\n",
    "                    \"messages\": [\n",
    "                        {\"type\": m.__class__.__name__, \"content\": m.content}\n",
    "                        if hasattr(m, \"content\")  # BaseMessage subclasses\n",
    "                        else m  # already JSON-friendly\n",
    "                        for m in args[0].get(\"messages\", [])\n",
    "                    ],\n",
    "                }\n",
    "            ),\n",
    "            # ---- selected tool IDs as a simple comma-separated string -----\n",
    "            f\"{BASE_SCOPE}.selected_tool_ids\": \", \".join(\n",
    "                ret.get(\"selected_tools\", [])\n",
    "            )\n",
    "            if isinstance(ret, dict)\n",
    "            else \"\",\n",
    "        },\n",
    "    )\n",
    "    def select_tools(\n",
    "        state: ToolState,\n",
    "    ) -> Command[Literal[\"research_agent\", END]]:\n",
    "        messages = state[\"messages\"]\n",
    "        last = messages[-1]\n",
    "        query = last[\"content\"] if isinstance(last, dict) else last.content\n",
    "        print(\"selecting tools based on\", query)\n",
    "\n",
    "        # 1. pull top-k with their scores\n",
    "        results: list[tuple[Document, float]] = (\n",
    "            vector_store.similarity_search_with_score(\n",
    "                query,\n",
    "                k=5,  # look at top-5 candidates\n",
    "            )\n",
    "        )\n",
    "        print(\"tool search results\", results)\n",
    "\n",
    "        # 2. filter by minimum cosine-similarity\n",
    "        MIN_SIMILARITY = 0.7\n",
    "        filtered = [doc for doc, score in results if score >= MIN_SIMILARITY]\n",
    "\n",
    "        # 3a. no sufficiently similar tool → end\n",
    "        if not filtered:\n",
    "            print(\"no tool selected\")\n",
    "            msg = HumanMessage(\n",
    "                content=\"Sorry, I don’t have a tool that’s relevant enough to answer that.\",\n",
    "                name=\"assistant\",\n",
    "            )\n",
    "            return Command(\n",
    "                update={\"messages\": messages + [msg]},\n",
    "                goto=END,\n",
    "            )\n",
    "\n",
    "        # 3b. otherwise select those tools and move on\n",
    "        selected_ids = [doc.id for doc in filtered]\n",
    "        print(\"tools selected\", selected_ids)\n",
    "        return Command(\n",
    "            update={\n",
    "                \"selected_tools\": selected_ids,\n",
    "            },\n",
    "            goto=\"research_agent\",\n",
    "        )\n",
    "\n",
    "    # Warning: This executes code locally, which can be unsafe when not sandboxed\n",
    "\n",
    "    repl = PythonREPL()\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "    @tool\n",
    "    @instrument(\n",
    "        span_type=\"PYTHON_REPL_TOOL\",\n",
    "        attributes={\n",
    "            f\"{BASE_SCOPE}.python_tool_input_code\": \"code\",\n",
    "        },\n",
    "    )\n",
    "    def python_repl_tool(code: str):\n",
    "        \"\"\"\n",
    "        Run arbitrary Python, grab the CURRENT matplotlib figure (if any),\n",
    "        save it to ./langgraph_saved_images_snowflaketools/v2/chart_<uuid>.png,\n",
    "        and return a first-line `CHART_PATH=…`.\n",
    "        \"\"\"\n",
    "        import matplotlib\n",
    "\n",
    "        matplotlib.use(\"Agg\")  # headless safety\n",
    "        import os\n",
    "        import uuid\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        # ------------------ run user code & capture stdout ------------------\n",
    "        repl.run(code)\n",
    "\n",
    "        # ------------------ locate a figure (if generated) ------------------\n",
    "        fig = plt.gcf()\n",
    "        has_axes = bool(fig.axes)  # True if something was plotted\n",
    "\n",
    "        # ------------------ always save if we have a figure -----------------\n",
    "        chart_path = \"\"\n",
    "        if has_axes:\n",
    "            target_dir = \"./langgraph_saved_images_snowflaketools/v2\"\n",
    "            os.makedirs(target_dir, exist_ok=True)\n",
    "            chart_path = os.path.join(\n",
    "                target_dir, f\"chart_{uuid.uuid4().hex}.png\"\n",
    "            )\n",
    "            fig.savefig(chart_path, format=\"png\")\n",
    "            plt.close(fig)\n",
    "\n",
    "        # ------------------ tool result (1st line = CHART_PATH) -------------\n",
    "        return f\"CHART_PATH={chart_path if chart_path else 'NONE'}\\n\"\n",
    "\n",
    "    def get_next_node(last_message: BaseMessage, goto: str):\n",
    "        if \"FINAL ANSWER\" in last_message.content:\n",
    "            # Any agent decided the work is done\n",
    "            return END\n",
    "        return goto\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"RESEARCH_NODE\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            f\"{BASE_SCOPE}.research_node_input\": args[0][\"messages\"][\n",
    "                -1\n",
    "            ].content,\n",
    "            f\"{BASE_SCOPE}.research_node_response\": ret.update[\"messages\"][\n",
    "                -1\n",
    "            ].content\n",
    "            if hasattr(ret, \"update\")\n",
    "            else json.dumps(ret, indent=4, sort_keys=True),\n",
    "            f\"{BASE_SCOPE}.tool_messages\": [\n",
    "                dumps(message)\n",
    "                for message in ret.update[\"messages\"]\n",
    "                if isinstance(message, ToolMessage)\n",
    "            ]\n",
    "            if hasattr(ret, \"update\")\n",
    "            else \"No tool call\",\n",
    "        },\n",
    "    )\n",
    "    @instrument(\n",
    "        span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            SpanAttributes.RETRIEVAL.QUERY_TEXT: args[0][\"messages\"][\n",
    "                -1\n",
    "            ].content,\n",
    "            SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS: [\n",
    "                ret.update[\"messages\"][-1].content\n",
    "            ]\n",
    "            if hasattr(ret, \"update\")\n",
    "            else [json.dumps(ret, indent=4, sort_keys=True)],\n",
    "        },\n",
    "    )\n",
    "    def research_agent_node(\n",
    "        state: ToolState,\n",
    "    ) -> Command[Literal[\"chart_generator\"]]:\n",
    "        \"\"\"\n",
    "        Always binds the selected tools and invokes the bound agent.\n",
    "        Stops on FINAL ANSWER or moves to chart_generator.\n",
    "        \"\"\"\n",
    "        # grab (non-empty) list of selected tool IDs\n",
    "        selected_ids = state[\"selected_tools\"]\n",
    "\n",
    "        # bind only those tools\n",
    "        selected_tools = [tool_registry[tid] for tid in selected_ids]\n",
    "        bound_llm = llm.bind_tools(selected_tools)\n",
    "        bound_agent = create_react_agent(\n",
    "            bound_llm,\n",
    "            tools=selected_tools,  # already bound\n",
    "            prompt=make_system_prompt(\n",
    "                \"You can only do research. You are working with both a chart generator and a chart summarizer colleagues.\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # run it\n",
    "        result = bound_agent.invoke(state)\n",
    "\n",
    "        # decide if we’re done\n",
    "        last = result[\"messages\"][-1]\n",
    "        goto = get_next_node(last, \"chart_generator\")\n",
    "\n",
    "        # tag the origin of the final message\n",
    "        result[\"messages\"][-1] = HumanMessage(\n",
    "            content=last.content,\n",
    "            name=\"research_agent\",\n",
    "        )\n",
    "\n",
    "        return Command(\n",
    "            update={\"messages\": result[\"messages\"]},\n",
    "            goto=goto,\n",
    "        )\n",
    "\n",
    "    # Chart generator agent and node\n",
    "    # NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION, WHICH CAN BE UNSAFE WHEN NOT SANDBOXED\n",
    "    chart_agent = create_react_agent(\n",
    "        llm,\n",
    "        [python_repl_tool],\n",
    "        prompt=make_system_prompt(\n",
    "            \"You can only generate charts. The generated chart should be save at a local directory at current directory PATH './langgraph_saved_images_snowflaketools/v2' , and this PATH should be sent to your colleague. You are working with a chart summarizer colleague.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    def extract_chart_path(text: str) -> str | None:\n",
    "        \"\"\"\n",
    "        Returns the first CHART_PATH=… found in `text`, else None.\n",
    "        \"\"\"\n",
    "        m = re.search(r\"^CHART_PATH=(.+)$\", text, flags=re.MULTILINE)\n",
    "        return m.group(1).strip() if m else None\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"CHART_GENERATOR_NODE\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            f\"{BASE_SCOPE}.chart_node_input\": args[0][\"messages\"][-1].content,\n",
    "            f\"{BASE_SCOPE}.chart_node_response\": (\n",
    "                ret.update[\"messages\"][-1].content\n",
    "                if ret and hasattr(ret, \"update\") and ret.update\n",
    "                else \"No update response\"\n",
    "            ),\n",
    "        },\n",
    "    )\n",
    "    def chart_node(\n",
    "        state: MessagesState,\n",
    "    ) -> Command[Literal[\"chart_summarizer\"]]:\n",
    "        \"\"\"\n",
    "        This function represents the chart generation node in the workflow.\n",
    "        It invokes the chart generation agent to create a chart based on the provided state.\n",
    "        The generated chart is saved to a specified directory, and its path is extracted from the tool messages.\n",
    "        A summary prompt is then prepared to send to the chart summarizer agent.\n",
    "        If the chart path is not found, the workflow ends; otherwise, it transitions to the chart summarizer node.\n",
    "        \"\"\"\n",
    "        # 1. let the agent run\n",
    "        result = chart_agent.invoke(state)\n",
    "\n",
    "        # 2. try to grab a path from any ToolMessage\n",
    "        chart_path = None\n",
    "        for msg in result[\"messages\"]:\n",
    "            if isinstance(msg, ToolMessage):\n",
    "                chart_path = extract_chart_path(msg.content)\n",
    "                if chart_path and chart_path.upper() != \"NONE\":\n",
    "                    break\n",
    "\n",
    "        print(f\"Chart Path: {chart_path!r}\")\n",
    "\n",
    "        # 3. If no valid chart path, return to researcher with error message\n",
    "        if not chart_path or chart_path.upper() == \"NONE\":\n",
    "            result[\"messages\"].append(\n",
    "                HumanMessage(\n",
    "                    content=\"Failed to generate chart. Please try again with different parameters.\",\n",
    "                    name=\"chart_generator\",\n",
    "                )\n",
    "            )\n",
    "            return Command(\n",
    "                update={\"messages\": result[\"messages\"]}, goto=\"research_agent\"\n",
    "            )\n",
    "\n",
    "        # 4. Add chart path to messages for downstream nodes\n",
    "        result[\"messages\"].append(\n",
    "            HumanMessage(\n",
    "                content=f\"CHART_PATH={chart_path}\", name=\"chart_generator\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # 5. Add summary prompt\n",
    "        summary_prompt = \"Please summarise the chart in ≤ 3 sentences.\"\n",
    "        result[\"messages\"].append(\n",
    "            HumanMessage(name=\"chart_generator\", content=summary_prompt)\n",
    "        )\n",
    "\n",
    "        return Command(\n",
    "            update={\"messages\": result[\"messages\"]}, goto=\"chart_summarizer\"\n",
    "        )\n",
    "\n",
    "    # Build the image captioning agent.\n",
    "    # If you have any specific image processing tools (e.g., for extracting chart images),\n",
    "    # you can add them in the tools list. For now, we leave it empty.\n",
    "    chart_summary_agent = create_react_agent(\n",
    "        llm,\n",
    "        tools=[],  # Add image processing tools if available/needed.\n",
    "        prompt=make_system_prompt(\n",
    "            \"\"\"You can only generate charts with Python.\n",
    "        ALWAYS:\n",
    "        1. Save the figure as PNG to './langgraph_saved_images_snowflaketools/v2'.\n",
    "        2. Do NOT display the image inline.\n",
    "        3. End your reply with `CHART_PATH=<absolute-or-relative-path>`.\n",
    "        You are working with a summariser colleague.\"\"\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    reflection_prompt_template = PromptTemplate(\n",
    "        input_variables=[\"user_query\", \"chart_summary\"],\n",
    "        template=\"\"\"\\\n",
    "        You are an AI assistant tasked with reflecting on the quality of a chart summary. The user has asked the following question:\n",
    "        \"{user_query}\"\n",
    "\n",
    "        You are given the following chart summary:\n",
    "        \"{chart_summary}\"\n",
    "\n",
    "        Your task is to evaluate how well the chart summary answers the user's question. Consider the following:\n",
    "        - Does the summary capture the **key insights** and trends from the chart, even if in a more general form?\n",
    "        - Does it provide **adequate context** to address the user's query, even if it's not exhaustive?\n",
    "        - If the summary provides some context but could benefit from more details, consider it sufficient for now unless significant details are missing.\n",
    "\n",
    "        If the summary **generally** addresses the question, respond with 'Task complete'. If the summary **lacks significant** details or clarity, then respond with specific details on how the answer should be improved and what information is needed. Avoid being overly critical unless the summary completely misses key elements necessary to answer the query.\n",
    "\n",
    "        Please provide your answer in a **concise and encouraging** manner.\n",
    "        \"\"\",\n",
    "    )\n",
    "\n",
    "    # Create the chain using the prompt template and the LLM (ChatOpenAI)\n",
    "    reflection_chain = reflection_prompt_template | llm\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"CHART_SUMMARY_REFLECTION\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            f\"{BASE_SCOPE}.chart_summary_reflection_input_user_query\": args[0],\n",
    "            f\"{BASE_SCOPE}.chart_summary_reflection_input_chart_summary\": args[\n",
    "                1\n",
    "            ],\n",
    "            f\"{BASE_SCOPE}.chart_summary_reflection_response\": ret,\n",
    "        },\n",
    "    )\n",
    "    def perform_reflection(user_query: str, chart_summary: str) -> str:\n",
    "        \"\"\"\n",
    "        This function uses an LLM to reflect on the quality of a chart summary\n",
    "        and determine if the task is complete or requires further refinement.\n",
    "        \"\"\"\n",
    "        # Call the chain with the user query and chart summary\n",
    "        reflection_result = reflection_chain.invoke({\n",
    "            \"user_query\": user_query,\n",
    "            \"chart_summary\": chart_summary,\n",
    "        })\n",
    "        return reflection_result.content\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"CHART_SUMMARY_NODE\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            f\"{BASE_SCOPE}.summary_node_input\": args[0][\"messages\"][-1].content,\n",
    "            f\"{BASE_SCOPE}.summary_node_output\": (\n",
    "                ret.update[\"messages\"][-1].content\n",
    "                if hasattr(ret, \"update\")\n",
    "                else \"NO SUMMARY GENERATED\"\n",
    "            ),\n",
    "        },\n",
    "    )\n",
    "    def chart_summary_node(state: MessagesState) -> Command[Literal[END]]:\n",
    "        \"\"\"\n",
    "        This function represents the chart summarizer node in the workflow graph.\n",
    "        It uses the chart summary agent to generate a concise summary for the chart image\n",
    "        provided by the chart generator node. The summary is limited to three sentences\n",
    "        and is based on the chart image saved at the specified local path.\n",
    "        \"\"\"\n",
    "        # 1. Extract chart path from messages\n",
    "        chart_path = None\n",
    "        for msg in state[\"messages\"]:\n",
    "            if isinstance(msg, ToolMessage) and \"CHART_PATH=\" in msg.content:\n",
    "                chart_path = extract_chart_path(msg.content)\n",
    "                if chart_path and chart_path.upper() != \"NONE\":\n",
    "                    break\n",
    "\n",
    "        print(f\"Chart Path in Chart Summary Node: {chart_path!r}\")\n",
    "\n",
    "        # 2. If no valid chart path, return to researcher with error message\n",
    "        if not chart_path or chart_path.upper() == \"NONE\":\n",
    "            return Command(\n",
    "                update={\n",
    "                    \"messages\": state[\"messages\"]\n",
    "                    + [\n",
    "                        HumanMessage(\n",
    "                            content=\"No valid chart was generated. Please try again.\",\n",
    "                            name=\"chart_summarizer\",\n",
    "                        )\n",
    "                    ]\n",
    "                },\n",
    "                goto=\"research_agent\",\n",
    "            )\n",
    "\n",
    "        # 3. Run the summarizer\n",
    "        result = chart_summary_agent.invoke(state)\n",
    "        if not result or \"messages\" not in result:\n",
    "            return Command(\n",
    "                update={\n",
    "                    \"messages\": state[\"messages\"]\n",
    "                    + [\n",
    "                        HumanMessage(\n",
    "                            content=\"Failed to generate chart summary. Please try again.\",\n",
    "                            name=\"chart_summarizer\",\n",
    "                        )\n",
    "                    ]\n",
    "                },\n",
    "                goto=\"research_agent\",\n",
    "            )\n",
    "\n",
    "        # 4. Add reflection\n",
    "        user_query = state[\"messages\"][-2].content\n",
    "        chart_summary = result[\"messages\"][-1].content\n",
    "        reflection = perform_reflection(user_query, chart_summary)\n",
    "\n",
    "        # 5. Determine next node\n",
    "        goto = (\n",
    "            END\n",
    "            if \"Task complete\" in reflection or \"FINAL ANSWER\" in reflection\n",
    "            else \"research_agent\"\n",
    "        )\n",
    "        result[\"messages\"][-1] = HumanMessage(\n",
    "            name=\"chart_summarizer\", content=chart_summary\n",
    "        )\n",
    "\n",
    "        return Command(update={\"messages\": result[\"messages\"]}, goto=goto)\n",
    "\n",
    "    workflow = StateGraph(ToolState)\n",
    "    workflow.add_node(\"select_tools\", select_tools)\n",
    "    workflow.add_node(\"research_agent\", research_agent_node)\n",
    "    workflow.add_node(\"chart_generator\", chart_node)\n",
    "    workflow.add_node(\"chart_summarizer\", chart_summary_node)\n",
    "\n",
    "    # Update transitions: begin with tool selection then go to research agent.\n",
    "    workflow.add_edge(START, \"select_tools\")\n",
    "    # workflow.add_edge(\"select_tools\", END)\n",
    "    workflow.add_edge(\"select_tools\", \"research_agent\")\n",
    "    workflow.add_edge(\"research_agent\", \"chart_generator\")\n",
    "    workflow.add_edge(\"chart_generator\", \"chart_summarizer\")\n",
    "    workflow.add_edge(\"chart_summarizer\", END)\n",
    "    compiled_graph_with_search = workflow.compile()\n",
    "\n",
    "    return compiled_graph_with_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff89dcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "class TruAgent:\n",
    "    def __init__(self):\n",
    "        self.graph = build_graph_with_search()\n",
    "\n",
    "    @instrument(\n",
    "        span_type=SpanAttributes.SpanType.RECORD_ROOT,\n",
    "        attributes={\n",
    "            SpanAttributes.RECORD_ROOT.INPUT: \"query\",\n",
    "            SpanAttributes.RECORD_ROOT.OUTPUT: \"return\",\n",
    "        },\n",
    "    )\n",
    "    def invoke_agent_graph(self, query: str) -> str:\n",
    "        events = self.graph.stream(\n",
    "            {\n",
    "                \"messages\": [(\"user\", query)],\n",
    "            },\n",
    "            # Maximum number of steps to take in the graph\n",
    "            {\"recursion_limit\": 50},\n",
    "        )\n",
    "\n",
    "        # resp_messages = []\n",
    "\n",
    "        for event in events:\n",
    "            # Grab the payload if it exists\n",
    "            payload = next(iter(event.values()), None)\n",
    "            if payload is None:\n",
    "                continue  # skip this event if no payload\n",
    "\n",
    "            messages = payload.get(\"messages\")\n",
    "        return (\n",
    "            messages[-1].content\n",
    "            if messages and hasattr(messages[-1], \"content\")\n",
    "            else \"\"\n",
    "        )\n",
    "\n",
    "\n",
    "tru_agent = TruAgent()\n",
    "\n",
    "tru_agent_app = TruApp(\n",
    "    tru_agent,\n",
    "    app_name=APP_NAME,\n",
    "    app_version=\"doc and web search\",\n",
    "    connector=trulens_sf_connector,\n",
    "    main_method=tru_agent.invoke_agent_graph,\n",
    ")\n",
    "\n",
    "st_1 = datetime.datetime.fromtimestamp(time.time()).strftime(\n",
    "    \"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    run_name=\"Multi-agent demo run - document and web search - better reflection\",\n",
    "    description=\"this is a run with access to cortex search and tavily + qualitative caption\",\n",
    "    dataset_name=\"Research test dataset\",\n",
    "    source_type=\"DATAFRAME\",\n",
    "    label=\"langgraph demo\",\n",
    "    dataset_spec={\n",
    "        \"RECORD_ROOT.INPUT\": \"query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "run: Run = tru_agent_app.add_run(run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004f71a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(tru_agent.graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23788cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.start(input_df=user_queries_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70f360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while run.get_status() == \"INVOCATION_IN_PROGRESS\":\n",
    "    time.sleep(3)\n",
    "\n",
    "run.compute_metrics([\"groundedness\", \"context_relevance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac25d0ad",
   "metadata": {},
   "source": [
    "### Use Cortex Agent to gain access to querying structured SEC data without complicating the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8817dbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "import requests\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "\n",
    "class CortexAgentArgs(BaseModel):\n",
    "    query: str\n",
    "\n",
    "\n",
    "class CortexAgentTool(StructuredTool):\n",
    "    name: str = \"CortexAgent\"\n",
    "    description: str = \"answers questions using the federal reserve meeting minutes and structured data from the SEC\"\n",
    "    args_schema: type[BaseModel] = CortexAgentArgs\n",
    "    session: Session\n",
    "\n",
    "\n",
    "def run(self, query: str, **kwargs) -> str:\n",
    "    print(\"calling agent\")\n",
    "    payload = {\n",
    "        \"model\": \"claude-3-5-sonnet\",\n",
    "        \"response_instruction\": \"You are a helpful AI assistant.\",\n",
    "        \"experimental\": {},\n",
    "        \"tools\": [\n",
    "            {\n",
    "                \"tool_spec\": {\n",
    "                    \"type\": \"cortex_analyst_text_to_sql\",\n",
    "                    \"name\": \"SEC_ANALYST\",\n",
    "                }\n",
    "            },\n",
    "            {\"tool_spec\": {\"type\": \"cortex_search\", \"name\": \"FOMC_SEARCH\"}},\n",
    "            {\"tool_spec\": {\"type\": \"sql_exec\", \"name\": \"sql_execution_tool\"}},\n",
    "        ],\n",
    "        \"tool_resources\": {\n",
    "            \"SEC_ANALYST\": {\n",
    "                \"semantic_model_file\": \"@agents_db.notebooks.semantic_models/sec_filings.yaml\"\n",
    "            },\n",
    "            \"FOMC_SEARCH\": {\n",
    "                \"name\": \"CORTEX_SEARCH_TUTORIAL_DB.PUBLIC.FOMC_SEARCH_SERVICE\"\n",
    "            },\n",
    "        },\n",
    "        \"tool_choice\": {\"type\": \"auto\"},\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": query}]}\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    api_url = \"http://SFDEVREL-SFDEVREL_ENTERPRISE.snowflakecomputing.com/api/v2/cortex/agent:run\"\n",
    "    pat = os.getenv(\"SNOWFLAKE_PAT\")\n",
    "    if not pat:\n",
    "        raise RuntimeError(\"Environment variable SNOWFLAKE_PAT is not set\")\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {pat}\",\n",
    "        \"X-Snowflake-Authorization-Token-Type\": \"PROGRAMMATIC_ACCESS_TOKEN\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    response = requests.post(api_url, json=payload, headers=headers)\n",
    "    print(\"agent response\", response)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(response.status_code)\n",
    "        print(response.text)\n",
    "        return f\"Failed Cortex Agents API call: {response.status_code} - {response.text}\"\n",
    "\n",
    "    # Extract content from delta\n",
    "    data = response.json()\n",
    "    contents = data.get(\"delta\", {}).get(\"content\", [])\n",
    "    result_parts = [\n",
    "        chunk.get(\"text\", \"\")\n",
    "        for chunk in contents\n",
    "        if chunk.get(\"type\") == \"text\"\n",
    "    ]\n",
    "    result_text = \" \".join(result_parts).strip()\n",
    "\n",
    "    # Fallback if content is empty\n",
    "    return result_text or json.dumps(data, indent=2)\n",
    "\n",
    "\n",
    "def build_graph_with_agent(search_max_results: int = 5):\n",
    "    def make_system_prompt(suffix: str) -> str:\n",
    "        return (\n",
    "            \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "            \" Use the provided tools to progress towards answering the question.\"\n",
    "            \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "            \" will help where you left off. Execute what you can to make progress.\"\n",
    "            \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "            \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "            f\"\\n{suffix}\"\n",
    "        )\n",
    "\n",
    "    search = SerpAPIWrapper()\n",
    "\n",
    "    # search_tool = TavilySearchResults(max_results=search_max_results)\n",
    "\n",
    "    search_tool = Tool(\n",
    "        name=\"web_search\",\n",
    "        description=\"Search the web for current information, such as weather or news\",\n",
    "        func=search.run,\n",
    "    )\n",
    "\n",
    "    # Instantiate CortexAgentTool\n",
    "    cortex_agent_tool = CortexAgentTool(session=snowpark_session_trulens)\n",
    "\n",
    "    wrapped_cortex_agent_tool = Tool(\n",
    "        name=cortex_agent_tool.name,\n",
    "        description=cortex_agent_tool.description,\n",
    "        func=cortex_agent_tool.run,\n",
    "        return_direct=False,  # set to True only if you want the agent to stop after using it\n",
    "    )\n",
    "\n",
    "    tool_registry = {\n",
    "        str(uuid.uuid4()): search_tool,\n",
    "        str(uuid.uuid4()): wrapped_cortex_agent_tool,  # CortexAgentTool here\n",
    "    }\n",
    "\n",
    "    # Update your tool documents indexing accordingly\n",
    "    tool_documents = [\n",
    "        Document(\n",
    "            page_content=tool.name,\n",
    "            id=tool_id,\n",
    "            metadata={\n",
    "                \"tool_name\": tool.name,\n",
    "                \"tool_description\": tool.description,\n",
    "            },\n",
    "        )\n",
    "        for tool_id, tool in tool_registry.items()\n",
    "    ]\n",
    "    vector_store = InMemoryVectorStore(embedding=OpenAIEmbeddings())\n",
    "    vector_store.add_documents(tool_documents)\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "    vector_store = InMemoryVectorStore(embedding=OpenAIEmbeddings())\n",
    "    vector_store.add_documents(tool_documents)\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"SELECT_TOOLS\",\n",
    "        attributes=lambda ret, exc, *args, **kw: {\n",
    "            # ---- state as JSON-text (OTLP needs a scalar) -----------------\n",
    "            f\"{BASE_SCOPE}.select_tools_input_state\": json.dumps(  # ← turns dict → str\n",
    "                {\n",
    "                    **{k: v for k, v in args[0].items() if k != \"messages\"},\n",
    "                    \"messages\": [\n",
    "                        {\"type\": m.__class__.__name__, \"content\": m.content}\n",
    "                        if hasattr(m, \"content\")  # BaseMessage subclasses\n",
    "                        else m  # already JSON-friendly\n",
    "                        for m in args[0].get(\"messages\", [])\n",
    "                    ],\n",
    "                }\n",
    "            ),\n",
    "            # ---- selected tool IDs as a simple comma-separated string -----\n",
    "            f\"{BASE_SCOPE}.selected_tool_ids\": \", \".join(\n",
    "                ret.get(\"selected_tools\", [])\n",
    "            )\n",
    "            if isinstance(ret, dict)\n",
    "            else \"\",\n",
    "        },\n",
    "    )\n",
    "    def select_tools(\n",
    "        state: ToolState,\n",
    "    ) -> Command[Literal[\"research_agent\", END]]:\n",
    "        messages = state[\"messages\"]\n",
    "        last = messages[-1]\n",
    "        query = last[\"content\"] if isinstance(last, dict) else last.content\n",
    "        print(\"selecting tools based on\", query)\n",
    "\n",
    "        # 1. pull top-k with their scores\n",
    "        results: list[tuple[Document, float]] = (\n",
    "            vector_store.similarity_search_with_score(\n",
    "                query,\n",
    "                k=5,  # look at top-5 candidates\n",
    "            )\n",
    "        )\n",
    "        print(\"tool search results\", results)\n",
    "\n",
    "        # 2. filter by minimum cosine-similarity\n",
    "        MIN_SIMILARITY = 0.7\n",
    "        filtered = [doc for doc, score in results if score >= MIN_SIMILARITY]\n",
    "\n",
    "        # 3a. no sufficiently similar tool → end\n",
    "        if not filtered:\n",
    "            print(\"no tool selected\")\n",
    "            msg = HumanMessage(\n",
    "                content=\"Sorry, I don’t have a tool that’s relevant enough to answer that.\",\n",
    "                name=\"assistant\",\n",
    "            )\n",
    "            return Command(\n",
    "                update={\"messages\": messages + [msg]},\n",
    "                goto=END,\n",
    "            )\n",
    "\n",
    "        # 3b. otherwise select those tools and move on\n",
    "        selected_ids = [doc.id for doc in filtered]\n",
    "        print(\"tools selected\", selected_ids)\n",
    "        return Command(\n",
    "            update={\n",
    "                \"selected_tools\": selected_ids,\n",
    "            },\n",
    "            goto=\"research_agent\",\n",
    "        )\n",
    "\n",
    "    # Warning: This executes code locally, which can be unsafe when not sandboxed\n",
    "\n",
    "    repl = PythonREPL()\n",
    "\n",
    "    @tool\n",
    "    @instrument(\n",
    "        span_type=\"PYTHON_REPL_TOOL\",\n",
    "        attributes={\n",
    "            f\"{BASE_SCOPE}.python_tool_input_code\": \"code\",\n",
    "        },\n",
    "    )\n",
    "    def python_repl_tool(code: str):\n",
    "        \"\"\"\n",
    "        Run arbitrary Python, grab the CURRENT matplotlib figure (if any),\n",
    "        save it to ./langgraph_saved_images_snowflaketools/v3/chart_<uuid>.png,\n",
    "        and return a first-line `CHART_PATH=…`.\n",
    "        \"\"\"\n",
    "        import matplotlib\n",
    "\n",
    "        matplotlib.use(\"Agg\")  # headless safety\n",
    "        import os\n",
    "        import uuid\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        # ------------------ run user code & capture stdout ------------------\n",
    "        repl.run(code)\n",
    "\n",
    "        # ------------------ locate a figure (if generated) ------------------\n",
    "        fig = plt.gcf()\n",
    "        has_axes = bool(fig.axes)  # True if something was plotted\n",
    "\n",
    "        # ------------------ always save if we have a figure -----------------\n",
    "        chart_path = \"\"\n",
    "        if has_axes:\n",
    "            target_dir = \"./langgraph_saved_images_snowflaketools/v3\"\n",
    "            os.makedirs(target_dir, exist_ok=True)\n",
    "            chart_path = os.path.join(\n",
    "                target_dir, f\"chart_{uuid.uuid4().hex}.png\"\n",
    "            )\n",
    "            fig.savefig(chart_path, format=\"png\")\n",
    "            plt.close(fig)\n",
    "\n",
    "        # ------------------ tool result (1st line = CHART_PATH) -------------\n",
    "        return f\"CHART_PATH={chart_path if chart_path else 'NONE'}\\n\"\n",
    "\n",
    "    def get_next_node(last_message: BaseMessage, goto: str):\n",
    "        if \"FINAL ANSWER\" in last_message.content:\n",
    "            # Any agent decided the work is done\n",
    "            return END\n",
    "        return goto\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"RESEARCH_NODE\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            f\"{BASE_SCOPE}.research_node_input\": args[0][\"messages\"][\n",
    "                -1\n",
    "            ].content,\n",
    "            f\"{BASE_SCOPE}.research_node_response\": ret.update[\"messages\"][\n",
    "                -1\n",
    "            ].content\n",
    "            if hasattr(ret, \"update\")\n",
    "            else json.dumps(ret, indent=4, sort_keys=True),\n",
    "            f\"{BASE_SCOPE}.tool_messages\": [\n",
    "                dumps(message)\n",
    "                for message in ret.update[\"messages\"]\n",
    "                if isinstance(message, ToolMessage)\n",
    "            ]\n",
    "            if hasattr(ret, \"update\")\n",
    "            else \"No tool call\",\n",
    "        },\n",
    "    )\n",
    "    @instrument(\n",
    "        span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            SpanAttributes.RETRIEVAL.QUERY_TEXT: args[0][\"messages\"][\n",
    "                -1\n",
    "            ].content,\n",
    "            SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS: [\n",
    "                ret.update[\"messages\"][-1].content\n",
    "            ]\n",
    "            if hasattr(ret, \"update\")\n",
    "            else [json.dumps(ret, indent=4, sort_keys=True)],\n",
    "        },\n",
    "    )\n",
    "    def research_agent_node(\n",
    "        state: ToolState,\n",
    "    ) -> Command[Literal[\"chart_generator\"]]:\n",
    "        \"\"\"\n",
    "        Always binds the selected tools and invokes the bound agent.\n",
    "        Stops on FINAL ANSWER or moves to chart_generator.\n",
    "        \"\"\"\n",
    "        # grab (non-empty) list of selected tool IDs\n",
    "        selected_ids = state[\"selected_tools\"]\n",
    "\n",
    "        # bind only those tools\n",
    "        selected_tools = [tool_registry[tid] for tid in selected_ids]\n",
    "        bound_llm = llm.bind_tools(selected_tools)\n",
    "        bound_agent = create_react_agent(\n",
    "            bound_llm,\n",
    "            tools=selected_tools,  # already bound\n",
    "            prompt=make_system_prompt(\n",
    "                \"You can only do research. You are working with both a chart generator and a chart summarizer colleagues.\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # run it\n",
    "        result = bound_agent.invoke(state)\n",
    "\n",
    "        # decide if we’re done\n",
    "        last = result[\"messages\"][-1]\n",
    "        goto = get_next_node(last, \"chart_generator\")\n",
    "\n",
    "        # tag the origin of the final message\n",
    "        result[\"messages\"][-1] = HumanMessage(\n",
    "            content=last.content,\n",
    "            name=\"research_agent\",\n",
    "        )\n",
    "\n",
    "        return Command(\n",
    "            update={\"messages\": result[\"messages\"]},\n",
    "            goto=goto,\n",
    "        )\n",
    "\n",
    "    # Chart generator agent and node\n",
    "    # NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION, WHICH CAN BE UNSAFE WHEN NOT SANDBOXED\n",
    "    chart_agent = create_react_agent(\n",
    "        llm,\n",
    "        [python_repl_tool],\n",
    "        prompt=make_system_prompt(\n",
    "            \"You can only generate charts. The generated chart should be save at a local directory at current directory PATH './langgraph_saved_images_snowflaketools/v3' , and this PATH should be sent to your colleague. You are working with a chart summarizer colleague.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    def extract_chart_path(text: str) -> str | None:\n",
    "        \"\"\"\n",
    "        Returns the first CHART_PATH=… found in `text`, else None.\n",
    "        \"\"\"\n",
    "        m = re.search(r\"^CHART_PATH=(.+)$\", text, flags=re.MULTILINE)\n",
    "        return m.group(1).strip() if m else None\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"CHART_GENERATOR_NODE\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            f\"{BASE_SCOPE}.chart_node_input\": args[0][\"messages\"][-1].content,\n",
    "            f\"{BASE_SCOPE}.chart_node_response\": (\n",
    "                ret.update[\"messages\"][-1].content\n",
    "                if ret and hasattr(ret, \"update\") and ret.update\n",
    "                else \"No update response\"\n",
    "            ),\n",
    "        },\n",
    "    )\n",
    "    def chart_node(\n",
    "        state: MessagesState,\n",
    "    ) -> Command[Literal[\"chart_summarizer\"]]:\n",
    "        \"\"\"\n",
    "        This function represents the chart generation node in the workflow.\n",
    "        It invokes the chart generation agent to create a chart based on the provided state.\n",
    "        The generated chart is saved to a specified directory, and its path is extracted from the tool messages.\n",
    "        A summary prompt is then prepared to send to the chart summarizer agent.\n",
    "        If the chart path is not found, the workflow ends; otherwise, it transitions to the chart summarizer node.\n",
    "        \"\"\"\n",
    "        print(\"generating chart\")\n",
    "        # 1. let the agent run\n",
    "        result = chart_agent.invoke(state)\n",
    "        print(\"chart result\", result)\n",
    "\n",
    "        # 2. try to grab a path from any ToolMessage\n",
    "        chart_path = None\n",
    "        for msg in result[\"messages\"]:\n",
    "            if isinstance(msg, ToolMessage):\n",
    "                chart_path = extract_chart_path(msg.content)\n",
    "                if chart_path and chart_path.upper() != \"NONE\":\n",
    "                    break\n",
    "\n",
    "        print(f\"Chart Path: {chart_path!r}\")\n",
    "\n",
    "        # 3. If no valid chart path, return to researcher with error message\n",
    "        if not chart_path or chart_path.upper() == \"NONE\":\n",
    "            result[\"messages\"].append(\n",
    "                HumanMessage(\n",
    "                    content=\"Failed to generate chart. Please try again with different parameters.\",\n",
    "                    name=\"chart_generator\",\n",
    "                )\n",
    "            )\n",
    "            return Command(\n",
    "                update={\"messages\": result[\"messages\"]}, goto=\"research_agent\"\n",
    "            )\n",
    "\n",
    "        # 4. Add chart path to messages for downstream nodes\n",
    "        result[\"messages\"].append(\n",
    "            HumanMessage(\n",
    "                content=f\"CHART_PATH={chart_path}\", name=\"chart_generator\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # 5. Add summary prompt\n",
    "        summary_prompt = \"Please summarise the chart in ≤ 3 sentences.\"\n",
    "        result[\"messages\"].append(\n",
    "            HumanMessage(name=\"chart_generator\", content=summary_prompt)\n",
    "        )\n",
    "\n",
    "        return Command(\n",
    "            update={\"messages\": result[\"messages\"]}, goto=\"chart_summarizer\"\n",
    "        )\n",
    "\n",
    "    # Build the image captioning agent.\n",
    "    # If you have any specific image processing tools (e.g., for extracting chart images),\n",
    "    # you can add them in the tools list. For now, we leave it empty.\n",
    "    chart_summary_agent = create_react_agent(\n",
    "        llm,\n",
    "        tools=[],  # Add image processing tools if available/needed.\n",
    "        prompt=make_system_prompt(\n",
    "            \"\"\"You can only generate charts with Python.\n",
    "        ALWAYS:\n",
    "        1. Save the figure as PNG to './langgraph_saved_images_snowflaketools/v3'.\n",
    "        2. Do NOT display the image inline.\n",
    "        3. End your reply with `CHART_PATH=<absolute-or-relative-path>`.\n",
    "        You are working with a summariser colleague.\"\"\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    reflection_prompt_template = PromptTemplate(\n",
    "        input_variables=[\"user_query\", \"chart_summary\"],\n",
    "        template=\"\"\"\\\n",
    "        You are an AI assistant tasked with reflecting on the quality of a chart summary. The user has asked the following question:\n",
    "        \"{user_query}\"\n",
    "\n",
    "        You are given the following chart summary:\n",
    "        \"{chart_summary}\"\n",
    "\n",
    "        Your task is to evaluate how well the chart summary answers the user's question. Consider the following:\n",
    "        - Does the summary capture the **key insights** and trends from the chart, even if in a more general form?\n",
    "        - Does it provide **adequate context** to address the user's query, even if it's not exhaustive?\n",
    "        - If the summary provides some context but could benefit from more details, consider it sufficient for now unless significant details are missing.\n",
    "\n",
    "        If the summary **generally** addresses the question, respond with 'Task complete'. If the summary **lacks significant** details or clarity, then respond with specific details on how the answer should be improved and what information is needed. Avoid being overly critical unless the summary completely misses key elements necessary to answer the query.\n",
    "\n",
    "        Please provide your answer in a **concise and encouraging** manner.\n",
    "        \"\"\",\n",
    "    )\n",
    "\n",
    "    # Create the chain using the prompt template and the LLM (ChatOpenAI)\n",
    "    reflection_chain = reflection_prompt_template | llm\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"CHART_SUMMARY_REFLECTION\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            f\"{BASE_SCOPE}.chart_summary_reflection_input_user_query\": args[0],\n",
    "            f\"{BASE_SCOPE}.chart_summary_reflection_input_chart_summary\": args[\n",
    "                1\n",
    "            ],\n",
    "            f\"{BASE_SCOPE}.chart_summary_reflection_response\": ret,\n",
    "        },\n",
    "    )\n",
    "    def perform_reflection(user_query: str, chart_summary: str) -> str:\n",
    "        \"\"\"\n",
    "        This function uses an LLM to reflect on the quality of a chart summary\n",
    "        and determine if the task is complete or requires further refinement.\n",
    "        \"\"\"\n",
    "        # Call the chain with the user query and chart summary\n",
    "        reflection_result = reflection_chain.invoke({\n",
    "            \"user_query\": user_query,\n",
    "            \"chart_summary\": chart_summary,\n",
    "        })\n",
    "        return reflection_result.content\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"CHART_SUMMARY_NODE\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            f\"{BASE_SCOPE}.summary_node_input\": args[0][\"messages\"][-1].content,\n",
    "            f\"{BASE_SCOPE}.summary_node_output\": (\n",
    "                ret.update[\"messages\"][-1].content\n",
    "                if hasattr(ret, \"update\")\n",
    "                else \"NO SUMMARY GENERATED\"\n",
    "            ),\n",
    "        },\n",
    "    )\n",
    "    def chart_summary_node(state: MessagesState) -> Command[Literal[END]]:\n",
    "        \"\"\"\n",
    "        This function represents the chart summarizer node in the workflow graph.\n",
    "        It uses the chart summary agent to generate a concise summary for the chart image\n",
    "        provided by the chart generator node. The summary is limited to three sentences\n",
    "        and is based on the chart image saved at the specified local path.\n",
    "        \"\"\"\n",
    "        # 1. Extract chart path from messages\n",
    "        chart_path = None\n",
    "        for msg in state[\"messages\"]:\n",
    "            if isinstance(msg, ToolMessage) and \"CHART_PATH=\" in msg.content:\n",
    "                chart_path = extract_chart_path(msg.content)\n",
    "                if chart_path and chart_path.upper() != \"NONE\":\n",
    "                    break\n",
    "\n",
    "        print(f\"Chart Path in Chart Summary Node: {chart_path!r}\")\n",
    "\n",
    "        # 2. If no valid chart path, return to researcher with error message\n",
    "        if not chart_path or chart_path.upper() == \"NONE\":\n",
    "            return Command(\n",
    "                update={\n",
    "                    \"messages\": state[\"messages\"]\n",
    "                    + [\n",
    "                        HumanMessage(\n",
    "                            content=\"No valid chart was generated. Please try again.\",\n",
    "                            name=\"chart_summarizer\",\n",
    "                        )\n",
    "                    ]\n",
    "                },\n",
    "                goto=\"research_agent\",\n",
    "            )\n",
    "\n",
    "        # 3. Run the summarizer\n",
    "        result = chart_summary_agent.invoke(state)\n",
    "        if not result or \"messages\" not in result:\n",
    "            return Command(\n",
    "                update={\n",
    "                    \"messages\": state[\"messages\"]\n",
    "                    + [\n",
    "                        HumanMessage(\n",
    "                            content=\"Failed to generate chart summary. Please try again.\",\n",
    "                            name=\"chart_summarizer\",\n",
    "                        )\n",
    "                    ]\n",
    "                },\n",
    "                goto=\"research_agent\",\n",
    "            )\n",
    "\n",
    "        # 4. Add reflection\n",
    "        user_query = state[\"messages\"][-2].content\n",
    "        chart_summary = result[\"messages\"][-1].content\n",
    "        reflection = perform_reflection(user_query, chart_summary)\n",
    "\n",
    "        # 5. Determine next node\n",
    "        goto = (\n",
    "            END\n",
    "            if \"Task complete\" in reflection or \"FINAL ANSWER\" in reflection\n",
    "            else \"research_agent\"\n",
    "        )\n",
    "        result[\"messages\"][-1] = HumanMessage(\n",
    "            name=\"chart_summarizer\", content=chart_summary\n",
    "        )\n",
    "\n",
    "        return Command(update={\"messages\": result[\"messages\"]}, goto=goto)\n",
    "\n",
    "    workflow = StateGraph(ToolState)\n",
    "    workflow.add_node(\"select_tools\", select_tools)\n",
    "    workflow.add_node(\"research_agent\", research_agent_node)\n",
    "    workflow.add_node(\"chart_generator\", chart_node)\n",
    "    workflow.add_node(\"chart_summarizer\", chart_summary_node)\n",
    "\n",
    "    # Update transitions: begin with tool selection then go to research agent.\n",
    "    workflow.add_edge(START, \"select_tools\")\n",
    "    # workflow.add_edge(\"select_tools\", END)\n",
    "    workflow.add_edge(\"select_tools\", \"research_agent\")\n",
    "    workflow.add_edge(\"research_agent\", \"chart_generator\")\n",
    "    workflow.add_edge(\"chart_generator\", \"chart_summarizer\")\n",
    "    workflow.add_edge(\"chart_summarizer\", END)\n",
    "    compiled_graph_with_agent = workflow.compile()\n",
    "\n",
    "    return compiled_graph_with_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae6e403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TruAgent:\n",
    "    def __init__(self):\n",
    "        self.graph = build_graph_with_agent()\n",
    "\n",
    "    @instrument(\n",
    "        span_type=SpanAttributes.SpanType.RECORD_ROOT,\n",
    "        attributes={\n",
    "            SpanAttributes.RECORD_ROOT.INPUT: \"query\",\n",
    "            SpanAttributes.RECORD_ROOT.OUTPUT: \"return\",\n",
    "        },\n",
    "    )\n",
    "    def invoke_agent_graph(self, query: str) -> str:\n",
    "        events = self.graph.stream(\n",
    "            {\n",
    "                \"messages\": [(\"user\", query)],\n",
    "            },\n",
    "            # Maximum number of steps to take in the graph\n",
    "            {\"recursion_limit\": 100},\n",
    "        )\n",
    "\n",
    "        # resp_messages = []\n",
    "\n",
    "        for event in events:\n",
    "            # Grab the payload if it exists\n",
    "            payload = next(iter(event.values()), None)\n",
    "            if payload is None:\n",
    "                continue  # skip this event if no payload\n",
    "\n",
    "            messages = payload.get(\"messages\")\n",
    "        return (\n",
    "            messages[-1].content\n",
    "            if messages and hasattr(messages[-1], \"content\")\n",
    "            else \"\"\n",
    "        )\n",
    "\n",
    "\n",
    "tru_agent = TruAgent()\n",
    "\n",
    "tru_agent_app = TruApp(\n",
    "    tru_agent,\n",
    "    app_name=APP_NAME,\n",
    "    app_version=\"cortex agent + web search\",\n",
    "    connector=trulens_sf_connector,\n",
    "    main_method=tru_agent.invoke_agent_graph,\n",
    ")\n",
    "\n",
    "st_1 = datetime.datetime.fromtimestamp(time.time()).strftime(\n",
    "    \"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    run_name=\"Multi-agent demo run - cortex agent + web search 16\",\n",
    "    description=\"this is a run with access to cortex agent, with internally uses cortex search and analyst as tools\",\n",
    "    dataset_name=\"Research test dataset\",\n",
    "    source_type=\"DATAFRAME\",\n",
    "    label=\"langgraph demo\",\n",
    "    dataset_spec={\n",
    "        \"RECORD_ROOT.INPUT\": \"query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "run: Run = tru_agent_app.add_run(run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde20b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "user_queries = [\n",
    "    \"What has been the temperature in London so far in 2025? Create a line chart that illustrates this data?\",\n",
    "    \"In 2023, how did the fed funds rate fluctuate? What were the key drivers? Create a line chart that illustrates this data, including a caption with the key drivers.\",\n",
    "    \"What is the total holding value reported by BlackRock Fund Advisors in their SEC filings during 2023?? Create a line chart that illustrates this data\",\n",
    "    \"Which filing managers submitted the most SEC filings in 2023 year to date? Create a bar chart that illustrates this data\",\n",
    "]\n",
    "\n",
    "user_queries_df = pd.DataFrame(user_queries, columns=[\"query\"])\n",
    "\n",
    "run.start(input_df=user_queries_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faa503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while run.get_status() == \"INVOCATION_IN_PROGRESS\":\n",
    "    time.sleep(3)\n",
    "\n",
    "run.compute_metrics([\"groundedness\", \"context_relevance\", \"answer_relevance\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oss_rag_stack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
