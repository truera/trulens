{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39fd1948-b5c3-48c4-b10e-2ae7e8c83334",
   "metadata": {},
   "source": [
    "# Multi-agent network with Snowflake tools for querying unstructured and structured data\n",
    "\n",
    "Adapted from the original [Langgraph multi-agent notebook example](https://github.com/langchain-ai/langgraph/blob/main/docs/docs/tutorials/multi_agent/multi-agent-collaboration.ipynb)\n",
    "\n",
    "A single agent can usually operate effectively using a handful of tools within a single domain, but even using powerful models like `gpt-4`, it can be less effective at using many tools. \n",
    "\n",
    "This notebook is an extension of the multi-agent-collaboration notebook, showing how access to more tools - particularly with private data can enhance the ability of a data agent.\n",
    "\n",
    "We will slowly build up the agent with more tools, starting with web search, then adding document search via Cortex Search, and lastly replacing document search with a Cortex Agent that can both document search and query snowflake tables in sql via Cortex Analyst.\n",
    "\n",
    "We also make some useful improvements to the agentic flow in this notebook to handle the more complex set of tools: namely a reflection loop and safe exit. These improvements dramatically improve the efficiency of the agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7b6dcc-c985-46e2-8457-7e6b0298b950",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "# pip install -U langchain_community langchain_openai langchain_experimental matplotlib langgraph pygraphviz google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c0e4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = (\n",
    "    \"Finance Data and Research Agent\"  # set this app name for your use case\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e48d5f",
   "metadata": {},
   "source": [
    "## Set keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bdc791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# need both API keys\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\"\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"...\"\n",
    "\n",
    "os.environ[\"SNOWFLAKE_ACCOUNT\"] = \"SFDEVREL_ENTERPRISE\"\n",
    "os.environ[\"SNOWFLAKE_USER\"] = \"JREINI\"\n",
    "os.environ[\"SNOWFLAKE_USER_PASSWORD\"] = \"...\"\n",
    "os.environ[\"SNOWFLAKE_DATABASE\"] = \"AGENTS_DB\"\n",
    "os.environ[\"SNOWFLAKE_SCHEMA\"] = \"NOTEBOOKS\"\n",
    "os.environ[\"SNOWFLAKE_ROLE\"] = \"CORTEX_USER_ROLE\"\n",
    "os.environ[\"SNOWFLAKE_WAREHOUSE\"] = \"CONTAINER_RUNTIME_WH\"\n",
    "os.environ[\"SNOWFLAKE_PAT\"] = \"...\"\n",
    "\n",
    "os.environ[\"TRULENS_OTEL_TRACING\"] = (\n",
    "    \"1\"  # to enable OTEL tracing -> note the Snowsight UI experience for now is limited to PuPr customers, not yet supported for OSS.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58f2f08",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e874860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import List, Literal\n",
    "import uuid\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "from langchain.load.dump import dumps\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import StructuredTool\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langgraph.graph import END\n",
    "from langgraph.graph import START\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.managed.is_last_step import RemainingSteps\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.types import Command\n",
    "from pydantic import BaseModel\n",
    "from snowflake.snowpark import Session\n",
    "from trulens.apps.app import TruApp\n",
    "from trulens.connectors.snowflake import SnowflakeConnector\n",
    "from trulens.core.otel.instrument import instrument\n",
    "from trulens.core.run import Run\n",
    "from trulens.core.run import RunConfig\n",
    "from trulens.otel.semconv.trace import BASE_SCOPE\n",
    "from trulens.otel.semconv.trace import SpanAttributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bb1f53",
   "metadata": {},
   "source": [
    "## Create TruLens/Snowflake Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3c39f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowflake account for trulens\n",
    "snowflake_connection_parameters = {\n",
    "    \"account\": os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "    \"user\": os.environ[\"SNOWFLAKE_USER\"],\n",
    "    \"password\": os.environ[\"SNOWFLAKE_USER_PASSWORD\"],\n",
    "    \"database\": os.environ[\"SNOWFLAKE_DATABASE\"],\n",
    "    \"schema\": os.environ[\"SNOWFLAKE_SCHEMA\"],\n",
    "    \"role\": os.environ[\"SNOWFLAKE_ROLE\"],\n",
    "    \"warehouse\": os.environ[\"SNOWFLAKE_WAREHOUSE\"],\n",
    "}\n",
    "snowpark_session_trulens = Session.builder.configs(\n",
    "    snowflake_connection_parameters\n",
    ").create()\n",
    "\n",
    "\n",
    "trulens_sf_connector = SnowflakeConnector(\n",
    "    snowpark_session=snowpark_session_trulens\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a283e-ea04-40c1-b792-f9e5f7d81203",
   "metadata": {},
   "source": [
    "### Define the agent with web search and charting tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cab43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolState(MessagesState):\n",
    "    selected_tools: List[str]\n",
    "    chart_path: str\n",
    "    remaining_steps: RemainingSteps\n",
    "\n",
    "\n",
    "def build_graph():\n",
    "    def canned_end_node(state: ToolState) -> Command[Literal[\"__end__\"]]:\n",
    "        print(\"starting CANNED END\", flush=True)\n",
    "\n",
    "        return Command(goto=END)\n",
    "\n",
    "    def make_system_prompt(suffix: str) -> str:\n",
    "        return (\n",
    "            \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "            \" Use the provided tools to progress towards answering the question.\"\n",
    "            \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "            \" will help where you left off. Execute what you can to make progress.\"\n",
    "            \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "            \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "            f\"\\n{suffix}\"\n",
    "        )\n",
    "\n",
    "    search = SerpAPIWrapper()\n",
    "\n",
    "    search_tool = Tool(\n",
    "        name=\"web_search\",\n",
    "        description=\"Search the web for current information, such as weather or news\",\n",
    "        func=search.run,\n",
    "    )\n",
    "\n",
    "    # 4) Register it under a UUID and turn that into a Document\n",
    "    tool_id = str(uuid.uuid4())\n",
    "    tool_registry = {tool_id: search_tool}\n",
    "\n",
    "    tool_documents = [\n",
    "        Document(\n",
    "            page_content=search_tool.name,  # your human-readable blurb\n",
    "            id=tool_id,  # must match the registry key\n",
    "            metadata={\n",
    "                \"tool_name\": tool.name,\n",
    "                \"tool_description\": tool.description,\n",
    "            },\n",
    "        )\n",
    "        for tool_id, tool in tool_registry.items()\n",
    "    ]\n",
    "    vector_store = InMemoryVectorStore(embedding=OpenAIEmbeddings())\n",
    "    vector_store.add_documents(tool_documents)\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"SELECT_TOOLS\",\n",
    "        attributes=lambda ret, exc, *args, **kw: {\n",
    "            # ---- state as JSON-text (OTLP needs a scalar) -----------------\n",
    "            f\"{BASE_SCOPE}.select_tools_input_state\": json.dumps(  # ← turns dict → str\n",
    "                {\n",
    "                    **{k: v for k, v in args[0].items() if k != \"messages\"},\n",
    "                    \"messages\": [\n",
    "                        {\"type\": m.__class__.__name__, \"content\": m.content}\n",
    "                        if hasattr(m, \"content\")  # BaseMessage subclasses\n",
    "                        else m  # already JSON-friendly\n",
    "                        for m in args[0].get(\"messages\", [])\n",
    "                    ],\n",
    "                }\n",
    "            ),\n",
    "            # ---- selected tool IDs as a simple comma-separated string -----\n",
    "            f\"{BASE_SCOPE}.selected_tool_ids\": \", \".join(\n",
    "                ret.get(\"selected_tools\", [])\n",
    "            )\n",
    "            if isinstance(ret, dict)\n",
    "            else \"\",\n",
    "        },\n",
    "    )\n",
    "    def select_tools(\n",
    "        state: ToolState,\n",
    "    ) -> Command[Literal[\"research_agent\", END]]:\n",
    "        print(state[\"remaining_steps\"])\n",
    "        if state[\"remaining_steps\"] <= 2:\n",
    "            print(\"Bailing out\", flush=True)\n",
    "            return Command(\n",
    "                update={\"messages\": state[\"messages\"]},\n",
    "                goto=\"canned_end\",\n",
    "            )\n",
    "        messages = state[\"messages\"]\n",
    "        last = messages[-1]\n",
    "        query = last[\"content\"] if isinstance(last, dict) else last.content\n",
    "        print(\"selecting tools based on\", query)\n",
    "\n",
    "        # 1. pull top-k with their scores\n",
    "        results: list[tuple[Document, float]] = (\n",
    "            vector_store.similarity_search_with_score(\n",
    "                query,\n",
    "                k=5,  # look at top-5 candidates\n",
    "            )\n",
    "        )\n",
    "        print(\"tool search results\", results, flush=True)\n",
    "\n",
    "        # 2. filter by minimum cosine-similarity\n",
    "        MIN_SIMILARITY = 0.6\n",
    "        filtered = [doc for doc, score in results if score >= MIN_SIMILARITY]\n",
    "\n",
    "        # 3a. no sufficiently similar tool → end\n",
    "        if not filtered:\n",
    "            print(\"no tool selected\", flush=True)\n",
    "            msg = HumanMessage(\n",
    "                content=\"Sorry, I don’t have a tool that’s relevant enough to answer that.\",\n",
    "                name=\"assistant\",\n",
    "            )\n",
    "            return Command(\n",
    "                update={\"messages\": messages + [msg]},\n",
    "                goto=END,\n",
    "            )\n",
    "\n",
    "        # 3b. otherwise select those tools and move on\n",
    "        selected_ids = [doc.id for doc in filtered]\n",
    "        print(\"tools selected\", selected_ids, flush=True)\n",
    "        return Command(\n",
    "            update={\n",
    "                \"selected_tools\": selected_ids,\n",
    "            },\n",
    "            goto=\"research_agent\",\n",
    "        )\n",
    "\n",
    "    # Warning: This executes code locally, which can be unsafe when not sandboxed\n",
    "\n",
    "    repl = PythonREPL()\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "    @tool\n",
    "    @instrument(\n",
    "        span_type=\"PYTHON_REPL_TOOL\",\n",
    "        attributes={\n",
    "            f\"{BASE_SCOPE}.python_tool_input_code\": \"code\",\n",
    "        },\n",
    "    )\n",
    "    def python_repl_tool(code: str):\n",
    "        \"\"\"\n",
    "        Run arbitrary Python, grab the CURRENT matplotlib figure (if any),\n",
    "        save it to ./langgraph_saved_images_snowflaketools/v1/chart_<uuid>.png,\n",
    "        and return a first-line `CHART_PATH=…`.\n",
    "        \"\"\"\n",
    "        import os\n",
    "        import uuid\n",
    "\n",
    "        import matplotlib\n",
    "\n",
    "        matplotlib.use(\"Agg\")\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        # 1) Run the user’s code\n",
    "        repl.run(code)\n",
    "\n",
    "        # 2) Check for a figure\n",
    "        fig = plt.gcf()\n",
    "        if fig.axes:\n",
    "            target_dir = \"./langgraph_saved_images_snowflaketools/v1\"\n",
    "            os.makedirs(target_dir, exist_ok=True)\n",
    "            path = os.path.join(target_dir, f\"chart_{uuid.uuid4().hex}.png\")\n",
    "            print(path, flush=True)\n",
    "            fig.savefig(path, format=\"png\")\n",
    "            plt.close(fig)\n",
    "        else:\n",
    "            path = \"NONE\"\n",
    "\n",
    "        # 3) Return only the CHART_PATH line\n",
    "        return f\"CHART_PATH={path}\\n\"\n",
    "\n",
    "    def get_next_node(last_message: BaseMessage, goto: str):\n",
    "        if \"FINAL ANSWER\" in last_message.content:\n",
    "            # Any agent decided the work is done\n",
    "            return END\n",
    "        return goto\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"RESEARCH_NODE\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            f\"{BASE_SCOPE}.research_node_input\": args[0][\"messages\"][\n",
    "                -1\n",
    "            ].content,\n",
    "            f\"{BASE_SCOPE}.research_node_response\": ret.update[\"messages\"][\n",
    "                -1\n",
    "            ].content\n",
    "            if hasattr(ret, \"update\")\n",
    "            else json.dumps(ret, indent=4, sort_keys=True),\n",
    "            f\"{BASE_SCOPE}.tool_messages\": [\n",
    "                dumps(message)\n",
    "                for message in ret.update[\"messages\"]\n",
    "                if isinstance(message, ToolMessage)\n",
    "            ]\n",
    "            if hasattr(ret, \"update\")\n",
    "            else \"No tool call\",\n",
    "        },\n",
    "    )\n",
    "    @instrument(\n",
    "        span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            SpanAttributes.RETRIEVAL.QUERY_TEXT: args[0][\"messages\"][\n",
    "                -1\n",
    "            ].content,\n",
    "            SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS: [\n",
    "                msg.content\n",
    "                for msg in ret.update[\"messages\"]\n",
    "                if isinstance(msg, ToolMessage) and msg.content\n",
    "            ]\n",
    "            if hasattr(ret, \"update\") and \"messages\" in ret.update\n",
    "            else [],\n",
    "        },\n",
    "    )\n",
    "    def research_agent_node(\n",
    "        state: ToolState,\n",
    "    ) -> Command[Literal[\"chart_generator\"]]:\n",
    "        print(state[\"remaining_steps\"])\n",
    "        if state[\"remaining_steps\"] <= 2:\n",
    "            print(\"Bailing out\", flush=True)\n",
    "            return Command(\n",
    "                update={\"messages\": state[\"messages\"]},\n",
    "                goto=\"canned_end\",\n",
    "            )\n",
    "        sys.__stdout__.write(\"🔍 [research_agent_node] start\\n\")\n",
    "        sys.__stdout__.flush()\n",
    "\n",
    "        # 1) bind & invoke as before\n",
    "        selected_tools = [tool_registry[tid] for tid in state[\"selected_tools\"]]\n",
    "        bound_llm = llm.bind_tools(selected_tools)\n",
    "        bound_agent = create_react_agent(\n",
    "            bound_llm,\n",
    "            tools=selected_tools,\n",
    "            prompt=make_system_prompt(\"You can only do research…\"),\n",
    "        )\n",
    "\n",
    "        sys.__stdout__.write(\"  ⏳ invoking bound_agent.invoke()\\n\")\n",
    "        sys.__stdout__.flush()\n",
    "        result = bound_agent.invoke(state)\n",
    "\n",
    "        # 2) debug‐dump\n",
    "        sys.__stdout__.write(\"  📬 raw research messages:\\n\")\n",
    "        for m in result[\"messages\"]:\n",
    "            sys.__stdout__.write(\n",
    "                f\"    [{m.__class__.__name__}] {getattr(m, 'content', m)!r}\\n\"\n",
    "            )\n",
    "        sys.__stdout__.flush()\n",
    "\n",
    "        # 3) strip out any tool_calls on those messages\n",
    "        clean_messages = []\n",
    "        for msg in result[\"messages\"]:\n",
    "            # deep‐copy so we don't mutate the original if you care\n",
    "            m = deepcopy(msg)\n",
    "            if hasattr(m, \"tool_calls\"):\n",
    "                # either empty the list or delete the attr altogether\n",
    "                m.tool_calls = []\n",
    "            clean_messages.append(m)\n",
    "\n",
    "        # 4) routing\n",
    "        last = clean_messages[-1]\n",
    "        goto = get_next_node(last, \"chart_generator\")\n",
    "        sys.__stdout__.write(f\"  ➡ next goto = {goto}\\n\\n\")\n",
    "        sys.__stdout__.flush()\n",
    "\n",
    "        # 5) tag the final message as coming from your research agent\n",
    "        clean_messages[-1] = HumanMessage(\n",
    "            content=last.content, name=\"research_agent\"\n",
    "        )\n",
    "\n",
    "        return Command(\n",
    "            update={\n",
    "                \"messages\": clean_messages,\n",
    "                \"chart_path\": state.get(\"chart_path\", \"\"),\n",
    "            },\n",
    "            goto=goto,\n",
    "        )\n",
    "\n",
    "    # Chart generator agent and node\n",
    "    # NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION, WHICH CAN BE UNSAFE WHEN NOT SANDBOXED\n",
    "    # 1) Define the chart‐agent: it only returns JSON with a \"code\" field\n",
    "    chart_agent = create_react_agent(\n",
    "        llm,\n",
    "        [python_repl_tool],\n",
    "        prompt=make_system_prompt(\n",
    "            \"\"\"You can only generate charts by returning a single JSON object, for example:\n",
    "        {\n",
    "        \"code\": \"<your python plotting code here>\"\n",
    "        }\n",
    "        —where <your python plotting code> uses matplotlib to create exactly one figure.\n",
    "        Do NOT include any prose or tool‐call wrappers.\"\"\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    def extract_chart_path(text: str) -> str | None:\n",
    "        \"\"\"\n",
    "        Scan every line of tool stdout for 'CHART_PATH=' and return\n",
    "        whatever follows, trimmed.  Returns None if no such line exists.\n",
    "        \"\"\"\n",
    "        for line in text.splitlines():\n",
    "            if \"CHART_PATH=\" in line:\n",
    "                # split on the first '=', strip whitespace\n",
    "                return line.split(\"CHART_PATH=\", 1)[1].strip()\n",
    "        return None\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"CHART_GENERATOR_NODE\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            f\"{BASE_SCOPE}.chart_node_input\": args[0][\"messages\"][-1].content,\n",
    "            f\"{BASE_SCOPE}.chart_node_response\": (\n",
    "                ret.update[\"messages\"][-1].content\n",
    "                if ret and hasattr(ret, \"update\") and ret.update\n",
    "                else \"No update response\"\n",
    "            ),\n",
    "        },\n",
    "    )\n",
    "    def chart_node(state: ToolState) -> Command[Literal[\"chart_summarizer\"]]:\n",
    "        print(state[\"remaining_steps\"])\n",
    "        if state[\"remaining_steps\"] <= 2:\n",
    "            print(\"Bailing out\", flush=True)\n",
    "            return Command(\n",
    "                update={\"messages\": state[\"messages\"]},\n",
    "                goto=\"canned_end\",\n",
    "            )\n",
    "        # 0) If a path is already in state, skip\n",
    "        # extract the current human query\n",
    "        current_query = state[\"messages\"][-1].content\n",
    "\n",
    "        # if we already generated a chart for _this_ query, skip\n",
    "        if state.get(\"last_query\") == current_query and state.get(\"chart_path\"):\n",
    "            print(\n",
    "                f\"⚡️ skipping chart_node, existing path = {state['chart_path']}\",\n",
    "                flush=True,\n",
    "            )\n",
    "            return Command(\n",
    "                update={\"messages\": state[\"messages\"]}, goto=\"chart_summarizer\"\n",
    "            )\n",
    "\n",
    "        # it's a new query (or first run) → clear any old chart_path and remember this query\n",
    "        state.pop(\"chart_path\", None)\n",
    "        state[\"last_query\"] = current_query\n",
    "\n",
    "        # 1) Remember how many messages we had\n",
    "        len_before = len(state[\"messages\"])\n",
    "\n",
    "        # 2) Run the agent exactly once\n",
    "        agent_out = chart_agent.invoke(state)\n",
    "\n",
    "        print(agent_out, flush=True)\n",
    "        all_msgs = agent_out[\"messages\"]\n",
    "\n",
    "        # 3) Look at only the brand-new messages for our chart tool output\n",
    "        new_segment = all_msgs[len_before:]\n",
    "        tool_msgs = [\n",
    "            m\n",
    "            for m in new_segment\n",
    "            if isinstance(m, ToolMessage) and \"CHART_PATH=\" in m.content\n",
    "        ]\n",
    "\n",
    "        if not tool_msgs:\n",
    "            # If none found, trigger your retry logic\n",
    "            print(\n",
    "                \"⚠️ chart_node: no CHART_PATH in new messages, retrying\",\n",
    "                flush=True,\n",
    "            )\n",
    "            print(state[\"remaining_steps\"])\n",
    "            if state[\"remaining_steps\"] <= 2:\n",
    "                print(\"Bailing out\", flush=True)\n",
    "                return Command(\n",
    "                    update={\"messages\": state[\"messages\"]},\n",
    "                    goto=\"canned_end\",\n",
    "                )\n",
    "            return Command(\n",
    "                update={\"messages\": state[\"messages\"]},\n",
    "                goto=\"research_agent\",\n",
    "            )\n",
    "\n",
    "        # 4) Parse the last one in case there are multiples\n",
    "        tool_msg = tool_msgs[-1]\n",
    "        tool_stdout = tool_msg.content\n",
    "        print(f\"chart_node 🖨 tool_stdout:\\n{tool_stdout}\", flush=True)\n",
    "\n",
    "        chart_path = extract_chart_path(tool_stdout)\n",
    "        print(f\"chart_node 📂 parsed chart_path = {chart_path!r}\", flush=True)\n",
    "        # 5) Build your new messages list: include only that new ToolMessage\n",
    "        new_msgs = state[\"messages\"][:] + [tool_msg]\n",
    "\n",
    "        # 6) Success! stash path into state and append the CHART_PATH marker\n",
    "        new_msgs.append(\n",
    "            HumanMessage(\n",
    "                content=f\"CHART_PATH={chart_path}\", name=\"chart_generator\"\n",
    "            )\n",
    "        )\n",
    "        return Command(\n",
    "            update={\"messages\": new_msgs, \"chart_path\": chart_path},\n",
    "            goto=\"chart_summarizer\",\n",
    "        )\n",
    "\n",
    "    reflection_prompt_template = PromptTemplate(\n",
    "        input_variables=[\"user_query\", \"chart_summary\"],\n",
    "        template=\"\"\"\\\n",
    "        You are an AI assistant tasked with reflecting on the quality of a chart summary. The user has asked the following question:\n",
    "        \"{user_query}\"\n",
    "\n",
    "        You are given the following chart summary:\n",
    "        \"{chart_summary}\"\n",
    "\n",
    "        Your task is to evaluate how well the chart summary answers the user's question. Consider the following:\n",
    "        - Does the summary capture the **key insights** and trends from the chart, even if in a more general form?\n",
    "        - Does it provide **adequate context** to address the user's query, even if it's not exhaustive?\n",
    "        - If the summary provides some context but could benefit from more details, consider it sufficient for now unless significant details are missing.\n",
    "\n",
    "        If the summary **generally** addresses the question, respond with 'Task complete'. If the summary **lacks significant** details or clarity, then respond with specific details on how the answer should be improved and what information is needed. Avoid being overly critical unless the summary completely misses key elements necessary to answer the query.\n",
    "\n",
    "        Please provide your answer in a **concise and encouraging** manner.\n",
    "        \"\"\",\n",
    "    )\n",
    "\n",
    "    # Create the chain using the prompt template and the LLM (ChatOpenAI)\n",
    "    reflection_chain = reflection_prompt_template | llm\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"CHART_SUMMARY_REFLECTION\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            f\"{BASE_SCOPE}.chart_summary_reflection_input_user_query\": args[0],\n",
    "            f\"{BASE_SCOPE}.chart_summary_reflection_input_chart_summary\": args[\n",
    "                1\n",
    "            ],\n",
    "            f\"{BASE_SCOPE}.chart_summary_reflection_response\": ret,\n",
    "        },\n",
    "    )\n",
    "    def perform_reflection(user_query: str, chart_summary: str) -> str:\n",
    "        \"\"\"\n",
    "        This function uses an LLM to reflect on the quality of a chart summary\n",
    "        and determine if the task is complete or requires further refinement.\n",
    "        \"\"\"\n",
    "        print(\"doing reflection...\")\n",
    "        # Call the chain with the user query and chart summary\n",
    "        reflection_result = reflection_chain.invoke({\n",
    "            \"user_query\": user_query,\n",
    "            \"chart_summary\": chart_summary,\n",
    "        })\n",
    "        print(\"reflection_result: \", reflection_result.content)\n",
    "        return reflection_result.content\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"CHART_SUMMARY_NODE\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            f\"{BASE_SCOPE}.summary_node_input\": args[0][\"messages\"][-1].content,\n",
    "            f\"{BASE_SCOPE}.summary_node_output\": (\n",
    "                ret.update[\"messages\"][-1].content\n",
    "                if hasattr(ret, \"update\")\n",
    "                else \"NO SUMMARY GENERATED\"\n",
    "            ),\n",
    "        },\n",
    "    )\n",
    "    def chart_summary_node(state: ToolState) -> Command[Literal[\"__end__\"]]:\n",
    "        print(\"▶️ entering chart_summary_node\", flush=True)\n",
    "        print(state[\"remaining_steps\"])\n",
    "        if state[\"remaining_steps\"] <= 2:\n",
    "            print(\"Bailing out\", flush=True)\n",
    "            return Command(\n",
    "                update={\"messages\": state[\"messages\"]},\n",
    "                goto=\"canned_end\",\n",
    "            )\n",
    "\n",
    "        # 1) find the chart_path in state\n",
    "        chart_path = state.get(\"chart_path\", \"\")\n",
    "        print(f\"  using state.chart_path = {chart_path!r}\", flush=True)\n",
    "        if not chart_path:\n",
    "            return Command(\n",
    "                update={\n",
    "                    \"messages\": state[\"messages\"]\n",
    "                    + [\n",
    "                        HumanMessage(\n",
    "                            \"No valid chart was generated. Please try again.\",\n",
    "                            name=\"chart_summarizer\",\n",
    "                        )\n",
    "                    ]\n",
    "                },\n",
    "                goto=\"select_tools\",\n",
    "            )\n",
    "\n",
    "        # 2) strip *everything* except human utterances\n",
    "        human_history = [\n",
    "            m for m in state[\"messages\"] if isinstance(m, HumanMessage)\n",
    "        ]\n",
    "\n",
    "        # ensure our CHART_PATH marker is last\n",
    "        if not human_history or not human_history[-1].content.startswith(\n",
    "            \"CHART_PATH=\"\n",
    "        ):\n",
    "            human_history.append(\n",
    "                HumanMessage(\n",
    "                    f\"CHART_PATH={chart_path}\", name=\"chart_summarizer\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            \"  human_history:\", [m.content for m in human_history], flush=True\n",
    "        )\n",
    "\n",
    "        # 3) build your ChatCompletion prompt\n",
    "        system = SystemMessage(\n",
    "            content=make_system_prompt(\n",
    "                \"You are an AI assistant whose *only* job is to summarise a chart image. \"\n",
    "                \"Input is a message CHART_PATH=… pointing at a saved PNG. \"\n",
    "                \"Output a concise (≤3 sentences) summary of the key trends.\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        messages_for_llm = (\n",
    "            [system]\n",
    "            + human_history\n",
    "            + [\n",
    "                HumanMessage(\n",
    "                    \"Please summarise the above chart in ≤3 sentences.\"\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 4) call the LLM directly—no tools, no React agent\n",
    "        print(\"📝 calling ChatOpenAI directly for summary\", flush=True)\n",
    "        ai_msg: AIMessage = llm(messages_for_llm)\n",
    "        summary = ai_msg.content\n",
    "        print(f\"📋 chart summary: {summary!r}\", flush=True)\n",
    "\n",
    "        # 5) reflect as before\n",
    "        user_query = state[\"messages\"][0].content\n",
    "        print(\"🔍 reflecting on summary quality\", flush=True)\n",
    "        reflection = perform_reflection(user_query, summary)\n",
    "        clean_ref = reflection.strip().lower()\n",
    "        print(f\"💡 reflection: {reflection!r}\", flush=True)\n",
    "\n",
    "        # 6) decide where to go\n",
    "        if \"task complete\" in clean_ref:\n",
    "            print(\"✅ done\", flush=True)\n",
    "            return Command(\n",
    "                update={\n",
    "                    \"messages\": state[\"messages\"]\n",
    "                    + [HumanMessage(summary, name=\"chart_summarizer\")]\n",
    "                },\n",
    "                goto=END,\n",
    "            )\n",
    "        else:\n",
    "            print(\"🔁 need to retry\", flush=True)\n",
    "            return Command(\n",
    "                update={\n",
    "                    \"messages\": state[\"messages\"]\n",
    "                    + [\n",
    "                        HumanMessage(summary, name=\"chart_summarizer\"),\n",
    "                        HumanMessage(reflection, name=\"chart_reflection\"),\n",
    "                    ]\n",
    "                },\n",
    "                goto=\"select_tools\",\n",
    "            )\n",
    "\n",
    "    workflow = StateGraph(ToolState)\n",
    "    workflow.add_node(\"select_tools\", select_tools)\n",
    "    workflow.add_node(\"research_agent\", research_agent_node)\n",
    "    workflow.add_node(\"chart_generator\", chart_node)\n",
    "    workflow.add_node(\"chart_summarizer\", chart_summary_node)\n",
    "    workflow.add_node(\"canned_end\", canned_end_node)\n",
    "\n",
    "    # Update transitions: begin with tool selection then go to research agent.\n",
    "    workflow.add_edge(START, \"select_tools\")\n",
    "\n",
    "    # workflow.add_edge(\"select_tools\", END)\n",
    "    workflow.add_edge(\"select_tools\", \"research_agent\")\n",
    "    workflow.add_edge(\"research_agent\", \"chart_generator\")\n",
    "    workflow.add_edge(\"chart_generator\", \"chart_summarizer\")\n",
    "    workflow.add_edge(\"chart_summarizer\", END)\n",
    "\n",
    "    workflow.add_edge(\"canned_end\", END)\n",
    "\n",
    "    compiled_graph = workflow.compile()\n",
    "\n",
    "    return compiled_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca89763c",
   "metadata": {},
   "source": [
    "## Register the agent and create a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58ffc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TruAgent:\n",
    "    def __init__(self):\n",
    "        self.graph = build_graph()\n",
    "\n",
    "    @instrument(\n",
    "        span_type=SpanAttributes.SpanType.RECORD_ROOT,\n",
    "        attributes={\n",
    "            SpanAttributes.RECORD_ROOT.INPUT: \"query\",\n",
    "            SpanAttributes.RECORD_ROOT.OUTPUT: \"return\",\n",
    "        },\n",
    "    )\n",
    "    def invoke_agent_graph(self, query: str) -> str:\n",
    "        try:\n",
    "            # rebuild the graph for each query\n",
    "            self.graph = build_graph()\n",
    "            # Initialize state with proper message format\n",
    "            state = {\"messages\": [HumanMessage(content=query)]}\n",
    "\n",
    "            # Stream events with recursion limit\n",
    "            events = self.graph.stream(\n",
    "                state,\n",
    "                {\"recursion_limit\": 15},\n",
    "            )\n",
    "\n",
    "            # Track all messages through the conversation\n",
    "            all_messages = []\n",
    "            for event in events:\n",
    "                # Get the payload from the event\n",
    "                _, payload = next(iter(event.items()))\n",
    "                if not payload:  # Skip empty payloads\n",
    "                    continue\n",
    "\n",
    "                messages = payload.get(\"messages\")\n",
    "                if not messages:\n",
    "                    continue\n",
    "                all_messages.extend(messages)\n",
    "\n",
    "            # Return the last message's content if available\n",
    "            return (\n",
    "                all_messages[-1].content\n",
    "                if all_messages and hasattr(all_messages[-1], \"content\")\n",
    "                else \"\"\n",
    "            )\n",
    "        except:\n",
    "            return \"I ran into an issue, and cannot answer your question.\"\n",
    "\n",
    "\n",
    "tru_agent = TruAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b361154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_agent_app = TruApp(\n",
    "    tru_agent,\n",
    "    app_name=APP_NAME,\n",
    "    app_version=\"web search\",\n",
    "    connector=trulens_sf_connector,\n",
    "    main_method=tru_agent.invoke_agent_graph,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe95b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_1 = datetime.datetime.fromtimestamp(time.time()).strftime(\n",
    "    \"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    run_name=\"Multi-agent demo run\",\n",
    "    description=\"this is a run with access to web search and charting capabilities\",\n",
    "    dataset_name=\"Research test dataset\",\n",
    "    source_type=\"DATAFRAME\",\n",
    "    label=\"langgraph demo\",\n",
    "    dataset_spec={\n",
    "        \"RECORD_ROOT.INPUT\": \"query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "run: Run = tru_agent_app.add_run(run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed35594",
   "metadata": {},
   "source": [
    "## Display the agent's graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c325b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(tru_agent.graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad18dba0",
   "metadata": {},
   "source": [
    "## Start the run\n",
    "\n",
    "This runs the agent in batch using the queries in the `input_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba1cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "user_queries = [\n",
    "    \"In 2023, how did the fed funds rate fluctuate? What were the key drivers? Create a line chart that illustrates this data, including a caption with the key drivers.\",\n",
    "    \"What is the total holding value reported by BlackRock Fund Advisors in their SEC filings during 2023?? Create a line chart that illustrates this data\",\n",
    "]\n",
    "\n",
    "user_queries_df = pd.DataFrame(user_queries, columns=[\"query\"])\n",
    "\n",
    "run.start(input_df=user_queries_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1a8c8f",
   "metadata": {},
   "source": [
    "## Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3d1614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while run.get_status() == \"INVOCATION_IN_PROGRESS\":\n",
    "    time.sleep(3)\n",
    "\n",
    "run.compute_metrics([\"groundedness\", \"context_relevance\", \"answer_relevance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e580b028",
   "metadata": {},
   "source": [
    "Web is not as precise as it could be if it had access to private minutes data. Let's supplement web search with a document search.\n",
    "\n",
    "## Add Cortex Search to the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765e30a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.core import Root\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "\n",
    "class CortexSearchArgs(BaseModel):\n",
    "    query: str\n",
    "\n",
    "\n",
    "# --- Define a new Cortex Search Tool to perform document search via Cortex ---\n",
    "class CortexSearchTool(StructuredTool):\n",
    "    name: str = \"CortexSearch\"\n",
    "    description: str = \"Searches documents using Cortex Search via Snowflake.\"\n",
    "    args_schema: type[BaseModel] = CortexSearchArgs\n",
    "    session: Session\n",
    "\n",
    "    def run(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Executes a search query using the Cortex Search service in Snowflake.\n",
    "\n",
    "        Args:\n",
    "            query (str): The search query string.\n",
    "\n",
    "        Returns:\n",
    "            str: A JSON string containing the search results, limited to 10 entries.\n",
    "        \"\"\"\n",
    "        root = Root(self.session)\n",
    "        search_service = (\n",
    "            root.databases[\"CORTEX_SEARCH_TUTORIAL_DB\"]\n",
    "            .schemas[\"PUBLIC\"]\n",
    "            .cortex_search_services[\"FOMC_SEARCH_SERVICE\"]\n",
    "        )\n",
    "        resp = search_service.search(query=query, columns=[\"chunk\"], limit=10)\n",
    "        return resp.to_json()\n",
    "\n",
    "\n",
    "def build_graph_with_search():\n",
    "    def canned_end_node(state: ToolState) -> Command[Literal[\"__end__\"]]:\n",
    "        print(\"starting CANNED END\", flush=True)\n",
    "\n",
    "        return Command(goto=END)\n",
    "\n",
    "    def make_system_prompt(suffix: str) -> str:\n",
    "        return (\n",
    "            \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "            \" Use the provided tools to progress towards answering the question.\"\n",
    "            \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "            \" will help where you left off. Execute what you can to make progress.\"\n",
    "            \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "            \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "            f\"\\n{suffix}\"\n",
    "        )\n",
    "\n",
    "    search = SerpAPIWrapper()\n",
    "\n",
    "    search_tool = Tool(\n",
    "        name=\"web_search\",\n",
    "        description=\"Search the web for current information, such as weather or news\",\n",
    "        func=search.run,\n",
    "    )\n",
    "\n",
    "    # # Create document search tool using Cortex Search (uses your Snowflake session)\n",
    "    cortex_search_tool = CortexSearchTool(session=snowpark_session_trulens)\n",
    "\n",
    "    # wrap so sync-compatible\n",
    "    wrapped_cortex_search_tool = Tool(\n",
    "        name=cortex_search_tool.name,\n",
    "        description=cortex_search_tool.description,\n",
    "        func=cortex_search_tool.run,\n",
    "        return_direct=False,  # set to True only if you want the agent to stop after using it\n",
    "    )\n",
    "    # The tool registry now includes both the web and document search tools.\n",
    "    tool_registry = {\n",
    "        str(uuid.uuid4()): search_tool,\n",
    "        str(uuid.uuid4()): wrapped_cortex_search_tool,\n",
    "    }\n",
    "\n",
    "    # Index tool descriptions in a vector store for semantic tool retrieval\n",
    "    tool_documents = [\n",
    "        Document(\n",
    "            page_content=search_tool.name,  # your human-readable blurb\n",
    "            id=tool_id,  # must match the registry key\n",
    "            metadata={\n",
    "                \"tool_name\": tool.name,\n",
    "                \"tool_description\": tool.description,\n",
    "            },\n",
    "        )\n",
    "        for tool_id, tool in tool_registry.items()\n",
    "    ]\n",
    "    vector_store = InMemoryVectorStore(embedding=OpenAIEmbeddings())\n",
    "    vector_store.add_documents(tool_documents)\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"SELECT_TOOLS\",\n",
    "        attributes=lambda ret, exc, *args, **kw: {\n",
    "            # ---- state as JSON-text (OTLP needs a scalar) -----------------\n",
    "            f\"{BASE_SCOPE}.select_tools_input_state\": json.dumps(  # ← turns dict → str\n",
    "                {\n",
    "                    **{k: v for k, v in args[0].items() if k != \"messages\"},\n",
    "                    \"messages\": [\n",
    "                        {\"type\": m.__class__.__name__, \"content\": m.content}\n",
    "                        if hasattr(m, \"content\")  # BaseMessage subclasses\n",
    "                        else m  # already JSON-friendly\n",
    "                        for m in args[0].get(\"messages\", [])\n",
    "                    ],\n",
    "                }\n",
    "            ),\n",
    "            # ---- selected tool IDs as a simple comma-separated string -----\n",
    "            f\"{BASE_SCOPE}.selected_tool_ids\": \", \".join(\n",
    "                ret.get(\"selected_tools\", [])\n",
    "            )\n",
    "            if isinstance(ret, dict)\n",
    "            else \"\",\n",
    "        },\n",
    "    )\n",
    "    def select_tools(\n",
    "        state: ToolState,\n",
    "    ) -> Command[Literal[\"research_agent\", END]]:\n",
    "        messages = state[\"messages\"]\n",
    "        last = messages[-1]\n",
    "        query = last[\"content\"] if isinstance(last, dict) else last.content\n",
    "        print(\"selecting tools based on\", query)\n",
    "\n",
    "        # 1. pull top-k with their scores\n",
    "        results: list[tuple[Document, float]] = (\n",
    "            vector_store.similarity_search_with_score(\n",
    "                query,\n",
    "                k=5,  # look at top-5 candidates\n",
    "            )\n",
    "        )\n",
    "        print(\"tool search results\", results)\n",
    "\n",
    "        # 2. filter by minimum cosine-similarity\n",
    "        MIN_SIMILARITY = 0.7\n",
    "        filtered = [doc for doc, score in results if score >= MIN_SIMILARITY]\n",
    "\n",
    "        # 3a. no sufficiently similar tool → end\n",
    "        if not filtered:\n",
    "            print(\"no tool selected\")\n",
    "            msg = HumanMessage(\n",
    "                content=\"Sorry, I don’t have a tool that’s relevant enough to answer that.\",\n",
    "                name=\"assistant\",\n",
    "            )\n",
    "            return Command(\n",
    "                update={\"messages\": messages + [msg]},\n",
    "                goto=END,\n",
    "            )\n",
    "\n",
    "        # 3b. otherwise select those tools and move on\n",
    "        selected_ids = [doc.id for doc in filtered]\n",
    "        print(\"tools selected\", selected_ids)\n",
    "        return Command(\n",
    "            update={\n",
    "                \"selected_tools\": selected_ids,\n",
    "            },\n",
    "            goto=\"research_agent\",\n",
    "        )\n",
    "\n",
    "    # Warning: This executes code locally, which can be unsafe when not sandboxed\n",
    "\n",
    "    repl = PythonREPL()\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "    @tool\n",
    "    @instrument(\n",
    "        span_type=\"PYTHON_REPL_TOOL\",\n",
    "        attributes={\n",
    "            f\"{BASE_SCOPE}.python_tool_input_code\": \"code\",\n",
    "        },\n",
    "    )\n",
    "    def python_repl_tool(code: str):\n",
    "        \"\"\"\n",
    "        Run arbitrary Python, grab the CURRENT matplotlib figure (if any),\n",
    "        save it to ./langgraph_saved_images_snowflaketools/v1/chart_<uuid>.png,\n",
    "        and return a first-line `CHART_PATH=…`.\n",
    "        \"\"\"\n",
    "        import os\n",
    "        import uuid\n",
    "\n",
    "        import matplotlib\n",
    "\n",
    "        matplotlib.use(\"Agg\")\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        # 1) Run the user’s code\n",
    "        repl.run(code)\n",
    "\n",
    "        # 2) Check for a figure\n",
    "        fig = plt.gcf()\n",
    "        if fig.axes:\n",
    "            target_dir = \"./langgraph_saved_images_snowflaketools/v1\"\n",
    "            os.makedirs(target_dir, exist_ok=True)\n",
    "            path = os.path.join(target_dir, f\"chart_{uuid.uuid4().hex}.png\")\n",
    "            print(path, flush=True)\n",
    "            fig.savefig(path, format=\"png\")\n",
    "            plt.close(fig)\n",
    "        else:\n",
    "            path = \"NONE\"\n",
    "\n",
    "        # 3) Return only the CHART_PATH line\n",
    "        return f\"CHART_PATH={path}\\n\"\n",
    "\n",
    "    def get_next_node(last_message: BaseMessage, goto: str):\n",
    "        if \"FINAL ANSWER\" in last_message.content:\n",
    "            # Any agent decided the work is done\n",
    "            return END\n",
    "        return goto\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"RESEARCH_NODE\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            f\"{BASE_SCOPE}.research_node_input\": args[0][\"messages\"][\n",
    "                -1\n",
    "            ].content,\n",
    "            f\"{BASE_SCOPE}.research_node_response\": ret.update[\"messages\"][\n",
    "                -1\n",
    "            ].content\n",
    "            if hasattr(ret, \"update\")\n",
    "            else json.dumps(ret, indent=4, sort_keys=True),\n",
    "            f\"{BASE_SCOPE}.tool_messages\": [\n",
    "                dumps(message)\n",
    "                for message in ret.update[\"messages\"]\n",
    "                if isinstance(message, ToolMessage)\n",
    "            ]\n",
    "            if hasattr(ret, \"update\")\n",
    "            else \"No tool call\",\n",
    "        },\n",
    "    )\n",
    "    @instrument(\n",
    "        span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            SpanAttributes.RETRIEVAL.QUERY_TEXT: args[0][\"messages\"][\n",
    "                -1\n",
    "            ].content,\n",
    "            SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS: [\n",
    "                ret.update[\"messages\"][-1].content\n",
    "            ]\n",
    "            if hasattr(ret, \"update\")\n",
    "            else [json.dumps(ret, indent=4, sort_keys=True)],\n",
    "        },\n",
    "    )\n",
    "    def research_agent_node(\n",
    "        state: ToolState,\n",
    "    ) -> Command[Literal[\"chart_generator\"]]:\n",
    "        sys.__stdout__.write(\"🔍 [research_agent_node] start\\n\")\n",
    "        sys.__stdout__.flush()\n",
    "\n",
    "        # 1) bind & invoke as before\n",
    "        selected_tools = [tool_registry[tid] for tid in state[\"selected_tools\"]]\n",
    "        bound_llm = llm.bind_tools(selected_tools)\n",
    "        bound_agent = create_react_agent(\n",
    "            bound_llm,\n",
    "            tools=selected_tools,\n",
    "            prompt=make_system_prompt(\"You can only do research…\"),\n",
    "        )\n",
    "\n",
    "        sys.__stdout__.write(\"  ⏳ invoking bound_agent.invoke()\\n\")\n",
    "        sys.__stdout__.flush()\n",
    "        result = bound_agent.invoke(state)\n",
    "\n",
    "        # 2) debug‐dump\n",
    "        sys.__stdout__.write(\"  📬 raw research messages:\\n\")\n",
    "        for m in result[\"messages\"]:\n",
    "            sys.__stdout__.write(\n",
    "                f\"    [{m.__class__.__name__}] {getattr(m, 'content', m)!r}\\n\"\n",
    "            )\n",
    "        sys.__stdout__.flush()\n",
    "\n",
    "        # 3) strip out any tool_calls on those messages\n",
    "        clean_messages = []\n",
    "        for msg in result[\"messages\"]:\n",
    "            # deep‐copy so we don't mutate the original if you care\n",
    "            m = deepcopy(msg)\n",
    "            if hasattr(m, \"tool_calls\"):\n",
    "                # either empty the list or delete the attr altogether\n",
    "                m.tool_calls = []\n",
    "            clean_messages.append(m)\n",
    "\n",
    "        # 4) routing\n",
    "        last = clean_messages[-1]\n",
    "        goto = get_next_node(last, \"chart_generator\")\n",
    "        sys.__stdout__.write(f\"  ➡ next goto = {goto}\\n\\n\")\n",
    "        sys.__stdout__.flush()\n",
    "\n",
    "        # 5) tag the final message as coming from your research agent\n",
    "        clean_messages[-1] = HumanMessage(\n",
    "            content=last.content, name=\"research_agent\"\n",
    "        )\n",
    "\n",
    "        return Command(\n",
    "            update={\n",
    "                \"messages\": clean_messages,\n",
    "                \"chart_path\": state.get(\"chart_path\", \"\"),\n",
    "            },\n",
    "            goto=goto,\n",
    "        )\n",
    "\n",
    "    # Chart generator agent and node\n",
    "    # NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION, WHICH CAN BE UNSAFE WHEN NOT SANDBOXED\n",
    "    # 1) Define the chart‐agent: it only returns JSON with a \"code\" field\n",
    "    chart_agent = create_react_agent(\n",
    "        llm,\n",
    "        [python_repl_tool],\n",
    "        prompt=make_system_prompt(\n",
    "            \"\"\"You can only generate charts by returning a single JSON object, for example:\n",
    "        {\n",
    "        \"code\": \"<your python plotting code here>\"\n",
    "        }\n",
    "        —where <your python plotting code> uses matplotlib to create exactly one figure.\n",
    "        Do NOT include any prose or tool‐call wrappers.\"\"\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    def extract_chart_path(text: str) -> str | None:\n",
    "        \"\"\"\n",
    "        Scan every line of tool stdout for 'CHART_PATH=' and return\n",
    "        whatever follows, trimmed.  Returns None if no such line exists.\n",
    "        \"\"\"\n",
    "        for line in text.splitlines():\n",
    "            if \"CHART_PATH=\" in line:\n",
    "                # split on the first '=', strip whitespace\n",
    "                return line.split(\"CHART_PATH=\", 1)[1].strip()\n",
    "        return None\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"CHART_GENERATOR_NODE\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            f\"{BASE_SCOPE}.chart_node_input\": args[0][\"messages\"][-1].content,\n",
    "            f\"{BASE_SCOPE}.chart_node_response\": (\n",
    "                ret.update[\"messages\"][-1].content\n",
    "                if ret and hasattr(ret, \"update\") and ret.update\n",
    "                else \"No update response\"\n",
    "            ),\n",
    "        },\n",
    "    )\n",
    "    def chart_node(state: ToolState) -> Command[Literal[\"chart_summarizer\"]]:\n",
    "        print(state[\"remaining_steps\"])\n",
    "        if state[\"remaining_steps\"] <= 2:\n",
    "            print(\"Bailing out\", flush=True)\n",
    "            return Command(\n",
    "                update={\"messages\": state[\"messages\"]},\n",
    "                goto=\"canned_end\",\n",
    "            )\n",
    "        # 0) If a path is already in state, skip\n",
    "        # extract the current human query\n",
    "        current_query = state[\"messages\"][-1].content\n",
    "\n",
    "        # if we already generated a chart for _this_ query, skip\n",
    "        if state.get(\"last_query\") == current_query and state.get(\"chart_path\"):\n",
    "            print(\n",
    "                f\"⚡️ skipping chart_node, existing path = {state['chart_path']}\",\n",
    "                flush=True,\n",
    "            )\n",
    "            return Command(\n",
    "                update={\"messages\": state[\"messages\"]}, goto=\"chart_summarizer\"\n",
    "            )\n",
    "\n",
    "        # it's a new query (or first run) → clear any old chart_path and remember this query\n",
    "        state.pop(\"chart_path\", None)\n",
    "        state[\"last_query\"] = current_query\n",
    "\n",
    "        # 1) Remember how many messages we had\n",
    "        len_before = len(state[\"messages\"])\n",
    "\n",
    "        # 2) Run the agent exactly once\n",
    "        agent_out = chart_agent.invoke(state)\n",
    "\n",
    "        print(agent_out, flush=True)\n",
    "        all_msgs = agent_out[\"messages\"]\n",
    "\n",
    "        # 3) Look at only the brand-new messages for our chart tool output\n",
    "        new_segment = all_msgs[len_before:]\n",
    "        tool_msgs = [\n",
    "            m\n",
    "            for m in new_segment\n",
    "            if isinstance(m, ToolMessage) and \"CHART_PATH=\" in m.content\n",
    "        ]\n",
    "\n",
    "        if not tool_msgs:\n",
    "            # If none found, trigger your retry logic\n",
    "            print(\n",
    "                \"⚠️ chart_node: no CHART_PATH in new messages, retrying\",\n",
    "                flush=True,\n",
    "            )\n",
    "            print(state[\"remaining_steps\"])\n",
    "            if state[\"remaining_steps\"] <= 2:\n",
    "                print(\"Bailing out\", flush=True)\n",
    "                return Command(\n",
    "                    update={\"messages\": state[\"messages\"]},\n",
    "                    goto=\"canned_end\",\n",
    "                )\n",
    "            return Command(\n",
    "                update={\"messages\": state[\"messages\"]},\n",
    "                goto=\"research_agent\",\n",
    "            )\n",
    "\n",
    "        # 4) Parse the last one in case there are multiples\n",
    "        tool_msg = tool_msgs[-1]\n",
    "        tool_stdout = tool_msg.content\n",
    "        print(f\"chart_node 🖨 tool_stdout:\\n{tool_stdout}\", flush=True)\n",
    "\n",
    "        chart_path = extract_chart_path(tool_stdout)\n",
    "        print(f\"chart_node 📂 parsed chart_path = {chart_path!r}\", flush=True)\n",
    "        # 5) Build your new messages list: include only that new ToolMessage\n",
    "        new_msgs = state[\"messages\"][:] + [tool_msg]\n",
    "\n",
    "        # 6) Success! stash path into state and append the CHART_PATH marker\n",
    "        new_msgs.append(\n",
    "            HumanMessage(\n",
    "                content=f\"CHART_PATH={chart_path}\", name=\"chart_generator\"\n",
    "            )\n",
    "        )\n",
    "        return Command(\n",
    "            update={\"messages\": new_msgs, \"chart_path\": chart_path},\n",
    "            goto=\"chart_summarizer\",\n",
    "        )\n",
    "\n",
    "    reflection_prompt_template = PromptTemplate(\n",
    "        input_variables=[\"user_query\", \"chart_summary\"],\n",
    "        template=\"\"\"\\\n",
    "        You are an AI assistant tasked with reflecting on the quality of a chart summary. The user has asked the following question:\n",
    "        \"{user_query}\"\n",
    "\n",
    "        You are given the following chart summary:\n",
    "        \"{chart_summary}\"\n",
    "\n",
    "        Your task is to evaluate how well the chart summary answers the user's question. Consider the following:\n",
    "        - Does the summary capture the **key insights** and trends from the chart, even if in a more general form?\n",
    "        - Does it provide **adequate context** to address the user's query, even if it's not exhaustive?\n",
    "        - If the summary provides some context but could benefit from more details, consider it sufficient for now unless significant details are missing.\n",
    "\n",
    "        If the summary **generally** addresses the question, respond with 'Task complete'. If the summary **lacks significant** details or clarity, then respond with specific details on how the answer should be improved and what information is needed. Avoid being overly critical unless the summary completely misses key elements necessary to answer the query.\n",
    "\n",
    "        Please provide your answer in a **concise and encouraging** manner.\n",
    "        \"\"\",\n",
    "    )\n",
    "\n",
    "    # Create the chain using the prompt template and the LLM (ChatOpenAI)\n",
    "    reflection_chain = reflection_prompt_template | llm\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"CHART_SUMMARY_REFLECTION\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            f\"{BASE_SCOPE}.chart_summary_reflection_input_user_query\": args[0],\n",
    "            f\"{BASE_SCOPE}.chart_summary_reflection_input_chart_summary\": args[\n",
    "                1\n",
    "            ],\n",
    "            f\"{BASE_SCOPE}.chart_summary_reflection_response\": ret,\n",
    "        },\n",
    "    )\n",
    "    def perform_reflection(user_query: str, chart_summary: str) -> str:\n",
    "        \"\"\"\n",
    "        This function uses an LLM to reflect on the quality of a chart summary\n",
    "        and determine if the task is complete or requires further refinement.\n",
    "        \"\"\"\n",
    "        print(\"doing reflection...\")\n",
    "        # Call the chain with the user query and chart summary\n",
    "        reflection_result = reflection_chain.invoke({\n",
    "            \"user_query\": user_query,\n",
    "            \"chart_summary\": chart_summary,\n",
    "        })\n",
    "        print(\"reflection_result: \", reflection_result.content)\n",
    "        return reflection_result.content\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"CHART_SUMMARY_NODE\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            f\"{BASE_SCOPE}.summary_node_input\": args[0][\"messages\"][-1].content,\n",
    "            f\"{BASE_SCOPE}.summary_node_output\": (\n",
    "                ret.update[\"messages\"][-1].content\n",
    "                if hasattr(ret, \"update\")\n",
    "                else \"NO SUMMARY GENERATED\"\n",
    "            ),\n",
    "        },\n",
    "    )\n",
    "    def chart_summary_node(state: ToolState) -> Command[Literal[\"__end__\"]]:\n",
    "        print(\"▶️ entering chart_summary_node\", flush=True)\n",
    "        print(state[\"remaining_steps\"])\n",
    "        if state[\"remaining_steps\"] <= 2:\n",
    "            print(\"Bailing out\", flush=True)\n",
    "            return Command(\n",
    "                update={\"messages\": state[\"messages\"]},\n",
    "                goto=\"canned_end\",\n",
    "            )\n",
    "\n",
    "        # 1) find the chart_path in state\n",
    "        chart_path = state.get(\"chart_path\", \"\")\n",
    "        print(f\"  using state.chart_path = {chart_path!r}\", flush=True)\n",
    "        if not chart_path:\n",
    "            return Command(\n",
    "                update={\n",
    "                    \"messages\": state[\"messages\"]\n",
    "                    + [\n",
    "                        HumanMessage(\n",
    "                            \"No valid chart was generated. Please try again.\",\n",
    "                            name=\"chart_summarizer\",\n",
    "                        )\n",
    "                    ]\n",
    "                },\n",
    "                goto=\"select_tools\",\n",
    "            )\n",
    "\n",
    "        # 2) strip *everything* except human utterances\n",
    "        human_history = [\n",
    "            m for m in state[\"messages\"] if isinstance(m, HumanMessage)\n",
    "        ]\n",
    "\n",
    "        # ensure our CHART_PATH marker is last\n",
    "        if not human_history or not human_history[-1].content.startswith(\n",
    "            \"CHART_PATH=\"\n",
    "        ):\n",
    "            human_history.append(\n",
    "                HumanMessage(\n",
    "                    f\"CHART_PATH={chart_path}\", name=\"chart_summarizer\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            \"  human_history:\", [m.content for m in human_history], flush=True\n",
    "        )\n",
    "\n",
    "        # 3) build your ChatCompletion prompt\n",
    "        system = SystemMessage(\n",
    "            content=make_system_prompt(\n",
    "                \"You are an AI assistant whose *only* job is to summarise a chart image. \"\n",
    "                \"Input is a message CHART_PATH=… pointing at a saved PNG. \"\n",
    "                \"Output a concise (≤3 sentences) summary of the key trends.\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        messages_for_llm = (\n",
    "            [system]\n",
    "            + human_history\n",
    "            + [\n",
    "                HumanMessage(\n",
    "                    \"Please summarise the above chart in ≤3 sentences.\"\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 4) call the LLM directly—no tools, no React agent\n",
    "        print(\"📝 calling ChatOpenAI directly for summary\", flush=True)\n",
    "        ai_msg: AIMessage = llm(messages_for_llm)\n",
    "        summary = ai_msg.content\n",
    "        print(f\"📋 chart summary: {summary!r}\", flush=True)\n",
    "\n",
    "        # 5) reflect as before\n",
    "        user_query = state[\"messages\"][0].content\n",
    "        print(\"🔍 reflecting on summary quality\", flush=True)\n",
    "        reflection = perform_reflection(user_query, summary)\n",
    "        clean_ref = reflection.strip().lower()\n",
    "        print(f\"💡 reflection: {reflection!r}\", flush=True)\n",
    "\n",
    "        # 6) decide where to go\n",
    "        if \"task complete\" in clean_ref:\n",
    "            print(\"✅ done\", flush=True)\n",
    "            return Command(\n",
    "                update={\n",
    "                    \"messages\": state[\"messages\"]\n",
    "                    + [HumanMessage(summary, name=\"chart_summarizer\")]\n",
    "                },\n",
    "                goto=END,\n",
    "            )\n",
    "        else:\n",
    "            print(\"🔁 need to retry\", flush=True)\n",
    "            return Command(\n",
    "                update={\n",
    "                    \"messages\": state[\"messages\"]\n",
    "                    + [\n",
    "                        HumanMessage(summary, name=\"chart_summarizer\"),\n",
    "                        HumanMessage(reflection, name=\"chart_reflection\"),\n",
    "                    ]\n",
    "                },\n",
    "                goto=\"select_tools\",\n",
    "            )\n",
    "\n",
    "    workflow = StateGraph(ToolState)\n",
    "    workflow.add_node(\"select_tools\", select_tools)\n",
    "    workflow.add_node(\"research_agent\", research_agent_node)\n",
    "    workflow.add_node(\"chart_generator\", chart_node)\n",
    "    workflow.add_node(\"chart_summarizer\", chart_summary_node)\n",
    "    workflow.add_node(\"canned_end\", canned_end_node)\n",
    "\n",
    "    # Update transitions: begin with tool selection then go to research agent.\n",
    "    workflow.add_edge(START, \"select_tools\")\n",
    "\n",
    "    # workflow.add_edge(\"select_tools\", END)\n",
    "    workflow.add_edge(\"select_tools\", \"research_agent\")\n",
    "    workflow.add_edge(\"research_agent\", \"chart_generator\")\n",
    "    workflow.add_edge(\"chart_generator\", \"chart_summarizer\")\n",
    "    workflow.add_edge(\"chart_summarizer\", END)\n",
    "\n",
    "    workflow.add_edge(\"canned_end\", END)\n",
    "\n",
    "    compiled_graph = workflow.compile()\n",
    "\n",
    "    return compiled_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff89dcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TruAgent:\n",
    "    def __init__(self):\n",
    "        self.graph = build_graph()\n",
    "\n",
    "    @instrument(\n",
    "        span_type=SpanAttributes.SpanType.RECORD_ROOT,\n",
    "        attributes={\n",
    "            SpanAttributes.RECORD_ROOT.INPUT: \"query\",\n",
    "            SpanAttributes.RECORD_ROOT.OUTPUT: \"return\",\n",
    "        },\n",
    "    )\n",
    "    def invoke_agent_graph(self, query: str) -> str:\n",
    "        try:\n",
    "            # rebuild the graph for each query\n",
    "            self.graph = build_graph()\n",
    "            # Initialize state with proper message format\n",
    "            state = {\"messages\": [HumanMessage(content=query)]}\n",
    "\n",
    "            # Stream events with recursion limit\n",
    "            events = self.graph.stream(\n",
    "                state,\n",
    "                {\"recursion_limit\": 15},\n",
    "            )\n",
    "\n",
    "            # Track all messages through the conversation\n",
    "            all_messages = []\n",
    "            for event in events:\n",
    "                # Get the payload from the event\n",
    "                _, payload = next(iter(event.items()))\n",
    "                if not payload:  # Skip empty payloads\n",
    "                    continue\n",
    "\n",
    "                messages = payload.get(\"messages\")\n",
    "                if not messages:\n",
    "                    continue\n",
    "                all_messages.extend(messages)\n",
    "\n",
    "            # Return the last message's content if available\n",
    "            return (\n",
    "                all_messages[-1].content\n",
    "                if all_messages and hasattr(all_messages[-1], \"content\")\n",
    "                else \"\"\n",
    "            )\n",
    "        except:\n",
    "            return \"I ran into an issue, and cannot answer your question.\"\n",
    "\n",
    "\n",
    "tru_agent = TruAgent()\n",
    "\n",
    "tru_agent_app = TruApp(\n",
    "    tru_agent,\n",
    "    app_name=APP_NAME,\n",
    "    app_version=\"doc and web search\",\n",
    "    connector=trulens_sf_connector,\n",
    "    main_method=tru_agent.invoke_agent_graph,\n",
    ")\n",
    "\n",
    "st_1 = datetime.datetime.fromtimestamp(time.time()).strftime(\n",
    "    \"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    run_name=\"Multi-agent demo run - document and web search\",\n",
    "    description=\"this is a run with access to cortex search and web search\",\n",
    "    dataset_name=\"Research test dataset\",\n",
    "    source_type=\"DATAFRAME\",\n",
    "    label=\"langgraph demo\",\n",
    "    dataset_spec={\n",
    "        \"RECORD_ROOT.INPUT\": \"query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "run: Run = tru_agent_app.add_run(run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004f71a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(tru_agent.graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23788cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.start(input_df=user_queries_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70f360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while run.get_status() == \"INVOCATION_IN_PROGRESS\":\n",
    "    time.sleep(3)\n",
    "\n",
    "run.compute_metrics([\"groundedness\", \"context_relevance\", \"answer_relevance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac25d0ad",
   "metadata": {},
   "source": [
    "### Use Cortex Agent to gain access to querying structured SEC data without complicating the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8817dbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "import requests\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "\n",
    "class CortexAgentArgs(BaseModel):\n",
    "    query: str\n",
    "\n",
    "\n",
    "class CortexAgentTool(StructuredTool):\n",
    "    name: str = \"CortexAgent\"\n",
    "    description: str = \"answers questions using the federal reserve meeting minutes and structured data from the SEC\"\n",
    "    args_schema: type[BaseModel] = CortexAgentArgs\n",
    "    session: Session\n",
    "\n",
    "\n",
    "def run(self, query: str, **kwargs) -> str:\n",
    "    print(\"calling agent\")\n",
    "    payload = {\n",
    "        \"model\": \"claude-3-5-sonnet\",\n",
    "        \"response_instruction\": \"You are a helpful AI assistant.\",\n",
    "        \"experimental\": {},\n",
    "        \"tools\": [\n",
    "            {\n",
    "                \"tool_spec\": {\n",
    "                    \"type\": \"cortex_analyst_text_to_sql\",\n",
    "                    \"name\": \"SEC_ANALYST\",\n",
    "                }\n",
    "            },\n",
    "            {\"tool_spec\": {\"type\": \"cortex_search\", \"name\": \"FOMC_SEARCH\"}},\n",
    "            {\"tool_spec\": {\"type\": \"sql_exec\", \"name\": \"sql_execution_tool\"}},\n",
    "        ],\n",
    "        \"tool_resources\": {\n",
    "            \"SEC_ANALYST\": {\n",
    "                \"semantic_model_file\": \"@agents_db.notebooks.semantic_models/sec_filings.yaml\"\n",
    "            },\n",
    "            \"FOMC_SEARCH\": {\n",
    "                \"name\": \"CORTEX_SEARCH_TUTORIAL_DB.PUBLIC.FOMC_SEARCH_SERVICE\"\n",
    "            },\n",
    "        },\n",
    "        \"tool_choice\": {\"type\": \"auto\"},\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": query}]}\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    api_url = \"http://SFDEVREL-SFDEVREL_ENTERPRISE.snowflakecomputing.com/api/v2/cortex/agent:run\"\n",
    "    pat = os.getenv(\"SNOWFLAKE_PAT\")\n",
    "    if not pat:\n",
    "        raise RuntimeError(\"Environment variable SNOWFLAKE_PAT is not set\")\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {pat}\",\n",
    "        \"X-Snowflake-Authorization-Token-Type\": \"PROGRAMMATIC_ACCESS_TOKEN\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    response = requests.post(api_url, json=payload, headers=headers)\n",
    "    print(\"agent response\", response)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(response.status_code)\n",
    "        print(response.text)\n",
    "        return f\"Failed Cortex Agents API call: {response.status_code} - {response.text}\"\n",
    "\n",
    "    # Extract content from delta\n",
    "    data = response.json()\n",
    "    contents = data.get(\"delta\", {}).get(\"content\", [])\n",
    "    result_parts = [\n",
    "        chunk.get(\"text\", \"\")\n",
    "        for chunk in contents\n",
    "        if chunk.get(\"type\") == \"text\"\n",
    "    ]\n",
    "    result_text = \" \".join(result_parts).strip()\n",
    "\n",
    "    # Fallback if content is empty\n",
    "    return result_text or json.dumps(data, indent=2)\n",
    "\n",
    "\n",
    "def build_graph_with_agent():\n",
    "    def canned_end_node(state: ToolState) -> Command[Literal[\"__end__\"]]:\n",
    "        print(\"starting CANNED END\", flush=True)\n",
    "\n",
    "        return Command(goto=END)\n",
    "\n",
    "    def make_system_prompt(suffix: str) -> str:\n",
    "        return (\n",
    "            \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "            \" Use the provided tools to progress towards answering the question.\"\n",
    "            \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "            \" will help where you left off. Execute what you can to make progress.\"\n",
    "            \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "            \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "            f\"\\n{suffix}\"\n",
    "        )\n",
    "\n",
    "    search = SerpAPIWrapper()\n",
    "\n",
    "    search_tool = Tool(\n",
    "        name=\"web_search\",\n",
    "        description=\"Search the web for current information, such as weather or news\",\n",
    "        func=search.run,\n",
    "    )\n",
    "\n",
    "    # Instantiate CortexAgentTool\n",
    "    cortex_agent_tool = CortexAgentTool(session=snowpark_session_trulens)\n",
    "\n",
    "    wrapped_cortex_agent_tool = Tool(\n",
    "        name=cortex_agent_tool.name,\n",
    "        description=cortex_agent_tool.description,\n",
    "        func=cortex_agent_tool.run,\n",
    "        return_direct=False,  # set to True only if you want the agent to stop after using it\n",
    "    )\n",
    "\n",
    "    tool_registry = {\n",
    "        str(uuid.uuid4()): search_tool,\n",
    "        str(uuid.uuid4()): wrapped_cortex_agent_tool,  # CortexAgentTool here\n",
    "    }\n",
    "\n",
    "    # Update your tool documents indexing accordingly\n",
    "    tool_documents = [\n",
    "        Document(\n",
    "            page_content=tool.name,\n",
    "            id=tool_id,\n",
    "            metadata={\n",
    "                \"tool_name\": tool.name,\n",
    "                \"tool_description\": tool.description,\n",
    "            },\n",
    "        )\n",
    "        for tool_id, tool in tool_registry.items()\n",
    "    ]\n",
    "    vector_store = InMemoryVectorStore(embedding=OpenAIEmbeddings())\n",
    "    vector_store.add_documents(tool_documents)\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "    vector_store = InMemoryVectorStore(embedding=OpenAIEmbeddings())\n",
    "    vector_store.add_documents(tool_documents)\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"SELECT_TOOLS\",\n",
    "        attributes=lambda ret, exc, *args, **kw: {\n",
    "            # ---- state as JSON-text (OTLP needs a scalar) -----------------\n",
    "            f\"{BASE_SCOPE}.select_tools_input_state\": json.dumps(  # ← turns dict → str\n",
    "                {\n",
    "                    **{k: v for k, v in args[0].items() if k != \"messages\"},\n",
    "                    \"messages\": [\n",
    "                        {\"type\": m.__class__.__name__, \"content\": m.content}\n",
    "                        if hasattr(m, \"content\")  # BaseMessage subclasses\n",
    "                        else m  # already JSON-friendly\n",
    "                        for m in args[0].get(\"messages\", [])\n",
    "                    ],\n",
    "                }\n",
    "            ),\n",
    "            # ---- selected tool IDs as a simple comma-separated string -----\n",
    "            f\"{BASE_SCOPE}.selected_tool_ids\": \", \".join(\n",
    "                ret.get(\"selected_tools\", [])\n",
    "            )\n",
    "            if isinstance(ret, dict)\n",
    "            else \"\",\n",
    "        },\n",
    "    )\n",
    "    def select_tools(\n",
    "        state: ToolState,\n",
    "    ) -> Command[Literal[\"research_agent\", END]]:\n",
    "        messages = state[\"messages\"]\n",
    "        last = messages[-1]\n",
    "        query = last[\"content\"] if isinstance(last, dict) else last.content\n",
    "        print(\"selecting tools based on\", query)\n",
    "\n",
    "        # 1. pull top-k with their scores\n",
    "        results: list[tuple[Document, float]] = (\n",
    "            vector_store.similarity_search_with_score(\n",
    "                query,\n",
    "                k=5,  # look at top-5 candidates\n",
    "            )\n",
    "        )\n",
    "        print(\"tool search results\", results)\n",
    "\n",
    "        # 2. filter by minimum cosine-similarity\n",
    "        MIN_SIMILARITY = 0.7\n",
    "        filtered = [doc for doc, score in results if score >= MIN_SIMILARITY]\n",
    "\n",
    "        # 3a. no sufficiently similar tool → end\n",
    "        if not filtered:\n",
    "            print(\"no tool selected\")\n",
    "            msg = HumanMessage(\n",
    "                content=\"Sorry, I don’t have a tool that’s relevant enough to answer that.\",\n",
    "                name=\"assistant\",\n",
    "            )\n",
    "            return Command(\n",
    "                update={\"messages\": messages + [msg]},\n",
    "                goto=END,\n",
    "            )\n",
    "\n",
    "        # 3b. otherwise select those tools and move on\n",
    "        selected_ids = [doc.id for doc in filtered]\n",
    "        print(\"tools selected\", selected_ids)\n",
    "        return Command(\n",
    "            update={\n",
    "                \"selected_tools\": selected_ids,\n",
    "            },\n",
    "            goto=\"research_agent\",\n",
    "        )\n",
    "\n",
    "    # Warning: This executes code locally, which can be unsafe when not sandboxed\n",
    "\n",
    "    repl = PythonREPL()\n",
    "\n",
    "    @tool\n",
    "    @instrument(\n",
    "        span_type=\"PYTHON_REPL_TOOL\",\n",
    "        attributes={\n",
    "            f\"{BASE_SCOPE}.python_tool_input_code\": \"code\",\n",
    "        },\n",
    "    )\n",
    "    def python_repl_tool(code: str):\n",
    "        \"\"\"\n",
    "        Run arbitrary Python, grab the CURRENT matplotlib figure (if any),\n",
    "        save it to ./langgraph_saved_images_snowflaketools/v3/chart_<uuid>.png,\n",
    "        and return a first-line `CHART_PATH=…`.\n",
    "        \"\"\"\n",
    "        import matplotlib\n",
    "\n",
    "        matplotlib.use(\"Agg\")  # headless safety\n",
    "        import os\n",
    "        import uuid\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        # ------------------ run user code & capture stdout ------------------\n",
    "        repl.run(code)\n",
    "\n",
    "        # ------------------ locate a figure (if generated) ------------------\n",
    "        fig = plt.gcf()\n",
    "        has_axes = bool(fig.axes)  # True if something was plotted\n",
    "\n",
    "        # ------------------ always save if we have a figure -----------------\n",
    "        chart_path = \"\"\n",
    "        if has_axes:\n",
    "            target_dir = \"./langgraph_saved_images_snowflaketools/v3\"\n",
    "            os.makedirs(target_dir, exist_ok=True)\n",
    "            chart_path = os.path.join(\n",
    "                target_dir, f\"chart_{uuid.uuid4().hex}.png\"\n",
    "            )\n",
    "            fig.savefig(chart_path, format=\"png\")\n",
    "            plt.close(fig)\n",
    "\n",
    "        # ------------------ tool result (1st line = CHART_PATH) -------------\n",
    "        return f\"CHART_PATH={chart_path if chart_path else 'NONE'}\\n\"\n",
    "\n",
    "    def get_next_node(last_message: BaseMessage, goto: str):\n",
    "        if \"FINAL ANSWER\" in last_message.content:\n",
    "            # Any agent decided the work is done\n",
    "            return END\n",
    "        return goto\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"RESEARCH_NODE\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            f\"{BASE_SCOPE}.research_node_input\": args[0][\"messages\"][\n",
    "                -1\n",
    "            ].content,\n",
    "            f\"{BASE_SCOPE}.research_node_response\": ret.update[\"messages\"][\n",
    "                -1\n",
    "            ].content\n",
    "            if hasattr(ret, \"update\")\n",
    "            else json.dumps(ret, indent=4, sort_keys=True),\n",
    "            f\"{BASE_SCOPE}.tool_messages\": [\n",
    "                dumps(message)\n",
    "                for message in ret.update[\"messages\"]\n",
    "                if isinstance(message, ToolMessage)\n",
    "            ]\n",
    "            if hasattr(ret, \"update\")\n",
    "            else \"No tool call\",\n",
    "        },\n",
    "    )\n",
    "    @instrument(\n",
    "        span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            SpanAttributes.RETRIEVAL.QUERY_TEXT: args[0][\"messages\"][\n",
    "                -1\n",
    "            ].content,\n",
    "            SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS: [\n",
    "                ret.update[\"messages\"][-1].content\n",
    "            ]\n",
    "            if hasattr(ret, \"update\")\n",
    "            else [json.dumps(ret, indent=4, sort_keys=True)],\n",
    "        },\n",
    "    )\n",
    "    def research_agent_node(\n",
    "        state: ToolState,\n",
    "    ) -> Command[Literal[\"chart_generator\"]]:\n",
    "        \"\"\"\n",
    "        Always binds the selected tools and invokes the bound agent.\n",
    "        Stops on FINAL ANSWER or moves to chart_generator.\n",
    "        \"\"\"\n",
    "        # grab (non-empty) list of selected tool IDs\n",
    "        selected_ids = state[\"selected_tools\"]\n",
    "\n",
    "        # bind only those tools\n",
    "        selected_tools = [tool_registry[tid] for tid in selected_ids]\n",
    "        bound_llm = llm.bind_tools(selected_tools)\n",
    "        bound_agent = create_react_agent(\n",
    "            bound_llm,\n",
    "            tools=selected_tools,  # already bound\n",
    "            prompt=make_system_prompt(\n",
    "                \"You can only do research. You are working with both a chart generator and a chart summarizer colleagues.\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # run it\n",
    "        result = bound_agent.invoke(state)\n",
    "\n",
    "        # decide if we’re done\n",
    "        last = result[\"messages\"][-1]\n",
    "        goto = get_next_node(last, \"chart_generator\")\n",
    "\n",
    "        # tag the origin of the final message\n",
    "        result[\"messages\"][-1] = HumanMessage(\n",
    "            content=last.content,\n",
    "            name=\"research_agent\",\n",
    "        )\n",
    "\n",
    "        return Command(\n",
    "            update={\"messages\": result[\"messages\"]},\n",
    "            goto=goto,\n",
    "        )\n",
    "\n",
    "    # Chart generator agent and node\n",
    "    # NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION, WHICH CAN BE UNSAFE WHEN NOT SANDBOXED\n",
    "    # 1) Define the chart‐agent: it only returns JSON with a \"code\" field\n",
    "    chart_agent = create_react_agent(\n",
    "        llm,\n",
    "        [python_repl_tool],\n",
    "        prompt=make_system_prompt(\n",
    "            \"\"\"You can only generate charts by returning a single JSON object, for example:\n",
    "        {\n",
    "        \"code\": \"<your python plotting code here>\"\n",
    "        }\n",
    "        —where <your python plotting code> uses matplotlib to create exactly one figure.\n",
    "        Do NOT include any prose or tool‐call wrappers.\"\"\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    def extract_chart_path(text: str) -> str | None:\n",
    "        \"\"\"\n",
    "        Scan every line of tool stdout for 'CHART_PATH=' and return\n",
    "        whatever follows, trimmed.  Returns None if no such line exists.\n",
    "        \"\"\"\n",
    "        for line in text.splitlines():\n",
    "            if \"CHART_PATH=\" in line:\n",
    "                # split on the first '=', strip whitespace\n",
    "                return line.split(\"CHART_PATH=\", 1)[1].strip()\n",
    "        return None\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"CHART_GENERATOR_NODE\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            f\"{BASE_SCOPE}.chart_node_input\": args[0][\"messages\"][-1].content,\n",
    "            f\"{BASE_SCOPE}.chart_node_response\": (\n",
    "                ret.update[\"messages\"][-1].content\n",
    "                if ret and hasattr(ret, \"update\") and ret.update\n",
    "                else \"No update response\"\n",
    "            ),\n",
    "        },\n",
    "    )\n",
    "    def chart_node(state: ToolState) -> Command[Literal[\"chart_summarizer\"]]:\n",
    "        print(state[\"remaining_steps\"])\n",
    "        if state[\"remaining_steps\"] <= 2:\n",
    "            print(\"Bailing out\", flush=True)\n",
    "            return Command(\n",
    "                update={\"messages\": state[\"messages\"]},\n",
    "                goto=\"canned_end\",\n",
    "            )\n",
    "        # 0) If a path is already in state, skip\n",
    "        # extract the current human query\n",
    "        current_query = state[\"messages\"][-1].content\n",
    "\n",
    "        # if we already generated a chart for _this_ query, skip\n",
    "        if state.get(\"last_query\") == current_query and state.get(\"chart_path\"):\n",
    "            print(\n",
    "                f\"⚡️ skipping chart_node, existing path = {state['chart_path']}\",\n",
    "                flush=True,\n",
    "            )\n",
    "            return Command(\n",
    "                update={\"messages\": state[\"messages\"]}, goto=\"chart_summarizer\"\n",
    "            )\n",
    "\n",
    "        # it's a new query (or first run) → clear any old chart_path and remember this query\n",
    "        state.pop(\"chart_path\", None)\n",
    "        state[\"last_query\"] = current_query\n",
    "\n",
    "        # 1) Remember how many messages we had\n",
    "        len_before = len(state[\"messages\"])\n",
    "\n",
    "        # 2) Run the agent exactly once\n",
    "        agent_out = chart_agent.invoke(state)\n",
    "\n",
    "        print(agent_out, flush=True)\n",
    "        all_msgs = agent_out[\"messages\"]\n",
    "\n",
    "        # 3) Look at only the brand-new messages for our chart tool output\n",
    "        new_segment = all_msgs[len_before:]\n",
    "        tool_msgs = [\n",
    "            m\n",
    "            for m in new_segment\n",
    "            if isinstance(m, ToolMessage) and \"CHART_PATH=\" in m.content\n",
    "        ]\n",
    "\n",
    "        if not tool_msgs:\n",
    "            # If none found, trigger your retry logic\n",
    "            print(\n",
    "                \"⚠️ chart_node: no CHART_PATH in new messages, retrying\",\n",
    "                flush=True,\n",
    "            )\n",
    "            print(state[\"remaining_steps\"])\n",
    "            if state[\"remaining_steps\"] <= 2:\n",
    "                print(\"Bailing out\", flush=True)\n",
    "                return Command(\n",
    "                    update={\"messages\": state[\"messages\"]},\n",
    "                    goto=\"canned_end\",\n",
    "                )\n",
    "            return Command(\n",
    "                update={\"messages\": state[\"messages\"]},\n",
    "                goto=\"research_agent\",\n",
    "            )\n",
    "\n",
    "        # 4) Parse the last one in case there are multiples\n",
    "        tool_msg = tool_msgs[-1]\n",
    "        tool_stdout = tool_msg.content\n",
    "        print(f\"chart_node 🖨 tool_stdout:\\n{tool_stdout}\", flush=True)\n",
    "\n",
    "        chart_path = extract_chart_path(tool_stdout)\n",
    "        print(f\"chart_node 📂 parsed chart_path = {chart_path!r}\", flush=True)\n",
    "        # 5) Build your new messages list: include only that new ToolMessage\n",
    "        new_msgs = state[\"messages\"][:] + [tool_msg]\n",
    "\n",
    "        # 6) Success! stash path into state and append the CHART_PATH marker\n",
    "        new_msgs.append(\n",
    "            HumanMessage(\n",
    "                content=f\"CHART_PATH={chart_path}\", name=\"chart_generator\"\n",
    "            )\n",
    "        )\n",
    "        return Command(\n",
    "            update={\"messages\": new_msgs, \"chart_path\": chart_path},\n",
    "            goto=\"chart_summarizer\",\n",
    "        )\n",
    "\n",
    "    reflection_prompt_template = PromptTemplate(\n",
    "        input_variables=[\"user_query\", \"chart_summary\"],\n",
    "        template=\"\"\"\\\n",
    "        You are an AI assistant tasked with reflecting on the quality of a chart summary. The user has asked the following question:\n",
    "        \"{user_query}\"\n",
    "\n",
    "        You are given the following chart summary:\n",
    "        \"{chart_summary}\"\n",
    "\n",
    "        Your task is to evaluate how well the chart summary answers the user's question. Consider the following:\n",
    "        - Does the summary capture the **key insights** and trends from the chart, even if in a more general form?\n",
    "        - Does it provide **adequate context** to address the user's query, even if it's not exhaustive?\n",
    "        - If the summary provides some context but could benefit from more details, consider it sufficient for now unless significant details are missing.\n",
    "\n",
    "        If the summary **generally** addresses the question, respond with 'Task complete'. If the summary **lacks significant** details or clarity, then respond with specific details on how the answer should be improved and what information is needed. Avoid being overly critical unless the summary completely misses key elements necessary to answer the query.\n",
    "\n",
    "        Please provide your answer in a **concise and encouraging** manner.\n",
    "        \"\"\",\n",
    "    )\n",
    "\n",
    "    # Create the chain using the prompt template and the LLM (ChatOpenAI)\n",
    "    reflection_chain = reflection_prompt_template | llm\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"CHART_SUMMARY_REFLECTION\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            f\"{BASE_SCOPE}.chart_summary_reflection_input_user_query\": args[0],\n",
    "            f\"{BASE_SCOPE}.chart_summary_reflection_input_chart_summary\": args[\n",
    "                1\n",
    "            ],\n",
    "            f\"{BASE_SCOPE}.chart_summary_reflection_response\": ret,\n",
    "        },\n",
    "    )\n",
    "    def perform_reflection(user_query: str, chart_summary: str) -> str:\n",
    "        \"\"\"\n",
    "        This function uses an LLM to reflect on the quality of a chart summary\n",
    "        and determine if the task is complete or requires further refinement.\n",
    "        \"\"\"\n",
    "        print(\"doing reflection...\")\n",
    "        # Call the chain with the user query and chart summary\n",
    "        reflection_result = reflection_chain.invoke({\n",
    "            \"user_query\": user_query,\n",
    "            \"chart_summary\": chart_summary,\n",
    "        })\n",
    "        print(\"reflection_result: \", reflection_result.content)\n",
    "        return reflection_result.content\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"CHART_SUMMARY_NODE\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            f\"{BASE_SCOPE}.summary_node_input\": args[0][\"messages\"][-1].content,\n",
    "            f\"{BASE_SCOPE}.summary_node_output\": (\n",
    "                ret.update[\"messages\"][-1].content\n",
    "                if hasattr(ret, \"update\")\n",
    "                else \"NO SUMMARY GENERATED\"\n",
    "            ),\n",
    "        },\n",
    "    )\n",
    "    def chart_summary_node(state: ToolState) -> Command[Literal[\"__end__\"]]:\n",
    "        print(\"▶️ entering chart_summary_node\", flush=True)\n",
    "        print(state[\"remaining_steps\"])\n",
    "        if state[\"remaining_steps\"] <= 2:\n",
    "            print(\"Bailing out\", flush=True)\n",
    "            return Command(\n",
    "                update={\"messages\": state[\"messages\"]},\n",
    "                goto=\"canned_end\",\n",
    "            )\n",
    "\n",
    "        # 1) find the chart_path in state\n",
    "        chart_path = state.get(\"chart_path\", \"\")\n",
    "        print(f\"  using state.chart_path = {chart_path!r}\", flush=True)\n",
    "        if not chart_path:\n",
    "            return Command(\n",
    "                update={\n",
    "                    \"messages\": state[\"messages\"]\n",
    "                    + [\n",
    "                        HumanMessage(\n",
    "                            \"No valid chart was generated. Please try again.\",\n",
    "                            name=\"chart_summarizer\",\n",
    "                        )\n",
    "                    ]\n",
    "                },\n",
    "                goto=\"select_tools\",\n",
    "            )\n",
    "\n",
    "        # 2) strip *everything* except human utterances\n",
    "        human_history = [\n",
    "            m for m in state[\"messages\"] if isinstance(m, HumanMessage)\n",
    "        ]\n",
    "\n",
    "        # ensure our CHART_PATH marker is last\n",
    "        if not human_history or not human_history[-1].content.startswith(\n",
    "            \"CHART_PATH=\"\n",
    "        ):\n",
    "            human_history.append(\n",
    "                HumanMessage(\n",
    "                    f\"CHART_PATH={chart_path}\", name=\"chart_summarizer\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            \"  human_history:\", [m.content for m in human_history], flush=True\n",
    "        )\n",
    "\n",
    "        # 3) build your ChatCompletion prompt\n",
    "        system = SystemMessage(\n",
    "            content=make_system_prompt(\n",
    "                \"You are an AI assistant whose *only* job is to summarise a chart image. \"\n",
    "                \"Input is a message CHART_PATH=… pointing at a saved PNG. \"\n",
    "                \"Output a concise (≤3 sentences) summary of the key trends.\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        messages_for_llm = (\n",
    "            [system]\n",
    "            + human_history\n",
    "            + [\n",
    "                HumanMessage(\n",
    "                    \"Please summarise the above chart in ≤3 sentences.\"\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 4) call the LLM directly—no tools, no React agent\n",
    "        print(\"📝 calling ChatOpenAI directly for summary\", flush=True)\n",
    "        ai_msg: AIMessage = llm(messages_for_llm)\n",
    "        summary = ai_msg.content\n",
    "        print(f\"📋 chart summary: {summary!r}\", flush=True)\n",
    "\n",
    "        # 5) reflect as before\n",
    "        user_query = state[\"messages\"][0].content\n",
    "        print(\"🔍 reflecting on summary quality\", flush=True)\n",
    "        reflection = perform_reflection(user_query, summary)\n",
    "        clean_ref = reflection.strip().lower()\n",
    "        print(f\"💡 reflection: {reflection!r}\", flush=True)\n",
    "\n",
    "        # 6) decide where to go\n",
    "        if \"task complete\" in clean_ref:\n",
    "            print(\"✅ done\", flush=True)\n",
    "            return Command(\n",
    "                update={\n",
    "                    \"messages\": state[\"messages\"]\n",
    "                    + [HumanMessage(summary, name=\"chart_summarizer\")]\n",
    "                },\n",
    "                goto=END,\n",
    "            )\n",
    "        else:\n",
    "            print(\"🔁 need to retry\", flush=True)\n",
    "            return Command(\n",
    "                update={\n",
    "                    \"messages\": state[\"messages\"]\n",
    "                    + [\n",
    "                        HumanMessage(summary, name=\"chart_summarizer\"),\n",
    "                        HumanMessage(reflection, name=\"chart_reflection\"),\n",
    "                    ]\n",
    "                },\n",
    "                goto=\"select_tools\",\n",
    "            )\n",
    "\n",
    "    workflow = StateGraph(ToolState)\n",
    "    workflow.add_node(\"select_tools\", select_tools)\n",
    "    workflow.add_node(\"research_agent\", research_agent_node)\n",
    "    workflow.add_node(\"chart_generator\", chart_node)\n",
    "    workflow.add_node(\"chart_summarizer\", chart_summary_node)\n",
    "    workflow.add_node(\"canned_end\", canned_end_node)\n",
    "\n",
    "    # Update transitions: begin with tool selection then go to research agent.\n",
    "    workflow.add_edge(START, \"select_tools\")\n",
    "\n",
    "    # workflow.add_edge(\"select_tools\", END)\n",
    "    workflow.add_edge(\"select_tools\", \"research_agent\")\n",
    "    workflow.add_edge(\"research_agent\", \"chart_generator\")\n",
    "    workflow.add_edge(\"chart_generator\", \"chart_summarizer\")\n",
    "    workflow.add_edge(\"chart_summarizer\", END)\n",
    "\n",
    "    workflow.add_edge(\"canned_end\", END)\n",
    "\n",
    "    compiled_graph = workflow.compile()\n",
    "\n",
    "    return compiled_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae6e403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TruAgent:\n",
    "    def __init__(self):\n",
    "        self.graph = build_graph()\n",
    "\n",
    "    @instrument(\n",
    "        span_type=SpanAttributes.SpanType.RECORD_ROOT,\n",
    "        attributes={\n",
    "            SpanAttributes.RECORD_ROOT.INPUT: \"query\",\n",
    "            SpanAttributes.RECORD_ROOT.OUTPUT: \"return\",\n",
    "        },\n",
    "    )\n",
    "    def invoke_agent_graph(self, query: str) -> str:\n",
    "        try:\n",
    "            # rebuild the graph for each query\n",
    "            self.graph = build_graph()\n",
    "            # Initialize state with proper message format\n",
    "            state = {\"messages\": [HumanMessage(content=query)]}\n",
    "\n",
    "            # Stream events with recursion limit\n",
    "            events = self.graph.stream(\n",
    "                state,\n",
    "                {\"recursion_limit\": 15},\n",
    "            )\n",
    "\n",
    "            # Track all messages through the conversation\n",
    "            all_messages = []\n",
    "            for event in events:\n",
    "                # Get the payload from the event\n",
    "                _, payload = next(iter(event.items()))\n",
    "                if not payload:  # Skip empty payloads\n",
    "                    continue\n",
    "\n",
    "                messages = payload.get(\"messages\")\n",
    "                if not messages:\n",
    "                    continue\n",
    "                all_messages.extend(messages)\n",
    "\n",
    "            # Return the last message's content if available\n",
    "            return (\n",
    "                all_messages[-1].content\n",
    "                if all_messages and hasattr(all_messages[-1], \"content\")\n",
    "                else \"\"\n",
    "            )\n",
    "        except:\n",
    "            return \"I ran into an issue, and cannot answer your question.\"\n",
    "\n",
    "\n",
    "tru_agent = TruAgent()\n",
    "\n",
    "tru_agent_app = TruApp(\n",
    "    tru_agent,\n",
    "    app_name=APP_NAME,\n",
    "    app_version=\"doc, sql and web search\",\n",
    "    connector=trulens_sf_connector,\n",
    "    main_method=tru_agent.invoke_agent_graph,\n",
    ")\n",
    "\n",
    "st_1 = datetime.datetime.fromtimestamp(time.time()).strftime(\n",
    "    \"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    run_name=\"Multi-agent demo run - document, sql and web search\",\n",
    "    description=\"this is a run with access to cortex agent (search and analyst) and web search\",\n",
    "    dataset_name=\"Research test dataset\",\n",
    "    source_type=\"DATAFRAME\",\n",
    "    label=\"langgraph demo\",\n",
    "    dataset_spec={\n",
    "        \"RECORD_ROOT.INPUT\": \"query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "run: Run = tru_agent_app.add_run(run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde20b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.start(input_df=user_queries_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faa503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while run.get_status() == \"INVOCATION_IN_PROGRESS\":\n",
    "    time.sleep(3)\n",
    "\n",
    "run.compute_metrics([\"groundedness\", \"context_relevance\", \"answer_relevance\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oss_rag_stack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
