{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\" GT_train.csv\")\n",
    "\n",
    "train_df.head()\n",
    "\n",
    "test_df = pd.read_csv(\"GT_test.csv\")\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Create confusion matrices for all models - simplified approach\n",
    "metric_columns = [\n",
    "    \"Logical Consistency\",\n",
    "    \"Execution Efficiency\",\n",
    "    \"Plan Adherence\",\n",
    "    \"Plan Quality\",\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "results = {}\n",
    "\n",
    "split = \"test\"\n",
    "\n",
    "if split == \"train\":\n",
    "    df = train_df\n",
    "elif split == \"test\":\n",
    "    df = test_df\n",
    "\n",
    "for idx, metric in enumerate(metric_columns):\n",
    "    gt_clean = df[f\"{metric}_GT\"]\n",
    "    metric_clean = df[f\"{metric}_score\"]\n",
    "\n",
    "    # Remove rows with NaN values\n",
    "    mask = pd.notna(gt_clean) & pd.notna(metric_clean)\n",
    "    gt_clean = gt_clean[mask]\n",
    "    metric_clean = metric_clean[mask]\n",
    "\n",
    "    mapping = {0: 0, 0.3333333333: 1, 0.6666666667: 2, 1: 3}\n",
    "\n",
    "    gt_bucketed = [mapping.get(val) for val in gt_clean]\n",
    "    metric_bucketed = [mapping.get(val) for val in metric_clean]\n",
    "\n",
    "    # Create confusion matrix using bucketed values\n",
    "    cm = confusion_matrix(gt_bucketed, metric_bucketed)\n",
    "\n",
    "    # Get unique labels for proper ordering\n",
    "    labels = [0, 1, 2, 3]\n",
    "\n",
    "    # Plot heatmap\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels,\n",
    "        ax=axes[idx],\n",
    "    )\n",
    "    axes[idx].set_title(f\"{metric} vs {metric} Ground Truth\")\n",
    "    axes[idx].set_xlabel(f\"{metric} Judge Score\")\n",
    "    axes[idx].set_ylabel(\"Ground Truth\")\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.trace(cm) / np.sum(cm)\n",
    "\n",
    "    # Calculate off-by-one accuracy\n",
    "    off_by_one_correct = sum(\n",
    "        1\n",
    "        for gt, pred in zip(gt_bucketed, metric_bucketed)\n",
    "        if (gt == 0 and pred == 0)\n",
    "        or (gt == 1 and pred in [1, 2])\n",
    "        or (gt == 2 and pred in [1, 2])\n",
    "        or (gt == 3 and pred == 3)\n",
    "    )\n",
    "    off_by_one_accuracy = off_by_one_correct / len(gt_bucketed)\n",
    "\n",
    "    # Store results\n",
    "    results[metric] = {\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"off_by_one_accuracy\": off_by_one_accuracy,\n",
    "        \"n_samples\": len(gt_bucketed),\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "    # Print classification report\n",
    "    print(f\"\\n=== {metric} vs Ground Truth ===\")\n",
    "    print(f\"Samples: {len(gt_bucketed)}\")\n",
    "    print(f\"Confusion Matrix Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"Off-by-One Accuracy: {off_by_one_accuracy:.3f}\")\n",
    "\n",
    "    # Calculate continuous metrics using original values\n",
    "    print(\n",
    "        f\"Mean Absolute Error: {mean_absolute_error(gt_bucketed, metric_bucketed):.3f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Normalized Mean Absolute Error: {mean_absolute_error(gt_bucketed, metric_bucketed) / mapping.get(1):.3f}\"\n",
    "    )\n",
    "    print(\"Classification Report:\")\n",
    "    print(\n",
    "        classification_report(\n",
    "            gt_bucketed, metric_bucketed, zero_division=\"warn\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "plt.suptitle(f\"{metric}: Confusion Matrices\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n=== SUMMARY COMPARISON ===\")\n",
    "print(\"Metric\\t\\t\\tAccuracy\\tOff-by-1 Acc\\tSamples\")\n",
    "print(\"-\" * 65)\n",
    "for model, stats in results.items():\n",
    "    print(\n",
    "        f\"{model:<20}\\t{stats['accuracy']:.3f}\\t\\t{stats['off_by_one_accuracy']:.3f}\\t\\t{stats['n_samples']}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
