{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install dspy-ai trulens-core trulens-providers-openai ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "# gpt_4o_mini = dspy.LM('openai/gpt-4o-mini')\n",
    "local_llama_3 = dspy.OllamaLocal(model=\"llama3.1:8b\")\n",
    "\n",
    "# start with local model first\n",
    "dspy.configure(lm=local_llama_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llama_3(\"hi there how are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare datasets for DSPy\n",
    "we'd be using XSum (EXtreme Summarization) as the training, dev, and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.datasets.dataset import Dataset\n",
    "import pandas as pd\n",
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    generate_qags_golden_set_groundedness,\n",
    ")\n",
    "\n",
    "# entire dataset\n",
    "xsum_df = pd.DataFrame(\n",
    "    list(\n",
    "        generate_qags_golden_set_groundedness(\n",
    "            \"../../src/benchmark/trulens/benchmark/benchmark_frameworks/experiments/data/qags_mturk_xsum.jsonl\",\n",
    "            max_samples_per_bucket=100,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "class XSumDataset(Dataset):\n",
    "    def __init__(self, df, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self._train = df.iloc[0 : int(len(df) * 0.8)].to_dict(orient=\"records\")\n",
    "\n",
    "        self._dev = df.iloc[int(len(df) * 0.8) : int(len(df) * 0.9)].to_dict(\n",
    "            orient=\"records\"\n",
    "        )\n",
    "\n",
    "        self._test = df.iloc[int(len(df) * 0.9) :].to_dict(orient=\"records\")\n",
    "\n",
    "\n",
    "dataset = XSumDataset(xsum_df, input_keys=[\"query\", \"expected_response\"])\n",
    "print(dataset.train[:3])\n",
    "\n",
    "\n",
    "data_train = []\n",
    "data_dev = []\n",
    "data_test = []\n",
    "for example in dataset.train:\n",
    "    data_train.append({\n",
    "        \"query\": example.query,\n",
    "        \"expected_score\": example.expected_score,\n",
    "        \"expected_response\": example.expected_response,\n",
    "    })\n",
    "\n",
    "\n",
    "for example in dataset.dev:\n",
    "    data_dev.append({\n",
    "        \"query\": example.query,\n",
    "        \"expected_score\": example.expected_score,\n",
    "        \"expected_response\": example.expected_response,\n",
    "    })\n",
    "\n",
    "for example in dataset.test:\n",
    "    data_test.append({\n",
    "        \"query\": example.query,\n",
    "        \"expected_score\": example.expected_score,\n",
    "        \"expected_response\": example.expected_response,\n",
    "    })\n",
    "df_train = pd.DataFrame(data_train)\n",
    "\n",
    "df_dev = pd.DataFrame(data_dev)\n",
    "\n",
    "df_test = pd.DataFrame(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev[\"expected_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_dev[\"expected_score\"].hist(bins=10)\n",
    "plt.xlabel(\"Expected Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Expected Scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define metric for DSPy pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "\n",
    "def evaluate_groundedness_score(\n",
    "    example, pred, trace=None, use_binary_threshold=True, threshold=0.5\n",
    ") -> Union[float, bool]:\n",
    "    gt_score = example.expected_score\n",
    "    pred_score = pred.output.score\n",
    "\n",
    "    if trace is None:\n",
    "        if use_binary_threshold:\n",
    "            gt_label = 1 if gt_score >= threshold else 0\n",
    "\n",
    "            pred_label = 1 if pred_score >= threshold else 0\n",
    "\n",
    "            return 1.0 if gt_label == pred_label else 0.0\n",
    "        else:\n",
    "            return 1.0 - abs(gt_score - pred_score)\n",
    "    else:\n",
    "        binary_gt_label = 1 if gt_score >= threshold else 0\n",
    "        binary_pred_label = 1 if pred_score >= threshold else 0\n",
    "\n",
    "        #  if we're doing bootstrapping, i.e. self-generating good demonstrations of each step\n",
    "        return binary_gt_label == binary_pred_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build our DSPy pipline with signatures, Assertions, CoT, metric, and dataset to implement TruLens groundedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.primitives import Prediction\n",
    "from dspy.teleprompt import MIPROv2\n",
    "from pydantic import BaseModel\n",
    "from pydantic import Field\n",
    "from trulens.providers.litellm import LiteLLM\n",
    "\n",
    "\n",
    "class Input(BaseModel):\n",
    "    source: str = Field(\n",
    "        description=\"Source context from the retrieved documents\"\n",
    "    )\n",
    "    statement: str = Field(\n",
    "        description=\"The generated response to the query that its groundedness shall be evaluated.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Output(BaseModel):\n",
    "    reasons: str = Field(\n",
    "        description=\"The reasons for why the groundedness score is given.\"\n",
    "    )\n",
    "    score: float = Field(\n",
    "        ge=0, le=1, description=\"The groundedness score for the answer\"\n",
    "    )\n",
    "\n",
    "\n",
    "class GroundednessSignature(dspy.Signature):\n",
    "    \"\"\"Your task is to evaluate if every sentence in the statement (except the trivial ones like stylistic sentences) is supported or entailed by the source context.\n",
    "    Generate a score the scale of 0.0 to 1.0 with reasons for the score.\"\"\"\n",
    "\n",
    "    input: Input = dspy.InputField()\n",
    "    output: Output = dspy.OutputField()\n",
    "\n",
    "\n",
    "# class OriginalStatementsInput(BaseModel):\n",
    "#     original_statements: List[str] = Field(description=\"Original statements that need to be refined by removing trivial claims.\")\n",
    "\n",
    "# class RefinedStatementsOutput(BaseModel):\n",
    "#     refined_statements: List[str] = Field(description=\"Refined statements after removing trivial claims.\")\n",
    "\n",
    "# class TrivialStatementRemovalSignature(dspy.Signature):\n",
    "#     \"\"\"You are a TRIVIAL STATEMENT REMOVAL classifier; providing the refined response by removing the trivial claims from the original response.\n",
    "#     Consider the following list of statements. Identify and remove sentences that are stylistic, contain trivial pleasantries, or lack substantive information relevant to the main content. Respond only with a list of the remaining statements in the format of a python list of strings.\n",
    "#     \"\"\"\n",
    "#     input: OriginalStatementsInput = dspy.InputField()\n",
    "#     output: RefinedStatementsOutput = dspy.OutputField()\n",
    "\n",
    "\n",
    "trulens_ollama_provider = LiteLLM(\n",
    "    model_engine=\"ollama/llama3.1:8b\", api_base=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "\n",
    "def tru_groundedness(source, statement):\n",
    "    tru_res = trulens_ollama_provider.groundedness_measure_with_cot_reasons(\n",
    "        source=source, statement=statement\n",
    "    )\n",
    "\n",
    "    score = tru_res[0]\n",
    "\n",
    "    if pd.isna(score):\n",
    "        score = 0.0\n",
    "    reasons = tru_res[1][\"reasons\"]\n",
    "\n",
    "    return Prediction(\n",
    "        reasoning=reasons, output=Output(reasons=reasons, score=score)\n",
    "    )\n",
    "\n",
    "\n",
    "class GroundednessDSPy(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_score_and_reasons = dspy.TypedChainOfThought(\n",
    "            GroundednessSignature\n",
    "        )\n",
    "\n",
    "    def forward(self, query: str, expected_response: str):\n",
    "        input_pair = Input(source=query, statement=expected_response)\n",
    "\n",
    "        try:\n",
    "            output = self.generate_score_and_reasons(input=input_pair)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            output = Prediction(\n",
    "                reasoning=\"Error: Unable to generate reasons or score\",\n",
    "                output=Output(\n",
    "                    reasons=\"Error: Unable to generate reasons or score\",\n",
    "                    score=0.0,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "trainset, devset = dataset.train, dataset.dev\n",
    "\n",
    "\n",
    "groundedness_dspy = GroundednessDSPy()\n",
    "\n",
    "\n",
    "# Set up the evaluator, which can be re-used in your code.\n",
    "# evaluate = Evaluate(devset=devset[:], num_threads=8, display_progress=True,  display_table=True)\n",
    "\n",
    "\n",
    "def evaluate_on_devset(groundedness_fn, devset):\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    dspy_metric_scores = []\n",
    "    for example in devset:\n",
    "        pred = groundedness_fn(example.query, example.expected_response)\n",
    "\n",
    "        gt_label = 1 if example.expected_score >= 0.5 else 0\n",
    "        pred_label = 1 if pred.output.score >= 0.5 else 0\n",
    "\n",
    "        dspy_metric_scores.append(\n",
    "            evaluate_groundedness_score(\n",
    "                example, pred, use_binary_threshold=True, threshold=0.5\n",
    "            )\n",
    "        )\n",
    "\n",
    "        tp += 1 if pred_label == 1 and gt_label == 1 else 0\n",
    "        tn += 1 if pred_label == 0 and gt_label == 0 else 0\n",
    "        fp += 1 if pred_label == 1 and gt_label == 0 else 0\n",
    "        fn += 1 if pred_label == 0 and gt_label == 1 else 0\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    print(f\"Precision: {precision}, Recall: {recall}, F1: {f1}\")\n",
    "    return dspy_metric_scores, precision, recall, f1\n",
    "\n",
    "\n",
    "print(\"Evaluate on the dev set on DSPy baseline model\")\n",
    "\n",
    "evaluate_on_devset(groundedness_dspy, devset)\n",
    "\n",
    "print(\"Evaluate on the dev set on TruLens baseline model\")\n",
    "\n",
    "evaluate_on_devset(tru_groundedness, devset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use MIPROv2 without fewshot examples (0-shot only):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mipro_optimizer = MIPROv2(\n",
    "    metric=evaluate_groundedness_score,\n",
    "    auto=\"light\",\n",
    "    verbose=True,\n",
    ")\n",
    "print(\"Optimizing zero-shot program with MIPRO...\")\n",
    "\n",
    "zeroshot_optimized_program = mipro_optimizer.compile(\n",
    "    groundedness_dspy.deepcopy(),\n",
    "    trainset=trainset,\n",
    "    max_bootstrapped_demos=0,  # ZERO FEW-SHOT EXAMPLES\n",
    "    max_labeled_demos=0,  # ZERO FEW-SHOT EXAMPLES\n",
    "    requires_permission_to_run=False,\n",
    ")\n",
    "\n",
    "# Save optimize program for future use\n",
    "zeroshot_optimized_program.save(\"mipro_zeroshot_optimized\")\n",
    "\n",
    "# Evaluate optimized program\n",
    "print(\"Evluate 0-shot optimized program...\")\n",
    "# evaluate(zeroshot_optimized_program, devset=devset[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_on_devset(zeroshot_optimized_program, devset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fewshot_optimizer = MIPROv2(\n",
    "    metric=evaluate_groundedness_score,\n",
    "    auto=\"medium\",\n",
    ")\n",
    "fewshot_optimized_program = fewshot_optimizer.compile(\n",
    "    groundedness_dspy.deepcopy(),\n",
    "    trainset=trainset,\n",
    "    max_bootstrapped_demos=5,  # FEW-SHOT EXAMPLES\n",
    "    max_labeled_demos=5,  # FEW-SHOT EXAMPLES\n",
    "    requires_permission_to_run=False,\n",
    ")\n",
    "\n",
    "# Save optimize program for future use\n",
    "fewshot_optimized_program.save(\"mipro_fewshot_optimized\")\n",
    "\n",
    "# Evaluate optimized program\n",
    "print(\"Evluate few-shot optimized program...\")\n",
    "\n",
    "evaluate_on_devset(fewshot_optimized_program, devset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.providers.litellm import LiteLLM\n",
    "\n",
    "trulens_ollama_provider = LiteLLM(\n",
    "    model_engine=\"ollama/llama3.1:8b\", api_base=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "\n",
    "def tru_groundedness(source, statement, filter_trivial_statements=False):\n",
    "    return trulens_ollama_provider.groundedness_measure_with_cot_reasons(\n",
    "        source=source,\n",
    "        statement=statement,\n",
    "        filter_trivial_statements=filter_trivial_statements,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_scores = df_train[\"gt_scores\"] = (\n",
    "    df_train[\"expected_score\"].apply(lambda x: 1 if x >= 0.5 else 0).to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_scores = []\n",
    "for i, row in df_train.iterrows():\n",
    "    expected_score = row[\"expected_score\"]\n",
    "    groundedness_output = tru_groundedness(\n",
    "        row[\"query\"], row[\"expected_response\"], filter_trivial_statements=True\n",
    "    )\n",
    "\n",
    "    print(groundedness_output)\n",
    "\n",
    "    predicted_scores.append(1 if groundedness_output[0] >= 0.5 else 0)\n",
    "\n",
    "\n",
    "print(len(gt_scores), len(predicted_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "precision = precision_score(gt_scores, predicted_scores)\n",
    "recall = recall_score(gt_scores, predicted_scores)\n",
    "f1 = f1_score(gt_scores, predicted_scores)\n",
    "\n",
    "print(\n",
    "    f\"(with trivial filtering) Precision with: {precision}, Recall: {recall}, F1: {f1}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "precision = precision_score(gt_scores, predicted_scores)\n",
    "recall = recall_score(gt_scores, predicted_scores)\n",
    "f1 = f1_score(gt_scores, predicted_scores)\n",
    "\n",
    "print(f\"Precision: {precision}, Recall: {recall}, F1: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement TruLens' `groundedness_with_cot_reasons` in AdalFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, Optional, Tuple, Union\n",
    "import warnings\n",
    "\n",
    "import adalflow as adal\n",
    "from adalflow.optim.types import ParameterType\n",
    "from dspy.datasets.dataset import Dataset\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    generate_qags_golden_set_groundedness,\n",
    ")\n",
    "from trulens.feedback import generated as feedback_generated\n",
    "from trulens.feedback.v2.feedback import Groundedness\n",
    "\n",
    "# entire dataset\n",
    "xsum_df = pd.DataFrame(\n",
    "    list(\n",
    "        generate_qags_golden_set_groundedness(\n",
    "            \"../../src/benchmark/trulens/benchmark/benchmark_frameworks/experiments/data/qags_mturk_xsum.jsonl\",\n",
    "            max_samples_per_bucket=100,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "nltk.download(\"punkt_tab\", quiet=True)\n",
    "\n",
    "\n",
    "class XSumDataset(Dataset):\n",
    "    def __init__(self, df, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # Set the random seed for reproducibility\n",
    "        np.random.seed(42)\n",
    "\n",
    "        # Shuffle the dataframe\n",
    "        df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "        # Split into 80% train, 10% dev, 10% test\n",
    "        train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "        dev_df, test_df = train_test_split(\n",
    "            temp_df, test_size=0.5, random_state=42\n",
    "        )\n",
    "\n",
    "        # Store as dictionaries for easy access\n",
    "        self._train = train_df.to_dict(orient=\"records\")\n",
    "        self._dev = dev_df.to_dict(orient=\"records\")\n",
    "        self._test = test_df.to_dict(orient=\"records\")\n",
    "\n",
    "\n",
    "dataset = XSumDataset(xsum_df, input_keys=[\"query\", \"expected_response\"])\n",
    "print(dataset.train[:3])\n",
    "\n",
    "\n",
    "data_train = []\n",
    "data_dev = []\n",
    "data_test = []\n",
    "for example in dataset.train:\n",
    "    data_train.append({\n",
    "        \"query\": example.query,\n",
    "        \"expected_score\": example.expected_score,\n",
    "        \"expected_response\": example.expected_response,\n",
    "    })\n",
    "\n",
    "\n",
    "for example in dataset.dev:\n",
    "    data_dev.append({\n",
    "        \"query\": example.query,\n",
    "        \"expected_score\": example.expected_score,\n",
    "        \"expected_response\": example.expected_response,\n",
    "    })\n",
    "\n",
    "for example in dataset.test:\n",
    "    data_test.append({\n",
    "        \"query\": example.query,\n",
    "        \"expected_score\": example.expected_score,\n",
    "        \"expected_response\": example.expected_response,\n",
    "    })\n",
    "df_train = pd.DataFrame(data_train)\n",
    "\n",
    "df_dev = pd.DataFrame(data_dev)\n",
    "\n",
    "df_test = pd.DataFrame(data_test)\n",
    "\n",
    "print(\n",
    "    f\"len(df_train): {len(df_train)}; len(df_dev): {len(df_dev)}; len(df_test): {len(df_test)}\"\n",
    ")\n",
    "\n",
    "few_shot_template = r\"\"\"<START_OF_SYSTEM_PROMPT>\n",
    "{{system_prompt}}\n",
    "{# Few shot demos #}\n",
    "{% if few_shot_demos is not none %}\n",
    "Here are some examples:\n",
    "{{few_shot_demos}}\n",
    "{% endif %}\n",
    "<END_OF_SYSTEM_PROMPT>\n",
    "<START_OF_USER>\n",
    "{{user_prompt}}\n",
    "<END_OF_USER>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class GroundednessTaskPipeline(adal.Component):\n",
    "    def __init__(self, model_client: adal.ModelClient, model_kwargs: Dict):\n",
    "        super().__init__()\n",
    "\n",
    "        system_prompt = adal.Parameter(\n",
    "            data=Groundedness.system_prompt,\n",
    "            role_desc=\"To give task instruction to the language model in the system prompt\",\n",
    "            requires_opt=True,\n",
    "            param_type=ParameterType.PROMPT,\n",
    "        )\n",
    "        few_shot_demos = adal.Parameter(\n",
    "            data=None,\n",
    "            role_desc=\"To provide few shot demos to the language model\",\n",
    "            requires_opt=True,  # Changed to True for few-shot learning\n",
    "            param_type=ParameterType.DEMOS,\n",
    "        )\n",
    "\n",
    "        self.evaluate_hypothesis = adal.Generator(\n",
    "            model_client=model_client,\n",
    "            model_kwargs=model_kwargs,\n",
    "            template=few_shot_template,\n",
    "            prompt_kwargs={\n",
    "                \"system_prompt\": system_prompt,\n",
    "                \"few_shot_demos\": few_shot_demos,\n",
    "            },\n",
    "            use_cache=True,\n",
    "            output_processors=self.parse_single_groundedness_output,\n",
    "        )\n",
    "\n",
    "    @adal.fun_to_component\n",
    "    def parse_single_groundedness_output(response: str) -> Tuple[float, Dict]:\n",
    "        score, reason = None, None\n",
    "        if response and \"Supporting Evidence\" in response:\n",
    "            score = -1\n",
    "            supporting_evidence = None\n",
    "            criteria = None\n",
    "            for line in response.split(\"\\n\"):\n",
    "                if \"Score\" in line:\n",
    "                    score = (\n",
    "                        feedback_generated.re_configured_rating(\n",
    "                            line,\n",
    "                            min_score_val=0,\n",
    "                            max_score_val=3,\n",
    "                        )\n",
    "                    ) / 3\n",
    "                criteria_lines = []\n",
    "                supporting_evidence_lines = []\n",
    "                collecting_criteria = False\n",
    "                collecting_evidence = False\n",
    "\n",
    "                for line in response.split(\"\\n\"):\n",
    "                    if \"Criteria:\" in line:\n",
    "                        criteria_lines.append(\n",
    "                            line.split(\"Criteria:\", 1)[1].strip()\n",
    "                        )\n",
    "                        collecting_criteria = True\n",
    "                        collecting_evidence = False\n",
    "                    elif \"Supporting Evidence:\" in line:\n",
    "                        supporting_evidence_lines.append(\n",
    "                            line.split(\"Supporting Evidence:\", 1)[1].strip()\n",
    "                        )\n",
    "                        collecting_evidence = True\n",
    "                        collecting_criteria = False\n",
    "                    elif collecting_criteria:\n",
    "                        if \"Supporting Evidence:\" not in line:\n",
    "                            criteria_lines.append(line.strip())\n",
    "                        else:\n",
    "                            collecting_criteria = False\n",
    "                    elif collecting_evidence:\n",
    "                        if \"Criteria:\" not in line:\n",
    "                            supporting_evidence_lines.append(line.strip())\n",
    "                        else:\n",
    "                            collecting_evidence = False\n",
    "\n",
    "                criteria = \"\\n\".join(criteria_lines).strip()\n",
    "                supporting_evidence = \"\\n\".join(\n",
    "                    supporting_evidence_lines\n",
    "                ).strip()\n",
    "            reason = {\n",
    "                \"reason\": (\n",
    "                    f\"{'Criteria: ' + str(criteria)}\\n\"\n",
    "                    f\"{'Supporting Evidence: ' + str(supporting_evidence)}\"\n",
    "                )\n",
    "            }\n",
    "            score = score\n",
    "            reason = reason\n",
    "\n",
    "        else:\n",
    "            if not response:\n",
    "                score = 0\n",
    "                reason = {\"reason\": \"No response generated.\"}\n",
    "            else:\n",
    "                score = (\n",
    "                    feedback_generated.re_configured_rating(\n",
    "                        response,\n",
    "                        min_score_val=0,\n",
    "                        max_score_val=3,\n",
    "                    )\n",
    "                ) / 3\n",
    "                warnings.warn(\n",
    "                    \"No supporting evidence provided. Returning score only.\",\n",
    "                    UserWarning,\n",
    "                )\n",
    "                score = score\n",
    "                reason = {}\n",
    "\n",
    "        score_pattern = re.compile(r\"Score:\\s*([0-9.]+)\")\n",
    "        match = score_pattern.search(reason.get(\"reason\", \"\"))\n",
    "        normalized_reason = None\n",
    "        if match:\n",
    "            original_reason_score = float(match.group(1))\n",
    "            normalized_reason_score = (original_reason_score) / 3\n",
    "\n",
    "            # Ensure the formatting matches exactly\n",
    "            original_string = f\"Score: {int(original_reason_score)}\"\n",
    "            replacement_string = f\"Score: {normalized_reason_score}\"\n",
    "            normalized_reason = reason.copy()\n",
    "            normalized_reason[\"reason\"] = normalized_reason[\"reason\"].replace(\n",
    "                original_string, replacement_string\n",
    "            )\n",
    "\n",
    "        if normalized_reason is not None:\n",
    "            return score, normalized_reason\n",
    "        else:\n",
    "            return score, reason\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        premise: str,\n",
    "        hypothesis: str,\n",
    "        id: Optional[str] = None,\n",
    "    ) -> Union[adal.GeneratorOutput, adal.Parameter]:\n",
    "        # TODO - add trivial statement prompt to be another parameter to optimize\n",
    "\n",
    "        # def evaluate_hypothesis(index, hypothesis):\n",
    "        user_prompt = \"\"\"SOURCE: {premise}\n",
    "\n",
    "        Hypothesis: {hypothesis}\n",
    "\n",
    "        Please answer with the template below for all statement sentences:\n",
    "\n",
    "        Criteria: <Statement Sentence>\n",
    "        Supporting Evidence: <Identify and describe the location in the source where the information matches the statement. Provide a detailed, human-readable summary indicating the path or key details. if nothing matches, say NOTHING FOUND. For the case where the statement is an abstention, say ABSTENTION>\n",
    "        Score: <Output a number based on the scoring output space / range>\n",
    "        \"\"\".format(premise=premise, hypothesis=hypothesis)\n",
    "\n",
    "        return self.evaluate_hypothesis(\n",
    "            prompt_kwargs={\"user_prompt\": user_prompt}, id=id\n",
    "        )\n",
    "\n",
    "        # groundedness_scores = {}\n",
    "        # reasons_str = \"\"\n",
    "        # hypotheses = sent_tokenize(statement)\n",
    "        # results = []\n",
    "\n",
    "        # with ThreadPoolExecutor() as executor:\n",
    "        #     futures = [\n",
    "        #         executor.submit(evaluate_hypothesis, i, hypothesis)\n",
    "        #         for i, hypothesis in enumerate(hypotheses)\n",
    "        #     ]\n",
    "\n",
    "        #     for future in as_completed(futures):\n",
    "        #         results.append(future.result())\n",
    "\n",
    "        # results.sort(key=lambda x: x[0])  # Sort results by index\n",
    "\n",
    "        # for i, score, reason in results:\n",
    "        #     groundedness_scores[f\"statement_{i}\"] = score\n",
    "        #     reason_str = (\n",
    "        #         reason[\"reason\"]\n",
    "        #         if reason is not None and \"reason\" in reason\n",
    "        #         else \"reason not generated\"\n",
    "        #     )\n",
    "        #     reasons_str += f\"STATEMENT {i}:\\n{reason_str}\\n\"\n",
    "\n",
    "        # # Calculate the average groundedness score from the scores dictionary\n",
    "        # average_groundedness_score = float(\n",
    "        #     np.mean(list(groundedness_scores.values()))\n",
    "        # )\n",
    "\n",
    "        # return average_groundedness_score, {\"reasons\": reasons_str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adalflow.components.model_client.ollama_client import OllamaClient\n",
    "from adalflow.components.model_client.openai_client import OpenAIClient\n",
    "\n",
    "gpt_mini_model = {\n",
    "    \"model_client\": OpenAIClient(),\n",
    "    \"model_kwargs\": {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"max_tokens\": 2000,\n",
    "        \"temperature\": 0.0,\n",
    "        \"top_p\": 0.99,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"stop\": None,\n",
    "    },\n",
    "}\n",
    "\n",
    "llama_3_1_model = {\n",
    "    \"model_client\": OllamaClient(),\n",
    "    \"model_kwargs\": {\"model\": \"llama3.1:8b\"},\n",
    "}\n",
    "\n",
    "gpt_4o_model = {\n",
    "    \"model_client\": OpenAIClient(),\n",
    "    \"model_kwargs\": {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"max_tokens\": 4000,\n",
    "        \"temperature\": 0.0,\n",
    "        \"top_p\": 0.99,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"stop\": None,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "task_pipeline = GroundednessTaskPipeline(**gpt_mini_model)\n",
    "print(task_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = task_pipeline(\n",
    "    premise=\"All fruits not edible\", hypothesis=\" Apple is edible\"\n",
    ")\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start auto prompt optimization with Adalflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_pipeline.train()  # set to train mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from dataclasses import field\n",
    "import uuid\n",
    "\n",
    "from adalflow.datasets.types import Example\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class XSumData(Example):\n",
    "    __doc__ = \"\"\"A dataclass for representing examples in the XSum dataset.\"\"\"\n",
    "\n",
    "    id: str = field(\n",
    "        metadata={\"desc\": \"The unique identifier of the example\", \"type\": \"id\"},\n",
    "        default=str(uuid.uuid4()),\n",
    "    )\n",
    "    query: Optional[str] = field(\n",
    "        metadata={\"desc\": \"The source context from the retrieved documents.\"},\n",
    "        default=None,\n",
    "    )\n",
    "\n",
    "    expected_response: Optional[str] = field(\n",
    "        metadata={\n",
    "            \"desc\": \"The generated response to the query that its groundedness shall be evaluated.\"\n",
    "        },\n",
    "        default=None,\n",
    "    )\n",
    "\n",
    "    expected_score: Optional[float] = field(\n",
    "        metadata={\"desc\": \"The expected groundedness score for the answer.\"},\n",
    "        default=None,\n",
    "    )\n",
    "\n",
    "    # __input_fields__ = [\n",
    "    #     \"id\",\n",
    "    #     \"query\",\n",
    "    #     \"expected_response\",\n",
    "    #     \"expected_score\"\n",
    "    # ]  # follow this order too.\n",
    "    # __output_fields__ = [\"expected_score\"]\n",
    "\n",
    "\n",
    "train_dataset = [\n",
    "    XSumData(\n",
    "        query=row[\"query\"],\n",
    "        expected_response=row[\"expected_response\"],\n",
    "        expected_score=row[\"expected_score\"],\n",
    "    )\n",
    "    for _, row in df_train.iterrows()\n",
    "]\n",
    "val_dataset = [\n",
    "    XSumData(\n",
    "        query=row[\"query\"],\n",
    "        expected_response=row[\"expected_response\"],\n",
    "        expected_score=row[\"expected_score\"],\n",
    "    )\n",
    "    for _, row in df_dev.iterrows()\n",
    "]\n",
    "test_dataset = [\n",
    "    XSumData(\n",
    "        query=row[\"query\"],\n",
    "        expected_response=row[\"expected_response\"],\n",
    "        expected_score=row[\"expected_score\"],\n",
    "    )\n",
    "    for _, row in df_test.iterrows()\n",
    "]\n",
    "\n",
    "\n",
    "def groundedness_eval_fn(y: float, y_gt: float) -> float:\n",
    "    y_binary = 1 if y >= 0.5 else 0\n",
    "    y_gt_binary = 1 if y_gt >= 0.5 else 0\n",
    "    return 1 if y_binary == y_gt_binary else 0\n",
    "\n",
    "\n",
    "def weighted_groundedness_loss(\n",
    "    y: float, y_gt: float, false_positive_weight: float = 2.0\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Penalizes false positives more heavily and keeps the loss in [0, 1].\n",
    "    \"\"\"\n",
    "    y_binary = 1 if y >= 0.5 else 0\n",
    "    y_gt_binary = 1 if y_gt >= 0.5 else 0\n",
    "\n",
    "    # Identify the type of error\n",
    "    if y_binary == 1 and y_gt_binary == 0:  # False positive\n",
    "        penalty = false_positive_weight\n",
    "    elif y_binary != y_gt_binary:  # Other mismatches (false negatives)\n",
    "        penalty = 1.0\n",
    "    else:  # Correct predictions\n",
    "        return 0.0\n",
    "\n",
    "    # Normalize the penalty to keep the loss in [0, 1]\n",
    "    normalized_loss = penalty / (false_positive_weight + 1.0)\n",
    "\n",
    "    return normalized_loss\n",
    "\n",
    "\n",
    "class GroundednessAdalComponent(adal.AdalComponent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_client: adal.ModelClient,\n",
    "        model_kwargs: Dict,\n",
    "        backward_engine_model_config: Dict = None,\n",
    "        teacher_model_config: Dict = None,\n",
    "        text_optimizer_model_config: Dict = None,\n",
    "    ):\n",
    "        task = GroundednessTaskPipeline(model_client, model_kwargs)\n",
    "        # eval_fn = AnswerMatchAcc(type=\"exact_match\").compute_single_item\n",
    "        eval_fn = groundedness_eval_fn\n",
    "        loss_fn = adal.EvalFnToTextLoss(\n",
    "            eval_fn=lambda y, y_gt: weighted_groundedness_loss(\n",
    "                y, y_gt, false_positive_weight=2.0\n",
    "            ),\n",
    "            eval_fn_desc=(\n",
    "                \"Weighted loss to penalize false positives: \"\n",
    "                \"1 if y_binary == y_gt_binary, else weighted penalty for FP cases.\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        super().__init__(task=task, eval_fn=eval_fn, loss_fn=loss_fn)\n",
    "        self.backward_engine_model_config = backward_engine_model_config\n",
    "        self.teacher_model_config = teacher_model_config\n",
    "        self.text_optimizer_model_config = text_optimizer_model_config\n",
    "\n",
    "    def prepare_task(self, sample: XSumData):\n",
    "        return self.task.call, {\n",
    "            \"premise\": sample.query,\n",
    "            \"hypothesis\": sample.expected_response,\n",
    "            \"id\": sample.id,\n",
    "        }\n",
    "\n",
    "    def prepare_loss(self, sample: XSumData, pred: adal.Parameter):\n",
    "        # prepare the gt and pred for the loss function\n",
    "        y_gt = adal.Parameter(\n",
    "            name=\"y_gt\",\n",
    "            data=sample.expected_score,\n",
    "            eval_input=sample.expected_score,\n",
    "            requires_opt=False,\n",
    "        )\n",
    "\n",
    "        pred.eval_input = pred.full_response.data[0]\n",
    "        return self.loss_fn, {\"kwargs\": {\"y\": pred, \"y_gt\": y_gt}}\n",
    "\n",
    "    def prepare_eval(self, sample: XSumData, y_pred: adal.GeneratorOutput):\n",
    "        # print(\"ok printing prepare eval\")\n",
    "\n",
    "        # print(f\"Y_pred: {y_pred}\")\n",
    "\n",
    "        y_label = -1\n",
    "        if (\n",
    "            y_pred\n",
    "            and y_pred.data\n",
    "            and len(y_pred.data) > 0\n",
    "            and isinstance(y_pred.data[0], float)\n",
    "        ):\n",
    "            y_label = y_pred.data[0]\n",
    "        return self.eval_fn, {\"y\": y_label, \"y_gt\": sample.expected_score}\n",
    "\n",
    "    def configure_backward_engine(self):\n",
    "        super().configure_backward_engine_helper(\n",
    "            **self.backward_engine_model_config\n",
    "        )\n",
    "\n",
    "    def configure_teacher_generator(self):\n",
    "        super().configure_teacher_generator_helper(**self.teacher_model_config)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        to = super().configure_text_optimizer_helper(\n",
    "            **self.text_optimizer_model_config\n",
    "        )\n",
    "        do = super().configure_demo_optimizer_helper()  # Add demo optimizer\n",
    "        return to + do  # Return both text and demo optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose(\n",
    "    model_client: adal.ModelClient,\n",
    "    model_kwargs: Dict,\n",
    ") -> Dict:\n",
    "    trainset, valset, testset = (\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "        test_dataset,\n",
    "    )  # use max_samples=10 to test the code\n",
    "    # use max_samples=10 to test the code\n",
    "\n",
    "    adal_component = GroundednessAdalComponent(model_client, model_kwargs)\n",
    "    trainer = adal.Trainer(adaltask=adal_component)\n",
    "    trainer.diagnose(dataset=trainset, split=\"train\")\n",
    "    trainer.diagnose(dataset=valset, split=\"val\")\n",
    "    trainer.diagnose(dataset=testset, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnose(**llama_3_1_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_batch_size=4,  # larger batch size is not that effective, probably because of llm's lost in the middle\n",
    "    raw_shots: int = 0,\n",
    "    bootstrap_shots: int = 2,\n",
    "    max_steps=1,\n",
    "    num_workers=4,\n",
    "    strategy=\"random\",\n",
    "    optimization_order=\"sequential\",\n",
    "    debug=False,\n",
    "    resume_from_ckpt=None,\n",
    "    exclude_input_fields_from_bootstrap_demos=False,\n",
    "):\n",
    "    adal_component = GroundednessAdalComponent(\n",
    "        **llama_3_1_model,\n",
    "        teacher_model_config=gpt_4o_model,\n",
    "        text_optimizer_model_config=gpt_4o_model,\n",
    "        backward_engine_model_config=gpt_4o_model,\n",
    "    )\n",
    "    print(adal_component)\n",
    "    trainer = adal.Trainer(\n",
    "        train_batch_size=train_batch_size,\n",
    "        adaltask=adal_component,\n",
    "        strategy=strategy,\n",
    "        max_steps=max_steps,\n",
    "        num_workers=num_workers,\n",
    "        raw_shots=raw_shots,\n",
    "        bootstrap_shots=bootstrap_shots,\n",
    "        debug=debug,\n",
    "        weighted_sampling=True,\n",
    "        optimization_order=optimization_order,\n",
    "        exclude_input_fields_from_bootstrap_demos=exclude_input_fields_from_bootstrap_demos,\n",
    "    )\n",
    "    print(trainer)\n",
    "\n",
    "    # train_dataset, val_dataset, test_dataset = load_datasets()\n",
    "    trainer.fit(\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        debug=debug,\n",
    "        resume_from_ckpt=resume_from_ckpt,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    debug=False,\n",
    "    max_steps=12,\n",
    "    strategy=\"constrained\",\n",
    "    raw_shots=0,\n",
    "    bootstrap_shots=1,\n",
    "    exclude_input_fields_from_bootstrap_demos=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
