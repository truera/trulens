{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning with Snowflake Cortex to improve robustness and alignment of your feedback functions\n",
    "\n",
    "[The Snowflake Cortex Fine-Tuning function](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-finetuning) offers a way to customize large language models for your specific task. This notebook focuses on how to improve the evaluation capability of TruLens groundedness feedback function by fine-tuning LLMs available on Snowflake Cortex using benchmark datasets with human annotation (i.e. SummEval). \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.tests.test_cases import generate_summeval_groundedness_golden_set\n",
    "\n",
    "# generator for groundedness golden set\n",
    "test_cases_gen = generate_summeval_groundedness_golden_set(\n",
    "    \"../tests/datasets/summeval/summeval_test_100.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split, train_split, val_split = [], [], []\n",
    "\n",
    "\n",
    "for i in range(500):\n",
    "    test_split.append(next(test_cases_gen))\n",
    "\n",
    "for i in range(500, 1300):\n",
    "    train_split.append(next(test_cases_gen))\n",
    "\n",
    "for i in range(1300, 1600):\n",
    "    val_split.append(next(test_cases_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"SNOWFLAKE_ACCOUNT\"] = \"xxx-xxx\"\n",
    "os.environ[\"SNOWFLAKE_USER\"] = \"...\"\n",
    "os.environ[\"SNOWFLAKE_USER_PASSWORD\"] = \"...\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "os.environ[\"SNOWFLAKE_DATABASE\"] = \"...\"\n",
    "os.environ[\"SNOWFLAKE_SCHEMA\"] = \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract current prompts we use for groundedness without COT for supervised fine-tuning datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.feedback import prompts\n",
    "\n",
    "system_prompt: str = prompts.LLM_GROUNDEDNESS_SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_train_data, preprocessed_val_data = [], []\n",
    "for row in train_split:\n",
    "    source, hypothesis = row[\"query\"], row[\"response\"]\n",
    "    user_prompt: str = \"\"\"\n",
    "        SOURCE: {source}\n",
    "        \n",
    "        Hypothesis: {hypothesis}\n",
    "        Please answer with just the score. nothing else.\n",
    "        \"\"\".format(source=f\"{source}\", hypothesis=f\"{hypothesis}\")\n",
    "    llm_messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "    full_prompt = str(llm_messages)\n",
    "\n",
    "    human_score_scale_on_5 = row[\"human_score\"]\n",
    "    human_score_scale_on_10 = 10 * ((human_score_scale_on_5 - 1) / 4)\n",
    "\n",
    "    preprocessed_train_data.append({\n",
    "        \"prompt\": full_prompt,\n",
    "        \"completion\": str(human_score_scale_on_10),\n",
    "    })\n",
    "\n",
    "for row in val_split:\n",
    "    source, hypothesis = row[\"query\"], row[\"response\"]\n",
    "    user_prompt: str = \"\"\"\n",
    "        SOURCE: {source}\n",
    "        \n",
    "        Hypothesis: {hypothesis}\n",
    "        Please answer with just the score. nothing else.\n",
    "        \"\"\".format(source=f\"{source}\", hypothesis=f\"{hypothesis}\")\n",
    "    llm_messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "    full_prompt = str(llm_messages)\n",
    "\n",
    "    human_score_scale_on_5 = row[\"human_score\"]\n",
    "    human_score_scale_on_10 = 10 * ((human_score_scale_on_5 - 1) / 4)\n",
    "\n",
    "    preprocessed_val_data.append({\n",
    "        \"prompt\": full_prompt,\n",
    "        \"completion\": str(human_score_scale_on_10),\n",
    "    })\n",
    "\n",
    "assert len(preprocessed_train_data) == len(train_split)\n",
    "assert len(preprocessed_val_data) == len(val_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing dataset for Snowflake table - only 2 column names are `prompt` and `completion` are accepted by Cortex Finetuning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "\n",
    "# Connect to Snowflake\n",
    "conn = snowflake.connector.connect(\n",
    "    user=os.environ[\"SNOWFLAKE_USER\"],\n",
    "    password=os.environ[\"SNOWFLAKE_USER_PASSWORD\"],\n",
    "    account=os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"USE DATABASE DHUANG_LLM\")\n",
    "# SQL command to create the table\n",
    "create_table_sql_train = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS summeval_train_data (\n",
    "    prompt STRING,\n",
    "    completion STRING\n",
    ");\n",
    "\"\"\"\n",
    "create_table_sql_val = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS summeval_val_data (\n",
    "    prompt STRING,\n",
    "    completion STRING\n",
    ");\n",
    "\"\"\"\n",
    "# Execute the SQL command to create the table\n",
    "cur.execute(create_table_sql_train)\n",
    "cur.execute(create_table_sql_val)\n",
    "\n",
    "# Data to be inserted\n",
    "\n",
    "# Insert data into the table\n",
    "insert_sql_train = (\n",
    "    \"INSERT INTO summeval_train_data (prompt, completion) VALUES (%s, %s)\"\n",
    ")\n",
    "for row in preprocessed_train_data:\n",
    "    cur.execute(insert_sql_train, (row[\"prompt\"], row[\"completion\"]))\n",
    "\n",
    "insert_sql_val = (\n",
    "    \"INSERT INTO summeval_val_data (prompt, completion) VALUES (%s, %s)\"\n",
    ")\n",
    "for row in preprocessed_val_data:\n",
    "    cur.execute(insert_sql_val, (row[\"prompt\"], row[\"completion\"]))\n",
    "\n",
    "# Commit the transaction\n",
    "conn.commit()\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kick off Cortex FineTune Job in your Snowflake account to fine-tune `Mixtral-8x7b` (as a feedback function provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "\n",
    "# Connect to Snowflake\n",
    "conn = snowflake.connector.connect(\n",
    "    user=os.environ[\"SNOWFLAKE_USER\"],\n",
    "    password=os.environ[\"SNOWFLAKE_USER_PASSWORD\"],\n",
    "    account=os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"USE DATABASE DHUANG_LLM\")\n",
    "\n",
    "finetune_sql = \"\"\"\n",
    "\n",
    "SELECT SNOWFLAKE.CORTEX.FINETUNE(\n",
    "  'CREATE',\n",
    "  'ft_mixtral_8x7b_06152024',\n",
    "  'mixtral-8x7b',\n",
    "  'SELECT prompt, completion FROM summeval_train_data',\n",
    "  'SELECT prompt, completion FROM summeval_val_data'\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "cur.execute(finetune_sql)\n",
    "\n",
    "\n",
    "# Commit the transaction\n",
    "conn.commit()\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the base models to establish baseline performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.apps.basic import TruBasicApp\n",
    "from trulens.core import Feedback\n",
    "from trulens.core import Select\n",
    "from trulens.core import TruSession\n",
    "from trulens.feedback import GroundTruthAgreement\n",
    "from trulens.providers.cortex import Cortex\n",
    "\n",
    "session = TruSession()\n",
    "session.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.session import Session\n",
    "\n",
    "connection_params = {\n",
    "    \"account\": os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "    \"user\": os.environ[\"SNOWFLAKE_USER\"],\n",
    "    \"private_key_file\": os.environ[\"SNOWFLAKE_PRIVATE_KEY_FILE\"],\n",
    "    \"role\": os.environ[\"SNOWFLAKE_ROLE\"],\n",
    "    \"database\": os.environ[\"SNOWFLAKE_DATABASE\"],\n",
    "    \"schema\": os.environ[\"SNOWFLAKE_SCHEMA\"],\n",
    "    \"warehouse\": os.environ[\"SNOWFLAKE_WAREHOUSE\"],\n",
    "}\n",
    "snowpark_session = Session.builder.configs(connection_params).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cortex_provider_mixtral = Cortex(\n",
    "    snowpark_session,\n",
    "    model_engine=\"ft_mixtral_8x7b_06152024\",\n",
    ")\n",
    "\n",
    "f_groundedness_mixtral = Feedback(\n",
    "    cortex_provider_mixtral.groundedness_measure_with_cot_reasons,\n",
    "    name=\"groundedness Mixtral\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "    \"(CNN)Donald Sterling's racist remarks cost him an NBA team last year. But now it's his former female companion who has lost big. A Los Angeles judge has ordered V. Stiviano to pay back more than $2.6 million in gifts after Sterling's wife sued her. In the lawsuit, Rochelle \\\"Shelly\\\" Sterling accused Stiviano of targeting extremely wealthy older men. She claimed Donald Sterling used the couple's money to buy Stiviano a Ferrari, two Bentleys and a Range Rover, and that he helped her get a $1.8 million duplex. Who is V. Stiviano? Stiviano countered that there was nothing wrong with Donald Sterling giving her gifts and that she never took advantage of the former Los Angeles Clippers owner, who made much of his fortune in real estate. Shelly Sterling was thrilled with the court decision Tuesday, her lawyer told CNN affiliate KABC. \\\"This is a victory for the Sterling family in recovering the $2,630,000 that Donald lavished on a conniving mistress,\\\" attorney Pierce O'Donnell said in a statement. \\\"It also sets a precedent that the injured spouse can recover damages from the recipient of these ill-begotten gifts.\\\" Stiviano's gifts from Donald Sterling didn't just include uber-expensive items like luxury cars. According to the Los Angeles Times, the list also includes a $391 Easter bunny costume, a $299 two-speed blender and a $12 lace thong. Donald Sterling's downfall came after an audio recording surfaced of the octogenarian arguing with Stiviano. In the tape, Sterling chastises Stiviano for posting pictures on social media of her posing with African-Americans, including basketball legend Magic Johnson. \\\"In your lousy f**ing Instagrams, you don't have to have yourself with -- walking with black people,\\\" Sterling said in the audio first posted by TMZ. He also tells Stiviano not to bring Johnson to Clippers games and not to post photos with the Hall of Famer so Sterling's friends can see. \\\"Admire him, bring him here, feed him, f**k him, but don't put (Magic) on an Instagram for the world to have to see so they have to call me,\\\" Sterling said. NBA Commissioner Adam Silver banned Sterling from the league, fined him $2.5 million and pushed through a charge to terminate all of his ownership rights in the franchise. Fact check: Donald Sterling's claims vs. reality CNN's Dottie Evans contributed to this report.\",\n",
    ")\n",
    "summary = \"donald sterling , nba team last year . sterling 's wife sued for $ 2.6 million in gifts . sterling says he is the former female companion who has lost the . sterling has ordered v. stiviano to pay back $ 2.6 m in gifts after his wife sued . sterling also includes a $ 391 easter bunny costume , $ 299 and a $ 299 .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_groundedness_mixtral(text, summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cortex_provider_mixtral = Cortex(\n",
    "    snowpark_session,\n",
    "    model_engine=\"ft_mixtral_8x7b_06152024\",\n",
    ")\n",
    "\n",
    "f_groundedness_mixtral = Feedback(\n",
    "    cortex_provider_mixtral.groundedness_measure_with_cot_reasons,\n",
    "    name=\"groundedness Mixtral\",\n",
    ")\n",
    "\n",
    "\n",
    "def wrapped_groundedness_mixtral(input, output) -> float:\n",
    "    score = f_groundedness_mixtral(input, output)[0]\n",
    "    return score\n",
    "\n",
    "\n",
    "ground_truth = GroundTruthAgreement(\n",
    "    test_split, provider=cortex_provider_mixtral\n",
    ")\n",
    "# Call the numeric_difference method with app and record and aggregate to get the mean absolute error\n",
    "f_mae = (\n",
    "    Feedback(ground_truth.absolute_error, name=\"Mean Absolute Error\")\n",
    "    .on(Select.Record.calls[0].args.args[0])\n",
    "    .on(Select.Record.calls[0].args.args[1])\n",
    "    .on_output()\n",
    ")\n",
    "\n",
    "\n",
    "tru_wrapped_groundedness_mixtral = TruBasicApp(\n",
    "    wrapped_groundedness_mixtral,\n",
    "    app_name=\"groundedness\",\n",
    "    app_version=\"Mixtral-8x7b baseline\",\n",
    "    feedbacks=[f_mae],\n",
    ")\n",
    "\n",
    "for i in range(len(test_split)):\n",
    "    source = test_split[i][\"query\"]\n",
    "    response = test_split[i][\"response\"]\n",
    "\n",
    "    with tru_wrapped_groundedness_mixtral as recording:\n",
    "        try:\n",
    "            tru_wrapped_groundedness_mixtral.app(source, response)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cortex_provider_mistral_7b = Cortex(\n",
    "    snowpark_session,\n",
    "    model_engine=\"mistral-7b\",\n",
    ")\n",
    "\n",
    "f_groundedness_mistral_7b = Feedback(\n",
    "    cortex_provider_mistral_7b.groundedness_measure_with_cot_reasons,\n",
    "    name=\"groundedness Mistral-7b\",\n",
    ")\n",
    "\n",
    "\n",
    "def wrapped_groundedness_mistral_7b(input, output) -> float:\n",
    "    score = f_groundedness_mistral_7b(input, output)[0]\n",
    "    return score\n",
    "\n",
    "\n",
    "ground_truth = GroundTruthAgreement(\n",
    "    test_split, provider=cortex_provider_mistral_7b\n",
    ")\n",
    "# Call the numeric_difference method with app and record and aggregate to get the mean absolute error\n",
    "f_mae = (\n",
    "    Feedback(ground_truth.absolute_error, name=\"Mean Absolute Error\")\n",
    "    .on(Select.Record.calls[0].args.args[0])\n",
    "    .on(Select.Record.calls[0].args.args[1])\n",
    "    .on_output()\n",
    ")\n",
    "\n",
    "\n",
    "tru_wrapped_groundedness_mistral_7b = TruBasicApp(\n",
    "    wrapped_groundedness_mistral_7b,\n",
    "    app_name=\"groundedness\",\n",
    "    app_version=\"Mistral-7b baseline\",\n",
    "    feedbacks=[f_mae],\n",
    ")\n",
    "\n",
    "for i in range(len(test_split)):\n",
    "    source = test_split[i][\"query\"]\n",
    "    response = test_split[i][\"response\"]\n",
    "\n",
    "    with tru_wrapped_groundedness_mistral_7b as recording:\n",
    "        try:\n",
    "            tru_wrapped_groundedness_mistral_7b.app(source, response)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune `Mistral 7b`, in additional to `Mixtral-8x7b` for side-by-side comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "\n",
    "# Connect to Snowflake\n",
    "conn = snowflake.connector.connect(\n",
    "    user=os.environ[\"SNOWFLAKE_USER\"],\n",
    "    password=os.environ[\"SNOWFLAKE_USER_PASSWORD\"],\n",
    "    account=os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"USE DATABASE DHUANG_LLM\")\n",
    "\n",
    "finetune_sql = \"\"\"\n",
    "SELECT SNOWFLAKE.CORTEX.FINETUNE(\n",
    "  'CREATE',\n",
    "  'ft_mistral_7b_06182024',\n",
    "  'mistral-7b',\n",
    "  'SELECT prompt, completion FROM summeval_train_data',\n",
    "  'SELECT prompt, completion FROM summeval_val_data'\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "cur.execute(finetune_sql)\n",
    "\n",
    "\n",
    "# Commit the transaction\n",
    "conn.commit()\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = GroundTruthAgreement(\n",
    "    test_split, provider=cortex_provider_mistral_7b\n",
    ")\n",
    "# Call the numeric_difference method with app and record and aggregate to get the mean absolute error\n",
    "f_mae = (\n",
    "    Feedback(ground_truth.absolute_error, name=\"Mean Absolute Error\")\n",
    "    .on(Select.Record.calls[0].args.args[0])\n",
    "    .on(Select.Record.calls[0].args.args[1])\n",
    "    .on_output()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Groundedness performance after finetuning for both `Mixtral-8x7b` and `Mistral-7b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cortex_provider_mixtral_ft = Cortex(\n",
    "    snowpark_session,\n",
    "    model_engine=\"ft_mixtral_8x7b_06152024\",\n",
    ")\n",
    "\n",
    "f_groundedness_mixtral_ft = Feedback(\n",
    "    cortex_provider_mixtral_ft.groundedness_measure_with_cot_reasons,\n",
    "    name=\"groundedness Mixtral finetuned\",\n",
    ")\n",
    "\n",
    "\n",
    "def wrapped_groundedness_mixtral_finetuned(input, output) -> float:\n",
    "    score = f_groundedness_mixtral_ft(input, output)[0]\n",
    "    return score\n",
    "\n",
    "\n",
    "tru_wrapped_groundedness_mixtral_ft = TruBasicApp(\n",
    "    wrapped_groundedness_mixtral_finetuned,\n",
    "    app_name=\"groundedness\",\n",
    "    app_version=\"Mixtral-8x7b finetuned\",\n",
    "    feedbacks=[f_mae],\n",
    ")\n",
    "\n",
    "for i in range(len(test_split)):\n",
    "    source = test_split[i][\"query\"]\n",
    "    response = test_split[i][\"response\"]\n",
    "\n",
    "    with tru_wrapped_groundedness_mixtral_ft as recording:\n",
    "        try:\n",
    "            tru_wrapped_groundedness_mixtral_ft.app(source, response)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cortex_provider_mistral_7b_ft = Cortex(\n",
    "    snowpark_session,\n",
    "    model_engine=\"ft_mistral_7b_06182024\",\n",
    ")\n",
    "\n",
    "f_groundedness_mistral_7b = Feedback(\n",
    "    cortex_provider_mistral_7b_ft.groundedness_measure_with_cot_reasons,\n",
    "    name=\"groundedness Mistral-7b finetuned\",\n",
    ")\n",
    "\n",
    "\n",
    "def wrapped_groundedness_mistral_7b_finetuned(input, output) -> float:\n",
    "    score = f_groundedness_mistral_7b(input, output)[0]\n",
    "    return score\n",
    "\n",
    "\n",
    "tru_wrapped_groundedness_mistral_7b_ft = TruBasicApp(\n",
    "    wrapped_groundedness_mistral_7b_finetuned,\n",
    "    app_name=\"groundedness\",\n",
    "    app_version=\"Mistral-7b finetuned\",\n",
    "    feedbacks=[f_mae],\n",
    ")\n",
    "\n",
    "for i in range(len(test_split)):\n",
    "    source = test_split[i][\"query\"]\n",
    "    response = test_split[i][\"response\"]\n",
    "\n",
    "    with tru_wrapped_groundedness_mistral_7b_ft as recording:\n",
    "        try:\n",
    "            tru_wrapped_groundedness_mistral_7b_ft.app(source, response)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.get_leaderboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary analysis:\n",
    "\n",
    "\n",
    "We observe a slight improvements over baseline with Mixtral-8x7b model as our feedback provider. \n",
    "\n",
    "Notice in the Mistral-7b case, finetuning on a limited set of training data (800 input-output pairs) actually results in worse performance. This could be attributed to the model overfitting the train set and thus failing to generalize well on the test set. \n",
    "\n",
    "In another test run (not shown in this notebook), we also see potentially under-training on Llama3-8B, as the train loss recorded was about 20x of the loss values we got here for Mixtral-7b.\n",
    "\n",
    "Both of the above scenarios are expected with the current beta (private preview) version of Cortex Fine-Tuning API, and we believe better results can be achieved in the near releases.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check: some Statistical Exploration of the SummEval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_score_test, avg_score_val, avg_score_train = 0, 0, 0\n",
    "test_human_scores, val_human_scores, train_human_scores = [], [], []\n",
    "for row in test_split:\n",
    "    avg_score_test += row[\"human_score\"]\n",
    "    test_human_scores.append(row[\"human_score\"])\n",
    "print(avg_score_test)\n",
    "avg_score_test = avg_score_test / len(test_split)\n",
    "\n",
    "for row in val_split:\n",
    "    avg_score_val += row[\"human_score\"]\n",
    "    val_human_scores.append(row[\"human_score\"])\n",
    "\n",
    "print(avg_score_val)\n",
    "\n",
    "avg_score_val = avg_score_val / len(val_split)\n",
    "\n",
    "for row in train_split:\n",
    "    avg_score_train += row[\"human_score\"]\n",
    "    train_human_scores.append(row[\"human_score\"])\n",
    "\n",
    "print(avg_score_train)\n",
    "avg_score_train = avg_score_train / len(train_split)\n",
    "\n",
    "\n",
    "print(avg_score_train, avg_score_test, avg_score_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.hist(train_human_scores, bins=5, alpha=0.5, label=\"Train\", color=\"b\")\n",
    "plt.hist(val_human_scores, bins=5, alpha=0.5, label=\"Validation\", color=\"g\")\n",
    "plt.hist(test_human_scores, bins=5, alpha=0.5, label=\"Test\", color=\"r\")\n",
    "\n",
    "plt.xlabel(\"Scores\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Human Scores\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data for boxplot\n",
    "data = [train_human_scores, val_human_scores, test_human_scores]\n",
    "labels = [\"Train\", \"Validation\", \"Test\"]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create boxplot\n",
    "box = plt.boxplot(\n",
    "    data, labels=labels, patch_artist=True, showmeans=True, notch=True\n",
    ")\n",
    "\n",
    "# Customize boxplot\n",
    "colors = [\"skyblue\", \"lightgreen\", \"lightcoral\"]\n",
    "for patch, color in zip(box[\"boxes\"], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "# Add grid\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Add jittered points for better visualization of distributions\n",
    "for i in range(len(data)):\n",
    "    y = data[i]\n",
    "    x = np.random.normal(i + 1, 0.04, size=len(y))\n",
    "    plt.plot(x, y, \"r.\", alpha=0.6)\n",
    "\n",
    "# Axis labels and title\n",
    "plt.xlabel(\"Dataset\")\n",
    "plt.ylabel(\"Scores\")\n",
    "plt.title(\"Box Plot of Human Scores (1 to 5)\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
