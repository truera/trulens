{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Feedback Function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create snowpark session.\n",
    "\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "# snowflake_connection_parameters = {\n",
    "#     \"account\": os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "#     \"user\": os.environ[\"SNOWFLAKE_USER\"],\n",
    "#     \"password\": os.environ[\"SNOWFLAKE_USER_PASSWORD\"],\n",
    "#     \"database\": os.environ[\"SNOWFLAKE_DATABASE\"],\n",
    "#     \"schema\": os.environ[\"SNOWFLAKE_SCHEMA\"],\n",
    "#     \"role\": os.environ[\"SNOWFLAKE_ROLE\"],\n",
    "#     \"warehouse\": os.environ[\"SNOWFLAKE_WAREHOUSE\"],\n",
    "# }\n",
    "\n",
    "\n",
    "snowflake_connection_parameters = {\n",
    "    \"account\": \"SNOWHOUSE\",\n",
    "    \"user\": \"ajia\",\n",
    "    \"authenticator\": \"externalbrowser\",\n",
    "}\n",
    "snowpark_session = Session.builder.configs(\n",
    "    snowflake_connection_parameters\n",
    ").create()\n",
    "\n",
    "# TruSession is no longer required as long as snowflake connector exists\n",
    "# sf_connector = SnowflakeConnector(snowpark_session=snowpark_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.providers.cortex import Cortex\n",
    "\n",
    "provider = Cortex(\n",
    "    model_engine=\"claude-4-sonnet\",\n",
    "    snowpark_session=snowpark_session,\n",
    "    reasoning_effort=\"high\",\n",
    ")\n",
    "# provider = OpenAI(model_engine=\"o3\")\n",
    "\n",
    "# Create feedback functions without selectors (better for direct string evaluation)\n",
    "# These can be called directly on strings, which is what you want for your DataFrame processing\n",
    "\n",
    "feedback_functions = {\n",
    "    # \"Logical Consistency\": provider.logical_consistency_with_cot_reasons,\n",
    "    # \"Execution Efficiency\": provider.execution_efficiency_with_cot_reasons,\n",
    "    # \"Plan Adherence\": provider.plan_adherence_with_cot_reasons,\n",
    "    # \"Plan Quality\": provider.plan_quality_with_cot_reasons,\n",
    "    \"TRAIL\": provider.trail_with_cot_reasons,\n",
    "}\n",
    "\n",
    "for name in feedback_functions:\n",
    "    print(f\"- {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROCESS ALL GAIA FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from trail_traversal import build_span_tree\n",
    "from trail_traversal import reset_state\n",
    "\n",
    "gaia_dir = \"GAIA\"\n",
    "all_files = []\n",
    "for filename in os.listdir(gaia_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        reset_state()\n",
    "        print(filename)\n",
    "        filepath = os.path.join(gaia_dir, filename)\n",
    "        with open(filepath, \"r\") as f:\n",
    "            trace_data = json.load(f)\n",
    "        output_file = os.path.join(gaia_dir, filename.replace(\".json\", \".txt\"))\n",
    "        with open(output_file, \"w\") as f:\n",
    "            f.write(f\"Trace ID: {trace_data['trace_id']}\\n\\n\")\n",
    "        root_spans, all_spans = build_span_tree(trace_data)\n",
    "        for root_span in root_spans:\n",
    "            root_span.display(output_file=output_file)\n",
    "        all_files.append(output_file)\n",
    "\n",
    "print(len(all_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN EVALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAIA_trace_explanation = \"\"\"\n",
    "Agent Architecture and Trace Structure: The agent architecture consists of a primary manager Agent (also referred to as CodeAgent) that delegates tasks to a search_agent (also referred to as ToolCallingAgent).\n",
    "\n",
    "Overall Flow:\n",
    "Every trace consists of several spans (with span_id numbers and parent span_id numbers). Each trace begins with the manager (CodeAgent). The process follows a clear, hierarchical structure where the manager outlines a high-level plan and the search_agent executes the detailed, tool-based steps for each part of that plan.\n",
    "\n",
    "1. Manager Agent Initiation:\n",
    "The trace starts with the manager. In its initial child spans, you will observe the following sequence:\n",
    "- A preparatory survey is created based on the user's query.\n",
    "- A high-level plan is formulated from this survey.\n",
    "\n",
    "The Manager agent begins executing Step 1 of its plan.\n",
    "\n",
    "2. Manager Agent Step 1:\n",
    "Within the child span for Step 1, the Manager agent decides how to proceed given the initial fact survey and plan. The Manager agent will produce a thought, which may call the search_agent to perform the necessary actions or research.\n",
    "\n",
    "3. search_agent (ToolCallingAgent) Execution Loop:\n",
    "Once called, the search_agent begins its own execution loop. In its child spans, you will observe the following sequence:\n",
    "- A preparatory survey to the specific sub-task it received from the Manager agent.\n",
    "- A plan tailored to the specific sub-task it received from the Manager agent.\n",
    "\n",
    "The search_agent executes an initial set of up to four steps. Each step involves an LLM call to generate a tool-call, followed by the tool's execution.\n",
    "After these initial steps, search_agent synthesizes the information gathered into an updated fact list and refines its plan.\n",
    "The search_agent may then continue to execute more tool-steps based on this updated plan.\n",
    "\n",
    "This loop continues until the search_agent has gathered enough information to comprehensively answer the manager's sub-task, at which point it calls final_answer.\n",
    "\n",
    "4. Returning Control to the Manager agent\n",
    "The final_answer from the search_agent is returned to the Manager agent, concluding the Manager agent's Step 1. The Manager agent then proceeds to Step 2 of its high-level plan, using the result from the previous step as context. \n",
    "This entire cycle repeats for all subsequent steps in the Manager Agent's plan.\n",
    "\n",
    "Whenever you want to point out anything in the trace, cite the span_id number of the span that you are referring to.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logical_consistency_prompt = \"\"\"\n",
    "Track each agent's system instructions and conversation history, ensuring all subsequent outputs from that agent adhere to its established guidelines and prior dialogue, even when agents speak interchangeably. \n",
    "For the manager agent and each unique search_agent that may exist in the trace, evaluate the logical consistency for the agent's actions and responses. For each agent, ensure that each response is consistent with the system instructions and prior dialogue.\n",
    "You must structure your entire response: \n",
    "**Manager Agent** \n",
    "**System Instructions**\n",
    "[Paste all system instructions associated with the manager agent.]\n",
    "**Logical Consistency issues**\n",
    "[All Logical Consistency issues associated with the manager agent] \n",
    "\n",
    "**search_agent 0** (if exists)\n",
    "**System Instructions**\n",
    "[Paste all system instructions associated with the search_agent.]\n",
    "**Logical Consistency issues**\n",
    "[List all Logical Consistency issues associated with this search_agent] \n",
    "... \n",
    "**search_agent n** (if exists)\n",
    "**System Instructions**\n",
    "[Paste all system instructions associated with the search_agent.]\n",
    "**Logical Consistency issues**\n",
    "[List all Logical Consistency issues associated with this search_agent]\n",
    "\n",
    "Here are some examples of logical consistency issues:\n",
    "        {\n",
    "            \"evidence\": \"The plan output content ends with the last step of the plan instead of the `<end_plan>` tag.\",\n",
    "            \"description\": \"The plan generation step did not conclude with the required '<end_plan>' tag as specified in the instructions for plan generation.\",\n",
    "        },\n",
    "        {\n",
    "            \"evidence\": \"Thought: I recall that Girls Who Code published a statistic showing that in the 1980s, women accounted for 37% of computer scientists (or computer science graduates) and that over a span of three decades that figure dropped by 13 percentage points (from 37% down to 24%)\",\n",
    "            \"description\": \"The system recalled a statistic about Girls Who Code and the percentage of women in computer science, but this information was not verified using the search tool as planned. The system states \\\"Thought: I recall that\\nGirls Who Code published a statistic showing that in the 1980s, women accounted for 37% of computer scientists (or computer science graduates) and that over a span of three decades that figure dropped by 13 percentage points\\n(from 37% down to 24%). In other words, it took 30 years for that change to occur. Based on that well-circulated statistic that Girls Who Code highlighted, I will output 30 years as the final answer.\"\n",
    "        }\n",
    "Cite each issue with all corresponding span id numbers and the reason for the issue.\n",
    "\"\"\"\n",
    "\n",
    "execution_efficiency_prompt = \"\"\"\n",
    "Track each agent's system instructions and conversation history, ensuring all subsequent outputs from that agent adhere to its established guidelines and prior dialogue, even when agents speak interchangeably. \n",
    "For the manager agent and each unique search_agent that may exist in the trace, evaluate the execution efficiency for the agent's actions and responses. \n",
    "You must structure your entire response: \n",
    "**Manager Agent** \n",
    "[List each execution efficiency issue associated with the manager agent with an explanation and citation(s)] \n",
    "\n",
    "**search_agent 0** (if exists)\n",
    "[List each execution efficiency issue associated with this search_agent with an explanation and citation(s)] \n",
    "...\n",
    "**search_agent n** (if exists)\n",
    "[List each execution efficiency issue associated with this search_agent with an explanation and citation(s)] \n",
    "\n",
    "Here are some examples of execution efficiency issues:\n",
    "        {\n",
    "            \"evidence\": \"{'input.value': '{\\\"args\\\": [], \\\"sanitize_inputs_outputs\\\": true, \\\"kwargs\\\": {\\\"\\\": \\\"\\\"}}', 'openinference.span.kind': 'TOOL', 'pat.app': 'GAIA-Samples', 'pat.project.id': 'a69d64fc-5115-468e-95ed-0950bd37f06a', 'pat.project.name': 'gaia-annotation-samples', 'tool.description': 'Scroll the viewport DOWN one page-length in the current webpage and return the new viewport content.', 'tool.name': 'page_down', 'tool.parameters': '{}'}\",\n",
    "            \"description\": \"Resource Abuse error caused by a tool related mistake where the tool is repeatedly invoked with an invalid parameter (\\\"\\\": \\\"\\\" or \\\"\\\": {}), despite being defined with no parameters. This repeated misuse signals abnormal or excessive use of the tool with incorrect input, triggering a Resource Abuse error.\",\n",
    "        }\n",
    "Cite each issue with all corresponding span id numbers and the reason for the issue.\n",
    "\"\"\"\n",
    "\n",
    "plan_quality_prompt = \"\"\"\n",
    "Look for the keyword '[PLAN]' to identify plans for the manager agent and each unique search_agent that may exist in the trace. \n",
    "Your task is to evaluate the intrinsic quality of sequence of plans for each agent.\n",
    "You must structure your entire response: \n",
    "**Manager Agent** \n",
    "[Plan Quality issues] \n",
    "\n",
    "**search_agent 0** (if exists)\n",
    "[Plan Quality issues] \n",
    "\n",
    "... \n",
    "**search_agent n** (if exists)\n",
    "[Plan Quality issues]\n",
    "\n",
    "Here are some examples of plan quality issues:\n",
    "    {\n",
    "            \"evidence\": \"1. Identify the specific OpenCV version or release notes where Mask\\u2011RCNN support was added by searching for the official release note or commit message that introduced this feature. \\n2. Retrieve the commit history or changelog details for that version to determine the list of contributors responsible for adding Mask\\u2011RCNN support. \\n3. Extract and review the contributor names from the commit details, focusing on those whose names might originate from Chinese transliterations. \\n4. Research a reliable list of former Chinese heads of government with their names transliterated into the Latin alphabet. \\n5. Compare and cross-match the contributor names with the list of former Chinese heads of government to identify the one whose Latin name exactly matches. \\n6. Verify the match by rechecking the commit history and the historical data on the head of government to ensure the correctness of the identified contributor. \\n7. Conclude with the final contributor\\u2019s name as the correct answer.\",\n",
    "            \"description\": \"The model didn't define the tools needed in the plan, which may result in the model not using any tool since it needs to follow the plan.\",\n",
    "        },\n",
    "        {\n",
    "            \"evidence\": \"The plan listed in the output is the same as the plan generated in span 2, despite the system failing to execute steps 1 and 2 (via search_agent and inspect_file_as_text) in the preceding turns.\",\n",
    "            \"description\": \"The system generated an updated plan that was identical to the initial plan created before encountering tool execution failures, demonstrating a failure to integrate lessons learned from previous steps into its updated strategy.\",\n",
    "        },\n",
    "\n",
    "Cite each issue with all corresponding span id numbers and the reason for the issue.\n",
    "\"\"\"\n",
    "\n",
    "plan_adherence_prompt = \"\"\"\n",
    "Look for the keyword '[PLAN]' to identify plans for the manager agent and each unique search_agent that may exist in the trace. \n",
    "Each search_agent operates in a cycle: it first generates a plan, executes up to 4 tool calls based on that plan, and then re-plans. Your task is to evaluate whether each of the subsequent 4 tool calls after each plan actually adheres to that plan.\n",
    "You must structure your entire response: \n",
    "**Manager Agent** \n",
    "[Plan Adherence issues] \n",
    "\n",
    "**search_agent 0** (if exists)\n",
    "[Plan Adherence issues] \n",
    "... \n",
    "**search_agent n** (if exists)\n",
    "[Plan Adherence issues]\n",
    "\n",
    "Here are some examples of plan adherence issues:\n",
    "        {\n",
    "            \"evidence\": \"Plan step 1: 'Locate the official 2023 IPCC report (85 pages version) by using the search_agent tool'. Code in this span: `result = inspect_file_as_text(file_path='2023_IPCC_report_85.pdf', ...)`\",\n",
    "            \"description\": \"The system attempted to use the `inspect_file_as_text` tool with a hardcoded file path ('2023_IPCC_report_85.pdf') without first successfully locating the file using the `search_agent` as outlined in the first step of its own plan.\",\n",
    "        }\n",
    "        {\n",
    "            \"evidence\": \"The `search_agent` calls `final_answer` without having executed steps like systematically checking all submission pages, visiting detail pages for all candidates (e.g. Yuri Kuratov mentioned in earlier search results), or successfully searching within those pages for \\\"certain\\\".\",\n",
    "            \"description\": \"The LLM (search_agent) abandoned its most recent plan (generated in span d65ec360f7319e84), which involved systematically checking all pages and candidate papers for \\\"Yuri\\\" and \\\"certain\\\". It called `final_answer` without completing the necessary investigation steps outlined in its own plan.\",\n",
    "        }\n",
    "        \n",
    "Cite each issue with all corresponding span id numbers and the reason for the issue.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST SINGULAR TRACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = \"GAIA/041b7f9c8c76c2ca1a8e67c6769267c3.txt\"\n",
    "with open(test_file, \"r\") as f:\n",
    "    test_data = f.read()\n",
    "\n",
    "for i, (feedback_name, feedback_func) in enumerate(feedback_functions.items()):\n",
    "    if feedback_name == \"Logical Consistency\":\n",
    "        result = feedback_func(\n",
    "            test_data,\n",
    "            custom_instructions=GAIA_trace_explanation\n",
    "            + logical_consistency_prompt,\n",
    "        )\n",
    "    if feedback_name == \"Execution Efficiency\":\n",
    "        result = feedback_func(\n",
    "            test_data,\n",
    "            custom_instructions=GAIA_trace_explanation\n",
    "            + execution_efficiency_prompt,\n",
    "        )\n",
    "    if feedback_name == \"Plan Quality\":\n",
    "        result = feedback_func(\n",
    "            test_data,\n",
    "            custom_instructions=GAIA_trace_explanation + plan_quality_prompt,\n",
    "        )\n",
    "    if feedback_name == \"Plan Adherence\":\n",
    "        result = feedback_func(\n",
    "            test_data,\n",
    "            custom_instructions=GAIA_trace_explanation + plan_adherence_prompt,\n",
    "        )\n",
    "    if feedback_name == \"TRAIL\":\n",
    "        result = feedback_func(\n",
    "            test_data, custom_instructions=GAIA_trace_explanation\n",
    "        )\n",
    "    if isinstance(result, tuple) and len(result) == 2:\n",
    "        score, metadata = result\n",
    "        reason = metadata.get(\"reason\", \"\")\n",
    "        print(f\"{feedback_name}: {score} {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN ON ALL TRACES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "all_files = []\n",
    "gaia_dir = \"GAIA\"\n",
    "for filename in os.listdir(gaia_dir):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filepath = os.path.join(gaia_dir, filename)\n",
    "        all_files.append(filepath)\n",
    "\n",
    "all_files.sort()\n",
    "\n",
    "train_files, test_files = train_test_split(\n",
    "    all_files, test_size=0.5, random_state=42\n",
    ")\n",
    "train_files.sort()\n",
    "test_files.sort()\n",
    "print(f\"train_files: {train_files}\")\n",
    "print(f\"test_files: {test_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"test\"\n",
    "if split == \"train\":\n",
    "    all_files = train_files\n",
    "else:\n",
    "    all_files = test_files\n",
    "\n",
    "csv_path = \"trailprompt_test_withcontrolflow.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "\n",
    "print(f\"Status: {len(all_results)} completed, {len(all_files)} remaining\")\n",
    "print(f\" Next files to process: {all_files[:3]}\")\n",
    "\n",
    "# Process remaining files one by one\n",
    "for i, file in enumerate(all_files):\n",
    "    file_name = file.split(\".\")[0]\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Processing {i + 1}/{len(all_files)}: {file_name}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    try:\n",
    "        # Read file\n",
    "        with open(file, \"r\") as f:\n",
    "            gaia_file = f.read()\n",
    "\n",
    "        results = {\"filename\": file_name}\n",
    "\n",
    "        # Process each feedback function\n",
    "        for j, (feedback_name, feedback_func) in enumerate(\n",
    "            feedback_functions.items()\n",
    "        ):\n",
    "            print(\n",
    "                f\"[{j + 1}/{len(feedback_functions)}] Evaluating: {feedback_name}\"\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                if feedback_name == \"Logical Consistency\":\n",
    "                    result = feedback_func(\n",
    "                        gaia_file,\n",
    "                        custom_instructions=GAIA_trace_explanation\n",
    "                        + logical_consistency_prompt,\n",
    "                    )\n",
    "                if feedback_name == \"Execution Efficiency\":\n",
    "                    result = feedback_func(\n",
    "                        gaia_file,\n",
    "                        custom_instructions=GAIA_trace_explanation\n",
    "                        + execution_efficiency_prompt,\n",
    "                    )\n",
    "                if feedback_name == \"Plan Quality\":\n",
    "                    result = feedback_func(\n",
    "                        gaia_file,\n",
    "                        custom_instructions=GAIA_trace_explanation\n",
    "                        + plan_quality_prompt,\n",
    "                    )\n",
    "                if feedback_name == \"Plan Adherence\":\n",
    "                    result = feedback_func(\n",
    "                        gaia_file,\n",
    "                        custom_instructions=GAIA_trace_explanation\n",
    "                        + plan_adherence_prompt,\n",
    "                    )\n",
    "                if feedback_name == \"TRAIL\":\n",
    "                    result = feedback_func(\n",
    "                        gaia_file, custom_instructions=GAIA_trace_explanation\n",
    "                    )\n",
    "                if isinstance(result, tuple) and len(result) == 2:\n",
    "                    score, metadata = result\n",
    "                    results[f\"{feedback_name}_score\"] = score\n",
    "                    reason = metadata.get(\"reason\", \"\")\n",
    "                    results[f\"{feedback_name}_reasons\"] = reason\n",
    "                    print(f\"Score: {score}\")\n",
    "                else:\n",
    "                    print(\"Unexpected result format\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {str(e)[:100]}...\")\n",
    "                results[f\"{feedback_name}_score\"] = None\n",
    "                results[f\"{feedback_name}_reasons\"] = f\"Error: {str(e)[:200]}\"\n",
    "\n",
    "        # Add to results and save immediately\n",
    "        all_results.append(results)\n",
    "\n",
    "        results_df = pd.DataFrame([results])\n",
    "        results_df.to_csv(\n",
    "            csv_path, mode=\"a\", header=not os.path.exists(csv_path), index=False\n",
    "        )\n",
    "        print(\n",
    "            f\"Completed {file_name} | Total: {len(all_results)}/{len(all_files)}\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"FAILED {file_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Final save\n",
    "print(f\"\\nFinished processing {len(all_results)} files\")\n",
    "print(all_results)\n",
    "# final_df = pd.DataFrame(all_results)\n",
    "# final_df.to_csv(\"trail_benchmark_aug18.csv\", index=False)\n",
    "# print(\"Final results saved to: trail_benchmark_aug18.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
