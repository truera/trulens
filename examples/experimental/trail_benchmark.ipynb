{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Feedback Function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create snowpark session.\n",
    "\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "# snowflake_connection_parameters = {\n",
    "#     \"account\": os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "#     \"user\": os.environ[\"SNOWFLAKE_USER\"],\n",
    "#     \"password\": os.environ[\"SNOWFLAKE_USER_PASSWORD\"],\n",
    "#     \"database\": os.environ[\"SNOWFLAKE_DATABASE\"],\n",
    "#     \"schema\": os.environ[\"SNOWFLAKE_SCHEMA\"],\n",
    "#     \"role\": os.environ[\"SNOWFLAKE_ROLE\"],\n",
    "#     \"warehouse\": os.environ[\"SNOWFLAKE_WAREHOUSE\"],\n",
    "# }\n",
    "\n",
    "\n",
    "snowflake_connection_parameters = {\n",
    "    \"account\": \"SNOWHOUSE\",\n",
    "    \"user\": \"ajia\",\n",
    "    \"authenticator\": \"externalbrowser\",\n",
    "}\n",
    "snowpark_session = Session.builder.configs(\n",
    "    snowflake_connection_parameters\n",
    ").create()\n",
    "\n",
    "# TruSession is no longer required as long as snowflake connector exists\n",
    "# sf_connector = SnowflakeConnector(snowpark_session=snowpark_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.providers.cortex import Cortex\n",
    "\n",
    "provider = Cortex(\n",
    "    model_engine=\"claude-4-sonnet\",\n",
    "    snowpark_session=snowpark_session,\n",
    "    reasoning_effort=\"high\",\n",
    ")\n",
    "# provider = OpenAI(model_engine=\"o3\")\n",
    "\n",
    "feedback_functions = {\n",
    "    \"Logical Consistency\": provider.logical_consistency_with_cot_reasons,\n",
    "    # \"Execution Efficiency\": provider.execution_efficiency_with_cot_reasons,\n",
    "    # \"Plan Adherence\": provider.plan_adherence_with_cot_reasons,\n",
    "    # \"Plan Quality\": provider.plan_quality_with_cot_reasons,\n",
    "    # \"TRAIL\": provider.trail_with_cot_reasons,\n",
    "}\n",
    "\n",
    "for name in feedback_functions:\n",
    "    print(f\"- {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROCESS ALL GAIA FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from preprocess_trail_gaia import build_span_tree\n",
    "from preprocess_trail_gaia import reset_state\n",
    "\n",
    "gaia_dir = \"GAIA\"\n",
    "all_files = []\n",
    "for filename in os.listdir(gaia_dir):\n",
    "    if filename.endswith(\"0140b3f657eddf76ca82f72c49ac8e58.json\"):\n",
    "        reset_state()\n",
    "        print(filename)\n",
    "        filepath = os.path.join(gaia_dir, filename)\n",
    "        with open(filepath, \"r\") as f:\n",
    "            trace_data = json.load(f)\n",
    "        output_file = os.path.join(gaia_dir, filename.replace(\".json\", \".txt\"))\n",
    "        with open(output_file, \"w\") as f:\n",
    "            f.write(f\"Trace ID: {trace_data['trace_id']}\\n\\n\")\n",
    "        root_spans = build_span_tree(trace_data)\n",
    "        for root_span in root_spans:\n",
    "            root_span.display(output_file=output_file)\n",
    "        all_files.append(output_file)\n",
    "\n",
    "print(len(all_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAIA CUSTOM INSTRUCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of agent architecture and trace structure\n",
    "GAIA_trace_explanation = \"\"\"\n",
    "Agent Architecture and Trace Structure: The agent architecture consists of a primary manager Agent (also referred to as CodeAgent) that delegates tasks to a search_agent (also referred to as ToolCallingAgent).\n",
    "\n",
    "Overall Flow:\n",
    "Every trace consists of several spans (with span_id numbers and parent span_id numbers). Each trace begins with the manager (CodeAgent). The process follows a clear, hierarchical structure where the manager outlines a high-level plan and the search_agent executes the detailed, tool-based steps for each part of that plan.\n",
    "\n",
    "1. Manager Agent Initiation:\n",
    "The trace starts with the manager. In its initial child spans, you will observe the following sequence:\n",
    "- A preparatory survey is created based on the user's query.\n",
    "- A high-level plan is formulated from this survey.\n",
    "\n",
    "The Manager agent begins executing Step 1 of its plan.\n",
    "\n",
    "2. Manager Agent Step 1:\n",
    "Within the child span for Step 1, the Manager agent decides how to proceed given the initial fact survey and plan. The Manager agent will produce a thought, which may call the search_agent to perform the necessary actions or research.\n",
    "\n",
    "3. search_agent (ToolCallingAgent) Execution Loop:\n",
    "Once called, the search_agent begins its own execution loop. In its child spans, you will observe the following sequence:\n",
    "- A preparatory survey to the specific sub-task it received from the Manager agent.\n",
    "- A plan tailored to the specific sub-task it received from the Manager agent.\n",
    "\n",
    "The search_agent executes an initial set of up to four steps. Each step involves an LLM call to generate a tool-call, followed by the tool's execution.\n",
    "After these initial steps, search_agent synthesizes the information gathered into an updated fact list and refines its plan.\n",
    "The search_agent may then continue to execute more tool-steps based on this updated plan.\n",
    "\n",
    "This loop continues until the search_agent has gathered enough information to comprehensively answer the manager's sub-task, at which point it calls final_answer.\n",
    "\n",
    "4. Returning Control to the Manager agent\n",
    "The final_answer from the search_agent is returned to the Manager agent, concluding the Manager agent's Step 1. The Manager agent then proceeds to Step 2 of its high-level plan, using the result from the previous step as context. \n",
    "This entire cycle repeats for all subsequent steps in the Manager Agent's plan.\n",
    "\n",
    "Whenever you want to point out anything in the trace, cite the span_id number of the span that you are referring to.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom instructions for each metric (mostly around shaping the output)\n",
    "logical_consistency_prompt = \"\"\"\n",
    "Track each agent's system instructions and conversation history, ensuring all subsequent outputs from that agent adhere to its established guidelines and prior dialogue, even when agents speak interchangeably. \n",
    "For the manager agent and each unique search_agent that may exist in the trace, evaluate the logical consistency for the agent's actions and responses. For each agent, ensure that each response is consistent with the system instructions and prior dialogue.\n",
    "You must structure your entire response: \n",
    "**Manager Agent** \n",
    "**System Instructions**\n",
    "[Paste all system instructions associated with the manager agent.]\n",
    "**Logical Consistency issues**\n",
    "[All Logical Consistency issues associated with the manager agent] \n",
    "\n",
    "**search_agent 0** (if exists)\n",
    "**System Instructions**\n",
    "[Paste all system instructions associated with the search_agent.]\n",
    "**Logical Consistency issues**\n",
    "[List all Logical Consistency issues associated with this search_agent] \n",
    "... \n",
    "**search_agent n** (if exists)\n",
    "**System Instructions**\n",
    "[Paste all system instructions associated with the search_agent.]\n",
    "**Logical Consistency issues**\n",
    "[List all Logical Consistency issues associated with this search_agent]\n",
    "\n",
    "Here are some examples of logical consistency issues:\n",
    "        {\n",
    "            \"evidence\": \"The plan output content ends with the last step of the plan instead of the `<end_plan>` tag.\",\n",
    "            \"description\": \"The plan generation step did not conclude with the required '<end_plan>' tag as specified in the instructions for plan generation.\",\n",
    "        },\n",
    "        {\n",
    "            \"evidence\": \"Thought: I recall that Girls Who Code published a statistic showing that in the 1980s, women accounted for 37% of computer scientists (or computer science graduates) and that over a span of three decades that figure dropped by 13 percentage points (from 37% down to 24%)\",\n",
    "            \"description\": \"The system recalled a statistic about Girls Who Code and the percentage of women in computer science, but this information was not verified using the search tool as planned. The system states \\\"Thought: I recall that\\nGirls Who Code published a statistic showing that in the 1980s, women accounted for 37% of computer scientists (or computer science graduates) and that over a span of three decades that figure dropped by 13 percentage points\\n(from 37% down to 24%). In other words, it took 30 years for that change to occur. Based on that well-circulated statistic that Girls Who Code highlighted, I will output 30 years as the final answer.\"\n",
    "        }\n",
    "Cite each issue with all corresponding span id numbers and the reason for the issue.\n",
    "\"\"\"\n",
    "\n",
    "execution_efficiency_prompt = \"\"\"\n",
    "Track each agent's system instructions and conversation history, ensuring all subsequent outputs from that agent adhere to its established guidelines and prior dialogue, even when agents speak interchangeably. \n",
    "For the manager agent and each unique search_agent that may exist in the trace, evaluate the execution efficiency for the agent's actions and responses. \n",
    "You must structure your entire response: \n",
    "**Manager Agent** \n",
    "[List all execution efficiency issues associated with the manager agent] \n",
    "\n",
    "**search_agent 0** (if exists)\n",
    "[List all execution efficiency issues associated with this search_agent] \n",
    "...\n",
    "**search_agent n** (if exists)\n",
    "[List all execution efficiency issues associated with this search_agent] \n",
    "\n",
    "Here are some examples of execution efficiency issues:\n",
    "        {\n",
    "            \"evidence\": \"{'input.value': '{\\\"args\\\": [], \\\"sanitize_inputs_outputs\\\": true, \\\"kwargs\\\": {\\\"\\\": \\\"\\\"}}', 'openinference.span.kind': 'TOOL', 'pat.app': 'GAIA-Samples', 'pat.project.id': 'a69d64fc-5115-468e-95ed-0950bd37f06a', 'pat.project.name': 'gaia-annotation-samples', 'tool.description': 'Scroll the viewport DOWN one page-length in the current webpage and return the new viewport content.', 'tool.name': 'page_down', 'tool.parameters': '{}'}\",\n",
    "            \"description\": \"Resource Abuse error caused by a tool related mistake where the tool is repeatedly invoked with an invalid parameter (\\\"\\\": \\\"\\\" or \\\"\\\": {}), despite being defined with no parameters. This repeated misuse signals abnormal or excessive use of the tool with incorrect input, triggering a Resource Abuse error.\",\n",
    "        }\n",
    "Cite each issue with all corresponding span id numbers and the reason for the issue.\n",
    "\"\"\"\n",
    "\n",
    "plan_quality_prompt = \"\"\"\n",
    "Look for the keyword '[PLAN]' to identify plans for the manager agent and each unique search_agent that may exist in the trace. \n",
    "Your task is to evaluate the intrinsic quality of sequence of plans for each agent.\n",
    "You must structure your entire response: \n",
    "**Manager Agent** \n",
    "[Plan Quality issues] \n",
    "\n",
    "**search_agent 0** (if exists)\n",
    "[Plan Quality issues] \n",
    "\n",
    "... \n",
    "**search_agent n** (if exists)\n",
    "[Plan Quality issues]\n",
    "\n",
    "Here are some examples of plan quality issues:\n",
    "    {\n",
    "            \"evidence\": \"1. Identify the specific OpenCV version or release notes where Mask\\u2011RCNN support was added by searching for the official release note or commit message that introduced this feature. \\n2. Retrieve the commit history or changelog details for that version to determine the list of contributors responsible for adding Mask\\u2011RCNN support. \\n3. Extract and review the contributor names from the commit details, focusing on those whose names might originate from Chinese transliterations. \\n4. Research a reliable list of former Chinese heads of government with their names transliterated into the Latin alphabet. \\n5. Compare and cross-match the contributor names with the list of former Chinese heads of government to identify the one whose Latin name exactly matches. \\n6. Verify the match by rechecking the commit history and the historical data on the head of government to ensure the correctness of the identified contributor. \\n7. Conclude with the final contributor\\u2019s name as the correct answer.\",\n",
    "            \"description\": \"The model didn't define the tools needed in the plan, which may result in the model not using any tool since it needs to follow the plan.\",\n",
    "        },\n",
    "        {\n",
    "            \"evidence\": \"The plan listed in the output is the same as the plan generated in span 2, despite the system failing to execute steps 1 and 2 (via search_agent and inspect_file_as_text) in the preceding turns.\",\n",
    "            \"description\": \"The system generated an updated plan that was identical to the initial plan created before encountering tool execution failures, demonstrating a failure to integrate lessons learned from previous steps into its updated strategy.\",\n",
    "        },\n",
    "\n",
    "Cite each issue with all corresponding span id numbers and the reason for the issue.\n",
    "\"\"\"\n",
    "\n",
    "plan_adherence_prompt = \"\"\"\n",
    "Look for the keyword '[PLAN]' to identify plans for the manager agent and each unique search_agent that may exist in the trace. \n",
    "Each search_agent operates in a cycle: it first generates a plan, executes up to 4 tool calls based on that plan, and then re-plans. Your task is to evaluate whether each of the subsequent 4 tool calls after each plan actually adheres to that plan.\n",
    "You must structure your entire response: \n",
    "**Manager Agent** \n",
    "[Plan Adherence issues] \n",
    "\n",
    "**search_agent 0** (if exists)\n",
    "[Plan Adherence issues] \n",
    "... \n",
    "**search_agent n** (if exists)\n",
    "[Plan Adherence issues]\n",
    "\n",
    "Here are some examples of plan adherence issues:\n",
    "        {\n",
    "            \"evidence\": \"Plan step 1: 'Locate the official 2023 IPCC report (85 pages version) by using the search_agent tool'. Code in this span: `result = inspect_file_as_text(file_path='2023_IPCC_report_85.pdf', ...)`\",\n",
    "            \"description\": \"The system attempted to use the `inspect_file_as_text` tool with a hardcoded file path ('2023_IPCC_report_85.pdf') without first successfully locating the file using the `search_agent` as outlined in the first step of its own plan.\",\n",
    "        }\n",
    "        {\n",
    "            \"evidence\": \"The `search_agent` calls `final_answer` without having executed steps like systematically checking all submission pages, visiting detail pages for all candidates (e.g. Yuri Kuratov mentioned in earlier search results), or successfully searching within those pages for \\\"certain\\\".\",\n",
    "            \"description\": \"The LLM (search_agent) abandoned its most recent plan (generated in span d65ec360f7319e84), which involved systematically checking all pages and candidate papers for \\\"Yuri\\\" and \\\"certain\\\". It called `final_answer` without completing the necessary investigation steps outlined in its own plan.\",\n",
    "        }\n",
    "        \n",
    "Cite each issue with all corresponding span id numbers and the reason for the issue.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAIA: TEST SINGULAR TRACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = \"GAIA/fb3333ca30eb8af56d4f31839ca9e317.txt\"\n",
    "with open(test_file, \"r\") as f:\n",
    "    test_data = f.read()\n",
    "\n",
    "for i, (feedback_name, feedback_func) in enumerate(feedback_functions.items()):\n",
    "    if feedback_name == \"Logical Consistency\":\n",
    "        result = feedback_func(\n",
    "            test_data,\n",
    "            custom_instructions=GAIA_trace_explanation\n",
    "            + logical_consistency_prompt,\n",
    "        )\n",
    "    if feedback_name == \"Execution Efficiency\":\n",
    "        result = feedback_func(\n",
    "            test_data,\n",
    "            custom_instructions=GAIA_trace_explanation\n",
    "            + execution_efficiency_prompt,\n",
    "        )\n",
    "    if feedback_name == \"Plan Quality\":\n",
    "        result = feedback_func(\n",
    "            test_data,\n",
    "            custom_instructions=GAIA_trace_explanation + plan_quality_prompt,\n",
    "        )\n",
    "    if feedback_name == \"Plan Adherence\":\n",
    "        result = feedback_func(\n",
    "            test_data,\n",
    "            custom_instructions=GAIA_trace_explanation + plan_adherence_prompt,\n",
    "        )\n",
    "    if feedback_name == \"TRAIL\":\n",
    "        result = feedback_func(\n",
    "            test_data, custom_instructions=GAIA_trace_explanation\n",
    "        )\n",
    "    if isinstance(result, tuple) and len(result) == 2:\n",
    "        score, metadata = result\n",
    "        reason = metadata.get(\"reason\", \"\")\n",
    "        print(f\"{feedback_name}: {score} {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAIA: RUN ON ALL TRACES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "all_files = []\n",
    "gaia_dir = \"GAIA\"\n",
    "for filename in os.listdir(gaia_dir):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filepath = os.path.join(gaia_dir, filename)\n",
    "        all_files.append(filepath)\n",
    "\n",
    "all_files.sort()\n",
    "\n",
    "train_files, test_files = train_test_split(\n",
    "    all_files, test_size=0.5, random_state=42\n",
    ")\n",
    "train_files.sort()\n",
    "test_files.sort()\n",
    "print(f\"train_files: {train_files}\")\n",
    "print(f\"test_files: {test_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"train\"\n",
    "if split == \"train\":\n",
    "    all_files = train_files\n",
    "else:\n",
    "    all_files = test_files\n",
    "\n",
    "csv_path = \"planning_train_eval_trail_sep6.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "\n",
    "print(f\"Status: {len(all_results)} completed, {len(all_files)} remaining\")\n",
    "print(f\" Next files to process: {all_files[:3]}\")\n",
    "\n",
    "# Process remaining files one by one\n",
    "for i, file in enumerate(all_files):\n",
    "    file_name = file.split(\".\")[0]\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Processing {i + 1}/{len(all_files)}: {file_name}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    try:\n",
    "        # Read file\n",
    "        with open(file, \"r\") as f:\n",
    "            gaia_file = f.read()\n",
    "\n",
    "        results = {\"filename\": file_name}\n",
    "\n",
    "        # Process each feedback function\n",
    "        for j, (feedback_name, feedback_func) in enumerate(\n",
    "            feedback_functions.items()\n",
    "        ):\n",
    "            print(\n",
    "                f\"[{j + 1}/{len(feedback_functions)}] Evaluating: {feedback_name}\"\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                if feedback_name == \"Logical Consistency\":\n",
    "                    result = feedback_func(\n",
    "                        gaia_file,\n",
    "                        custom_instructions=GAIA_trace_explanation\n",
    "                        + logical_consistency_prompt,\n",
    "                    )\n",
    "                if feedback_name == \"Execution Efficiency\":\n",
    "                    result = feedback_func(\n",
    "                        gaia_file,\n",
    "                        custom_instructions=GAIA_trace_explanation\n",
    "                        + execution_efficiency_prompt,\n",
    "                    )\n",
    "                if feedback_name == \"Plan Quality\":\n",
    "                    result = feedback_func(\n",
    "                        gaia_file,\n",
    "                        custom_instructions=GAIA_trace_explanation\n",
    "                        + plan_quality_prompt,\n",
    "                    )\n",
    "                if feedback_name == \"Plan Adherence\":\n",
    "                    result = feedback_func(\n",
    "                        gaia_file,\n",
    "                        custom_instructions=GAIA_trace_explanation\n",
    "                        + plan_adherence_prompt,\n",
    "                    )\n",
    "                if feedback_name == \"TRAIL\":\n",
    "                    result = feedback_func(\n",
    "                        gaia_file, custom_instructions=GAIA_trace_explanation\n",
    "                    )\n",
    "                if isinstance(result, tuple) and len(result) == 2:\n",
    "                    score, metadata = result\n",
    "                    results[f\"{feedback_name}_score\"] = score\n",
    "                    reason = metadata.get(\"reason\", \"\")\n",
    "                    results[f\"{feedback_name}_reasons\"] = reason\n",
    "                    print(f\"Score: {score}\")\n",
    "                else:\n",
    "                    print(\"Unexpected result format\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {str(e)[:100]}...\")\n",
    "                results[f\"{feedback_name}_score\"] = None\n",
    "                results[f\"{feedback_name}_reasons\"] = f\"Error: {str(e)[:200]}\"\n",
    "\n",
    "        # Add to results and save immediately\n",
    "        all_results.append(results)\n",
    "\n",
    "        results_df = pd.DataFrame([results])\n",
    "        results_df.to_csv(\n",
    "            csv_path, mode=\"a\", header=not os.path.exists(csv_path), index=False\n",
    "        )\n",
    "        print(\n",
    "            f\"Completed {file_name} | Total: {len(all_results)}/{len(all_files)}\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"FAILED {file_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Final save\n",
    "print(f\"\\nFinished processing {len(all_results)} files\")\n",
    "print(all_results)\n",
    "# final_df = pd.DataFrame(all_results)\n",
    "# final_df.to_csv(\"trail_benchmark_aug18.csv\", index=False)\n",
    "# print(\"Final results saved to: trail_benchmark_aug18.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROCESS ALL SWE-BENCH TRACES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from preprocess_swebench import build_span_tree\n",
    "from preprocess_swebench import reset_state\n",
    "\n",
    "swebench_dir = \"SWE_Bench\"\n",
    "all_files = []\n",
    "for filename in os.listdir(swebench_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        reset_state()\n",
    "        print(filename)\n",
    "        filepath = os.path.join(swebench_dir, filename)\n",
    "        with open(filepath, \"r\") as f:\n",
    "            trace_data = json.load(f)\n",
    "        output_file = os.path.join(\n",
    "            swebench_dir, filename.replace(\".json\", \".txt\")\n",
    "        )\n",
    "        with open(output_file, \"w\") as f:\n",
    "            f.write(f\"Trace ID: {trace_data['trace_id']}\\n\\n\")\n",
    "        root_spans = build_span_tree(trace_data)\n",
    "        for root_span in root_spans:\n",
    "            root_span.display(output_file=output_file)\n",
    "        all_files.append(output_file)\n",
    "\n",
    "print(len(all_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SWE-BENCH CUSTOM INSTRUCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of agent architecture and trace structure\n",
    "SWEBench_trace_explanation = \"\"\"\n",
    "Agent Architecture and Trace Structure: The agent architecture consists of a CodeAgent that has access to a sandboxed environment, a python interpreter, and the \"gitingest\" library that can turn any Git reposistory into a text digest of its codebaes.\n",
    "\n",
    "Overall Flow:\n",
    "Every trace consists of several spans (with span_id numbers and parent span_id numbers). Each trace begins with the CodeAgent which performs actions through a cycle of steps, with existing variables and knowledge being incorporated into the agentâ€™s context. Specifically, the CodeAgent will plan forward to proceed in a series of steps, in a cycle of 'Thought:', 'Code:', and 'Observation:' sequences.  \n",
    "\n",
    "At each step, in the 'Thought:' sequence, the CodeAgent should first explain its reasoning towards solving the task and the tools that it wants to use. Then in the 'Code:' sequence, it should write the code in simple Python. The code sequence must end with '<end_code>' sequence. During each intermediate step, the CodeAgent can use 'print()' to save whatever important information it will then need. These print outputs will then appear in the 'Observation:' field, which will be available as input for the next step.\n",
    "Each tool call and tool response will also be shown in each step. \n",
    "\n",
    "In the end, the CodeAgent will have to return a final answer using the `final_answer` tool\n",
    "\n",
    "Whenever you want to point out anything in the trace, cite the span_id number of the span that you are referring to.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom instructions for each metric (mostly around shaping the output)\n",
    "swe_logical_consistency_prompt = \"\"\"\n",
    "Evaluate the logical consistency for the agent's actions and responses. Ensure that each response is consistent with the system instructions and prior dialogue.\n",
    "You must structure your entire response: \n",
    "**Agent** \n",
    "**System Instructions**\n",
    "[Paste all system instructions associated with the agent]\n",
    "**Logical Consistency issues**\n",
    "[List ALL Logical Consistency issues associated with the agent] \n",
    "\n",
    "Here are some examples of logical consistency issues:\n",
    "        {\n",
    "            \"description\": \"The model did not adhere to the size of the file as stated in instruction, where the instruction say \\\" STRICTLY DO NOT print file contents to the terminal for analysis at all costs. If you are unsure about the file size, simply print upto the first 500 characters to scan the contents of the file and then find the required information using regex.\\\".\",\n",
    "        },\n",
    "        {\n",
    "            \"description\": \"The system provided output in the final shard, \\nbut this information was not verified in the code or using any tool.\",\n",
    "        }\n",
    "Cite each issue with all corresponding span id numbers and the reason for the issue.\n",
    "\"\"\"\n",
    "\n",
    "swe_execution_efficiency_prompt = \"\"\"\n",
    "Evaluate the execution efficiency for the agent's actions and responses. \n",
    "You must structure your entire response: \n",
    "**Agent** \n",
    "[List all execution efficiency issues associated with the agent.] \n",
    "\n",
    "Here are some examples of execution efficiency issues:\n",
    "        {\n",
    "            \"description\": \"The model repeatedly printed the tree, trying to find the jsonrep.py file path, without reaching useful results, which might be considered abusing the resources and not using the code effectively.\",\n",
    "        }\n",
    "Be specific with each issue and cite each issue with all corresponding span id numbers and the reason for the issue.\n",
    "\"\"\"\n",
    "\n",
    "swe_plan_quality_prompt = \"\"\"\n",
    "Each of the 'Thought' sections identified will be considered a step in the plan. Your task is to evaluate the quality of the entire plan, or the entire sequence of 'Thought' sections.\n",
    "You must structure your entire response: \n",
    "**Agent** \n",
    "[Plan Quality issues] \n",
    "\n",
    "Cite each issue with all corresponding span id numbers and the reason for the issue.\n",
    "\"\"\"\n",
    "\n",
    "swe_plan_adherence_prompt = \"\"\"\n",
    "Each of the 'Thought' sections identified will be considered a step in the plan. Your task is to evaluate the quality of the entire plan, or the entire sequence of 'Thought' sections.\n",
    "Your task is to evaluate whether each of the subsequent tool calls after each \"Thought\" section actually adheres to that \"Thought\".\n",
    "You must structure your entire response: \n",
    "**Agent** \n",
    "[Plan Adherence issues] \n",
    "\n",
    "Here are some examples of plan adherence issues:\n",
    "        {\n",
    "            \"description\": \"The system did not follow the plan that was provided in the thought in the previous shard and fabricated a final answer instead.\",\n",
    "        }\n",
    "        \n",
    "Cite each issue with all corresponding span id numbers and the reason for the issue.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN ONE SWE-BENCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = \"SWE_Bench/2102eea2af6327834c8bd97b1488474c.txt\"\n",
    "with open(test_file, \"r\") as f:\n",
    "    test_data = f.read()\n",
    "\n",
    "for i, (feedback_name, feedback_func) in enumerate(feedback_functions.items()):\n",
    "    if feedback_name == \"Logical Consistency\":\n",
    "        result = feedback_func(\n",
    "            test_data,\n",
    "            custom_instructions=SWEBench_trace_explanation\n",
    "            + swe_logical_consistency_prompt,\n",
    "        )\n",
    "    if feedback_name == \"Execution Efficiency\":\n",
    "        result = feedback_func(\n",
    "            test_data,\n",
    "            custom_instructions=SWEBench_trace_explanation\n",
    "            + swe_execution_efficiency_prompt,\n",
    "        )\n",
    "    if feedback_name == \"Plan Quality\":\n",
    "        result = feedback_func(\n",
    "            test_data,\n",
    "            custom_instructions=SWEBench_trace_explanation\n",
    "            + swe_plan_quality_prompt,\n",
    "        )\n",
    "    if feedback_name == \"Plan Adherence\":\n",
    "        result = feedback_func(\n",
    "            test_data,\n",
    "            custom_instructions=SWEBench_trace_explanation\n",
    "            + swe_plan_adherence_prompt,\n",
    "        )\n",
    "    if feedback_name == \"TRAIL\":\n",
    "        result = feedback_func(\n",
    "            test_data, custom_instructions=SWEBench_trace_explanation\n",
    "        )\n",
    "    if isinstance(result, tuple) and len(result) == 2:\n",
    "        score, metadata = result\n",
    "        reason = metadata.get(\"reason\", \"\")\n",
    "        print(f\"{feedback_name}: {score} {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN ALL SWE-BENCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "all_files = []\n",
    "swebench_dir = \"SWE_Bench\"\n",
    "for filename in os.listdir(swebench_dir):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filepath = os.path.join(swebench_dir, filename)\n",
    "        all_files.append(filepath)\n",
    "\n",
    "all_files.sort()\n",
    "\n",
    "train_files, test_files = train_test_split(\n",
    "    all_files, test_size=0.5, random_state=42\n",
    ")\n",
    "train_files.sort()\n",
    "test_files.sort()\n",
    "print(f\"train_files: {train_files}\")\n",
    "print(f\"test_files: {test_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"train\"\n",
    "if split == \"train\":\n",
    "    all_files = train_files\n",
    "else:\n",
    "    all_files = test_files\n",
    "\n",
    "csv_path = \"swebench_train_eval_sep8.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_results = []\n",
    "\n",
    "print(f\"Status: {len(all_results)} completed, {len(all_files)} remaining\")\n",
    "print(f\" Next files to process: {all_files[:3]}\")\n",
    "\n",
    "# Process remaining files one by one\n",
    "for i, file in enumerate(all_files):\n",
    "    file_name = file.split(\".\")[0]\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Processing {i + 1}/{len(all_files)}: {file_name}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    try:\n",
    "        # Read file\n",
    "        with open(file, \"r\") as f:\n",
    "            swe_file = f.read()\n",
    "\n",
    "        results = {\"filename\": file_name}\n",
    "\n",
    "        # Process each feedback function\n",
    "        for j, (feedback_name, feedback_func) in enumerate(\n",
    "            feedback_functions.items()\n",
    "        ):\n",
    "            print(\n",
    "                f\"[{j + 1}/{len(feedback_functions)}] Evaluating: {feedback_name}\"\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                if feedback_name == \"Logical Consistency\":\n",
    "                    result = feedback_func(\n",
    "                        swe_file,\n",
    "                        custom_instructions=SWEBench_trace_explanation\n",
    "                        + swe_logical_consistency_prompt,\n",
    "                    )\n",
    "                if feedback_name == \"Execution Efficiency\":\n",
    "                    result = feedback_func(\n",
    "                        swe_file,\n",
    "                        custom_instructions=SWEBench_trace_explanation\n",
    "                        + swe_execution_efficiency_prompt,\n",
    "                    )\n",
    "                if feedback_name == \"Plan Quality\":\n",
    "                    result = feedback_func(\n",
    "                        swe_file,\n",
    "                        custom_instructions=SWEBench_trace_explanation\n",
    "                        + swe_plan_quality_prompt,\n",
    "                    )\n",
    "                if feedback_name == \"Plan Adherence\":\n",
    "                    result = feedback_func(\n",
    "                        swe_file,\n",
    "                        custom_instructions=SWEBench_trace_explanation\n",
    "                        + swe_plan_adherence_prompt,\n",
    "                    )\n",
    "                if feedback_name == \"TRAIL\":\n",
    "                    result = feedback_func(\n",
    "                        swe_file, custom_instructions=SWEBench_trace_explanation\n",
    "                    )\n",
    "                if isinstance(result, tuple) and len(result) == 2:\n",
    "                    score, metadata = result\n",
    "                    results[f\"{feedback_name}_score\"] = score\n",
    "                    reason = metadata.get(\"reason\", \"\")\n",
    "                    results[f\"{feedback_name}_reasons\"] = reason\n",
    "                    print(f\"Score: {score}\")\n",
    "                else:\n",
    "                    print(\"Unexpected result format\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {str(e)[:100]}...\")\n",
    "                results[f\"{feedback_name}_score\"] = None\n",
    "                results[f\"{feedback_name}_reasons\"] = f\"Error: {str(e)[:200]}\"\n",
    "\n",
    "        # Add to results and save immediately\n",
    "        all_results.append(results)\n",
    "\n",
    "        results_df = pd.DataFrame([results])\n",
    "        results_df.to_csv(\n",
    "            csv_path, mode=\"a\", header=not os.path.exists(csv_path), index=False\n",
    "        )\n",
    "        print(\n",
    "            f\"Completed {file_name} | Total: {len(all_results)}/{len(all_files)}\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"FAILED {file_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Final save\n",
    "print(f\"\\nFinished processing {len(all_results)} files\")\n",
    "print(all_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
