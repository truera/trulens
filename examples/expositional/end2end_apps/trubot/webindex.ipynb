{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape websites to create document retrieval stores.\n",
    "\n",
    "## Additional requirements:\n",
    "\n",
    "```bash\n",
    "pip install humanize pdfreader url_normalize tabulate unstructured langchain_community\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install humanize pdfreader url_normalize tabulate unstructured langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(Path().cwd().parent.parent.parent.parent.resolve()))\n",
    "\n",
    "from trulens_eval.keys import check_keys\n",
    "\n",
    "check_keys(\n",
    "    \"OPENAI_API_KEY\"\n",
    ")\n",
    "\n",
    "\"ignore me\"\n",
    "\n",
    "import datetime\n",
    "import io\n",
    "from multiprocessing import Event\n",
    "from pathlib import Path\n",
    "from queue import Queue\n",
    "import sqlite3\n",
    "import tempfile\n",
    "from threading import Thread\n",
    "from time import sleep\n",
    "from typing import Callable, Iterable, Sequence, Union\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import humanize\n",
    "from langchain.document_loaders import PagedPDFSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "import numpy as np\n",
    "import pdfreader\n",
    "import pinecone\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "from url_normalize import url_normalize\n",
    "\n",
    "from trulens_eval.utils.containers import first\n",
    "from trulens_eval.utils.text import UNICODE_CHECK\n",
    "\n",
    "TRUERA_BASE_URL = 'https://truera.com/'\n",
    "TRUERA_DOC_URL = 'https://docs.truera.com/1.34/public/'\n",
    "TRUERA_SUPPORT_URL = \"https://support.truera.com/hc/en-us/\"\n",
    "TRUERA_BLOG_URL = \"https://truera.com/ai-quality-blog/\"\n",
    "TRULENS_URL = \"https://trulens.org/\"\n",
    "TRUERA_URLS = [TRUERA_BASE_URL, TRUERA_DOC_URL, TRUERA_SUPPORT_URL, TRUERA_BLOG_URL, TRULENS_URL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScrape():\n",
    "    TABLE_PAGES = \"page\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        filename: Path = Path(\"scrape.sqlite\"),\n",
    "        n_threads: int = 8,\n",
    "        filters: Callable[[str], bool] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Web document downloader. Walks over links, collecting documents.\n",
    "\n",
    "        NOTE: This is not a serious scraper for large crawls.\n",
    "        \"\"\"\n",
    "\n",
    "        self.filename = filename\n",
    "        self.n_threads = n_threads\n",
    "        self._create_tables()\n",
    "\n",
    "        if isinstance(filters, str):\n",
    "            filter_func = lambda url: filters in url\n",
    "        elif isinstance(filters, Iterable):\n",
    "            filter_func = lambda url: any(map(lambda f: f in url, filters))\n",
    "        elif isinstance(filter, Callable):\n",
    "            filter_func = filters\n",
    "        else:\n",
    "            raise TypeError(f\"Unhandled filters type {type(filters)}\")\n",
    "\n",
    "        self.filter_func = filter_func\n",
    "\n",
    "    @staticmethod\n",
    "    def custom_normalize(url, base_url=None):\n",
    "        if url.startswith(\"tel:\"):\n",
    "            return url\n",
    "\n",
    "        if base_url is not None:\n",
    "            base_url = url_normalize(\n",
    "                urlparse(base_url)._replace(fragment=None, query=None).geturl()\n",
    "            )\n",
    "            if not base_url.endswith(\"/\"):\n",
    "                base_url += \"/\"\n",
    "            url = urljoin(base_url, url)\n",
    "        else:\n",
    "            url = urlparse(url)._replace(fragment=None, query=None).geturl()\n",
    "\n",
    "        url = url_normalize(url)\n",
    "\n",
    "        return url\n",
    "\n",
    "    def cursor(self):\n",
    "        connection = sqlite3.connect(self.filename)\n",
    "        cursor = connection.cursor()\n",
    "        return cursor, connection\n",
    "\n",
    "    def _create_tables(self):\n",
    "        c, conn = self.cursor()\n",
    "        c.execute(\n",
    "            f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {WebScrape.TABLE_PAGES} (\n",
    "                url VARCHAR(128),\n",
    "                type VARCHAR(64),\n",
    "                retrieved INTEGER,\n",
    "                content BYTES,\n",
    "                PRIMARY KEY (url)\n",
    "            )\n",
    "        \"\"\"\n",
    "        )\n",
    "        conn.commit()\n",
    "        c.close()\n",
    "\n",
    "    def get_urls(self) -> Iterable[sqlite3.Row]:\n",
    "        c, conn = self.cursor()\n",
    "\n",
    "        c.execute(\n",
    "            f\"\"\"\n",
    "            SELECT url\n",
    "            FROM {WebScrape.TABLE_PAGES}\n",
    "            \"\"\"\n",
    "        )\n",
    "        rows = c.fetchall()\n",
    "\n",
    "        c.close()\n",
    "\n",
    "        return map(first, rows)\n",
    "\n",
    "    def get_page(self, url: str) -> sqlite3.Row:\n",
    "        c, conn = self.cursor()\n",
    "\n",
    "        c.execute(\n",
    "            f\"\"\"\n",
    "            SELECT * \n",
    "            FROM {WebScrape.TABLE_PAGES} \n",
    "            WHERE url=?\"\"\", (url,)\n",
    "        )\n",
    "        row = c.fetchone()\n",
    "\n",
    "        c.close()\n",
    "        return row\n",
    "\n",
    "    def request(self, url: str):\n",
    "        return requests.get(url, stream=True)\n",
    "\n",
    "    def delete_page(self, url: str):\n",
    "        c, conn = self.cursor()\n",
    "\n",
    "        c.execute(\n",
    "            f\"\"\"\n",
    "            DELETE FROM {WebScrape.TABLE_PAGES}\n",
    "            WHERE url=?\n",
    "            \"\"\", (url,)\n",
    "        )\n",
    "        conn.commit()\n",
    "        c.close()\n",
    "\n",
    "        print(f\"page {url} deleted\")\n",
    "\n",
    "    def insert_page(self, url: str, type: str, content: bytes):\n",
    "        retrieved = datetime.datetime.now().timestamp()\n",
    "\n",
    "        c, conn = self.cursor()\n",
    "\n",
    "        size = len(content)\n",
    "\n",
    "        c.execute(\n",
    "            f\"\"\"\n",
    "            INSERT OR REPLACE \n",
    "            INTO {WebScrape.TABLE_PAGES} \n",
    "            VALUES (?, ?, ?, ?)\"\"\", (url, type, retrieved, content)\n",
    "        )\n",
    "        conn.commit()\n",
    "        c.close()\n",
    "\n",
    "        print(\n",
    "            f\"{UNICODE_CHECK} page {type} {humanize.naturalsize(size)} {url} -> {self.filename}\"\n",
    "        )\n",
    "\n",
    "    def scrape(self, url: Union[str, Sequence[str]], redownload: bool = False):\n",
    "        q = Queue(maxsize=1024 * 1024)\n",
    "\n",
    "        if isinstance(url, str):\n",
    "            q.put((url, None))\n",
    "        elif isinstance(url, Sequence):\n",
    "            for u in url:\n",
    "                q.put((u, None))\n",
    "\n",
    "        stopped = Event()\n",
    "        stopped.clear()\n",
    "\n",
    "        scraped = set()\n",
    "        threads = []\n",
    "\n",
    "        for _ in range(self.n_threads):\n",
    "            thread = Thread(\n",
    "                target=self._scrape,\n",
    "                kwargs=dict(\n",
    "                    queue=q,\n",
    "                    redownload=redownload,\n",
    "                    scraped=scraped,\n",
    "                    stopped=stopped\n",
    "                )\n",
    "            )\n",
    "            thread.start()\n",
    "            threads.append(thread)\n",
    "\n",
    "        sleep(1)\n",
    "\n",
    "        while not q.empty():\n",
    "            print(\"queue size:\", q.qsize())\n",
    "            sleep(1)\n",
    "\n",
    "        print(\"queue empty\")\n",
    "        stopped.set()\n",
    "\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "\n",
    "    def _scrape(\n",
    "        self, queue: Queue, stopped: Event, redownload: bool, scraped: set\n",
    "    ):\n",
    "        while not stopped.is_set():\n",
    "            if not queue.empty():\n",
    "                (url, from_url) = queue.get()\n",
    "            else:\n",
    "                sleep(1)\n",
    "                continue\n",
    "\n",
    "            url = WebScrape.custom_normalize(url)\n",
    "\n",
    "            if url in scraped:\n",
    "                continue\n",
    "\n",
    "            scraped.add(url)\n",
    "\n",
    "            page = self.get_page(url)\n",
    "            if page is not None:\n",
    "                ctype = page[1]\n",
    "                content = page[3]\n",
    "\n",
    "            if page is None or redownload:\n",
    "                try:\n",
    "                    res = self.request(url)\n",
    "                except Exception as e:\n",
    "                    print(f\"WARNING: {url} from {from_url}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                if not res.ok:\n",
    "                    print(f\"WARNING: {url} from {from_url}: {res.status_code}\")\n",
    "                    continue\n",
    "\n",
    "                if \"content-type\" not in res.headers:\n",
    "                    print(\n",
    "                        f\"WARNING: {url} from {from_url} lacks needed headers:\\n{list(res.headers.keys())}\"\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                ctype = res.headers['content-type']\n",
    "\n",
    "                if \"content-length\" in res.headers:\n",
    "                    size = int(res.headers['content-length'])\n",
    "                    if size > 100 * (1024**2):\n",
    "                        print(\n",
    "                            f\"WARNING: {url} from {from_url} is large {humanize.naturalsize(size)}\"\n",
    "                        )\n",
    "                        continue  # skipping\n",
    "\n",
    "                if ctype.startswith(\"image/\"):\n",
    "                    continue  # skipping\n",
    "\n",
    "                content = res.content\n",
    "                self.insert_page(url=url, type=ctype, content=res.content)\n",
    "\n",
    "            size = len(content)\n",
    "            if size > 100 * (1024**2):\n",
    "                print(\n",
    "                    f\"WARNING: {url} from {from_url}: is large: {humanize.naturalsize(size)}\"\n",
    "                )\n",
    "                pass\n",
    "\n",
    "            if ctype.startswith(\"text/html\"):\n",
    "                soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "                anchors = soup.findAll(\"a\")\n",
    "                sub_urls = [a.get(\"href\") for a in anchors]\n",
    "\n",
    "            elif ctype.startswith(\"application/pdf\"):\n",
    "\n",
    "                with io.BytesIO() as fh:\n",
    "                    fh.write(content)\n",
    "                    fh.seek(0)\n",
    "\n",
    "                    pdf = pdfreader.SimplePDFViewer(fh)\n",
    "\n",
    "                    sub_urls = []\n",
    "\n",
    "                    if pdf.annotations is not None:\n",
    "                        for annot in pdf.annotations:\n",
    "                            if annot.Subtype == \"Link\":\n",
    "                                sub_url = annot.A.URI\n",
    "                                if sub_url is not None:\n",
    "                                    sub_url = sub_url.decode('ascii')\n",
    "                                    if sub_url.startswith(\"http\"):\n",
    "                                        sub_urls.append(sub_url)\n",
    "\n",
    "            else:\n",
    "                print(\n",
    "                    f\"WARNING: {url} from {from_url}: unknown content type {ctype}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            for sub_url in sub_urls:\n",
    "                if sub_url in scraped:\n",
    "                    continue\n",
    "\n",
    "                if sub_url is None:\n",
    "                    continue\n",
    "\n",
    "                if sub_url.startswith(\"tel:\"):\n",
    "                    # print(f\"skip: {sub_url} from {url}: is tel\")\n",
    "                    scraped.add(sub_url)\n",
    "                    continue\n",
    "\n",
    "                sub_url = WebScrape.custom_normalize(sub_url, base_url=url)\n",
    "\n",
    "                if sub_url in scraped:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    parts = urlparse(sub_url)\n",
    "                    if parts.scheme is None:\n",
    "                        print(f\"WARNING: {sub_url} from {url}: no scheme\")\n",
    "                        scraped.add(sub_url)\n",
    "                        continue\n",
    "\n",
    "                    if parts.scheme not in [\"http\", \"https\"]:\n",
    "                        # print(f\"skip: {sub_url} from {url}: skip scheme {parts.scheme}\")\n",
    "                        scraped.add(sub_url)\n",
    "                        continue\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"WARNING: {sub_url} from {url}: {e}\")\n",
    "                    scraped.add(sub_url)\n",
    "                    continue\n",
    "\n",
    "                if self.filter_func(sub_url):\n",
    "                    # print(\"adding\", sub_url)\n",
    "                    queue.put((sub_url, url))\n",
    "                else:\n",
    "                    scraped.add(sub_url)\n",
    "                    pass\n",
    "\n",
    "    def get_documents(self):\n",
    "        docs = []\n",
    "\n",
    "        seen_texts = dict()\n",
    "\n",
    "        for url in tqdm(list(self.get_urls())):\n",
    "            canon_url = WebScrape.custom_normalize(url)\n",
    "            if url != canon_url:\n",
    "                s.delete_page(url=url)\n",
    "                continue\n",
    "\n",
    "            if url in {\n",
    "                    'https://truera.com/resources/',\n",
    "                    'https://truera.com/ai-quality-blog/',\n",
    "                    'https://truera.com/event/live-events/',\n",
    "                    'https://truera.com/ai-quality-research/ai-quality-education/',\n",
    "                    'https://medium.com/trulens/archive'\n",
    "            }:\n",
    "                print(\"skipping\", url)\n",
    "                continue\n",
    "            elif \"/page/\" in url:\n",
    "                print(\"skipping\", url)\n",
    "                continue\n",
    "            elif \"/category/\" in url:\n",
    "                print(\"skipping\", url)\n",
    "                continue\n",
    "            elif \"Datasheet\" in url and url.endswith(\".pdf\"):\n",
    "                print(\"skipping\", url)\n",
    "                continue\n",
    "            elif url.startswith(\"https://pypi.org/project/\"):\n",
    "                print(\"skipping\", url)\n",
    "                continue\n",
    "            elif \"trulens\" in url: # temporarily skipping anything with trulens\n",
    "                print(\"skipping\", url)\n",
    "                continue\n",
    "\n",
    "            row = s.get_page(url=url)\n",
    "            type = row[1]\n",
    "\n",
    "            if type.startswith(\"text/html\"):\n",
    "                loader = UnstructuredHTMLLoader\n",
    "            elif type.startswith(\"application/pdf\"):\n",
    "                loader = PagedPDFSplitter\n",
    "                # UnstructuredPDFLoader\n",
    "            elif type.startswith(\"image\"):\n",
    "                # s.delete_page(url=url)\n",
    "                continue\n",
    "            else:\n",
    "                # markdown: UnstructuredMarkdownLoader\n",
    "                # jupyter?\n",
    "                # github?\n",
    "                # print(url, type)\n",
    "                continue\n",
    "\n",
    "            content = row[3]\n",
    "            size = len(content)\n",
    "            if content is None:\n",
    "                raise ValueError(url)\n",
    "            if size == 0:\n",
    "                raise ValueError(f\"empty: {url}\")\n",
    "            if size >= 100 * (1024**2):\n",
    "                print(\n",
    "                    f\"WARNING: big content {url} {humanize.naturalsize(size)}\"\n",
    "                )\n",
    "\n",
    "            file = tempfile.NamedTemporaryFile(mode='bw')\n",
    "            file.write(row[3])\n",
    "            file.flush()\n",
    "\n",
    "            try:\n",
    "                new_docs = loader(file.name).load()\n",
    "                for new_doc in new_docs:\n",
    "                    new_doc.metadata['source'] = url\n",
    "\n",
    "                cont = new_doc.page_content\n",
    "\n",
    "                if cont in seen_texts:\n",
    "                    #print(\n",
    "                    #    f\"WARNING: {url} Already seen text in {seen_texts[cont]}. Skipping.\"\n",
    "                    #)\n",
    "                    continue\n",
    "                seen_texts[cont] = url\n",
    "\n",
    "                docs.append(new_doc)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"WARNING: {url} {type} {e}\")\n",
    "\n",
    "            file.close()\n",
    "\n",
    "        return docs\n",
    "\n",
    "\n",
    "s = WebScrape(\n",
    "    filters=lambda url: \n",
    "        (\"truera\" in url or \"trulens\" in url) \\\n",
    "        and \"github.com\" not in url \\\n",
    "        and \"support.truera.com\" not in url \\\n",
    "        and \"cbinsights.com\" not in url \\\n",
    "        and \"files.pythonhosted.org\" not in url \\\n",
    "        and \"libraries.io\" not in url\n",
    ")\n",
    "s.scrape(TRUERA_URLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = s.get_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = NLTKTextSplitter(chunk_size=1024, chunk_overlap=0)\n",
    "text_splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=0)\n",
    "chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_chunks = [c for c in chunks if len(c.page_content) >= 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_content = dict()\n",
    "unique_chunks = []\n",
    "for chunk in big_chunks:\n",
    "    content = chunk.page_content\n",
    "    #if content in seen_content:\n",
    "        #print(f\"{chunk.metadata} already seen in {seen_content[content]}\")\n",
    "        #continue\n",
    "    seen_content[content] = chunk.metadata\n",
    "    unique_chunks.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smallest chunk:\n",
    "print(unique_chunks[np.array([len(c.page_content) for c in unique_chunks]).argmin()])\n",
    "\n",
    "# number of chunks:\n",
    "print(len(unique_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings(model='text-embedding-ada-002')  # 1536 dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import DocArrayHnswSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which chunks to write to vector db. Options here to play around with various\n",
    "# drawbacks.\n",
    "\n",
    "output_chunks = big_chunks # unique_chunks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To DocArrayHnswSearch\n",
    "\n",
    "This is a local document store and retriever that requires no additional api keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DocArrayHnswSearch.from_documents(\n",
    "    output_chunks,\n",
    "    embedding, work_dir='hnswlib_trubot',\n",
    "    n_dim=1536,\n",
    "    max_elements=int(len(output_chunks) * 1.1)\n",
    ")\n",
    "#db = DocArrayHnswSearch.from_params(\n",
    "#    embedding=embedding,\n",
    "#    work_dir='hnswlib_trubot',\n",
    "#    n_dim=1536,\n",
    "#    max_elements=int(len(output_chunks) * 1.1)\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in db.similarity_search(\"Who is Shayak?\"):\n",
    "    print(\"====\")\n",
    "    print(doc.metadata)\n",
    "    print(doc.page_content)\n",
    "    print(\"====\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Pinecone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "check_keys(\n",
    "    \"PINECONE_API_KEY\",\n",
    "    \"PINECONE_ENV\"\n",
    ")\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=os.environ.get(\"PINECONE_API_KEY\"),  # find at app.pinecone.io\n",
    "    environment=os.environ.get(\"PINECONE_ENV\")  # next to api key in console\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create / upload an index of the docs to pinecone\n",
    "\n",
    "index_name = \"llmdemo\"\n",
    "# Delete if already exists:\n",
    "# pinecone.delete_index(index_name)\n",
    "# pinecone.create_index(index_name, dimension=1536)\n",
    "Pinecone.from_documents(output_chunks, embedding, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
