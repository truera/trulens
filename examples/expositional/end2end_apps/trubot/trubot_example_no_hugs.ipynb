{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruBot\n",
    "\n",
    "This is the first part of the TruBot example notebook without the use of huggingface-based feedback functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# If running from github repo, can use this:\n",
    "sys.path.append(str(Path().cwd().parent.parent.parent.parent.resolve()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API keys setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import Tru\n",
    "\n",
    "Tru().migrate_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.utils.keys import check_keys\n",
    "\n",
    "check_keys(\n",
    "    \"OPENAI_API_KEY\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import PrettyPrinter\n",
    "\n",
    "# Imports from LangChain to build app:\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_community.llms import OpenAI\n",
    "import numpy as np\n",
    "from trulens.core import Feedback\n",
    "from trulens.core import FeedbackMode\n",
    "from trulens.core import Select\n",
    "from trulens.core import Tru\n",
    "\n",
    "# Imports main tools:\n",
    "from trulens.external import OpenAI as fOpenAI\n",
    "\n",
    "pp = PrettyPrinter()\n",
    "\n",
    "# Tru object manages the database of apps, records, and feedbacks; and the\n",
    "# dashboard to display these.\n",
    "tru = Tru()\n",
    "\n",
    "# Start the dasshboard. If you running from github repo, you will need to adjust\n",
    "# the path the dashboard streamlit app starts in by providing the _dev argument.\n",
    "tru.start_dashboard(force=True, _dev=Path().cwd().parent.parent.resolve())\n",
    "\n",
    "# If needed, you can reset the trulens_eval dashboard database by running the\n",
    "# below line:\n",
    "\n",
    "# tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select vector db provider. Pinecone requires setting up a pinecone database\n",
    "# first while the hnsw database is included with trulens_eval.\n",
    "# db_host = \"pinecone\"\n",
    "db_host = \"hnsw\"\n",
    "\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "app_id = \"TruBot\"\n",
    "\n",
    "# Embedding for vector db.\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-ada-002\")  # 1536 dims\n",
    "\n",
    "if db_host == \"pinecone\":\n",
    "    check_keys(\"PINECONE_API_KEY\", \"PINECONE_ENV\")\n",
    "\n",
    "    # Pinecone configuration if using pinecone.\n",
    "\n",
    "    import os\n",
    "\n",
    "    from langchain_community.vectorstores import Pinecone\n",
    "    import pinecone\n",
    "\n",
    "    pinecone.init(\n",
    "        api_key=os.environ.get(\"PINECONE_API_KEY\"),  # find at app.pinecone.io\n",
    "        environment=os.environ.get(\n",
    "            \"PINECONE_ENV\"\n",
    "        ),  # next to api key in console\n",
    "    )\n",
    "\n",
    "    # If using pinecone, make sure you create your index under name 'llmdemo' or\n",
    "    # change the below.\n",
    "\n",
    "    def get_doc_search():\n",
    "        docsearch = Pinecone.from_existing_index(\n",
    "            index_name=\"llmdemo\", embedding=embedding\n",
    "        )\n",
    "\n",
    "        return docsearch\n",
    "\n",
    "elif db_host == \"hnsw\":\n",
    "    # Local pinecone alternative. Requires precomputed 'hnswlib_truera' folder.\n",
    "\n",
    "    from langchain.vectorstores import DocArrayHnswSearch\n",
    "\n",
    "    def get_doc_search():\n",
    "        # We need to create this object in the thread in which it is used so we\n",
    "        # wrap it in this function for later usage.\n",
    "\n",
    "        docsearch = DocArrayHnswSearch.from_params(\n",
    "            embedding=embedding,\n",
    "            work_dir=\"hnswlib_trubot\",\n",
    "            n_dim=1536,\n",
    "            max_elements=1024,\n",
    "        )\n",
    "\n",
    "        return docsearch\n",
    "else:\n",
    "    raise RuntimeError(\"Unhandled db_host, select either 'pinecone' or 'hnsw'.\")\n",
    "\n",
    "# LLM for completing prompts, and other tasks.\n",
    "llm = OpenAI(temperature=0, max_tokens=256)\n",
    "\n",
    "# Construct feedback functions.\n",
    "\n",
    "# API endpoints for models used in feedback functions:\n",
    "# hugs = Huggingface()\n",
    "openai = fOpenAI()\n",
    "\n",
    "# Language match between question/answer.\n",
    "# f_lang_match = Feedback(hugs.language_match).on_input_output()\n",
    "# By default this will evaluate feedback on main app input and main app output.\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = Feedback(openai.relevance).on_input_output()\n",
    "# By default this will evaluate feedback on main app input and main app output.\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(openai.context_relevance)\n",
    "    .on_input()\n",
    "    .on(\n",
    "        Select.Record.app.combine_docs_chain._call.args.inputs.input_documents[\n",
    "            :\n",
    "        ].page_content\n",
    "    )\n",
    "    .aggregate(np.min)\n",
    ")\n",
    "# First feedback argument is set to main app input, and the second is taken from\n",
    "# the context sources as passed to an internal `combine_docs_chain._call`.\n",
    "\n",
    "all_feedbacks = [\n",
    "    # f_lang_match,\n",
    "    f_qa_relevance,\n",
    "    f_context_relevance,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruBot Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v1_new_conversation(feedback_mode=FeedbackMode.WITH_APP):\n",
    "    \"\"\"\n",
    "    Create a _LangChain_ app for a new conversation with a question-answering bot.\n",
    "\n",
    "    Feedback_mode controls when feedback is evaluated:\n",
    "\n",
    "    - FeedbackMode.WITH_APP -- app will wait until feedback is evaluated before\n",
    "      returning from calls.\n",
    "\n",
    "    - FeedbackMode.WITH_APP_THREAD -- app will return from calls and evaluate\n",
    "      feedback in a new thread.\n",
    "\n",
    "    - FeedbackMode.DEFERRED -- app will return and a separate runner thread (see\n",
    "      usage later in this notebook) will evaluate feedback.\n",
    "    \"\"\"\n",
    "\n",
    "    # Blank conversation memory.\n",
    "    memory = ConversationSummaryBufferMemory(\n",
    "        max_token_limit=650,\n",
    "        llm=llm,\n",
    "        memory_key=\"chat_history\",\n",
    "        output_key=\"answer\",\n",
    "    )\n",
    "\n",
    "    docsearch = get_doc_search()\n",
    "\n",
    "    # Context retriever.\n",
    "    retriever = docsearch.as_retriever()\n",
    "\n",
    "    # Conversational app puts it all together.\n",
    "    app = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        memory=memory,\n",
    "        get_chat_history=lambda a: a,\n",
    "        max_tokens_limit=4096,\n",
    "    )\n",
    "\n",
    "    # Trulens instrumentation.\n",
    "    tc = Tru().Chain(\n",
    "        app_id=f\"{app_id}/v1\",\n",
    "        chain=app,\n",
    "        feedbacks=all_feedbacks,\n",
    "        feedback_mode=feedback_mode,\n",
    "    )\n",
    "\n",
    "    return tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the app with fresh memory:\n",
    "\n",
    "tc1 = v1_new_conversation()\n",
    "\n",
    "# Call the app:\n",
    "\n",
    "res, record = tc1.with_record(tc1.app, \"Who is Shayak?\")\n",
    "res\n",
    "\n",
    "# Notice the `source_documents` returned include chunks about Shameek and the\n",
    "# answer includes bits about Shameek as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The feedback should already be present in the dashboard, but we can check the\n",
    "# context_relevance here manually as well:\n",
    "feedback = f_context_relevance.run(record=record, app=tc1)\n",
    "feedback.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now a question about QII (quantitative input influence is a base technology\n",
    "# employed in TruEra's products) question but in a non-English language:\n",
    "\n",
    "# Start a new conversation as the app keeps prior questions in its memory which\n",
    "# may cause you some testing woes.\n",
    "tc1 = v1_new_conversation()\n",
    "\n",
    "# res, record = tc1.with_record(tc1.app, \"Co jest QII?\") # Polish\n",
    "res, record = tc1.with_record(tc1.app, \"Was ist QII?\")  # German\n",
    "res\n",
    "\n",
    "# Note here the response is in English. This example sometimes matches language\n",
    "# so other variants may need to be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
