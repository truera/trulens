{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterating with RAG on Milvus\n",
    "\n",
    "Setup:\n",
    "To get up and running, you'll first need to install Docker and Milvus. Find instructions below:\n",
    "* Docker Compose ([Instructions](https://docs.docker.com/compose/install/))\n",
    "* Milvus Standalone ([Instructions](https://milvus.io/docs/install_standalone-docker.md))\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/examples/expositional/vector-dbs/milvus/milvus_evals_build_better_rags.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Install dependencies\n",
    "Let's install some of the dependencies for this notebook if we don't have them already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install trulens trulens-apps-llamaindex trulens-providers-openai llama-index llama-index-vector-stores-milvus nltk html2text tenacity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add API keys\n",
    "For this quickstart, you will need Open AI and Huggingface keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import from LlamaIndex and TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddingst\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "from tenacity import retry\n",
    "from tenacity import stop_after_attempt\n",
    "from tenacity import wait_exponential\n",
    "from trulens.core import Feedback\n",
    "from trulens.core import TruSession\n",
    "from trulens.apps.llamaindex import TruLlama\n",
    "from trulens.providers.openai import OpenAI as fOpenAI\n",
    "\n",
    "session = TruSession()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we need to load documents. We can use SimpleWebPageReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import WikipediaReader\n",
    "\n",
    "cities = [\n",
    "    \"Los Angeles\",\n",
    "    \"Houston\",\n",
    "    \"Honolulu\",\n",
    "    \"Tucson\",\n",
    "    \"Mexico City\",\n",
    "    \"Cincinatti\",\n",
    "    \"Chicago\",\n",
    "]\n",
    "\n",
    "wiki_docs = []\n",
    "for city in cities:\n",
    "    try:\n",
    "        doc = WikipediaReader().load_data(pages=[city])\n",
    "        wiki_docs.extend(doc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading page for city {city}: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now write down our test prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "    \"What's the best national park near Honolulu\",\n",
    "    \"What are some famous universities in Tucson?\",\n",
    "    \"What bodies of water are near Chicago?\",\n",
    "    \"What is the name of Chicago's central business district?\",\n",
    "    \"What are the two most famous universities in Los Angeles?\",\n",
    "    \"What are some famous festivals in Mexico City?\",\n",
    "    \"What are some famous festivals in Los Angeles?\",\n",
    "    \"What professional sports teams are located in Los Angeles\",\n",
    "    \"How do you classify Houston's climate?\",\n",
    "    \"What landmarks should I know about in Cincinatti\",\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a prototype RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = MilvusVectorStore(\n",
    "    index_params={\"index_type\": \"IVF_FLAT\", \"metric_type\": \"L2\"},\n",
    "    search_params={\"nprobe\": 20},\n",
    "    overwrite=True,\n",
    ")\n",
    "llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "embed_v12 = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_v12\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    wiki_docs, storage_context=storage_context\n",
    ")\n",
    "query_engine = index.as_query_engine(top_k=5)\n",
    "\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(10),\n",
    "    wait=wait_exponential(multiplier=1, min=4, max=10),\n",
    ")\n",
    "def call_query_engine(prompt):\n",
    "    return query_engine.query(prompt)\n",
    "\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    call_query_engine(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize OpenAI-based feedback function collection class:\n",
    "provider = fOpenAI()\n",
    "\n",
    "# Define groundedness\n",
    "f_groundedness = (\n",
    "    Feedback(\n",
    "        provider.groundedness_measure_with_cot_reasons, name=\"Groundedness\"\n",
    "    )\n",
    "    .on(TruLlama.select_context())\n",
    "    .on_output()\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_answer_relevance = Feedback(\n",
    "    provider.relevance_with_cot_reasons, name=\"Answer Relevance\"\n",
    ").on_input_output()\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(\n",
    "        provider.context_relevance_with_cot_reasons, name=\"Context Relevance\"\n",
    "    )\n",
    "    .on_input()\n",
    "    .on(TruLlama.select_context())\n",
    "    .aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_params = [\"IVF_FLAT\", \"HNSW\"]\n",
    "embed_v12 = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    ")\n",
    "embed_ft3_v12 = HuggingFaceEmbeddings(\n",
    "    model_name=\"Sprylab/paraphrase-multilingual-MiniLM-L12-v2-fine-tuned-3\"\n",
    ")\n",
    "embed_ada = OpenAIEmbeddings(model_name=\"text-embedding-ada-002\")\n",
    "embed_models = [embed_v12, embed_ada]\n",
    "top_ks = [1, 3]\n",
    "chunk_sizes = [200, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "for index_param, embed_model, top_k, chunk_size in itertools.product(\n",
    "    index_params, embed_models, top_ks, chunk_sizes\n",
    "):\n",
    "    if embed_model == embed_v12:\n",
    "        embed_model_name = \"v12\"\n",
    "    elif embed_model == embed_ft3_v12:\n",
    "        embed_model_name = \"ft3_v12\"\n",
    "    elif embed_model == embed_ada:\n",
    "        embed_model_name = \"ada\"\n",
    "    vector_store = MilvusVectorStore(\n",
    "        index_params={\"index_type\": index_param, \"metric_type\": \"L2\"},\n",
    "        search_params={\"nprobe\": 20},\n",
    "        overwrite=True,\n",
    "    )\n",
    "    llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "        embed_model=embed_model, llm=llm, chunk_size=chunk_size\n",
    "    )\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        wiki_docs,\n",
    "        service_context=service_context,\n",
    "        storage_context=storage_context,\n",
    "    )\n",
    "    query_engine = index.as_query_engine(similarity_top_k=top_k)\n",
    "    tru_query_engine = TruLlama(\n",
    "        query_engine,\n",
    "        feedbacks=[f_groundedness, f_qa_relevance, f_context_relevance],\n",
    "        metadata={\n",
    "            \"index_param\": index_param,\n",
    "            \"embed_model\": embed_model_name,\n",
    "            \"top_k\": top_k,\n",
    "            \"chunk_size\": chunk_size,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    @retry(\n",
    "        stop=stop_after_attempt(10),\n",
    "        wait=wait_exponential(multiplier=1, min=4, max=10),\n",
    "    )\n",
    "    def call_tru_query_engine(prompt):\n",
    "        return tru_query_engine.query(prompt)\n",
    "\n",
    "    for prompt in test_prompts:\n",
    "        call_tru_query_engine(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore in a Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "run_dashboard(session)  # open a local streamlit app to explore\n",
    "\n",
    "# stop_dashboard(session) # stop if needed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or view results directly in your notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.get_records_and_feedback()[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pkg_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
