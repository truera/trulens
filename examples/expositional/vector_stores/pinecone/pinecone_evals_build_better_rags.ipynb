{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pinecone Configuration Choices on Downstream App Performance\n",
    "\n",
    "Large Language Models (LLMs) have a hallucination problem. Retrieval Augmented Generation (RAG) is an emerging paradigm that augments LLMs with a knowledge base – a source of truth set of docs often stored in a vector database like Pinecone, to mitigate this problem. To build an effective RAG-style LLM  application, it is important to experiment with various configuration choices while setting up the vector database and study their impact on performance metrics.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/examples/expositional/vector-dbs/pinecone/pinecone_evals_build_better_rags.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing dependencies\n",
    "\n",
    "The following cell invokes a shell command in the active Python environment for the packages we need to continue with this notebook. You can also run `pip install` directly in your terminal without the `!`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --pre trulens trulens-apps-langchain trulens-providers-openai langchain==0.0.315 openai==0.28.1 tiktoken==0.5.1 \"pinecone-client[grpc]==2.2.4\" pinecone-datasets==0.5.1 datasets==2.14.5 langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = \"...\"\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"...\"\n",
    "os.environ[\"PINECONE_ENVIRONMENT\"] = \"...\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Knowledge Base"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will download a pre-embedding dataset from pinecone-datasets. Allowing us to skip the embedding and preprocessing steps, if you'd rather work through those steps you can find the full notebook [here](https://github.com/pinecone-io/examples/blob/master/generation/langchain/handbook/05-langchain-retrieval-augmentation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone_datasets\n",
    "\n",
    "dataset = pinecone_datasets.load_dataset(\n",
    "    \"wikipedia-simple-text-embedding-ada-002-100K\"\n",
    ")\n",
    "dataset.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll format the dataset ready for upsert and reduce what we use to a subset of the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we drop sparse_values as they are not needed for this example\n",
    "dataset.documents.drop([\"metadata\"], axis=1, inplace=True)\n",
    "dataset.documents.rename(columns={\"blob\": \"metadata\"}, inplace=True)\n",
    "# we will use rows of the dataset up to index 30_000\n",
    "dataset.documents.drop(dataset.documents.index[30_000:], inplace=True)\n",
    "len(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move on to initializing our Pinecone vector database.\n",
    "\n",
    "## Vector Database\n",
    "\n",
    "To create our vector database we first need a [free API key from Pinecone](https://app.pinecone.io). Then we initialize like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "# find API key in console at app.pinecone.io\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "# find ENV (cloud region) next to API key in console\n",
    "PINECONE_ENVIRONMENT = os.getenv(\"PINECONE_ENVIRONMENT\")\n",
    "pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name_v1 = \"langchain-rag-cosine\"\n",
    "\n",
    "if index_name_v1 not in pinecone.list_indexes():\n",
    "    # we create a new index\n",
    "    pinecone.create_index(\n",
    "        name=index_name_v1,\n",
    "        metric=\"cosine\",  # we'll try each distance metric here\n",
    "        dimension=1536,  # 1536 dim of text-embedding-ada-002\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fetch index stats to confirm that it was created. Note that the total vector count here will be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index = pinecone.GRPCIndex(index_name_v1)\n",
    "# wait a moment for the index to be fully initialized\n",
    "time.sleep(1)\n",
    "\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upsert documents into the db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataset.iter_documents(batch_size=100):\n",
    "    index.upsert(batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm they've been added, the vector count should now be 30k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Vector Store and Querying\n",
    "\n",
    "Now that we've build our index we can switch over to LangChain. We need to initialize a LangChain vector store using the same index we just built. For this we will also need a LangChain embedding object, which we initialize like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# get openai api key from platform.openai.com\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "model_name = \"text-embedding-ada-002\"\n",
    "\n",
    "embed = OpenAIEmbeddings(model=model_name, openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now initialize the vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"\n",
    "\n",
    "# switch back to normal index for langchain\n",
    "index = pinecone.Index(index_name_v1)\n",
    "\n",
    "vectorstore = Pinecone(index, embed.embed_query, text_field)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG)\n",
    "\n",
    "In RAG we take the query as a question that is to be answered by a LLM, but the LLM must answer the question based on the information it is seeing being returned from the `vectorstore`.\n",
    "\n",
    "To do this we initialize a `RetrievalQA` object like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# completion llm\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "chain_v1 = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with TruLens\n",
    "\n",
    "Once we’ve set up our app, we should put together our feedback functions. As a reminder, feedback functions are an extensible method for evaluating LLMs. Here we’ll set up 3 feedback functions: `context_relevance`, `qa_relevance`, and `groundedness`. They’re defined as follows:\n",
    "\n",
    "- QS Relevance: query-statement relevance is the average of relevance (0 to 1) for each context chunk returned by the semantic search.\n",
    "- QA Relevance: question-answer relevance is the relevance (again, 0 to 1) of the final answer to the original question.\n",
    "- Groundedness: groundedness measures how well the generated response is supported by the evidence provided to the model where a score of 1 means each sentence is grounded by a retrieved context chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports main tools for eval\n",
    "import numpy as np\n",
    "from trulens.core import Feedback\n",
    "from trulens.core import Select\n",
    "from trulens.core import TruSession\n",
    "from trulens.apps.langchain import TruChain\n",
    "from trulens.providers.openai import OpenAI as fOpenAI\n",
    "\n",
    "tru = TruSession()\n",
    "\n",
    "# Initialize OpenAI-based feedback function collection class:\n",
    "provider = fOpenAI()\n",
    "\n",
    "# Define groundedness\n",
    "f_groundedness = (\n",
    "    Feedback(\n",
    "        provider.groundedness_measure_with_cot_reasons, name=\"Groundedness\"\n",
    "    )\n",
    "    .on(\n",
    "        TruChain.select_context(chain_v1).collect()  # context\n",
    "    )\n",
    "    .on_output()\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_answer_relevance = Feedback(\n",
    "    provider.relevance_with_cot_reasons, name=\"Answer Relevance\"\n",
    ").on_input_output()\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(\n",
    "        provider.context_relevance_with_cot_reasons, name=\"Context Relevance\"\n",
    "    )\n",
    "    .on_input()\n",
    "    .on(TruChain.select_context(chain_v1))\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "feedback_functions = [f_answer_relevance, f_context_relevance, f_groundedness]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap with TruLens\n",
    "tru_chain_recorder_v1 = TruChain(\n",
    "    chain_v1, app_name=\"WikipediaQA\", app_version=\"chain_1\", feedbacks=feedback_functions\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can submit queries to our application and have them tracked and evaluated by TruLens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Name some famous dental floss brands?\",\n",
    "    \"Which year did Cincinnati become the Capital of Ohio?\",\n",
    "    \"Which year was Hawaii's state song written?\",\n",
    "    \"How many countries are there in the world?\",\n",
    "    \"How many total major trophies has manchester united won?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_chain_recorder_v1 as recording:\n",
    "    for prompt in prompts:\n",
    "        chain_v1(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the TruLens Dashboard to view tracking and evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "run_dashboard(tru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using a free pinecone instance, only one index is allowed. Delete instance to make room for the next iteration.\n",
    "pinecone.delete_index(index_name_v1)\n",
    "time.sleep(\n",
    "    30\n",
    ")  # sleep for 30 seconds after deleting the index before creating a new one"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with Distance Metrics\n",
    "Now that we’ve walked through the process of building our tracked RAG application using cosine as the distance metric, all we have to do for the next two experiments is to rebuild the index with ‘euclidean’ or ‘dotproduct’ as the metric and following the rest of the steps above as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name_v2 = \"langchain-rag-euclidean\"\n",
    "pinecone.create_index(\n",
    "    name=index_name_v2,\n",
    "    metric=\"euclidean\",\n",
    "    dimension=1536,  # 1536 dim of text-embedding-ada-002\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pinecone.GRPCIndex(index_name_v2)\n",
    "# wait a moment for the index to be fully initialized\n",
    "time.sleep(1)\n",
    "\n",
    "# upsert documents\n",
    "for batch in dataset.iter_documents(batch_size=100):\n",
    "    index.upsert(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa still exists, and will now use our updated vector store\n",
    "# switch back to normal index for langchain\n",
    "index = pinecone.Index(index_name_v2)\n",
    "\n",
    "# update vectorstore with new index\n",
    "vectorstore = Pinecone(index, embed.embed_query, text_field)\n",
    "\n",
    "# recreate qa from vector store\n",
    "chain_v2 = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "# wrap with TruLens\n",
    "tru_chain_recorder_v2 = TruChain(\n",
    "    qa, app_name=\"WikipediaQA\", app_version=\"chain_2\", feedbacks=[qa_relevance, context_relevance]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_chain_recorder_v2 as recording:\n",
    "    for prompt in prompts:\n",
    "        chain_v2(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.delete_index(index_name_v2)\n",
    "time.sleep(\n",
    "    30\n",
    ")  # sleep for 30 seconds after deleting the index before creating a new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name_v3 = \"langchain-rag-dot\"\n",
    "pinecone.create_index(\n",
    "    name=index_name_v3,\n",
    "    metric=\"dotproduct\",\n",
    "    dimension=1536,  # 1536 dim of text-embedding-ada-002\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pinecone.GRPCIndex(index_name_v3)\n",
    "# wait a moment for the index to be fully initialized\n",
    "time.sleep(1)\n",
    "\n",
    "index.describe_index_stats()\n",
    "\n",
    "# upsert documents\n",
    "for batch in dataset.iter_documents(batch_size=100):\n",
    "    index.upsert(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch back to normal index for langchain\n",
    "index = pinecone.Index(index_name_v3)\n",
    "\n",
    "# update vectorstore with new index\n",
    "vectorstore = Pinecone(index, embed.embed_query, text_field)\n",
    "\n",
    "# recreate qa from vector store\n",
    "chain_v3 = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "# wrap with TruLens\n",
    "tru_chain_recorder_v3 = TruChain(\n",
    "    chain_v3, app_name=\"WikipediaQA\", app_version=\"chain_3\", feedbacks=feedback_functions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_chain_recorder_v3 as recording:\n",
    "    for prompt in prompts:\n",
    "        chain_v3(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see that both the euclidean and dot-product metrics performed at a lower latency than cosine at roughly the same evaluation quality. We can move forward with either. Since Euclidean is already loaded in Pinecone, we'll go with that one.\n",
    "\n",
    "After doing so, we can view our evaluations for all three LLM apps sitting on top of the different indices. All three apps are struggling with query-statement relevance. In other words, the context retrieved is only somewhat relevant to the original query.\n",
    "\n",
    "Diagnosis: Hallucination.\n",
    "\n",
    "Digging deeper into the Query Statement Relevance, we notice one problem in particular with a question about famous dental floss brands. The app responds correctly, but is not backed up by the context retrieved, which does not mention any specific brands.\n",
    "\n",
    "Using a less powerful model is a common way to reduce hallucination for some applications. We’ll evaluate ada-001 in our next experiment for this purpose.\n",
    "\n",
    "Changing different components of apps built with frameworks like LangChain is really easy. In this case we just need to call ‘text-ada-001’ from the langchain LLM store. Adding in easy evaluation with TruLens allows us to quickly iterate through different components to find our optimal app configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completion llm\n",
    "from langchain_community.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-ada-001\", temperature=0)\n",
    "\n",
    "\n",
    "chain_with_sources = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "# wrap with TruLens\n",
    "tru_chain_with_sources_recorder = TruChain(\n",
    "    chain_with_sources,\n",
    "    app_name=\"WikipediaQA\",\n",
    "    app_version=\"chain_4\"\n",
    "    feedbacks=[f_answer_relevance, f_context_relevance],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_chain_with_sources_recorder as recording:\n",
    "    for prompt in prompts:\n",
    "        chain_with_sources(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However this configuration with a less powerful model struggles to return a relevant answer given the context provided. For example, when asked “Which year was Hawaii’s state song written?”, the app retrieves context that contains the correct answer but fails to respond with that answer, instead simply responding with the name of the song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completion llm\n",
    "from langchain_community.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "chain_v5 = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever(top_k=1)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The way the top_k works with RetrievalQA is that the documents are still retrieved by our semantic search and but only the top_k are passed to the LLM. Howevever TruLens captures all of the context chunks that are being retrieved. In order to calculate an accurate QS Relevance metric that matches what's being passed to the LLM, we need to only calculate the relevance of the top context chunk retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_relevance = (\n",
    "    Feedback(provider.context_relevance, name=\"Context Relevance\")\n",
    "    .on_input()\n",
    "    .on(\n",
    "        Select.Record.app.combine_documents_chain._call.args.inputs.input_documents[\n",
    "            :1\n",
    "        ].page_content\n",
    "    )\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "# wrap with TruLens\n",
    "tru_chain_recorder_v5 = TruChain(\n",
    "    chain_v5, app_name=\"WikipediaQA\", app_version=\"chain_5\", feedbacks=feedback_functions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_chain_recorder_v5 as recording:\n",
    "    for prompt in prompts:\n",
    "        chain_v5(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final application has much improved context_relevance, qa_relevance and low latency!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "trulens_pinecone",
   "language": "python",
   "name": "trulens_pinecone"
  },
  "vscode": {
   "interpreter": {
    "hash": "c68aa9cfa264c12f07062d08edcac5e8f20877de71ce1cea15160e4e8ae95e66"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "059918bb59744634aaa181dc4ec256a2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "28a553d3a3704b3aa8b061b71b1fe2ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ee030d62f3a54f5288cccf954caa7d85",
       "IPY_MODEL_55cdb4e0b33a48b298f760e7ff2af0f9",
       "IPY_MODEL_9de7f27011b346f8b7a13fa649164ee7"
      ],
      "layout": "IPY_MODEL_f362a565ff90457f904233d4fc625119"
     }
    },
    "3c6290e0ee42461eb47dfcc5d5cd0629": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "55cdb4e0b33a48b298f760e7ff2af0f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_83ac28af70074e998663f6f247278a83",
      "max": 10000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3c6290e0ee42461eb47dfcc5d5cd0629",
      "value": 10000
     }
    },
    "83ac28af70074e998663f6f247278a83": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88a2b48b3b4f415797bab96eaa925aa7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9de7f27011b346f8b7a13fa649164ee7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_88a2b48b3b4f415797bab96eaa925aa7",
      "placeholder": "​",
      "style": "IPY_MODEL_c241146f1475404282c35bc09e7cc945",
      "value": " 10000/10000 [03:52&lt;00:00, 79.57it/s]"
     }
    },
    "c241146f1475404282c35bc09e7cc945": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ee030d62f3a54f5288cccf954caa7d85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_059918bb59744634aaa181dc4ec256a2",
      "placeholder": "​",
      "style": "IPY_MODEL_f762e8d37ab6441d87b2a66bfddd5239",
      "value": "100%"
     }
    },
    "f362a565ff90457f904233d4fc625119": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f762e8d37ab6441d87b2a66bfddd5239": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
