{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groundtruth evaluation for LlamaIndex applications\n",
    "\n",
    "Ground truth evaluation can be especially useful during early LLM experiments when you have a small set of example queries that are critical to get right. Ground truth evaluation works by comparing the similarity of an LLM response compared to its matching verified response.\n",
    "\n",
    "This example walks through how to set up ground truth eval for a LlamaIndex app.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/examples/expositional/frameworks/llama_index/llama_index_groundtruth.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import from TruLens and LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --pre trulens trulens-instrument-llamaindex trulens-providers-openai llama_index==0.10.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "import openai\n",
    "from trulens.core import Feedback\n",
    "from trulens.core import TruSession\n",
    "from trulens.feedback import GroundTruthAgreement\n",
    "from trulens.instrument.llamaindex import TruLlama\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "tru = TruSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.reset_database()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add API keys\n",
    "For this quickstart, you will need Open AI and Huggingface keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Simple LLM Application\n",
    "\n",
    "This example uses LlamaIndex which internally uses an OpenAI LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\"http://paulgraham.com/worked.html\"]\n",
    ")\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Feedback Function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI-based feedback function collection class:\n",
    "openai_provider = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_set = [\n",
    "    {\n",
    "        \"query\": \"What was the author's undergraduate major?\",\n",
    "        \"response\": \"He didn't choose a major, and customized his courses.\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What company did the author start in 1995?\",\n",
    "        \"response\": \"Viaweb, to make software for building online stores.\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Where did the author move in 1998 after selling Viaweb?\",\n",
    "        \"response\": \"California, after Yahoo acquired Viaweb.\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What did the author do after leaving Yahoo in 1999?\",\n",
    "        \"response\": \"He focused on painting and tried to improve his art skills.\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What program did the author start with Jessica Livingston in 2005?\",\n",
    "        \"response\": \"Y Combinator, to provide seed funding for startups.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_groundtruth = Feedback(\n",
    "    GroundTruthAgreement(golden_set, provider=openai_provider).agreement_measure, name=\"Ground Truth Eval\"\n",
    ").on_input_output()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrument the application with Ground Truth Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_query_engine_recorder = TruLlama(\n",
    "    query_engine,\n",
    "    app_name=\"LlamaIndex_App\",\n",
    "    feedbacks=[f_groundtruth],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the application for all queries in the golden set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run and evaluate on groundtruth questions\n",
    "for pair in golden_set:\n",
    "    with tru_query_engine_recorder as recording:\n",
    "        llm_response = query_engine.query(pair[\"query\"])\n",
    "        print(llm_response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore with the TruLens dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "run_dashboard(tru)  # open a local streamlit app to explore\n",
    "\n",
    "# stop_dashboard(tru) # stop if needed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or view results directly in your notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records, feedback = tru.get_records_and_feedback()\n",
    "records.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "7d153714b979d5e6d08dd8ec90712dd93bff2c9b6c1f0c118169738af3430cd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
