{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Planning in LlamaIndex\n",
    "\n",
    "Query planning is a useful tool to leverage the ability of LLMs to structure the user inputs into multiple different queries, either sequentially or in parallel before answering the questions. This method improvers the response by allowing the question to be decomposed into smaller, more answerable questions.\n",
    "\n",
    "Sub-question queries are one such method. Sub-question queries decompose the user input into multiple different sub-questions. This is great for answering complex questions that require knowledge from different documents.\n",
    "\n",
    "Relatedly, there are a great deal of configurations for this style of application that must be selected. In this example, we'll iterate through several of these choices and evaluate each with TruLens.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/examples/expositional/frameworks/llama_index/llama_index_queryplanning.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import from LlamaIndex and TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install trulens trulens-apps-llamaindex trulens-providers-openai llama_index  llama-index-readers-web==0.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.tools import ToolMetadata\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "from trulens.core import TruSession\n",
    "\n",
    "session = TruSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This is ONLY necessary in jupyter notebook.\n",
    "# Details: Jupyter runs an event-loop behind the scenes.\n",
    "#          This results in nested event-loops when we start an event-loop to make async queries.\n",
    "#          This is normally not allowed, we use nest_asyncio to allow it for convenience.\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set keys\n",
    "\n",
    "For this example we need an OpenAI key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the dashboard\n",
    "\n",
    "By starting the dashboard ahead of time, we can watch as the evaluations get logged. This is especially useful for longer-running applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "run_dashboard(session)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\"https://www.gutenberg.org/files/11/11-h/11-h.htm\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create base query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build index and query engine\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# create embedding-based query engine from index\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from trulens.apps.llamaindex import TruLlama\n",
    "from trulens.core import Feedback\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "# Initialize provider class\n",
    "provider = OpenAI()\n",
    "\n",
    "# select context to be used in feedback. the location of context is app specific.\n",
    "\n",
    "context = TruLlama.select_context(query_engine)\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(\n",
    "        provider.groundedness_measure_with_cot_reasons, name=\"Groundedness\"\n",
    "    )\n",
    "    .on(context.collect())  # collect context chunks into a list\n",
    "    .on_output()\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_answer_relevance = Feedback(\n",
    "    provider.relevance_with_cot_reasons, name=\"Answer Relevance\"\n",
    ").on_input_output()\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(\n",
    "        provider.context_relevance_with_cot_reasons, name=\"Context Relevance\"\n",
    "    )\n",
    "    .on_input()\n",
    "    .on(context)\n",
    "    .aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set configuration space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_types = [\"VectorStoreIndex\", \"SubQuestionQueryEngine\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set test prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set test prompts\n",
    "prompts = [\n",
    "    \"Describe Alice's growth from meeting the White Rabbit to challenging the Queen of Hearts?\",\n",
    "    \"Relate aspects of enchantment to the nostalgia that Alice experiences in Wonderland. Why is Alice both fascinated and frustrated by her encounters below-ground?\",\n",
    "    \"Describe the White Rabbit's function in Alice.\",\n",
    "    \"Describe some of the ways that Carroll achieves humor at Alice's expense.\",\n",
    "    \"Compare the Duchess' lullaby to the 'You Are Old, Father William' verse\",\n",
    "    \"Compare the sentiment of the Mouse's long tale, the Mock Turtle's story and the Lobster-Quadrille.\",\n",
    "    \"Summarize the role of the mad hatter in Alice's journey\",\n",
    "    \"How does the Mad Hatter influence the arc of the story throughout?\",\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through configuration space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "for query_engine_type in query_engine_types:\n",
    "\n",
    "    if query_engine_type == \"SubQuestionQueryEngine\":\n",
    "        query_engine_tools = [\n",
    "            QueryEngineTool(\n",
    "                    query_engine=query_engine,\n",
    "                    metadata=ToolMetadata(\n",
    "                        name=\"Alice in Wonderland\",\n",
    "                        description=\"THE MILLENNIUM FULCRUM EDITION 3.0\",\n",
    "                    ),\n",
    "                )\n",
    "            ]\n",
    "        query_engine = SubQuestionQueryEngine.from_defaults(\n",
    "                query_engine_tools=query_engine_tools,\n",
    "            )\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    tru_query_engine_recorder = TruLlama(\n",
    "            app_name=f\"Alice in Wonderland QA\",\n",
    "            app_version=f\"{query_engine_type}\",\n",
    "            metadata={\n",
    "                \"query_engine_type\": query_engine_type,\n",
    "            },\n",
    "            app=query_engine,\n",
    "            feedbacks=[f_groundedness, f_answer_relevance, f_context_relevance],\n",
    "        )\n",
    "\n",
    "        # tru_query_engine_recorder as context manager\n",
    "    with tru_query_engine_recorder as recording:\n",
    "        for prompt in prompts:\n",
    "            query_engine.query(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "7d153714b979d5e6d08dd8ec90712dd93bff2c9b6c1f0c118169738af3430cd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
