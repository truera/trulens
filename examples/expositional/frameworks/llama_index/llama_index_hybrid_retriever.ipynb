{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlamaIndex Hybrid Retriever + Reranking + Guardrails\n",
    "\n",
    "Hybrid Retrievers are a great way to combine the strengths of different retrievers. Combined with filtering and reranking, this can be especially powerful in retrieving only the most relevant context from multiple methods. TruLens can take us even farther to highlight the strengths of each component retriever along with measuring the success of the hybrid retriever.\n",
    "\n",
    "Last, we'll show how guardrails are an alternative approach to achieving the same goal: passing only relevant context to the LLM.\n",
    "\n",
    "This example walks through that process.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/examples/expositional/frameworks/llama_index/llama_index_hybrid_retriever.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --pre trulens llama_index llama-index-readers-file llama-index-llms-openai llama-index-retrievers-bm25 openai pypdf torch sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports main tools:\n",
    "from trulens.core import Feedback\n",
    "from trulens.core import TruSession\n",
    "from trulens.apps.llamaindex import TruLlama\n",
    "\n",
    "tru = TruSession()\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf --output IPCC_AR6_WGII_Chapter03.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"IPCC_AR6_WGII_Chapter03.pdf\"]\n",
    ").load_data()\n",
    "\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "# initialize storage context (by default it's in-memory)\n",
    "storage_context = StorageContext.from_defaults()\n",
    "storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "index = VectorStoreIndex(\n",
    "    nodes=nodes,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the top 10 most similar nodes using embeddings\n",
    "vector_retriever = VectorIndexRetriever(index)\n",
    "\n",
    "# retrieve the top 2 most similar nodes using bm25\n",
    "bm25_retriever = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Hybrid (Custom) Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import BaseRetriever\n",
    "\n",
    "\n",
    "class HybridRetriever(BaseRetriever):\n",
    "    def __init__(self, vector_retriever, bm25_retriever):\n",
    "        self.vector_retriever = vector_retriever\n",
    "        self.bm25_retriever = bm25_retriever\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query, **kwargs):\n",
    "        bm25_nodes = self.bm25_retriever.retrieve(query, **kwargs)\n",
    "        vector_nodes = self.vector_retriever.retrieve(query, **kwargs)\n",
    "\n",
    "        # combine the two lists of nodes\n",
    "        all_nodes = []\n",
    "        node_ids = set()\n",
    "        for n in bm25_nodes + vector_nodes:\n",
    "            if n.node.node_id not in node_ids:\n",
    "                all_nodes.append(n)\n",
    "                node_ids.add(n.node.node_id)\n",
    "        return all_nodes\n",
    "\n",
    "\n",
    "index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "hybrid_retriever = HybridRetriever(vector_retriever, bm25_retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "reranker = SentenceTransformerRerank(top_n=2, model=\"BAAI/bge-reranker-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=hybrid_retriever, node_postprocessors=[reranker]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "run_dashboard(tru, port=1234)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Context Relevance checks\n",
    "\n",
    "Include relevance checks for bm25, vector retrievers, hybrid retriever and the filtered hybrid retriever (after rerank and filter).\n",
    "\n",
    "This requires knowing the feedback selector for each. You can find this path by logging a run of your application and examining the application traces on the Evaluations page.\n",
    "\n",
    "Read more in our docs: https://www.trulens.org/trulens/evaluation/feedback_selectors/selecting_components/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from trulens.core.schema import Select\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "# Initialize provider class\n",
    "openai = OpenAI()\n",
    "\n",
    "bm25_context = Select.RecordCalls._retriever.bm25_retriever.retrieve.rets[\n",
    "    :\n",
    "].node.text\n",
    "vector_context = Select.RecordCalls._retriever.vector_retriever._retrieve.rets[\n",
    "    :\n",
    "].node.text\n",
    "hybrid_context = Select.RecordCalls._retriever.retrieve.rets[:].node.text\n",
    "hybrid_context_filtered = (\n",
    "    Select.RecordCalls._node_postprocessors[0]\n",
    "    ._postprocess_nodes.rets[:]\n",
    "    .node.text\n",
    ")\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance_bm25 = (\n",
    "    Feedback(openai.context_relevance, name=\"BM25\")\n",
    "    .on_input()\n",
    "    .on(bm25_context)\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "f_context_relevance_vector = (\n",
    "    Feedback(openai.context_relevance, name=\"Vector\")\n",
    "    .on_input()\n",
    "    .on(vector_context)\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "f_context_relevance_hybrid = (\n",
    "    Feedback(openai.context_relevance, name=\"Hybrid\")\n",
    "    .on_input()\n",
    "    .on(hybrid_context)\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "f_context_relevance_hybrid_filtered = (\n",
    "    Feedback(openai.context_relevance, name=\"Hybrid Filtered\")\n",
    "    .on_input()\n",
    "    .on(hybrid_context_filtered)\n",
    "    .aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add feedbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_recorder = TruLlama(\n",
    "    query_engine,\n",
    "    app_name=\"Hybrid Retriever Query Engine\",\n",
    "    feedbacks=[\n",
    "        f_context_relevance_bm25,\n",
    "        f_context_relevance_vector,\n",
    "        f_context_relevance_hybrid,\n",
    "        f_context_relevance_hybrid_filtered,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_recorder as recording:\n",
    "    response = query_engine.query(\n",
    "        \"What is the impact of climate change on the ocean?\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore in a Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "run_dashboard(tru)  # open a local streamlit app to explore\n",
    "\n",
    "# stop_dashboard(tru) # stop if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback Guardrails: an alternative to reranking/filtering\n",
    "\n",
    "TruLens feedback functions can be used as context filters in place of reranking. This is great for cases when you don't want to deal with another model (the reranker) or in cases when the feedback function is better aligned to human scores than a reranker. Notably, this feedback function can be any model of your choice - this is a great use of small, lightweight models that don't add as much latency to your app.\n",
    "\n",
    "To illustrate this, we'll set up a new query engine with only the hybrid retriever (no reranking)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = RetrieverQueryEngine.from_args(retriever=hybrid_retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll set up a feedback function and wrap the query engine with TruLens' `WithFeedbackFilterNodes`. This allows us to pass in any feedback function we'd like to use for filtering, even custom ones!\n",
    "\n",
    "In this example, we're using LLM-as-judge context relevance, but a small local model could be used here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core.guardrails.llama import WithFeedbackFilterNodes\n",
    "\n",
    "feedback = Feedback(openai.context_relevance)\n",
    "\n",
    "filtered_query_engine = WithFeedbackFilterNodes(\n",
    "    query_engine, feedback=feedback, threshold=0.75\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up for recording\n",
    "\n",
    "Here we'll introduce one last variation of the context relevance feedback function, this one pointed at the returned source nodes from the query engine's `synthesize` method. This will accurately capture which retrieved context gets past the filter and to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_context_filtered = (\n",
    "    Select.Record.app.query_engine.synthesize.rets.source_nodes[:].node.text\n",
    ")\n",
    "\n",
    "\n",
    "f_context_relevance_afterguardrails = (\n",
    "    Feedback(openai.context_relevance, name=\"After guardrails\")\n",
    "    .on_input()\n",
    "    .on(hybrid_context_filtered)\n",
    "    .aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_recorder = TruLlama(\n",
    "    filtered_query_engine,\n",
    "    app_name=\"Hybrid Retriever Query Engine with Guardrails\",\n",
    "    feedbacks=[f_context_relevance_afterguardrails],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_recorder as recording:\n",
    "    response = filtered_query_engine.query(\n",
    "        \"What is the impact of climate change on the ocean\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "d5737f6101ac92451320b0e41890107145710b89f85909f3780d702e7818f973"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
