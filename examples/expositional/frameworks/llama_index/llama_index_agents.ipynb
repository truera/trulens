{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LlamaIndex Agents + Ground Truth & Custom Evaluations\n",
    "\n",
    "In this example, we build an agent-based app with Llama Index to answer questions with the help of Yelp. We'll evaluate it using a few different feedback functions (some custom, some out-of-the-box)\n",
    "\n",
    "The first set of feedback functions complete what the non-hallucination triad. However because we're dealing with agents here,  we've added a fourth leg (query translation) to cover the additional interaction between the query planner and the agent. This combination provides a foundation for eliminating hallucination in LLM applications.\n",
    "\n",
    "1. Query Translation - The first step. Here we compare the similarity of the original user query to the query sent to the agent. This ensures that we're providing the agent with the correct question.\n",
    "2. Context or QS Relevance - Next, we compare the relevance of the context provided by the agent back to the original query. This ensures that we're providing context for the right question.\n",
    "3. Groundedness - Third, we ensure that the final answer is supported by the context. This ensures that the LLM is not extending beyond the information provided by the agent.\n",
    "4. Question Answer Relevance - Last, we want to make sure that the final answer provided is relevant to the user query. This last step confirms that the answer is not only supported but also useful to the end user.\n",
    "\n",
    "In this example, we'll add two additional feedback functions.\n",
    "\n",
    "5. Ratings usage - evaluate if the summarized context uses ratings as justification. Note: this may not be relevant for all queries.\n",
    "6. Ground truth eval - we want to make sure our app responds correctly. We will create a ground truth set for this evaluation.\n",
    "\n",
    "Last, we'll compare the evaluation of this app against a standalone LLM. May the best bot win?\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/expositional/frameworks/llama_index/llama_index_agents.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install TruLens and Llama-Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install trulens_eval==0.24.0 llama_index==0.10.33 llama-index-tools-yelp==0.1.2 openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running from github repo, uncomment the below to setup paths.\n",
    "# from pathlib import Path\n",
    "# import sys\n",
    "# trulens_path = Path().cwd().parent.parent.parent.parent.resolve()\n",
    "# sys.path.append(str(trulens_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup OpenAI Agent\n",
    "import os\n",
    "\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API keys. If you already have them in your var env., you can skip these steps.\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk...\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "os.environ[\"YELP_API_KEY\"] = \"...\"\n",
    "os.environ[\"YELP_CLIENT_ID\"] = \"...\"\n",
    "\n",
    "# If you already have keys in var env., use these to check instead:\n",
    "# from trulens.utils.keys import check_keys\n",
    "# check_keys(\"OPENAI_API_KEY\", \"YELP_API_KEY\", \"YELP_CLIENT_ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up our Llama-Index App\n",
    "\n",
    "For this app, we will use a tool from Llama-Index to connect to Yelp and allow the Agent to search for business and fetch reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and initialize our tool spec\n",
    "from llama_index.core.tools.tool_spec.load_and_search.base import (\n",
    "    LoadAndSearchToolSpec,\n",
    ")\n",
    "from llama_index.tools.yelp.base import YelpToolSpec\n",
    "\n",
    "# Add Yelp API key and client ID\n",
    "tool_spec = YelpToolSpec(\n",
    "    api_key=os.environ.get(\"YELP_API_KEY\"),\n",
    "    client_id=os.environ.get(\"YELP_CLIENT_ID\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gordon_ramsay_prompt = \"You answer questions about restaurants in the style of Gordon Ramsay, often insulting the asker.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Agent with our tools\n",
    "tools = tool_spec.to_tool_list()\n",
    "agent = OpenAIAgent.from_tools(\n",
    "    [\n",
    "        *LoadAndSearchToolSpec.from_defaults(tools[0]).to_tool_list(),\n",
    "        *LoadAndSearchToolSpec.from_defaults(tools[1]).to_tool_list(),\n",
    "    ],\n",
    "    verbose=True,\n",
    "    system_prompt=gordon_ramsay_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a standalone GPT3.5 for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI()\n",
    "\n",
    "chat_completion = client.chat.completions.create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import TruCustomApp\n",
    "from trulens.core import instrument\n",
    "\n",
    "\n",
    "class LLMStandaloneApp:\n",
    "    @instrument\n",
    "    def __call__(self, prompt):\n",
    "        return (\n",
    "            chat_completion(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": gordon_ramsay_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "            )\n",
    "            .choices[0]\n",
    "            .message.content\n",
    "        )\n",
    "\n",
    "\n",
    "llm_standalone = LLMStandaloneApp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and Tracking with TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports required for tracking and evaluation\n",
    "from trulens.core import Feedback\n",
    "from trulens.core import Select\n",
    "from trulens.core import Tru\n",
    "from trulens.ext.instrument.llamaindex import TruLlama\n",
    "from trulens.ext.provider.openai import OpenAI as fOpenAI\n",
    "from trulens.feedback import GroundTruthAgreement\n",
    "\n",
    "tru = Tru()\n",
    "# tru.reset_database() # if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation setup\n",
    "\n",
    "To set up our evaluation, we'll first create two new custom feedback functions: query_translation_score and ratings_usage. These are straight-forward prompts of the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_OpenAI(OpenAI):\n",
    "    def query_translation_score(self, question1: str, question2: str) -> float:\n",
    "        prompt = f\"Your job is to rate how similar two quesitons are on a scale of 1 to 10. Respond with the number only. QUESTION 1: {question1}; QUESTION 2: {question2}\"\n",
    "        return self.generate_score_and_reason(system_prompt=prompt)\n",
    "\n",
    "    def ratings_usage(self, last_context: str) -> float:\n",
    "        prompt = f\"Your job is to respond with a '1' if the following statement mentions ratings or reviews, and a '0' if not. STATEMENT: {last_context}\"\n",
    "        return self.generate_score_and_reason(system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of our feedback functions available, we can instantiate them. For many of our evals, we want to check on intermediate parts of our app such as the query passed to the yelp app, or the summarization of the Yelp content. We'll do so here using Select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unstable: perhaps reduce temperature?\n",
    "\n",
    "custom_provider = Custom_OpenAI()\n",
    "# Input to tool based on trimmed user input.\n",
    "f_query_translation = (\n",
    "    Feedback(custom_provider.query_translation_score, name=\"Query Translation\")\n",
    "    .on_input()\n",
    "    .on(Select.Record.app.query[0].args.str_or_query_bundle)\n",
    ")\n",
    "\n",
    "f_ratings_usage = Feedback(\n",
    "    custom_provider.ratings_usage, name=\"Ratings Usage\"\n",
    ").on(Select.Record.app.query[0].rets.response)\n",
    "\n",
    "# Result of this prompt: Given the context information and not prior knowledge, answer the query.\n",
    "# Query: address of Gumbo Social\n",
    "# Answer: \"\n",
    "provider = fOpenAI()\n",
    "# Context relevance between question and last context chunk (i.e. summary)\n",
    "f_context_relevance = (\n",
    "    Feedback(provider.context_relevance, name=\"Context Relevance\")\n",
    "    .on_input()\n",
    "    .on(Select.Record.app.query[0].rets.response)\n",
    ")\n",
    "\n",
    "# Groundedness\n",
    "f_groundedness = (\n",
    "    Feedback(\n",
    "        provider.groundedness_measure_with_cot_reasons, name=\"Groundedness\"\n",
    "    )\n",
    "    .on(Select.Record.app.query[0].rets.response)\n",
    "    .on_output()\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = Feedback(\n",
    "    provider.relevance, name=\"Answer Relevance\"\n",
    ").on_input_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground Truth Eval\n",
    "\n",
    "It's also useful in many cases to do ground truth eval with small golden sets. We'll do so here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_set = [\n",
    "    {\n",
    "        \"query\": \"Hello there mister AI. What's the vibe like at oprhan andy's in SF?\",\n",
    "        \"response\": \"welcoming and friendly\",\n",
    "    },\n",
    "    {\"query\": \"Is park tavern in San Fran open yet?\", \"response\": \"Yes\"},\n",
    "    {\n",
    "        \"query\": \"I'm in san francisco for the morning, does Juniper serve pastries?\",\n",
    "        \"response\": \"Yes\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What's the address of Gumbo Social in San Francisco?\",\n",
    "        \"response\": \"5176 3rd St, San Francisco, CA 94124\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are the reviews like of Gola in SF?\",\n",
    "        \"response\": \"Excellent, 4.6/5\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Where's the best pizza in New York City\",\n",
    "        \"response\": \"Joe's Pizza\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What's the best diner in Toronto?\",\n",
    "        \"response\": \"The George Street Diner\",\n",
    "    },\n",
    "]\n",
    "\n",
    "f_groundtruth = Feedback(\n",
    "    GroundTruthAgreement(golden_set).agreement_measure, name=\"Ground Truth Eval\"\n",
    ").on_input_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the dashboard\n",
    "\n",
    "By running the dashboard before we start to make app calls, we can see them come in 1 by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "run_dashboard(\n",
    "    tru,\n",
    "    # if running from github\n",
    "    # _dev=trulens_path,\n",
    "    # force=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrument Yelp App\n",
    "\n",
    "We can instrument our yelp app with TruLlama and utilize the full suite of evals we set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_agent = TruLlama(\n",
    "    agent,\n",
    "    app_id=\"YelpAgent\",\n",
    "    tags=\"agent prototype\",\n",
    "    feedbacks=[\n",
    "        f_qa_relevance,\n",
    "        f_groundtruth,\n",
    "        f_context_relevance,\n",
    "        f_groundedness,\n",
    "        f_query_translation,\n",
    "        f_ratings_usage,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_agent.print_instrumented()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrument Standalone LLM app.\n",
    "\n",
    "Since we don't have insight into the OpenAI innerworkings, we cannot run many of the evals on intermediate steps.\n",
    "\n",
    "We can still do QA relevance on input and output, and check for similarity of the answers compared to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_llm_standalone = TruCustomApp(\n",
    "    llm_standalone,\n",
    "    app_id=\"OpenAIChatCompletion\",\n",
    "    tags=\"comparison\",\n",
    "    feedbacks=[f_qa_relevance, f_groundtruth],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_llm_standalone.print_instrumented()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start using our apps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_set = [\n",
    "    \"What's the vibe like at oprhan andy's in SF?\",\n",
    "    \"What are the reviews like of Gola in SF?\",\n",
    "    \"Where's the best pizza in New York City\",\n",
    "    \"What's the address of Gumbo Social in San Francisco?\",\n",
    "    \"I'm in san francisco for the morning, does Juniper serve pastries?\",\n",
    "    \"What's the best diner in Toronto?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in prompt_set:\n",
    "    print(prompt)\n",
    "\n",
    "    with tru_llm_standalone as recording:\n",
    "        llm_standalone(prompt)\n",
    "    record_standalone = recording.get()\n",
    "\n",
    "    with tru_agent as recording:\n",
    "        agent.query(prompt)\n",
    "    record_agent = recording.get()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "7d153714b979d5e6d08dd8ec90712dd93bff2c9b6c1f0c118169738af3430cd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
