{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Assistants API\n",
    "\n",
    "The [Assistants API](https://platform.openai.com/docs/assistants/overview) allows you to build AI assistants within your own applications. An Assistant has instructions and can leverage models, tools, and knowledge to respond to user queries. The Assistants API currently supports three types of tools: Code Interpreter, Retrieval, and Function calling.\n",
    "\n",
    "TruLens can be easily integrated with the assistants API to provide the same observability tooling you are used to when building with other frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[**Important**] Notice in this example notebook, we are using Assistants API V1 (hence the pinned version of `openai` below) so that we can evaluate against retrieved source.\n",
    "At some very recent point in time as of April 2024, OpenAI removed the [\"quote\" attribute from file citation object in Assistants API V2](https://platform.openai.com/docs/api-reference/messages/object#messages/object-content) due to stability issue of this feature. See response from OpenAI staff https://community.openai.com/t/assistant-api-always-return-empty-annotations/489285/48\n",
    "\n",
    "Here's the migration guide for easier navigating between V1 and V2 of Assistants API: https://platform.openai.com/docs/assistants/migration/changing-beta-versions\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --pre trulens trulens-providers-openai openai==1.14.3 # pinned openai version to avoid breaking changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the assistant\n",
    "\n",
    "Let's create a new assistant that answers questions about the famous *Paul Graham Essay*.\n",
    "\n",
    "The easiest way to get it is to download it via this link and save it in a folder called data. You can do so with the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt -P data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import Tru\n",
    "from trulens.core.app.custom import instrument\n",
    "\n",
    "tru = Tru()\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a thread (V1 Assistants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "class RAG_with_OpenAI_Assistant:\n",
    "    def __init__(self):\n",
    "        client = OpenAI()\n",
    "        self.client = client\n",
    "\n",
    "        # upload the file\\\n",
    "        file = client.files.create(\n",
    "            file=open(\"data/paul_graham_essay.txt\", \"rb\"), purpose=\"assistants\"\n",
    "        )\n",
    "\n",
    "        # create the assistant with access to a retrieval tool\n",
    "        assistant = client.beta.assistants.create(\n",
    "            name=\"Paul Graham Essay Assistant\",\n",
    "            instructions=\"You are an assistant that answers questions about Paul Graham.\",\n",
    "            tools=[{\"type\": \"retrieval\"}],\n",
    "            model=\"gpt-4-turbo-preview\",\n",
    "            file_ids=[file.id],\n",
    "        )\n",
    "\n",
    "        self.assistant = assistant\n",
    "\n",
    "    @instrument\n",
    "    def retrieve_and_generate(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Retrieve relevant text by creating and running a thread with the OpenAI assistant.\n",
    "        \"\"\"\n",
    "        self.thread = self.client.beta.threads.create()\n",
    "        self.message = self.client.beta.threads.messages.create(\n",
    "            thread_id=self.thread.id, role=\"user\", content=query\n",
    "        )\n",
    "\n",
    "        run = self.client.beta.threads.runs.create(\n",
    "            thread_id=self.thread.id,\n",
    "            assistant_id=self.assistant.id,\n",
    "            instructions=\"Please answer any questions about Paul Graham.\",\n",
    "        )\n",
    "\n",
    "        # Wait for the run to complete\n",
    "        import time\n",
    "\n",
    "        while run.status in [\"queued\", \"in_progress\", \"cancelling\"]:\n",
    "            time.sleep(1)\n",
    "            run = self.client.beta.threads.runs.retrieve(\n",
    "                thread_id=self.thread.id, run_id=run.id\n",
    "            )\n",
    "\n",
    "        if run.status == \"completed\":\n",
    "            messages = self.client.beta.threads.messages.list(\n",
    "                thread_id=self.thread.id\n",
    "            )\n",
    "            response = messages.data[0].content[0].text.value\n",
    "            quote = (\n",
    "                messages.data[0]\n",
    "                .content[0]\n",
    "                .text.annotations[0]\n",
    "                .file_citation.quote\n",
    "            )\n",
    "        else:\n",
    "            response = \"Unable to retrieve information at this time.\"\n",
    "\n",
    "        return response, quote\n",
    "\n",
    "\n",
    "rag = RAG_with_OpenAI_Assistant()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create feedback functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from trulens.core import Feedback\n",
    "from trulens.core import Select\n",
    "from trulens.providers.openai import OpenAI as fOpenAI\n",
    "\n",
    "provider = fOpenAI()\n",
    "\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(\n",
    "        provider.groundedness_measure_with_cot_reasons, name=\"Groundedness\"\n",
    "    )\n",
    "    .on(Select.RecordCalls.retrieve_and_generate.rets[1])\n",
    "    .on(Select.RecordCalls.retrieve_and_generate.rets[0])\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_answer_relevance = (\n",
    "    Feedback(provider.relevance_with_cot_reasons, name=\"Answer Relevance\")\n",
    "    .on(Select.RecordCalls.retrieve_and_generate.args.query)\n",
    "    .on(Select.RecordCalls.retrieve_and_generate.rets[0])\n",
    ")\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(\n",
    "        provider.context_relevance_with_cot_reasons, name=\"Context Relevance\"\n",
    "    )\n",
    "    .on(Select.RecordCalls.retrieve_and_generate.args.query)\n",
    "    .on(Select.RecordCalls.retrieve_and_generate.rets[1])\n",
    "    .aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import TruCustomApp\n",
    "\n",
    "tru_rag = TruCustomApp(\n",
    "    rag,\n",
    "    app_id=\"OpenAI Assistant RAG\",\n",
    "    feedbacks=[f_groundedness, f_answer_relevance, f_context_relevance],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_rag:\n",
    "    rag.retrieve_and_generate(\"How did paul graham grow up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "run_dashboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-prospector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
