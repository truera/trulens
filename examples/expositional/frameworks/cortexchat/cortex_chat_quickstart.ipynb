{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cortex Chat + TruLens\n",
    "\n",
    "This quickstart assumes you already have a Cortex Search Service started, JWT token created and Cortex Chat Private Preview enabled for your account. If you need assistance getting started with Cortex Chat, please contact your Snowflake contact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set JWT Token and Chat URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SNOWFLAKE_JWT\"] = \"...\"\n",
    "os.environ[\"SNOWFLAKE_CHAT_URL\"] = \".../api/v2/cortex/chat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Cortex Chat App\n",
    "\n",
    "The `CortexChat` class below can be configured with your URL and model selection.\n",
    "\n",
    "It contains two methods: `handle_cortex_chat_response`, and `chat`.\n",
    "- `_handle_cortex_chat_response` serves to handle the streaming response, and expose the debugging information.\n",
    "- `chat` is a user-facing method that allows you to input a `query` and receive a `response` and `citaiton`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from trulens.apps.custom import instrument\n",
    "\n",
    "class CortexChat:\n",
    "    def __init__(self, url: str, model: str = \"mistral-large\"):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the CortexChat class.\n",
    "        Parameters:\n",
    "        url (str): The URL of the chat service.\n",
    "        model (str): The model to be used for chat. Defaults to \"mistral-large\".\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        self.url = url\n",
    "        self.model = model\n",
    "\n",
    "    @instrument\n",
    "    def _handle_cortex_chat_response(self, response: requests.Response) -> tuple[str, str, str]:\n",
    "        \"\"\"\n",
    "        Process the response from the Cortex Chat API.\n",
    "        Args:\n",
    "            response: The response object from the Cortex Chat API.\n",
    "        Returns:\n",
    "            A tuple containing the extracted text, citation, and debug information from the response.\n",
    "        Raises:\n",
    "            json.JSONDecodeError: If there is an error decoding the JSON data in the response.\n",
    "        \"\"\"\n",
    "\n",
    "        text = \"\"\n",
    "        citation = \"\"\n",
    "        debug_info = \"\"\n",
    "        previous_line = \"\"\n",
    "        \n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                decoded_line = line.decode('utf-8')\n",
    "                if decoded_line == \"event: done\":\n",
    "                    return text, citation, debug_info\n",
    "                if decoded_line.startswith('data:'):\n",
    "                    try:\n",
    "                        data = json.loads(decoded_line[5:])\n",
    "                        if data['delta']['content'][0]['type'] == \"text\":\n",
    "                            print(data['delta']['content'][0]['text']['value'], end = '')\n",
    "                            text += data['delta']['content'][0]['text']['value']\n",
    "                        if data['delta']['content'][0]['type'] == \"citation\":\n",
    "                            citation = data['delta']['content'][0]['citation']\n",
    "                        if data['delta']['content'][0]['type'] == \"debug_info\":\n",
    "                            debug_info = data['delta']['content'][0]['debug_info']\n",
    "                            print(debug_info)\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"Error decoding JSON: {decoded_line}\")\n",
    "                        print(f\"Previous line: {previous_line}\")\n",
    "                previous_line = decoded_line\n",
    "\n",
    "    @instrument           \n",
    "    def chat(self, query: str) -> tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Sends a chat query to the Cortex Chat API and returns the response.\n",
    "        Args:\n",
    "            query (str): The chat query to send.\n",
    "        Returns:\n",
    "            tuple: A tuple containing the text response and citation.\n",
    "        Raises:\n",
    "            None\n",
    "        Example:\n",
    "            >>> cortex = CortexChat()\n",
    "            >>> response = cortex.chat(\"Hello, how are you?\")\n",
    "            >>> print(response)\n",
    "            (\"I'm good, thank you!\", \"Cortex Chat API v1.0\")\n",
    "        \"\"\"\n",
    "\n",
    "        url = self.url\n",
    "        headers = {\n",
    "            'X-Snowflake-Authorization-Token-Type': 'KEYPAIR_JWT',\n",
    "            'Content-Type': 'application/json',\n",
    "            'Accept': 'application/json',\n",
    "            'Authorization': f\"Bearer {os.environ.get('SNOWFLAKE_JWT')}\"\n",
    "        }\n",
    "        data = {\n",
    "            \"query\": query,\n",
    "            \"model\": self.model,\n",
    "            \"debug\": True,\n",
    "            \"search_services\": [{\n",
    "                \"name\": \"JOSH_DB.DATA.JOSH_CORTEX_SEARCH_SERVICE\",\n",
    "                \"max_results\": 10,\n",
    "            }],\n",
    "            \"prompt\": \"{{.Question}} {{.Context}}\",\n",
    "        }\n",
    "\n",
    "        response = requests.post(url, headers=headers, json=data, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            text, citation, _ = self._handle_cortex_chat_response(response)\n",
    "            return text, citation\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} - {response.text}\")\n",
    "\n",
    "cortex = CortexChat(os.environ[\"SNOWFLAKE_CHAT_URL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a TruLens session\n",
    "\n",
    "Start a TruLens session connected to Snowflake so we can log traces and evaluations in our Snowflake account.\n",
    "\n",
    "Learn more about how to [log in Snowflake](https://www.trulens.org/trulens/tracking/logging/where_to_log/log_in_snowflake/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import TruSession\n",
    "from trulens.connectors.snowflake import SnowflakeConnector\n",
    "\n",
    "connection_params = {\n",
    "    \"account\": \"...\",\n",
    "    \"user\": \"...\",\n",
    "    \"password\": \"...\",\n",
    "    \"database\": \"...\",\n",
    "    \"schema\": \"...\",\n",
    "    \"warehouse\": \"...\",\n",
    "    \"role\": \"...\"\n",
    "}\n",
    "\n",
    "connector = SnowflakeConnector(**connection_params)\n",
    "session = TruSession(connector=connector)\n",
    "\n",
    "session.reset_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Feedback Functions\n",
    "\n",
    "Here we initialize the [RAG Triad](https://www.trulens.org/trulens/getting_started/core_concepts/rag_triad/) to provide feedback on the Chat API responses.\n",
    "\n",
    "If you'd like, you can also choose from a wide variety of [stock feedback functions](https://www.trulens.org/trulens/evaluation/feedback_implementations/stock/) or even create [custom feedback functions](https://www.trulens.org/trulens/evaluation/feedback_implementations/custom_feedback_functions/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from trulens.core import Feedback\n",
    "from trulens.core import Select\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "provider = OpenAI(model_engine=\"gpt-4\")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_answer_relevance = (\n",
    "    Feedback(provider.relevance_with_cot_reasons, name=\"Answer Relevance\")\n",
    "    .on_input()\n",
    "    .on_output()\n",
    ")\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(\n",
    "        provider.groundedness_measure_with_cot_reasons, name=\"Groundedness\"\n",
    "    )\n",
    "    .on(Select.RecordCalls._handle_cortex_chat_response.rets[2][\"retrieved_results\"].collect())\n",
    "    .on_output()\n",
    ")\n",
    "\n",
    "# Context relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(\n",
    "        provider.context_relevance_with_cot_reasons, name=\"Context Relevance\"\n",
    "    )\n",
    "    .on_input()\n",
    "    .on(Select.RecordCalls._handle_cortex_chat_response.rets[2][\"retrieved_results\"][:])\n",
    "    .aggregate(np.mean)  # choose a different aggregation method if you wish\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the TruLens recorder and run the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.apps.custom import TruCustomApp\n",
    "\n",
    "tru_recorder = TruCustomApp(\n",
    "    cortex,\n",
    "    app_name=\"Cortex Chat\",\n",
    "    app_version=\"mistral-large\",\n",
    "    feedbacks=[f_answer_relevance, f_groundedness, f_context_relevance],\n",
    ")\n",
    "\n",
    "with tru_recorder as recording:\n",
    "    # Example usage\n",
    "    user_query = \"Hello! What kind of service does Gregory have?\"\n",
    "    cortex.chat(user_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "run_dashboard(session)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit_trulens_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
