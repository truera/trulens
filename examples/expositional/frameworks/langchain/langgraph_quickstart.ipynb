{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _LangGraph_ Quickstart\n",
    "\n",
    "In this quickstart you will create a multi-agent collaboration system with LangGraph and learn how to log it and get feedback on an LLM response.\n",
    "\n",
    "For evaluation, we will leverage the RAG triad of groundedness, context relevance and answer relevance.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/examples/expositional/frameworks/langchain/langgraph_quickstart.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### Add API keys\n",
    "For this quickstart you will need a [Tavily](https://app.tavily.com/home) and OpenAI key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install trulens langgraph trulens-apps-langgraph trulens-providers-openai openai matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"tvly-dev-...\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import from TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import TruSession\n",
    "\n",
    "session = TruSession()\n",
    "session.reset_database()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_core.load.dump import dumps\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from trulens.core.otel.instrument import instrument\n",
    "from trulens.otel.semconv.trace import SpanAttributes\n",
    "\n",
    "tavily_tool = TavilySearchResults(max_results=5)\n",
    "\n",
    "# Warning: This executes code locally, which can be unsafe when not sandboxed\n",
    "\n",
    "repl = PythonREPL()\n",
    "\n",
    "\n",
    "@tool\n",
    "def python_repl_tool(\n",
    "    code: Annotated[str, \"The python code to execute to generate your chart.\"],\n",
    "):\n",
    "    \"\"\"Use this to execute python code. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    result_str = (\n",
    "        f\"Successfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\"\n",
    "    )\n",
    "    return (\n",
    "        result_str\n",
    "        + \"\\n\\nIf you have completed all tasks, respond with FINAL ANSWER.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Literal\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.types import Command\n",
    "from trulens.otel.semconv.trace import BASE_SCOPE\n",
    "\n",
    "\n",
    "def make_system_prompt(suffix: str) -> str:\n",
    "    return (\n",
    "        \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "        \" Use the provided tools to progress towards answering the question.\"\n",
    "        \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "        \" will help where you left off. Execute what you can to make progress.\"\n",
    "        \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "        \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "        f\"\\n{suffix}\"\n",
    "    )\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "\n",
    "def get_next_node(last_message: BaseMessage, goto: str):\n",
    "    if \"FINAL ANSWER\" in last_message.content:\n",
    "        # Any agent decided the work is done\n",
    "        return END\n",
    "    return goto\n",
    "\n",
    "\n",
    "# Research agent and node\n",
    "research_agent = create_react_agent(\n",
    "    llm,\n",
    "    tools=[tavily_tool],\n",
    "    prompt=make_system_prompt(\n",
    "        \"You can only do research. You are working with a chart generator colleague.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "@instrument(\n",
    "    span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
    "    attributes=lambda ret, exception, *args, **kwargs: {\n",
    "        SpanAttributes.RETRIEVAL.QUERY_TEXT: args[0][\"messages\"][-1].content,\n",
    "        SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS: [\n",
    "            json.loads(dumps(message)).get(\"kwargs\", {}).get(\"content\", \"\")\n",
    "            for message in ret.update[\"messages\"]\n",
    "            if isinstance(message, ToolMessage)\n",
    "        ]\n",
    "        if hasattr(ret, \"update\")\n",
    "        else \"No tool call\",\n",
    "    },\n",
    ")\n",
    "def research_node(\n",
    "    state: MessagesState,\n",
    ") -> Command[Literal[\"chart_generator\", END]]:\n",
    "    result = research_agent.invoke(state)\n",
    "    goto = get_next_node(result[\"messages\"][-1], \"chart_generator\")\n",
    "    # wrap in a human message, as not all providers allow\n",
    "    # AI message at the last position of the input messages list\n",
    "    result[\"messages\"][-1] = HumanMessage(\n",
    "        content=result[\"messages\"][-1].content, name=\"researcher\"\n",
    "    )\n",
    "    return Command(\n",
    "        update={\n",
    "            # share internal message history of research agent with other agents\n",
    "            \"messages\": result[\"messages\"],\n",
    "        },\n",
    "        goto=goto,\n",
    "    )\n",
    "\n",
    "\n",
    "# Chart generator agent and node\n",
    "# NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION, WHICH CAN BE UNSAFE WHEN NOT SANDBOXED\n",
    "chart_agent = create_react_agent(\n",
    "    llm,\n",
    "    [python_repl_tool],\n",
    "    prompt=make_system_prompt(\n",
    "        \"You can only generate charts. You are working with a researcher colleague.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "@instrument(\n",
    "    span_type=\"CHART_GENERATOR_NODE\",\n",
    "    attributes=lambda ret, exception, *args, **kwargs: {\n",
    "        f\"{BASE_SCOPE}.chart_node_input\": args[0][\"messages\"][-1].content,\n",
    "        f\"{BASE_SCOPE}.chart_node_response\": (\n",
    "            ret.update[\"messages\"][-1].content\n",
    "            if ret and hasattr(ret, \"update\") and ret.update\n",
    "            else \"No update response\"\n",
    "        ),\n",
    "    },\n",
    ")\n",
    "def chart_node(state: MessagesState) -> Command[Literal[\"researcher\", END]]:\n",
    "    result = chart_agent.invoke(state)\n",
    "    goto = get_next_node(result[\"messages\"][-1], \"researcher\")\n",
    "    # wrap in a human message, as not all providers allow\n",
    "    # AI message at the last position of the input messages list\n",
    "    result[\"messages\"][-1] = HumanMessage(\n",
    "        content=result[\"messages\"][-1].content, name=\"chart_generator\"\n",
    "    )\n",
    "    return Command(\n",
    "        update={\n",
    "            # share internal message history of chart agent with other agents\n",
    "            \"messages\": result[\"messages\"],\n",
    "        },\n",
    "        goto=goto,\n",
    "    )\n",
    "\n",
    "\n",
    "chart_summary_agent = create_react_agent(\n",
    "    llm,\n",
    "    tools=[],  # Add image processing tools if available/needed.\n",
    "    prompt=make_system_prompt(\n",
    "        \"You can only generate image captions. You are working with a researcher colleague and a chart generator colleague. \"\n",
    "        + \"Your task is to generate a concise summary for the provided chart image saved at a local PATH, where the PATH should be and only be provided by your chart generator colleague. The summary should be no more than 3 sentences.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "@instrument(\n",
    "    span_type=\"CHART_SUMMARY_NODE\",\n",
    "    attributes=lambda ret, exception, *args, **kwargs: {\n",
    "        f\"{BASE_SCOPE}.summary_node_input\": args[0][\"messages\"][-1].content,\n",
    "        f\"{BASE_SCOPE}.summary_node_output\": ret.update[\"messages\"][-1].content\n",
    "        if hasattr(ret, \"update\")\n",
    "        else \"NO SUMMARY GENERATED\",\n",
    "    },\n",
    ")\n",
    "def chart_summary_node(\n",
    "    state: MessagesState,\n",
    ") -> Command[Literal[\"researcher\", END]]:\n",
    "    result = chart_summary_agent.invoke(state)\n",
    "    # After captioning the image, we send control back (e.g., to the researcher)\n",
    "    goto = get_next_node(result[\"messages\"][-1], \"researcher\")\n",
    "    # Wrap the output message in a HumanMessage to maintain consistency in the conversation flow.\n",
    "    result[\"messages\"][-1] = HumanMessage(\n",
    "        content=result[\"messages\"][-1].content, name=\"chart_summarizer\"\n",
    "    )\n",
    "    return Command(\n",
    "        update={\"messages\": result[\"messages\"]},\n",
    "        goto=goto,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "from langgraph.graph import START\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"researcher\", research_node)\n",
    "workflow.add_node(\"chart_generator\", chart_node)\n",
    "workflow.add_node(\"chart_summarizer\", chart_summary_node)\n",
    "\n",
    "workflow.add_edge(START, \"researcher\")\n",
    "graph = workflow.compile()\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Feedback Function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from trulens.core import Metric, Selector\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "provider = OpenAI(model_engine=\"gpt-4.1-mini\")\n",
    "\n",
    "# Define a groundedness metric\n",
    "f_groundedness = Metric(\n",
    "    implementation=provider.groundedness_measure_with_cot_reasons,\n",
    "    name=\"Groundedness\",\n",
    "    selectors={\n",
    "        \"source\": Selector.select_context(collect_list=True),\n",
    "        \"statement\": Selector.select_record_output(),\n",
    "    },\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_answer_relevance = Metric(\n",
    "    implementation=provider.relevance_with_cot_reasons,\n",
    "    name=\"Answer Relevance\",\n",
    "    selectors={\n",
    "        \"prompt\": Selector.select_record_input(),\n",
    "        \"response\": Selector.select_record_output(),\n",
    "    },\n",
    ")\n",
    "\n",
    "# Context relevance between question and each context chunk.\n",
    "f_context_relevance = Metric(\n",
    "    implementation=provider.context_relevance_with_cot_reasons,\n",
    "    name=\"Context Relevance\",\n",
    "    selectors={\n",
    "        \"question\": Selector.select_record_input(),\n",
    "        \"context\": Selector.select_context(collect_list=False),\n",
    "    },\n",
    "    agg=np.mean,  # choose a different aggregation method if you wish\n",
    ")\n",
    "\n",
    "# Agent trajectory evals: logical consistency\n",
    "f_logical_consistency = Metric(\n",
    "    implementation=provider.logical_consistency_with_cot_reasons,\n",
    "    name=\"Logical Consistency\",\n",
    "    selectors={\n",
    "        \"trace\": Selector(trace_level=True),\n",
    "    },\n",
    ")\n",
    "\n",
    "# Agent trajectory evals: execution efficiency\n",
    "f_execution_efficiency = Metric(\n",
    "    implementation=provider.execution_efficiency_with_cot_reasons,\n",
    "    name=\"Execution Efficiency\",\n",
    "    selectors={\n",
    "        \"trace\": Selector(trace_level=True),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the graph with TruLens TruGraph, where nodes are auto instrumented \n",
    "\n",
    "TruGraph supports directly registering a compiled graph as the app instance to be instrumented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.apps.langgraph import TruGraph\n",
    "\n",
    "tru_recorder = TruGraph(\n",
    "    graph,\n",
    "    app_name=\"Multi-Agent Chart Generation\",\n",
    "    app_version=\"Base\",\n",
    "    feedbacks=[\n",
    "        f_answer_relevance,\n",
    "        f_context_relevance,\n",
    "        f_groundedness,\n",
    "        f_logical_consistency,\n",
    "        f_execution_efficiency,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Generate a chart showing the trend of the US GDP and National Debt over the last 20 years.\"\n",
    "with tru_recorder:\n",
    "    # Run the multi-agent graph with a sample query\n",
    "    graph.invoke(\n",
    "        {\"messages\": [(\"user\", query)]},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.get_leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "run_dashboard(session=session)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "build_mcp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
