{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _LangChain_ Stream\n",
    "\n",
    "One of the biggest pain-points developers discuss when trying to build useful LLM applications is latency; these applications often make multiple calls to LLM APIs, each one taking a few seconds. It can be quite a frustrating user experience to stare at a loading spinner for more than a couple seconds. Streaming helps reduce this perceived latency by returning the output of the LLM token by token, instead of all at once.\n",
    "\n",
    "This notebook demonstrates how to monitor a _LangChain_ streaming app with TruLens.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/examples/expositional/frameworks/langchain/langchain_stream.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import from LangChain and TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install trulens trulens.apps.langchain trulens-providers-huggingface 'langchain>=0.2.16' 'langchain-openai>=0.0.1rc0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from trulens.core import Feedback, TruSession\n",
    "from trulens.providers.huggingface import Huggingface\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### Add API keys\n",
    "For this example you will need Huggingface and OpenAI keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# import os\n",
    "# os.environ[\"HUGGINGFACE_API_KEY\"] = \"hf_...\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Async Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatllm = ChatOpenAI(\n",
    "    temperature=0.0,\n",
    "    streaming=True,  # important\n",
    ")\n",
    "llm = OpenAI(\n",
    "    temperature=0.0,\n",
    ")\n",
    "memory = ChatMessageHistory()\n",
    "\n",
    "# Setup a simple question/answer chain with streaming ChatOpenAI.\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"human_input\", \"chat_history\"],\n",
    "    template=\"\"\"\n",
    "    You are having a conversation with a person. Make small talk.\n",
    "    {chat_history}\n",
    "        Human: {human_input}\n",
    "        AI:\"\"\",\n",
    ")\n",
    "\n",
    "chain = RunnableWithMessageHistory(\n",
    "    prompt | chatllm,\n",
    "    lambda: memory, \n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a language match feedback function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = TruSession()\n",
    "session.reset_database()\n",
    "hugs = Huggingface()\n",
    "f_lang_match = Feedback(hugs.language_match).on_input_output()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up evaluation and tracking with TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to also get filled-in prompt templates in timeline:\n",
    "from trulens.core.instruments import instrument\n",
    "from trulens.apps.langchain import TruChain\n",
    "\n",
    "instrument.method(PromptTemplate, \"format\")\n",
    "\n",
    "tc = TruChain(chain, feedbacks=[f_lang_match], app_name=\"chat_with_memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.print_instrumented()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the TruLens dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "run_dashboard(session)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"Hi. How are you?\"\n",
    "\n",
    "async with tc as recording:\n",
    "    stream = chain.astream(\n",
    "        input=dict(human_input=message, chat_history=[]),\n",
    "    )\n",
    "\n",
    "    async for chunk in stream:\n",
    "        print(chunk.content, end=\"\")\n",
    "\n",
    "record = recording.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main output is a concatenation of chunk contents:\n",
    "\n",
    "record.main_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Costs may not include all costs fields but should include the number of chunks\n",
    "# received.\n",
    "\n",
    "record.cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedback is only evaluated once the chunks are all received.\n",
    "\n",
    "record.feedback_results[0].result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "7d153714b979d5e6d08dd8ec90712dd93bff2c9b6c1f0c118169738af3430cd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
