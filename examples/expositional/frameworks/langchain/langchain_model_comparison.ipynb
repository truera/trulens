{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Comparison\n",
    "\n",
    "When building an LLM application we have hundreds of different models to choose from, all with different costs/latency and performance characteristics. Importantly, performance of LLMs can be heterogeneous across different use cases. Rather than relying on standard benchmarks or leaderboard performance, we want to evaluate an LLM for the use case we need.\n",
    "\n",
    "Doing this sort of comparison is a core use case of TruLens. In this example, we'll walk through how to build a simple langchain app and evaluate across 3 different models: small flan, large flan and text-turbo-3.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/examples/expositional/frameworks/langchain/langchain_model_comparison.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install trulens trulens-providers-huggingface trulens-providers-openai langchain langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Imports main tools:\n",
    "from trulens.core import Feedback\n",
    "from trulens.core import TruSession\n",
    "from trulens.apps.langchain import TruChain\n",
    "from trulens.providers.huggingface import Huggingface\n",
    "from trulens.providers.openai import OpenAI as fOpenAI\n",
    "\n",
    "session = TruSession()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set API Keys\n",
    "\n",
    "For this example, we need API keys for the Huggingface, HuggingFaceHub, and OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACE_API_KEY\"] = \"...\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up feedback functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API endpoints for models used in feedback functions:\n",
    "hugs = Huggingface()\n",
    "openai = fOpenAI()\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = Feedback(openai.relevance).on_input_output()\n",
    "# By default this will evaluate feedback on main app input and main app output.\n",
    "\n",
    "all_feedbacks = [f_qa_relevance]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a couple sizes of Flan and ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=os.environ['HUGGINGFACE_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "# initialize the models\n",
    "hub_llm_smallflan = HuggingFaceEndpoint(\n",
    "    model=\"google/flan-t5-small\", \n",
    "    temperature=1e-10, \n",
    "    max_new_tokens=250\n",
    ")\n",
    "\n",
    "gemma_2b = HuggingFaceEndpoint(\n",
    "    model=\"google/gemma-2-2b\", \n",
    "    temperature=1e-10\n",
    ")\n",
    "\n",
    "gpt = OpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# create prompt template > LLM chain\n",
    "smallflan_chain = LLMChain(prompt=prompt, llm=hub_llm_smallflan)\n",
    "\n",
    "gemma_2b_chain = LLMChain(prompt=prompt, llm=gemma_2b)\n",
    "\n",
    "openai_gpt_chain = LLMChain(prompt=prompt, llm=gpt)\n",
    "\n",
    "# Trulens instrumentation.\n",
    "smallflan_app_recorder = TruChain(\n",
    "    app_name=\"langchain_model_comparison\", \n",
    "    app_version=\"small_flan\", \n",
    "    app=smallflan_chain, \n",
    "    feedbacks=all_feedbacks\n",
    ")\n",
    "\n",
    "gemma_2b_app_recorder = TruChain(\n",
    "    app_name=\"langchain_model_comparison\", \n",
    "    app_version=\"gemma-2b\", \n",
    "    app=gemma_2b_chain, \n",
    "    feedbacks=all_feedbacks\n",
    ")\n",
    "\n",
    "openai_gpt_app_recorder = TruChain(\n",
    "    app_name=\"langchain_model_comparison\", \n",
    "    app_version=\"GPT-4o-mini\", \n",
    "    app=openai_gpt_chain, \n",
    "    feedbacks=all_feedbacks\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the application with all 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Who won the superbowl in 2010?\",\n",
    "    \"What is the capital of Thailand?\",\n",
    "    \"Who developed the theory of evolution by natural selection?\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    with smallflan_app_recorder as recording:\n",
    "        smallflan_chain(prompt)\n",
    "    with gemma_2b_app_recorder as recording:\n",
    "        gemma_2b_chain(prompt)\n",
    "    with openai_gpt_app_recorder as recording:\n",
    "        openai_gpt_chain(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the TruLens dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "run_dashboard(session)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pkg_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
