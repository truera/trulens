{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39fd1948-b5c3-48c4-b10e-2ae7e8c83334",
   "metadata": {},
   "source": [
    "# Multi-agent Data Tasks\n",
    "\n",
    "A single agent can usually operate effectively using a handful of tools within a single domain, but even using powerful models like `gpt-4`, it can be less effective at using many tools. \n",
    "\n",
    "This notebook is an extension of the multi-agent-collaboration notebook, showing how access to more tools - particularly with private data - can enhance the ability of a data agent.\n",
    "\n",
    "We will build up the agent with more tools, starting with web search, then adding Cortex Agent that can both document search and query snowflake tables in sql via Cortex Analyst.\n",
    "\n",
    "Prerequisites: you must create your own Cortex Search Service and Cortex Analyst semantic model to be used in the Cortex Agents REST API call.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/examples/expositional/frameworks/langchain/langgraph-multi-agent-snowflake-tools.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7b6dcc-c985-46e2-8457-7e6b0298b950",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "# pip install -U langchain_community langchain_openai langchain_experimental matplotlib langgraph pygraphviz google-search-results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f989825f",
   "metadata": {},
   "source": [
    "## Choose an app name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f90afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"Finance Data Agent\"  # set this app name for your use case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e94631",
   "metadata": {},
   "source": [
    "## Set the resources for Cortex Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c0e4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEMANTIC_MODEL_FILE = \"@agents_db.notebooks.semantic_models/sec_filings.yaml\"\n",
    "CORTEX_SEARCH_SERVICE = \"CORTEX_SEARCH_TUTORIAL_DB.PUBLIC.FOMC_SEARCH_SERVICE\"\n",
    "ACCOUNT_URL = \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e48d5f",
   "metadata": {},
   "source": [
    "## Set keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bdc791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\"  # llm used by LangGraph\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"...\"  # web search\n",
    "\n",
    "# ai observablity\n",
    "os.environ[\"SNOWFLAKE_ACCOUNT\"] = \"...\"\n",
    "os.environ[\"SNOWFLAKE_USER\"] = \"...\"\n",
    "os.environ[\"SNOWFLAKE_USER_PASSWORD\"] = \"...\"\n",
    "os.environ[\"SNOWFLAKE_DATABASE\"] = \"AGENTS_DB\"\n",
    "os.environ[\"SNOWFLAKE_SCHEMA\"] = \"NOTEBOOKS\"\n",
    "os.environ[\"SNOWFLAKE_ROLE\"] = \"CORTEX_USER_ROLE\"\n",
    "os.environ[\"SNOWFLAKE_WAREHOUSE\"] = \"CONTAINER_RUNTIME_WH\"\n",
    "\n",
    "os.environ[\"SNOWFLAKE_PAT\"] = \"...\"  # cortex agent call\n",
    "\n",
    "os.environ[\"TRULENS_OTEL_TRACING\"] = (\n",
    "    \"1\"  # to enable OTEL tracing -> note the Snowsight UI experience for now is limited to PuPr customers, not yet supported for OSS.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58f2f08",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e874860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from typing import List, Literal\n",
    "import uuid\n",
    "\n",
    "from langchain.load.dump import dumps\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import StructuredTool\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langgraph.graph import END\n",
    "from langgraph.graph import START\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.types import Command\n",
    "from pydantic import BaseModel\n",
    "from snowflake.snowpark import Session\n",
    "from trulens.apps.app import TruApp\n",
    "from trulens.connectors.snowflake import SnowflakeConnector\n",
    "from trulens.core.otel.instrument import instrument\n",
    "from trulens.core.run import Run\n",
    "from trulens.core.run import RunConfig\n",
    "from trulens.otel.semconv.trace import BASE_SCOPE\n",
    "from trulens.otel.semconv.trace import SpanAttributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bb1f53",
   "metadata": {},
   "source": [
    "## Create TruLens/Snowflake Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3c39f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowflake account for trulens\n",
    "snowflake_connection_parameters = {\n",
    "    \"account\": os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "    \"user\": os.environ[\"SNOWFLAKE_USER\"],\n",
    "    \"password\": os.environ[\"SNOWFLAKE_USER_PASSWORD\"],\n",
    "    \"database\": os.environ[\"SNOWFLAKE_DATABASE\"],\n",
    "    \"schema\": os.environ[\"SNOWFLAKE_SCHEMA\"],\n",
    "    \"role\": os.environ[\"SNOWFLAKE_ROLE\"],\n",
    "    \"warehouse\": os.environ[\"SNOWFLAKE_WAREHOUSE\"],\n",
    "}\n",
    "snowpark_session_trulens = Session.builder.configs(\n",
    "    snowflake_connection_parameters\n",
    ").create()\n",
    "\n",
    "\n",
    "trulens_sf_connector = SnowflakeConnector(\n",
    "    snowpark_session=snowpark_session_trulens\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a283e-ea04-40c1-b792-f9e5f7d81203",
   "metadata": {},
   "source": [
    "### Define the agent with web search and charting tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd221f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment message state to also track selected tools and the chart path\n",
    "class ToolState(MessagesState):\n",
    "    selected_tools: List[str]\n",
    "    chart_path: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cab43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph():\n",
    "    def make_system_prompt(suffix: str) -> str:\n",
    "        return (\n",
    "            \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "            \" Use the provided tools to progress towards answering the question.\"\n",
    "            \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "            \" will help where you left off. Execute what you can to make progress.\"\n",
    "            \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "            \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "            f\"\\n{suffix}\"\n",
    "        )\n",
    "\n",
    "    search = SerpAPIWrapper()\n",
    "\n",
    "    search_tool = Tool(\n",
    "        name=\"web_search\",\n",
    "        description=\"Search the web for current information, such as weather or news\",\n",
    "        func=search.run,\n",
    "    )\n",
    "\n",
    "    tool_registry = {\n",
    "        str(uuid.uuid4()): search_tool,\n",
    "    }\n",
    "\n",
    "    # Update your tool documents indexing accordingly\n",
    "    tool_documents = [\n",
    "        Document(\n",
    "            page_content=f\"{tool.name}\\n\\n{tool.description}\",\n",
    "            id=tool_id,\n",
    "            metadata={\n",
    "                \"tool_name\": tool.name,\n",
    "                \"tool_description\": tool.description,\n",
    "            },\n",
    "        )\n",
    "        for tool_id, tool in tool_registry.items()\n",
    "    ]\n",
    "\n",
    "    vector_store = InMemoryVectorStore(embedding=OpenAIEmbeddings())\n",
    "    vector_store.add_documents(tool_documents)\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"SELECT_TOOLS\",\n",
    "        attributes=lambda ret, exc, *args, **kw: {\n",
    "            # ---- state as JSON-text (OTLP needs a scalar) -----------------\n",
    "            f\"{BASE_SCOPE}.select_tools_input_state\": json.dumps(  # ← turns dict → str\n",
    "                {\n",
    "                    **{k: v for k, v in args[0].items() if k != \"messages\"},\n",
    "                    \"messages\": [\n",
    "                        {\"type\": m.__class__.__name__, \"content\": m.content}\n",
    "                        if hasattr(m, \"content\")  # BaseMessage subclasses\n",
    "                        else m  # already JSON-friendly\n",
    "                        for m in args[0].get(\"messages\", [])\n",
    "                    ],\n",
    "                }\n",
    "            ),\n",
    "            # ---- selected tool IDs as a simple comma-separated string -----\n",
    "            f\"{BASE_SCOPE}.selected_tool_ids\": \", \".join(\n",
    "                ret.update.get(\"selected_tools\", [])\n",
    "            )\n",
    "            if \"selected_tools\" in ret.update\n",
    "            else \"\",\n",
    "            f\"{BASE_SCOPE}.selected_tool_names\": \", \".join(\n",
    "                tool_registry[tool_id].name\n",
    "                for tool_id in ret.update.get(\"selected_tools\", [])\n",
    "            )\n",
    "            if \"selected_tools\" in ret.update\n",
    "            else \"\",\n",
    "        },\n",
    "    )\n",
    "    def select_tools(\n",
    "        state: ToolState,\n",
    "    ) -> Command[Literal[\"research_agent\", END]]:\n",
    "        query = state[\"messages\"][-1].content\n",
    "        # 1) Do a full similarity search over all tools\n",
    "        results = vector_store.similarity_search_with_score(\n",
    "            query,\n",
    "            k=len(tool_documents),\n",
    "        )\n",
    "\n",
    "        # 3) Pick the single best match\n",
    "        best_doc, best_score = max(results, key=lambda x: x[1])\n",
    "\n",
    "        # 4) If it’s truly too low, bail out (optional)\n",
    "        MIN_SCORE = 0.5\n",
    "        if best_score < MIN_SCORE:\n",
    "            return Command(\n",
    "                update={\n",
    "                    \"messages\": state[\"messages\"]\n",
    "                    + [\n",
    "                        HumanMessage(\n",
    "                            content=\"Sorry, I don’t have a tool that’s relevant enough to answer that.\",\n",
    "                            name=\"assistant\",\n",
    "                        )\n",
    "                    ]\n",
    "                },\n",
    "                goto=END,\n",
    "            )\n",
    "\n",
    "        # 5) Otherwise select that one\n",
    "        return Command(\n",
    "            update={\"selected_tools\": [best_doc.id]},\n",
    "            goto=\"research_agent\",\n",
    "        )\n",
    "\n",
    "    def get_next_node(last_message: BaseMessage, goto: str):\n",
    "        if \"FINAL ANSWER\" in last_message.content:\n",
    "            # Any agent decided the work is done\n",
    "            return END\n",
    "        return goto\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"RESEARCH_NODE\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            f\"{BASE_SCOPE}.research_node_input_content\": args[0][\"messages\"][\n",
    "                -1\n",
    "            ].content,\n",
    "            f\"{BASE_SCOPE}.research_node_selected_tool_names\": (\n",
    "                \", \".join(\n",
    "                    tool_registry.get(tool_id, \"\").name\n",
    "                    for tool_id in args[0].get(\"selected_tools\", [])\n",
    "                )\n",
    "                if \"selected_tools\" in args[0]\n",
    "                and len(args[0][\"selected_tools\"]) > 0\n",
    "                else \"No tools selected\"\n",
    "            ),\n",
    "            f\"{BASE_SCOPE}.planned_tool_call_names\": (\n",
    "                [\n",
    "                    call.get(\"function\", {}).get(\"name\", \"\")\n",
    "                    for call in (\n",
    "                        # if ret is a tuple, msg[1] is the AIMessage\n",
    "                        ret[0]\n",
    "                        .update[\"messages\"][1]\n",
    "                        .additional_kwargs.get(\"tool_calls\", [])\n",
    "                        if isinstance(ret, tuple)\n",
    "                        # otherwise it's ret.update[\"messages\"][1]\n",
    "                        else ret.update[\"messages\"][1].additional_kwargs.get(\n",
    "                            \"tool_calls\", []\n",
    "                        )\n",
    "                    )\n",
    "                ]\n",
    "                if (isinstance(ret, tuple) or hasattr(ret, \"update\"))\n",
    "                else []\n",
    "            ),\n",
    "            f\"{BASE_SCOPE}.planned_tool_call_args\": (\n",
    "                [\n",
    "                    call.get(\"function\", {}).get(\"arguments\", \"\")\n",
    "                    for call in (\n",
    "                        ret[0]\n",
    "                        .update[\"messages\"][1]\n",
    "                        .additional_kwargs.get(\"tool_calls\", [])\n",
    "                        if isinstance(ret, tuple)\n",
    "                        else ret.update[\"messages\"][1].additional_kwargs.get(\n",
    "                            \"tool_calls\", []\n",
    "                        )\n",
    "                    )\n",
    "                ]\n",
    "                if (isinstance(ret, tuple) or hasattr(ret, \"update\"))\n",
    "                else []\n",
    "            ),\n",
    "            f\"{BASE_SCOPE}.agent_response\": ret.update[\"messages\"][-1].content\n",
    "            if hasattr(ret, \"update\")\n",
    "            else json.dumps(ret, indent=4, sort_keys=True),\n",
    "            f\"{BASE_SCOPE}.web_search_results\": [\n",
    "                json.loads(dumps(message)).get(\"kwargs\", {}).get(\"content\", \"\")\n",
    "                for message in ret.update[\"messages\"]\n",
    "                if isinstance(message, ToolMessage)\n",
    "                and message.name == \"web_search\"\n",
    "            ]\n",
    "            if hasattr(ret, \"update\")\n",
    "            else \"No tool call\",\n",
    "        },\n",
    "    )\n",
    "    @instrument(\n",
    "        span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            SpanAttributes.RETRIEVAL.QUERY_TEXT: args[0][\"messages\"][\n",
    "                -1\n",
    "            ].content,\n",
    "            SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS: [\n",
    "                json.loads(dumps(message)).get(\"kwargs\", {}).get(\"content\", \"\")\n",
    "                for message in ret.update[\"messages\"]\n",
    "                if isinstance(message, ToolMessage)\n",
    "                and message.name == \"web_search\"\n",
    "            ]\n",
    "            if hasattr(ret, \"update\")\n",
    "            else \"No tool call\",\n",
    "        },\n",
    "    )\n",
    "    def research_agent_node(\n",
    "        state: ToolState,\n",
    "    ) -> Command[Literal[\"chart_generator\"]]:\n",
    "        \"\"\"\n",
    "        Always binds the selected tools and invokes the bound agent.\n",
    "        Stops on FINAL ANSWER or moves to chart_generator.\n",
    "        \"\"\"\n",
    "        # grab (non-empty) list of selected tool IDs\n",
    "        selected_ids = state[\"selected_tools\"]\n",
    "\n",
    "        # bind only those tools\n",
    "        selected_tools = [tool_registry[tid] for tid in selected_ids]\n",
    "        bound_llm = llm.bind_tools(selected_tools)\n",
    "        bound_agent = create_react_agent(\n",
    "            bound_llm,\n",
    "            tools=selected_tools,  # already bound\n",
    "            prompt=make_system_prompt(\n",
    "                \"You can only do research. You are working with both a chart generator and a chart summarizer colleagues.\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # run it\n",
    "        result = bound_agent.invoke(state)\n",
    "\n",
    "        # decide if we’re done\n",
    "        last = result[\"messages\"][-1]\n",
    "        goto = get_next_node(last, \"chart_generator\")\n",
    "\n",
    "        # tag the origin of the final message\n",
    "        result[\"messages\"][-1] = HumanMessage(\n",
    "            content=last.content,\n",
    "            name=\"research_agent\",\n",
    "        )\n",
    "\n",
    "        return Command(\n",
    "            update={\"messages\": result[\"messages\"]},\n",
    "            goto=goto,\n",
    "        )\n",
    "\n",
    "        # Warning: This executes code locally, which can be unsafe when not sandboxed\n",
    "\n",
    "    repl = PythonREPL()\n",
    "\n",
    "    @tool\n",
    "    @instrument(\n",
    "        span_type=\"PYTHON_REPL_TOOL\",\n",
    "        attributes={\n",
    "            f\"{BASE_SCOPE}.python_tool_input_code\": \"code\",\n",
    "        },\n",
    "    )\n",
    "    def python_repl_tool(code: str):\n",
    "        \"\"\"\n",
    "        Run arbitrary Python, grab the CURRENT matplotlib figure (if any),\n",
    "        save it to ./langgraph_saved_images_snowflaketools/v1/chart_<uuid>.png,\n",
    "        and return a first-line `CHART_PATH=…`.\n",
    "        \"\"\"\n",
    "        import matplotlib\n",
    "\n",
    "        matplotlib.use(\"Agg\")  # headless safety\n",
    "        import os\n",
    "        import uuid\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        # ------------------ run user code & capture stdout ------------------\n",
    "        repl.run(code)\n",
    "\n",
    "        # ------------------ locate a figure (if generated) ------------------\n",
    "        fig = plt.gcf()\n",
    "        has_axes = bool(fig.axes)  # True if something was plotted\n",
    "\n",
    "        # ------------------ always save if we have a figure -----------------\n",
    "        chart_path = \"\"\n",
    "        if has_axes:\n",
    "            target_dir = \"./langgraph_saved_images_snowflaketools/v1\"\n",
    "            os.makedirs(target_dir, exist_ok=True)\n",
    "            chart_path = os.path.join(\n",
    "                target_dir, f\"chart_{uuid.uuid4().hex}.png\"\n",
    "            )\n",
    "            fig.savefig(chart_path, format=\"png\")\n",
    "            plt.close(fig)\n",
    "\n",
    "        # ------------------ tool result (1st line = CHART_PATH) -------------\n",
    "        return f\"CHART_PATH={chart_path if chart_path else 'NONE'}\\n\"\n",
    "\n",
    "    # Chart generator agent and node\n",
    "    # NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION, WHICH CAN BE UNSAFE WHEN NOT SANDBOXED\n",
    "    # 1) Define the chart‐agent: it only returns JSON with a \"code\" field\n",
    "    chart_agent = create_react_agent(\n",
    "        llm,\n",
    "        [python_repl_tool],\n",
    "        prompt=make_system_prompt(\n",
    "            \"\"\"You can only generate charts by returning a single JSON object, for example:\n",
    "        {\n",
    "        \"code\": \"<your python plotting code here>\"\n",
    "        }\n",
    "        —where <your python plotting code> uses matplotlib to create exactly one figure.\n",
    "        The plot should always include axis titles and relevant labels at the minimum.\n",
    "        Do NOT include any prose or tool‐call wrappers.\"\"\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    def extract_chart_path(text: str) -> str | None:\n",
    "        \"\"\"\n",
    "        Scan every line of tool stdout for 'CHART_PATH=' and return\n",
    "        whatever follows, trimmed.  Returns None if no such line exists.\n",
    "        \"\"\"\n",
    "        for line in text.splitlines():\n",
    "            if \"CHART_PATH=\" in line:\n",
    "                # split on the first '=', strip whitespace\n",
    "                return line.split(\"CHART_PATH=\", 1)[1].strip()\n",
    "        return None\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"CHART_GENERATOR_NODE\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            f\"{BASE_SCOPE}.chart_node_input\": args[0][\"messages\"][-1].content,\n",
    "            f\"{BASE_SCOPE}.chart_node_response\": (\n",
    "                ret.update[\"messages\"][-1].content\n",
    "                if ret and hasattr(ret, \"update\") and ret.update\n",
    "                else \"No update response\"\n",
    "            ),\n",
    "        },\n",
    "    )\n",
    "    def chart_node(state: ToolState) -> Command[Literal[\"chart_summarizer\"]]:\n",
    "        # 0) If a path is already in state, skip\n",
    "        # extract the current human query\n",
    "        current_query = state[\"messages\"][-1].content\n",
    "\n",
    "        # if we already generated a chart for _this_ query, skip\n",
    "        if state.get(\"last_query\") == current_query and state.get(\"chart_path\"):\n",
    "            return Command(\n",
    "                update={\"messages\": state[\"messages\"]}, goto=\"chart_summarizer\"\n",
    "            )\n",
    "\n",
    "        # it's a new query (or first run) → clear any old chart_path and remember this query\n",
    "        state.pop(\"chart_path\", None)\n",
    "        state[\"last_query\"] = current_query\n",
    "\n",
    "        # 1) Remember how many messages we had\n",
    "        len_before = len(state[\"messages\"])\n",
    "\n",
    "        # 2) Run the agent exactly once\n",
    "        agent_out = chart_agent.invoke(state)\n",
    "\n",
    "        all_msgs = agent_out[\"messages\"]\n",
    "\n",
    "        # 3) Look at only the brand-new messages for our chart tool output\n",
    "        new_segment = all_msgs[len_before:]\n",
    "        tool_msgs = [\n",
    "            m\n",
    "            for m in new_segment\n",
    "            if isinstance(m, ToolMessage) and \"CHART_PATH=\" in m.content\n",
    "        ]\n",
    "\n",
    "        if not tool_msgs:\n",
    "            return Command(\n",
    "                update={\"messages\": state[\"messages\"]},\n",
    "                goto=\"research_agent\",\n",
    "            )\n",
    "\n",
    "        # 4) Parse the last one in case there are multiples\n",
    "        tool_msg = tool_msgs[-1]\n",
    "        tool_stdout = tool_msg.content\n",
    "\n",
    "        chart_path = extract_chart_path(tool_stdout)\n",
    "        # 5) Build your new messages list: include only that new ToolMessage\n",
    "        new_msgs = state[\"messages\"][:] + [tool_msg]\n",
    "\n",
    "        # 6) Success! stash path into state and append the CHART_PATH marker\n",
    "        new_msgs.append(\n",
    "            HumanMessage(\n",
    "                content=f\"CHART_PATH={chart_path}\", name=\"chart_generator\"\n",
    "            )\n",
    "        )\n",
    "        return Command(\n",
    "            update={\"messages\": new_msgs, \"chart_path\": chart_path},\n",
    "            goto=\"chart_summarizer\",\n",
    "        )\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"CHART_SUMMARY_NODE\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            # grab the state dict (kwarg wins, else first arg)\n",
    "            f\"{BASE_SCOPE}.summary_node_input\": (\n",
    "                (kwargs.get(\"state\") or args[0])[\"messages\"][-1].content\n",
    "            ),\n",
    "            f\"{BASE_SCOPE}.summary_node_output\": (\n",
    "                ret.update[\"messages\"][-1].content\n",
    "                if hasattr(ret, \"update\")\n",
    "                else \"NO SUMMARY GENERATED\"\n",
    "            ),\n",
    "        },\n",
    "    )\n",
    "    def chart_summary_node(state: ToolState) -> Command:\n",
    "        # 1) find the chart_path in state\n",
    "        chart_path = state.get(\"chart_path\", \"\")\n",
    "        if not chart_path:\n",
    "            return Command(\n",
    "                update={\n",
    "                    \"messages\": state[\"messages\"]\n",
    "                    + [\n",
    "                        HumanMessage(\n",
    "                            \"No valid chart was generated. Please try again.\",\n",
    "                            name=\"chart_summarizer\",\n",
    "                        )\n",
    "                    ]\n",
    "                },\n",
    "                goto=\"select_tools\",\n",
    "            )\n",
    "\n",
    "        # 2) strip *everything* except human utterances\n",
    "        human_history = [\n",
    "            m for m in state[\"messages\"] if isinstance(m, HumanMessage)\n",
    "        ]\n",
    "\n",
    "        # ensure our CHART_PATH marker is last\n",
    "        if not human_history or not human_history[-1].content.startswith(\n",
    "            \"CHART_PATH=\"\n",
    "        ):\n",
    "            human_history.append(\n",
    "                HumanMessage(\n",
    "                    f\"CHART_PATH={chart_path}\", name=\"chart_summarizer\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # 3) build your ChatCompletion prompt\n",
    "        system = SystemMessage(\n",
    "            content=make_system_prompt(\n",
    "                \"You are an AI assistant whose *only* job is to describe a chart image. \"\n",
    "                \"Input is a message CHART_PATH=… pointing at a saved PNG. \"\n",
    "                \"Include complete details and specifics of the chart image.\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        messages_for_llm = (\n",
    "            [system]\n",
    "            + human_history\n",
    "            + [HumanMessage(\"Please describe the above chart.\")]\n",
    "        )\n",
    "\n",
    "        # 4) call the LLM directly—no tools, no React agent\n",
    "        ai_msg: AIMessage = llm(messages_for_llm)\n",
    "        summary = ai_msg.content\n",
    "\n",
    "        return Command(\n",
    "            update={\n",
    "                \"messages\": state[\"messages\"]\n",
    "                + [\n",
    "                    HumanMessage(summary, name=\"chart_summarizer\"),\n",
    "                ]\n",
    "            },\n",
    "            goto=\"reflection\",\n",
    "        )\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"CHART_SUMMARY_REFLECTION\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            f\"{BASE_SCOPE}.chart_summary_reflection_input_user_query\": (\n",
    "                (kwargs.get(\"state\") or args[0])[\"messages\"][0].content\n",
    "            ),\n",
    "            f\"{BASE_SCOPE}.chart_summary_reflection_input_chart_summary\": (\n",
    "                (kwargs.get(\"state\") or args[0])[\"messages\"][-1].content\n",
    "            ),\n",
    "            # extract the summary string rather than returning the Command object\n",
    "            f\"{BASE_SCOPE}.chart_summary_reflection_response\": (\n",
    "                ret.update[\"messages\"][-1].content\n",
    "                if hasattr(ret, \"update\")\n",
    "                else \"\"\n",
    "            ),\n",
    "        },\n",
    "    )\n",
    "    def reflection_node(state: ToolState) -> Command:\n",
    "        \"\"\"\n",
    "        This function uses an LLM to reflect on the quality of a chart summary\n",
    "        and determine if the task is complete or requires further refinement.\n",
    "        \"\"\"\n",
    "\n",
    "        reflection_prompt_template = PromptTemplate(\n",
    "            input_variables=[\"user_query\", \"chart_summary\"],\n",
    "            template=\"\"\"\\\n",
    "        You are an AI assistant tasked with reflecting on the quality of a chart summary. The user has asked the following question:\n",
    "        \"{user_query}\"\n",
    "\n",
    "        You are given the following chart summary:\n",
    "        \"{chart_summary}\"\n",
    "\n",
    "        Your task is to evaluate how well the chart summary answers the user's question. Consider the following:\n",
    "        - Does it describe a chart that will be relevant for answering the user's query?\n",
    "\n",
    "        If the summary **generally** addresses the question, respond with 'Task complete'. If the summary **lacks significant** details or clarity, then respond with specific details on how the answer should be improved and what information is needed. Avoid being overly critical unless the summary completely misses key elements necessary to answer the query.\n",
    "\n",
    "        Please provide your answer in a **concise and encouraging** manner.\n",
    "        \"\"\",\n",
    "        )\n",
    "\n",
    "        # Create the chain using the prompt template and the LLM (ChatOpenAI)\n",
    "        reflection_chain = reflection_prompt_template | llm\n",
    "\n",
    "        user_query = state[\"messages\"][0].content\n",
    "        chart_summary = state[\"messages\"][-1].content\n",
    "        # Call the chain with the user query and chart summary\n",
    "        reflection_result = reflection_chain.invoke({\n",
    "            \"user_query\": user_query,\n",
    "            \"chart_summary\": chart_summary,\n",
    "        })\n",
    "        if \"Task complete\" in reflection_result.content:\n",
    "            return Command(\n",
    "                update={\n",
    "                    \"messages\": state[\"messages\"]\n",
    "                    + [\n",
    "                        HumanMessage(\n",
    "                            reflection_result.content, name=\"reflection\"\n",
    "                        )\n",
    "                    ]\n",
    "                    + [\n",
    "                        HumanMessage(\n",
    "                            f\"Chart saved at {state['chart_path']}. \\n The chart summary is: \\n {chart_summary}\",\n",
    "                            name=\"approved chart summary\",\n",
    "                        )\n",
    "                    ]\n",
    "                },\n",
    "                goto=END,\n",
    "            )\n",
    "        else:\n",
    "            return Command(\n",
    "                update={\n",
    "                    \"messages\": state[\"messages\"]\n",
    "                    + [\n",
    "                        HumanMessage(\n",
    "                            reflection_result.content, name=\"reflection\"\n",
    "                        )\n",
    "                    ]\n",
    "                },\n",
    "                goto=\"select_tools\",\n",
    "            )\n",
    "\n",
    "    workflow = StateGraph(ToolState)\n",
    "    workflow.add_node(\"select_tools\", select_tools)\n",
    "    workflow.add_node(\"research_agent\", research_agent_node)\n",
    "    workflow.add_node(\"chart_generator\", chart_node)\n",
    "    workflow.add_node(\"chart_summarizer\", chart_summary_node)\n",
    "    workflow.add_node(\"reflection\", reflection_node)\n",
    "\n",
    "    workflow.add_edge(START, \"select_tools\")\n",
    "    workflow.add_edge(\"select_tools\", \"research_agent\")\n",
    "    workflow.add_edge(\"research_agent\", \"chart_generator\")\n",
    "    workflow.add_edge(\"chart_generator\", \"chart_summarizer\")\n",
    "    workflow.add_edge(\"chart_summarizer\", \"reflection\")\n",
    "    workflow.add_edge(\"reflection\", END)\n",
    "\n",
    "    compiled_graph = workflow.compile()\n",
    "\n",
    "    return compiled_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca89763c",
   "metadata": {},
   "source": [
    "## Register the agent and create a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58ffc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TruAgent:\n",
    "    def __init__(self):\n",
    "        self.graph = build_graph()\n",
    "\n",
    "    @instrument(\n",
    "        span_type=SpanAttributes.SpanType.RECORD_ROOT,\n",
    "        attributes={\n",
    "            SpanAttributes.RECORD_ROOT.INPUT: \"query\",\n",
    "            SpanAttributes.RECORD_ROOT.OUTPUT: \"return\",\n",
    "        },\n",
    "    )\n",
    "    def invoke_agent_graph(self, query: str) -> str:\n",
    "        try:\n",
    "            # rebuild the graph for each query\n",
    "            self.graph = build_graph()\n",
    "            # Initialize state with proper message format\n",
    "            state = {\"messages\": [HumanMessage(content=query)]}\n",
    "\n",
    "            # Stream events with recursion limit\n",
    "            events = self.graph.stream(\n",
    "                state,\n",
    "                {\"recursion_limit\": 100},\n",
    "            )\n",
    "\n",
    "            # Track all messages through the conversation\n",
    "            all_messages = []\n",
    "            for event in events:\n",
    "                # Get the payload from the event\n",
    "                _, payload = next(iter(event.items()))\n",
    "                if not payload:  # Skip empty payloads\n",
    "                    continue\n",
    "\n",
    "                messages = payload.get(\"messages\")\n",
    "                if not messages:\n",
    "                    continue\n",
    "                all_messages.extend(messages)\n",
    "\n",
    "            # Return the last message's content if available\n",
    "            return (\n",
    "                all_messages[-1].content\n",
    "                if all_messages and hasattr(all_messages[-1], \"content\")\n",
    "                else \"\"\n",
    "            )\n",
    "        except Exception:\n",
    "            return \"I ran into an issue, and cannot answer your question.\"\n",
    "\n",
    "\n",
    "tru_agent = TruAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b361154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_agent_app = TruApp(\n",
    "    tru_agent,\n",
    "    app_name=APP_NAME,\n",
    "    app_version=\"web search\",\n",
    "    connector=trulens_sf_connector,\n",
    "    main_method=tru_agent.invoke_agent_graph,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe95b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_1 = datetime.datetime.fromtimestamp(time.time()).strftime(\n",
    "    \"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    run_name=\"Multi-agent demo run \" + st_1,\n",
    "    description=\"this is a run with access to web search and charting capabilities\",\n",
    "    dataset_name=\"Research test dataset\",\n",
    "    source_type=\"DATAFRAME\",\n",
    "    label=\"langgraph demo\",\n",
    "    dataset_spec={\n",
    "        \"RECORD_ROOT.INPUT\": \"query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "run: Run = tru_agent_app.add_run(run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad18dba0",
   "metadata": {},
   "source": [
    "## Start the run\n",
    "\n",
    "This runs the agent in batch using the queries in the `input_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c84b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "user_queries = [\n",
    "    \"Compare the summer high and low temperatures in London versus Atlanta. Create a simple bar chart showing the highs and lows side by side between the two cities.\",\n",
    "    \"Compare the 12-month PCE inflation rate to the interest rate paid on federal reserve balances. Create a simple bar chart showing both rates side by side with the unit as basis points rather than percentage points.\",\n",
    "    \"what were the top 10 funds in terms of holding values in their most recent 13F-HR filings? Create a bar chart to illustrate, and use billions as the unit.\",\n",
    "]\n",
    "\n",
    "user_queries_df = pd.DataFrame(user_queries, columns=[\"query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba1cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.start(input_df=user_queries_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1a8c8f",
   "metadata": {},
   "source": [
    "## Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3d1614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while run.get_status() == \"INVOCATION_IN_PROGRESS\":\n",
    "    time.sleep(3)\n",
    "\n",
    "run.compute_metrics([\"context_relevance\", \"answer_relevance\", \"groundedness\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac25d0ad",
   "metadata": {},
   "source": [
    "## Use Cortex Agent to gain access structured and unstructured data as a sub-agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8817dbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type\n",
    "\n",
    "import requests\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "\n",
    "class CortexAgentArgs(BaseModel):\n",
    "    query: str\n",
    "\n",
    "\n",
    "class CortexAgentTool(StructuredTool):\n",
    "    name: str = \"CortexAgent\"\n",
    "    description: str = \"answers questions using Fed minutes + SEC data\"\n",
    "\n",
    "    # ← annotate this override\n",
    "    args_schema: Type[CortexAgentArgs] = CortexAgentArgs\n",
    "\n",
    "    # now declare your extra fields, too:\n",
    "    session: Session\n",
    "    api_url: str\n",
    "    headers: dict\n",
    "\n",
    "    # allow extra attributes (optional if you declare all fields above)\n",
    "    model_config = {\"extra\": \"allow\"}\n",
    "\n",
    "    def __init__(self, session: Session, account_url: str):\n",
    "        # pass the declared fields into super()\n",
    "        super().__init__(\n",
    "            session=session,\n",
    "            api_url=f\"{account_url}/api/v2/cortex/agent:run\",\n",
    "            headers={},  # we'll populate it next\n",
    "        )\n",
    "        pat = os.getenv(\"SNOWFLAKE_PAT\")\n",
    "        if not pat:\n",
    "            raise RuntimeError(\"Set SNOWFLAKE_PAT\")\n",
    "        self.headers.update({\n",
    "            \"Authorization\": f\"Bearer {pat}\",\n",
    "            \"X-Snowflake-Authorization-Token-Type\": \"PROGRAMMATIC_ACCESS_TOKEN\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        })\n",
    "\n",
    "    def process_sse_response(self, resp):\n",
    "        \"\"\"\n",
    "        Process SSE stream lines, extracting any 'delta' payloads,\n",
    "        regardless of whether the JSON contains an 'event' field.\n",
    "        \"\"\"\n",
    "        text, sql, citations = \"\", \"\", []\n",
    "        for raw_line in resp.iter_lines(decode_unicode=True):\n",
    "            if not raw_line:\n",
    "                continue\n",
    "            raw_line = raw_line.strip()\n",
    "            # only handle data lines\n",
    "            if not raw_line.startswith(\"data:\"):\n",
    "                continue\n",
    "            payload = raw_line[len(\"data:\") :].strip()\n",
    "            if payload in (\"\", \"[DONE]\"):\n",
    "                continue\n",
    "            try:\n",
    "                evt = json.loads(payload)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            # Grab the 'delta' section, whether top-level or nested in 'data'\n",
    "            delta = evt.get(\"delta\") or evt.get(\"data\", {}).get(\"delta\")\n",
    "            if not isinstance(delta, dict):\n",
    "                continue\n",
    "            for item in delta.get(\"content\", []):\n",
    "                t = item.get(\"type\")\n",
    "                if t == \"text\":\n",
    "                    text += item.get(\"text\", \"\")\n",
    "                elif t == \"tool_results\":\n",
    "                    for result in item[\"tool_results\"].get(\"content\", []):\n",
    "                        if result.get(\"type\") == \"json\":\n",
    "                            j = result[\"json\"]\n",
    "                            text += j.get(\"text\", \"\")\n",
    "                            # capture SQL if present\n",
    "                            if \"sql\" in j:\n",
    "                                sql = j[\"sql\"]\n",
    "                            # capture any citations\n",
    "                            for s in j.get(\"searchResults\", []):\n",
    "                                citations.append({\n",
    "                                    \"source_id\": s.get(\"source_id\"),\n",
    "                                    \"doc_id\": s.get(\"doc_id\"),\n",
    "                                })\n",
    "        return text, sql, str(citations)\n",
    "\n",
    "    def run(self, query: str):\n",
    "        payload = {\n",
    "            \"model\": \"claude-3-5-sonnet\",\n",
    "            \"response_instruction\": \"You are a helpful AI assistant.\",\n",
    "            \"experimental\": {},\n",
    "            \"tools\": [\n",
    "                {\n",
    "                    \"tool_spec\": {\n",
    "                        \"type\": \"cortex_analyst_text_to_sql\",\n",
    "                        \"name\": \"Analyst1\",\n",
    "                    }\n",
    "                },\n",
    "                {\"tool_spec\": {\"type\": \"cortex_search\", \"name\": \"Search1\"}},\n",
    "                {\n",
    "                    \"tool_spec\": {\n",
    "                        \"type\": \"sql_exec\",\n",
    "                        \"name\": \"sql_execution_tool\",\n",
    "                    }\n",
    "                },\n",
    "            ],\n",
    "            \"tool_resources\": {\n",
    "                \"Analyst1\": {\"semantic_model_file\": SEMANTIC_MODEL_FILE},\n",
    "                \"Search1\": {\"name\": CORTEX_SEARCH_SERVICE},\n",
    "            },\n",
    "            \"tool_choice\": {\"type\": \"auto\"},\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": query}]}\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        resp = requests.post(\n",
    "            self.api_url, json=payload, headers=self.headers, stream=True\n",
    "        )\n",
    "\n",
    "        # parse SSE\n",
    "        text, sql, citations = self.process_sse_response(resp)\n",
    "\n",
    "        # execute SQL if returned\n",
    "        results = None\n",
    "        results_str = \"\"\n",
    "        if sql:\n",
    "            try:\n",
    "                results = self.session.sql(sql.replace(\";\", \"\")).collect()\n",
    "                results = pd.DataFrame(results)\n",
    "                results_str = results.to_string()\n",
    "            except Exception as e:\n",
    "                results_str = f\"SQL execution error: {e}\"\n",
    "        return text, citations, sql, results_str\n",
    "\n",
    "\n",
    "def build_graph_with_agent():\n",
    "    def make_system_prompt(suffix: str) -> str:\n",
    "        return (\n",
    "            \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "            \" Use the provided tools to progress towards answering the question.\"\n",
    "            \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "            \" will help where you left off. Execute what you can to make progress.\"\n",
    "            \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "            \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "            f\"\\n{suffix}\"\n",
    "        )\n",
    "\n",
    "    search = SerpAPIWrapper()\n",
    "\n",
    "    search_tool = Tool(\n",
    "        name=\"web_search\",\n",
    "        description=\"Search the web for current information, such as weather or news\",\n",
    "        func=search.run,\n",
    "    )\n",
    "\n",
    "    # Instantiate CortexAgentTool\n",
    "    account_url = ACCOUNT_URL\n",
    "    cortex_agent_tool = CortexAgentTool(\n",
    "        session=snowpark_session_trulens,\n",
    "        account_url=account_url,\n",
    "    )\n",
    "\n",
    "    wrapped_cortex_agent_tool = Tool(\n",
    "        name=cortex_agent_tool.name,\n",
    "        description=cortex_agent_tool.description,\n",
    "        func=cortex_agent_tool.run,\n",
    "        return_direct=False,  # set to True only if you want the agent to stop after using it\n",
    "    )\n",
    "\n",
    "    tool_registry = {\n",
    "        str(uuid.uuid4()): search_tool,\n",
    "        str(uuid.uuid4()): wrapped_cortex_agent_tool,\n",
    "    }\n",
    "\n",
    "    # Update your tool documents indexing accordingly\n",
    "    tool_documents = [\n",
    "        Document(\n",
    "            page_content=f\"{tool.name}\\n\\n{tool.description}\",\n",
    "            id=tool_id,\n",
    "            metadata={\n",
    "                \"tool_name\": tool.name,\n",
    "                \"tool_description\": tool.description,\n",
    "            },\n",
    "        )\n",
    "        for tool_id, tool in tool_registry.items()\n",
    "    ]\n",
    "\n",
    "    vector_store = InMemoryVectorStore(embedding=OpenAIEmbeddings())\n",
    "    vector_store.add_documents(tool_documents)\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"SELECT_TOOLS\",\n",
    "        attributes=lambda ret, exc, *args, **kw: {\n",
    "            # ---- state as JSON-text (OTLP needs a scalar) -----------------\n",
    "            f\"{BASE_SCOPE}.select_tools_input_state\": json.dumps(  # ← turns dict → str\n",
    "                {\n",
    "                    **{k: v for k, v in args[0].items() if k != \"messages\"},\n",
    "                    \"messages\": [\n",
    "                        {\"type\": m.__class__.__name__, \"content\": m.content}\n",
    "                        if hasattr(m, \"content\")  # BaseMessage subclasses\n",
    "                        else m  # already JSON-friendly\n",
    "                        for m in args[0].get(\"messages\", [])\n",
    "                    ],\n",
    "                }\n",
    "            ),\n",
    "            # ---- selected tool IDs as a simple comma-separated string -----\n",
    "            f\"{BASE_SCOPE}.selected_tool_ids\": \", \".join(\n",
    "                ret.update.get(\"selected_tools\", [])\n",
    "            )\n",
    "            if \"selected_tools\" in ret.update\n",
    "            else \"\",\n",
    "            f\"{BASE_SCOPE}.selected_tool_names\": \", \".join(\n",
    "                tool_registry[tool_id].name\n",
    "                for tool_id in ret.update.get(\"selected_tools\", [])\n",
    "            )\n",
    "            if \"selected_tools\" in ret.update\n",
    "            else \"\",\n",
    "        },\n",
    "    )\n",
    "    def select_tools(\n",
    "        state: ToolState,\n",
    "    ) -> Command[Literal[\"research_agent\", END]]:\n",
    "        query = state[\"messages\"][-1].content\n",
    "        # 1) Do a full similarity search over all tools\n",
    "        results = vector_store.similarity_search_with_score(\n",
    "            query,\n",
    "            k=len(tool_documents),\n",
    "        )\n",
    "\n",
    "        # 3) Pick the single best match\n",
    "        best_doc, best_score = max(results, key=lambda x: x[1])\n",
    "\n",
    "        # 4) If it’s truly too low, bail out (optional)\n",
    "        MIN_SCORE = 0.5\n",
    "        if best_score < MIN_SCORE:\n",
    "            return Command(\n",
    "                update={\n",
    "                    \"messages\": state[\"messages\"]\n",
    "                    + [\n",
    "                        HumanMessage(\n",
    "                            content=\"Sorry, I don’t have a tool that’s relevant enough to answer that.\",\n",
    "                            name=\"assistant\",\n",
    "                        )\n",
    "                    ]\n",
    "                },\n",
    "                goto=END,\n",
    "            )\n",
    "\n",
    "        # 5) Otherwise select that one\n",
    "        return Command(\n",
    "            update={\"selected_tools\": [best_doc.id]},\n",
    "            goto=\"research_agent\",\n",
    "        )\n",
    "\n",
    "    # Warning: This executes code locally, which can be unsafe when not sandboxed\n",
    "\n",
    "    repl = PythonREPL()\n",
    "\n",
    "    @tool\n",
    "    @instrument(\n",
    "        span_type=\"PYTHON_REPL_TOOL\",\n",
    "        attributes={\n",
    "            f\"{BASE_SCOPE}.python_tool_input_code\": \"code\",\n",
    "        },\n",
    "    )\n",
    "    def python_repl_tool(code: str):\n",
    "        \"\"\"\n",
    "        Run arbitrary Python, grab the CURRENT matplotlib figure (if any),\n",
    "        save it to ./langgraph_saved_images_snowflaketools/v2/chart_<uuid>.png,\n",
    "        and return a first-line `CHART_PATH=…`.\n",
    "        \"\"\"\n",
    "        import matplotlib\n",
    "\n",
    "        matplotlib.use(\"Agg\")  # headless safety\n",
    "        import os\n",
    "        import uuid\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        # ------------------ run user code & capture stdout ------------------\n",
    "        repl.run(code)\n",
    "\n",
    "        # ------------------ locate a figure (if generated) ------------------\n",
    "        fig = plt.gcf()\n",
    "        has_axes = bool(fig.axes)  # True if something was plotted\n",
    "\n",
    "        # ------------------ always save if we have a figure -----------------\n",
    "        chart_path = \"\"\n",
    "        if has_axes:\n",
    "            target_dir = \"./langgraph_saved_images_snowflaketools/v2\"\n",
    "            os.makedirs(target_dir, exist_ok=True)\n",
    "            chart_path = os.path.join(\n",
    "                target_dir, f\"chart_{uuid.uuid4().hex}.png\"\n",
    "            )\n",
    "            fig.savefig(chart_path, format=\"png\")\n",
    "            plt.close(fig)\n",
    "\n",
    "        # ------------------ tool result (1st line = CHART_PATH) -------------\n",
    "        return f\"CHART_PATH={chart_path if chart_path else 'NONE'}\\n\"\n",
    "\n",
    "    def get_next_node(last_message: BaseMessage, goto: str):\n",
    "        if \"FINAL ANSWER\" in last_message.content:\n",
    "            # Any agent decided the work is done\n",
    "            return END\n",
    "        return goto\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"RESEARCH_NODE\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            f\"{BASE_SCOPE}.research_node_input_content\": args[0][\"messages\"][\n",
    "                -1\n",
    "            ].content,\n",
    "            f\"{BASE_SCOPE}.research_node_selected_tool_names\": (\n",
    "                \", \".join(\n",
    "                    tool_registry.get(tool_id, \"\").name\n",
    "                    for tool_id in args[0].get(\"selected_tools\", [])\n",
    "                )\n",
    "                if \"selected_tools\" in args[0]\n",
    "                and len(args[0][\"selected_tools\"]) > 0\n",
    "                else \"No tools selected\"\n",
    "            ),\n",
    "            f\"{BASE_SCOPE}.planned_tool_call_names\": (\n",
    "                [\n",
    "                    call.get(\"function\", {}).get(\"name\", \"\")\n",
    "                    for call in (\n",
    "                        # if ret is a tuple, msg[1] is the AIMessage\n",
    "                        ret[0]\n",
    "                        .update[\"messages\"][1]\n",
    "                        .additional_kwargs.get(\"tool_calls\", [])\n",
    "                        if isinstance(ret, tuple)\n",
    "                        # otherwise it's ret.update[\"messages\"][1]\n",
    "                        else ret.update[\"messages\"][1].additional_kwargs.get(\n",
    "                            \"tool_calls\", []\n",
    "                        )\n",
    "                    )\n",
    "                ]\n",
    "                if (isinstance(ret, tuple) or hasattr(ret, \"update\"))\n",
    "                else []\n",
    "            ),\n",
    "            f\"{BASE_SCOPE}.planned_tool_call_args\": (\n",
    "                [\n",
    "                    call.get(\"function\", {}).get(\"arguments\", \"\")\n",
    "                    for call in (\n",
    "                        ret[0]\n",
    "                        .update[\"messages\"][1]\n",
    "                        .additional_kwargs.get(\"tool_calls\", [])\n",
    "                        if isinstance(ret, tuple)\n",
    "                        else ret.update[\"messages\"][1].additional_kwargs.get(\n",
    "                            \"tool_calls\", []\n",
    "                        )\n",
    "                    )\n",
    "                ]\n",
    "                if (isinstance(ret, tuple) or hasattr(ret, \"update\"))\n",
    "                else []\n",
    "            ),\n",
    "            f\"{BASE_SCOPE}.agent_response\": ret.update[\"messages\"][-1].content\n",
    "            if hasattr(ret, \"update\")\n",
    "            else json.dumps(ret, indent=4, sort_keys=True),\n",
    "            f\"{BASE_SCOPE}.web_search_results\": [\n",
    "                json.loads(dumps(message)).get(\"kwargs\", {}).get(\"content\", \"\")\n",
    "                for message in ret.update[\"messages\"]\n",
    "                if isinstance(message, ToolMessage)\n",
    "                and message.name == \"web_search\"\n",
    "            ]\n",
    "            if hasattr(ret, \"update\")\n",
    "            else \"No tool call\",\n",
    "            f\"{BASE_SCOPE}.cortex_agent_results\": [\n",
    "                json.loads(dumps(message)).get(\"kwargs\", {}).get(\"content\", \"\")\n",
    "                for message in ret.update[\"messages\"]\n",
    "                if isinstance(message, ToolMessage)\n",
    "                and message.name == \"CortexAgent\"\n",
    "            ]\n",
    "            if hasattr(ret, \"update\")\n",
    "            else \"No tool call\",\n",
    "            f\"{BASE_SCOPE}.cortex_agent_text\": [\n",
    "                ast.literal_eval(\n",
    "                    json.loads(dumps(m)).get(\"kwargs\", {}).get(\"content\", \"\")\n",
    "                )[0]\n",
    "                for m in ret.update[\"messages\"]\n",
    "                if isinstance(m, ToolMessage) and m.name == \"CortexAgent\"\n",
    "            ]\n",
    "            if hasattr(ret, \"update\")\n",
    "            else \"No tool call\",\n",
    "            f\"{BASE_SCOPE}.cortex_agent_citations\": [\n",
    "                ast.literal_eval(\n",
    "                    json.loads(dumps(m)).get(\"kwargs\", {}).get(\"content\", \"\")\n",
    "                )[1]\n",
    "                for m in ret.update[\"messages\"]\n",
    "                if isinstance(m, ToolMessage) and m.name == \"CortexAgent\"\n",
    "            ]\n",
    "            if hasattr(ret, \"update\")\n",
    "            else \"No tool call\",\n",
    "            f\"{BASE_SCOPE}.cortex_agent_sql\": [\n",
    "                ast.literal_eval(\n",
    "                    json.loads(dumps(m)).get(\"kwargs\", {}).get(\"content\", \"\")\n",
    "                )[2]\n",
    "                for m in ret.update[\"messages\"]\n",
    "                if isinstance(m, ToolMessage) and m.name == \"CortexAgent\"\n",
    "            ]\n",
    "            if hasattr(ret, \"update\")\n",
    "            else \"No tool call\",\n",
    "            f\"{BASE_SCOPE}.cortex_agent_sql_results\": [\n",
    "                ast.literal_eval(\n",
    "                    json.loads(dumps(m)).get(\"kwargs\", {}).get(\"content\", \"\")\n",
    "                )[3]\n",
    "                for m in ret.update[\"messages\"]\n",
    "                if isinstance(m, ToolMessage) and m.name == \"CortexAgent\"\n",
    "            ]\n",
    "            if hasattr(ret, \"update\")\n",
    "            else \"No tool call\",\n",
    "        },\n",
    "    )\n",
    "    @instrument(\n",
    "        span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            SpanAttributes.RETRIEVAL.QUERY_TEXT: args[0][\"messages\"][\n",
    "                -1\n",
    "            ].content,\n",
    "            SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS:        \n",
    "            [\n",
    "                (\n",
    "                    ast.literal_eval(\n",
    "                        json.loads(dumps(m)).get(\"kwargs\", {}).get(\"content\", \"\")\n",
    "                    )[3]\n",
    "                    if m.name == \"CortexAgent\"\n",
    "                    else\n",
    "                    json.loads(dumps(m)).get(\"kwargs\", {}).get(\"content\", \"\")\n",
    "                )\n",
    "                for m in ret.update[\"messages\"]\n",
    "                if isinstance(m, ToolMessage)\n",
    "                and m.name in (\"CortexAgent\", \"web_search\")\n",
    "            ]\n",
    "            if hasattr(ret, \"update\")\n",
    "            else \"No tool call\",\n",
    "        },\n",
    "    )\n",
    "    def research_agent_node(\n",
    "        state: ToolState,\n",
    "    ) -> Command[Literal[\"chart_generator\"]]:\n",
    "        \"\"\"\n",
    "        Always binds the selected tools and invokes the bound agent.\n",
    "        Stops on FINAL ANSWER or moves to chart_generator.\n",
    "        \"\"\"\n",
    "        # grab (non-empty) list of selected tool IDs\n",
    "        selected_ids = state[\"selected_tools\"]\n",
    "\n",
    "        # bind only those tools\n",
    "        selected_tools = [tool_registry[tid] for tid in selected_ids]\n",
    "        bound_llm = llm.bind_tools(selected_tools)\n",
    "        bound_agent = create_react_agent(\n",
    "            bound_llm,\n",
    "            tools=selected_tools,  # already bound\n",
    "            prompt=make_system_prompt(\n",
    "                \"You can only do research. You are working with both a chart generator and a chart summarizer colleagues.\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # run it\n",
    "        result = bound_agent.invoke(state)\n",
    "\n",
    "        # decide if we’re done\n",
    "        last = result[\"messages\"][-1]\n",
    "        goto = get_next_node(last, \"chart_generator\")\n",
    "\n",
    "        # tag the origin of the final message\n",
    "        result[\"messages\"][-1] = HumanMessage(\n",
    "            content=last.content,\n",
    "            name=\"research_agent\",\n",
    "        )\n",
    "\n",
    "        return Command(\n",
    "            update={\"messages\": result[\"messages\"]},\n",
    "            goto=goto,\n",
    "        )\n",
    "\n",
    "    # Chart generator agent and node\n",
    "    # NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION, WHICH CAN BE UNSAFE WHEN NOT SANDBOXED\n",
    "    # 1) Define the chart‐agent: it only returns JSON with a \"code\" field\n",
    "    chart_agent = create_react_agent(\n",
    "        llm,\n",
    "        [python_repl_tool],\n",
    "        prompt=make_system_prompt(\n",
    "            \"\"\"You can only generate charts by returning a single JSON object, for example:\n",
    "        {\n",
    "        \"code\": \"<your python plotting code here>\"\n",
    "        }\n",
    "        —where <your python plotting code> uses matplotlib to create exactly one figure.\n",
    "        The plot should always include axis titles and relevant labels at the minimum.\n",
    "        Do NOT include any prose or tool‐call wrappers.\"\"\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    def extract_chart_path(text: str) -> str | None:\n",
    "        \"\"\"\n",
    "        Scan every line of tool stdout for 'CHART_PATH=' and return\n",
    "        whatever follows, trimmed.  Returns None if no such line exists.\n",
    "        \"\"\"\n",
    "        for line in text.splitlines():\n",
    "            if \"CHART_PATH=\" in line:\n",
    "                # split on the first '=', strip whitespace\n",
    "                return line.split(\"CHART_PATH=\", 1)[1].strip()\n",
    "        return None\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"CHART_GENERATOR_NODE\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            f\"{BASE_SCOPE}.chart_node_input\": args[0][\"messages\"][-1].content,\n",
    "            f\"{BASE_SCOPE}.chart_node_response\": (\n",
    "                ret.update[\"messages\"][-1].content\n",
    "                if ret and hasattr(ret, \"update\") and ret.update\n",
    "                else \"No update response\"\n",
    "            ),\n",
    "        },\n",
    "    )\n",
    "    def chart_node(state: ToolState) -> Command[Literal[\"chart_summarizer\"]]:\n",
    "        # 0) If a path is already in state, skip\n",
    "        # extract the current human query\n",
    "        current_query = state[\"messages\"][-1].content\n",
    "\n",
    "        # if we already generated a chart for _this_ query, skip\n",
    "        if state.get(\"last_query\") == current_query and state.get(\"chart_path\"):\n",
    "            return Command(\n",
    "                update={\"messages\": state[\"messages\"]}, goto=\"chart_summarizer\"\n",
    "            )\n",
    "\n",
    "        # it's a new query (or first run) → clear any old chart_path and remember this query\n",
    "        state.pop(\"chart_path\", None)\n",
    "        state[\"last_query\"] = current_query\n",
    "\n",
    "        # 1) Remember how many messages we had\n",
    "        len_before = len(state[\"messages\"])\n",
    "\n",
    "        # 2) Run the agent exactly once\n",
    "        agent_out = chart_agent.invoke(state)\n",
    "\n",
    "        all_msgs = agent_out[\"messages\"]\n",
    "\n",
    "        # 3) Look at only the brand-new messages for our chart tool output\n",
    "        new_segment = all_msgs[len_before:]\n",
    "        tool_msgs = [\n",
    "            m\n",
    "            for m in new_segment\n",
    "            if isinstance(m, ToolMessage) and \"CHART_PATH=\" in m.content\n",
    "        ]\n",
    "\n",
    "        if not tool_msgs:\n",
    "            return Command(\n",
    "                update={\"messages\": state[\"messages\"]},\n",
    "                goto=\"research_agent\",\n",
    "            )\n",
    "\n",
    "        # 4) Parse the last one in case there are multiples\n",
    "        tool_msg = tool_msgs[-1]\n",
    "        tool_stdout = tool_msg.content\n",
    "\n",
    "        chart_path = extract_chart_path(tool_stdout)\n",
    "        # 5) Build your new messages list: include only that new ToolMessage\n",
    "        new_msgs = state[\"messages\"][:] + [tool_msg]\n",
    "\n",
    "        # 6) Success! stash path into state and append the CHART_PATH marker\n",
    "        new_msgs.append(\n",
    "            HumanMessage(\n",
    "                content=f\"CHART_PATH={chart_path}\", name=\"chart_generator\"\n",
    "            )\n",
    "        )\n",
    "        return Command(\n",
    "            update={\"messages\": new_msgs, \"chart_path\": chart_path},\n",
    "            goto=\"chart_summarizer\",\n",
    "        )\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"CHART_SUMMARY_NODE\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            # grab the state dict (kwarg wins, else first arg)\n",
    "            f\"{BASE_SCOPE}.summary_node_input\": (\n",
    "                (kwargs.get(\"state\") or args[0])[\"messages\"][-1].content\n",
    "            ),\n",
    "            f\"{BASE_SCOPE}.summary_node_output\": (\n",
    "                ret.update[\"messages\"][-1].content\n",
    "                if hasattr(ret, \"update\")\n",
    "                else \"NO SUMMARY GENERATED\"\n",
    "            ),\n",
    "        },\n",
    "    )\n",
    "    def chart_summary_node(state: ToolState) -> Command:\n",
    "        # 1) find the chart_path in state\n",
    "        chart_path = state.get(\"chart_path\", \"\")\n",
    "        if not chart_path:\n",
    "            return Command(\n",
    "                update={\n",
    "                    \"messages\": state[\"messages\"]\n",
    "                    + [\n",
    "                        HumanMessage(\n",
    "                            \"No valid chart was generated. Please try again.\",\n",
    "                            name=\"chart_summarizer\",\n",
    "                        )\n",
    "                    ]\n",
    "                },\n",
    "                goto=\"select_tools\",\n",
    "            )\n",
    "\n",
    "        # 2) strip *everything* except human utterances\n",
    "        human_history = [\n",
    "            m for m in state[\"messages\"] if isinstance(m, HumanMessage)\n",
    "        ]\n",
    "\n",
    "        # ensure our CHART_PATH marker is last\n",
    "        if not human_history or not human_history[-1].content.startswith(\n",
    "            \"CHART_PATH=\"\n",
    "        ):\n",
    "            human_history.append(\n",
    "                HumanMessage(\n",
    "                    f\"CHART_PATH={chart_path}\", name=\"chart_summarizer\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # 3) build your ChatCompletion prompt\n",
    "        system = SystemMessage(\n",
    "            content=make_system_prompt(\n",
    "                \"You are an AI assistant whose *only* job is to describe a chart image. \"\n",
    "                \"Input is a message CHART_PATH=… pointing at a saved PNG. \"\n",
    "                \"Include complete details and specifics of the chart image.\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        messages_for_llm = (\n",
    "            [system]\n",
    "            + human_history\n",
    "            + [HumanMessage(\"Please describe the above chart.\")]\n",
    "        )\n",
    "\n",
    "        # 4) call the LLM directly—no tools, no React agent\n",
    "        ai_msg: AIMessage = llm(messages_for_llm)\n",
    "        summary = ai_msg.content\n",
    "\n",
    "        return Command(\n",
    "            update={\n",
    "                \"messages\": state[\"messages\"]\n",
    "                + [\n",
    "                    HumanMessage(summary, name=\"chart_summarizer\"),\n",
    "                ]\n",
    "            },\n",
    "            goto=\"reflection\",\n",
    "        )\n",
    "\n",
    "    @instrument(\n",
    "        span_type=\"CHART_SUMMARY_REFLECTION\",\n",
    "        attributes=lambda ret, exception, *args, **kwargs: {\n",
    "            f\"{BASE_SCOPE}.chart_summary_reflection_input_user_query\": (\n",
    "                (kwargs.get(\"state\") or args[0])[\"messages\"][0].content\n",
    "            ),\n",
    "            f\"{BASE_SCOPE}.chart_summary_reflection_input_chart_summary\": (\n",
    "                (kwargs.get(\"state\") or args[0])[\"messages\"][-1].content\n",
    "            ),\n",
    "            # extract the summary string rather than returning the Command object\n",
    "            f\"{BASE_SCOPE}.chart_summary_reflection_response\": (\n",
    "                ret.update[\"messages\"][-1].content\n",
    "                if hasattr(ret, \"update\")\n",
    "                else \"\"\n",
    "            ),\n",
    "        },\n",
    "    )\n",
    "    def reflection_node(state: ToolState) -> Command:\n",
    "        \"\"\"\n",
    "        This function uses an LLM to reflect on the quality of a chart summary\n",
    "        and determine if the task is complete or requires further refinement.\n",
    "        \"\"\"\n",
    "\n",
    "        reflection_prompt_template = PromptTemplate(\n",
    "            input_variables=[\"user_query\", \"chart_summary\"],\n",
    "            template=\"\"\"\\\n",
    "        You are an AI assistant tasked with reflecting on the quality of a chart summary. The user has asked the following question:\n",
    "        \"{user_query}\"\n",
    "\n",
    "        You are given the following chart summary:\n",
    "        \"{chart_summary}\"\n",
    "\n",
    "        Your task is to evaluate how well the chart summary answers the user's question. Consider the following:\n",
    "        - Does it describe a chart that will be relevant for answering the user's query?\n",
    "\n",
    "        If the summary **generally** addresses the question, respond with 'Task complete'. If the summary **lacks significant** details or clarity, then respond with specific details on how the answer should be improved and what information is needed. Avoid being overly critical unless the summary completely misses key elements necessary to answer the query.\n",
    "\n",
    "        Please provide your answer in a **concise and encouraging** manner.\n",
    "        \"\"\",\n",
    "        )\n",
    "\n",
    "        # Create the chain using the prompt template and the LLM (ChatOpenAI)\n",
    "        reflection_chain = reflection_prompt_template | llm\n",
    "\n",
    "        user_query = state[\"messages\"][0].content\n",
    "        chart_summary = state[\"messages\"][-1].content\n",
    "        # Call the chain with the user query and chart summary\n",
    "        reflection_result = reflection_chain.invoke({\n",
    "            \"user_query\": user_query,\n",
    "            \"chart_summary\": chart_summary,\n",
    "        })\n",
    "        if \"Task complete\" in reflection_result.content:\n",
    "            return Command(\n",
    "                update={\n",
    "                    \"messages\": state[\"messages\"]\n",
    "                    + [\n",
    "                        HumanMessage(\n",
    "                            reflection_result.content, name=\"reflection\"\n",
    "                        )\n",
    "                    ]\n",
    "                    + [\n",
    "                        HumanMessage(\n",
    "                            f\"Chart saved at {state['chart_path']}. \\n The chart summary is: \\n {chart_summary}\",\n",
    "                            name=\"approved chart summary\",\n",
    "                        )\n",
    "                    ]\n",
    "                },\n",
    "                goto=END,\n",
    "            )\n",
    "        else:\n",
    "            return Command(\n",
    "                update={\n",
    "                    \"messages\": state[\"messages\"]\n",
    "                    + [\n",
    "                        HumanMessage(\n",
    "                            reflection_result.content, name=\"reflection\"\n",
    "                        )\n",
    "                    ]\n",
    "                },\n",
    "                goto=\"select_tools\",\n",
    "            )\n",
    "\n",
    "    workflow = StateGraph(ToolState)\n",
    "    workflow.add_node(\"select_tools\", select_tools)\n",
    "    workflow.add_node(\"research_agent\", research_agent_node)\n",
    "    workflow.add_node(\"chart_generator\", chart_node)\n",
    "    workflow.add_node(\"chart_summarizer\", chart_summary_node)\n",
    "    workflow.add_node(\"reflection\", reflection_node)\n",
    "\n",
    "    workflow.add_edge(START, \"select_tools\")\n",
    "    workflow.add_edge(\"select_tools\", \"research_agent\")\n",
    "    workflow.add_edge(\"research_agent\", \"chart_generator\")\n",
    "    workflow.add_edge(\"chart_generator\", \"chart_summarizer\")\n",
    "    workflow.add_edge(\"chart_summarizer\", \"reflection\")\n",
    "    workflow.add_edge(\"reflection\", END)\n",
    "\n",
    "    compiled_graph = workflow.compile()\n",
    "\n",
    "    return compiled_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae6e403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TruAgent:\n",
    "    def __init__(self):\n",
    "        self.graph = build_graph_with_agent()\n",
    "\n",
    "    @instrument(\n",
    "        span_type=SpanAttributes.SpanType.RECORD_ROOT,\n",
    "        attributes={\n",
    "            SpanAttributes.RECORD_ROOT.INPUT: \"query\",\n",
    "            SpanAttributes.RECORD_ROOT.OUTPUT: \"return\",\n",
    "        },\n",
    "    )\n",
    "    def invoke_agent_graph(self, query: str) -> str:\n",
    "        try:\n",
    "            # rebuild the graph for each query\n",
    "            self.graph = build_graph_with_agent()\n",
    "            # Initialize state with proper message format\n",
    "            state = {\"messages\": [HumanMessage(content=query)]}\n",
    "\n",
    "            # Stream events with recursion limit\n",
    "            events = self.graph.stream(\n",
    "                state,\n",
    "                {\"recursion_limit\": 60},\n",
    "            )\n",
    "\n",
    "            # Track all messages through the conversation\n",
    "            all_messages = []\n",
    "            for event in events:\n",
    "                # Get the payload from the event\n",
    "                _, payload = next(iter(event.items()))\n",
    "                if not payload:  # Skip empty payloads\n",
    "                    continue\n",
    "\n",
    "                messages = payload.get(\"messages\")\n",
    "                if not messages:\n",
    "                    continue\n",
    "                all_messages.extend(messages)\n",
    "\n",
    "            # Return the last message's content if available\n",
    "            return (\n",
    "                all_messages[-1].content\n",
    "                if all_messages and hasattr(all_messages[-1], \"content\")\n",
    "                else \"\"\n",
    "            )\n",
    "        except Exception:\n",
    "            return \"I ran into an issue, and cannot answer your question.\"\n",
    "\n",
    "\n",
    "tru_agent = TruAgent()\n",
    "\n",
    "tru_agent_app = TruApp(\n",
    "    tru_agent,\n",
    "    app_name=APP_NAME,\n",
    "    app_version=\"doc, sql and web search\",\n",
    "    connector=trulens_sf_connector,\n",
    "    main_method=tru_agent.invoke_agent_graph,\n",
    ")\n",
    "\n",
    "st_1 = datetime.datetime.fromtimestamp(time.time()).strftime(\n",
    "    \"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    run_name=\"Multi-agent demo run - document, sql and web search \" + st_1,\n",
    "    description=\"this is a run with access to cortex agent (search and analyst) and web search\",\n",
    "    dataset_name=\"Research test dataset\",\n",
    "    source_type=\"DATAFRAME\",\n",
    "    label=\"langgraph demo\",\n",
    "    dataset_spec={\n",
    "        \"RECORD_ROOT.INPUT\": \"query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "run: Run = tru_agent_app.add_run(run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde20b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.start(input_df=user_queries_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faa503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while run.get_status() == \"INVOCATION_IN_PROGRESS\":\n",
    "    time.sleep(3)\n",
    "\n",
    "run.compute_metrics([\"context_relevance\", \"answer_relevance\", \"groundedness\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oss_rag_stack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
