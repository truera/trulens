{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _LangChain_ Ensemble Retriever\n",
    "\n",
    "The _LangChain_ EnsembleRetriever takes a list of retrievers as input and ensemble the results of their get_relevant_documents() methods and rerank the results based on the Reciprocal Rank Fusion algorithm. With TruLens, we have the ability to evaluate the context of each component retriever along with the ensemble retriever. This example walks through that process.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/examples/expositional/frameworks/langchain/langchain_ensemble_retriever.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --pre trulens trulens-instrument-langchain trulens-providers-openai openai langchain langchain_community langchain_openai rank_bm25 faiss_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports main tools:\n",
    "# Imports from LangChain to build app\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from trulens.core import Feedback\n",
    "from trulens.core import TruSession\n",
    "from trulens.instrument.langchain import TruChain\n",
    "\n",
    "tru = TruSession()\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list_1 = [\n",
    "    \"I like apples\",\n",
    "    \"I like oranges\",\n",
    "    \"Apples and oranges are fruits\",\n",
    "]\n",
    "\n",
    "# initialize the bm25 retriever and faiss retriever\n",
    "bm25_retriever = BM25Retriever.from_texts(\n",
    "    doc_list_1, metadatas=[{\"source\": 1}] * len(doc_list_1)\n",
    ")\n",
    "bm25_retriever.k = 2\n",
    "\n",
    "doc_list_2 = [\n",
    "    \"You like apples\",\n",
    "    \"You like oranges\",\n",
    "]\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "faiss_vectorstore = FAISS.from_texts(\n",
    "    doc_list_2, embedding, metadatas=[{\"source\": 2}] * len(doc_list_2)\n",
    ")\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Context Relevance checks for each component retriever + ensemble\n",
    "\n",
    "This requires knowing the feedback selector for each. You can find this path by logging a run of your application and examining the application traces on the Evaluations page.\n",
    "\n",
    "Read more in our docs: https://www.trulens.org/trulens/selecting_components/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from trulens.core.schema import Select\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "# Initialize provider class\n",
    "openai = OpenAI()\n",
    "\n",
    "bm25_context = (\n",
    "    Select.RecordCalls.retrievers[0]\n",
    "    ._get_relevant_documents.rets[:]\n",
    "    .page_content\n",
    ")\n",
    "faiss_context = (\n",
    "    Select.RecordCalls.retrievers[1]\n",
    "    ._get_relevant_documents.rets[:]\n",
    "    .page_content\n",
    ")\n",
    "ensemble_context = Select.RecordCalls.invoke.rets[:].page_content\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance_bm25 = (\n",
    "    Feedback(openai.context_relevance, name=\"BM25\")\n",
    "    .on_input()\n",
    "    .on(bm25_context)\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "f_context_relevance_faiss = (\n",
    "    Feedback(openai.context_relevance, name=\"FAISS\")\n",
    "    .on_input()\n",
    "    .on(faiss_context)\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "f_context_relevance_ensemble = (\n",
    "    Feedback(openai.context_relevance, name=\"Ensemble\")\n",
    "    .on_input()\n",
    "    .on(ensemble_context)\n",
    "    .aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add feedbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_recorder = TruChain(\n",
    "    ensemble_retriever,\n",
    "    app_name=\"Ensemble Retriever\",\n",
    "    feedbacks=[\n",
    "        f_context_relevance_bm25,\n",
    "        f_context_relevance_faiss,\n",
    "        f_context_relevance_ensemble,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_recorder as recording:\n",
    "    ensemble_retriever.invoke(\"apples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See and compare results from each retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core.utils.trulens import get_feedback_result\n",
    "\n",
    "last_record = recording.records[-1]\n",
    "get_feedback_result(last_record, \"Ensemble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core.utils.trulens import get_feedback_result\n",
    "\n",
    "last_record = recording.records[-1]\n",
    "get_feedback_result(last_record, \"BM25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core.utils.trulens import get_feedback_result\n",
    "\n",
    "last_record = recording.records[-1]\n",
    "get_feedback_result(last_record, \"FAISS\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore in a Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "run_dashboard(tru)  # open a local streamlit app to explore\n",
    "\n",
    "# stop_dashboard(tru) # stop if needed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can run `trulens` from a command line in the same folder to start the dashboard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "d5737f6101ac92451320b0e41890107145710b89f85909f3780d702e7818f973"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
