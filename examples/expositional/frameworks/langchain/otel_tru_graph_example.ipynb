{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruGraph Tutorial: Instrumenting LangGraph Applications with OTel\n",
    "\n",
    "This notebook demonstrates how to use TruGraph to instrument LangGraph applications for evaluation and monitoring.\n",
    "\n",
    "## Overview\n",
    "\n",
    "TruGraph provides:\n",
    "- **Automatic detection** of LangGraph applications\n",
    "- **Combined instrumentation** of both LangChain and LangGraph components\n",
    "- **Multi-agent evaluation** capabilities\n",
    "- **Automatic @task instrumentation** with intelligent attribute extraction\n",
    "\n",
    "## Installation\n",
    "\n",
    "First, make sure you have the required packages installed:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "#!pip install trulens-apps-langgraph langgraph langchain-core langchain-openai langchain-community\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenTelemetry (OTel) Compatibility\n",
    "\n",
    "TruGraph supports both traditional TruLens instrumentation and OpenTelemetry (OTel) tracing mode:\n",
    "\n",
    "- **Traditional Mode** (default): Uses TruLens native instrumentation with `CombinedInstrument`\n",
    "- **OTel Mode**: Uses OpenTelemetry spans for interoperability with existing telemetry stacks\n",
    "\n",
    "To enable OTel mode, set the environment variable:\n",
    "```python\n",
    "import os\n",
    "os.environ[\"TRULENS_OTEL_TRACING\"] = \"1\"\n",
    "```\n",
    "\n",
    "‚ö†Ô∏è **Note**: When OTel tracing is enabled, TruGraph automatically detects the main method (`invoke` or `run`) and uses OTel-compatible instrumentation. The traditional instrumentation system is disabled in OTel mode.\n",
    "\n",
    "## Basic Setup\n",
    "\n",
    "Let's start by checking if LangGraph is available and importing the necessary components:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "from trulens.core.session import TruSession\n",
    "\n",
    "os.environ[\"TRULENS_OTEL_TRACING\"] = \"1\"\n",
    "\n",
    "session = TruSession()\n",
    "session.reset_database()\n",
    "# Check if LangGraph is available\n",
    "try:\n",
    "    from langgraph.graph import StateGraph, MessagesState, END\n",
    "    from langchain_core.messages import HumanMessage, AIMessage\n",
    "    from trulens.apps.langgraph import TruGraph\n",
    "    print(\"‚úÖ LangGraph and TruGraph are available!\")\n",
    "    LANGGRAPH_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå LangGraph not available: {e}\")\n",
    "    print(\"Please install with: pip install langgraph\")\n",
    "    LANGGRAPH_AVAILABLE = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Simple Multi-Agent Workflow\n",
    "\n",
    "Let's create a basic multi-agent workflow with a researcher and writer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LANGGRAPH_AVAILABLE:\n",
    "    # Define agent functions\n",
    " \n",
    "    def research_agent(state):\n",
    "        \"\"\"Agent that performs research on a topic.\"\"\"\n",
    "        messages = state.get(\"messages\", [])\n",
    "        if messages:\n",
    "            last_message = messages[-1]\n",
    "            if hasattr(last_message, \"content\"):\n",
    "                query = last_message.content\n",
    "            else:\n",
    "                query = str(last_message)\n",
    "            \n",
    "            # Simulate research (in a real app, this would call external APIs)\n",
    "            research_results = f\"Research findings for '{query}': This is a comprehensive analysis of the topic.\"\n",
    "            return {\"messages\": [AIMessage(content=research_results)]}\n",
    "        \n",
    "        return {\"messages\": [AIMessage(content=\"No research query provided\")]}\n",
    "    \n",
    "    def writer_agent(state):\n",
    "        \"\"\"Agent that writes articles based on research.\"\"\"\n",
    "        messages = state.get(\"messages\", [])\n",
    "        if messages:\n",
    "            last_message = messages[-1]\n",
    "            if hasattr(last_message, \"content\"):\n",
    "                research_content = last_message.content\n",
    "            else:\n",
    "                research_content = str(last_message)\n",
    "            \n",
    "            # Simulate article writing\n",
    "            article = f\"Article: Based on the research - {research_content[:100]}...\"\n",
    "            return {\"messages\": [AIMessage(content=article)]}\n",
    "        \n",
    "        return {\"messages\": [AIMessage(content=\"No research content provided\")]}\n",
    "    \n",
    "    # Create the workflow\n",
    "    workflow = StateGraph(MessagesState)\n",
    "    workflow.add_node(\"researcher\", research_agent)\n",
    "    workflow.add_node(\"writer\", writer_agent)\n",
    "    workflow.add_edge(\"researcher\", \"writer\")\n",
    "    workflow.add_edge(\"writer\", END)\n",
    "    workflow.set_entry_point(\"researcher\")\n",
    "    \n",
    "    # Compile the graph\n",
    "    graph = workflow.compile()\n",
    "    \n",
    "    print(\"‚úÖ Multi-agent workflow created successfully!\")\n",
    "    print(f\"Graph type: {type(graph)}\")\n",
    "    print(f\"Graph module: {graph.__module__}\")\n",
    "else:\n",
    "    print(\"‚ùå Skipping workflow creation - LangGraph not available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Test Auto-Detection\n",
    "\n",
    "Let's test whether TruSession can automatically detect our LangGraph application:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic @task Detection\n",
    "\n",
    "One of the key features of TruGraph is its ability to automatically detect and instrument functions decorated with LangGraph's `@task` decorator. This means you can use standard LangGraph patterns without any additional instrumentation code.\n",
    "\n",
    "### How it works:\n",
    "\n",
    "1. **Automatic Detection**: TruGraph automatically scans for functions decorated with `@task`\n",
    "2. **Smart Attribute Extraction**: It intelligently extracts information from function arguments:\n",
    "   - Handles `BaseChatModel` and `BaseModel` objects\n",
    "   - Extracts data from dataclasses and Pydantic models\n",
    "   - Skips non-serializable objects like LLM pools\n",
    "   - Captures return values and exceptions\n",
    "3. **Seamless Integration**: No additional decorators or code changes required\n",
    "\n",
    "### Example Usage:\n",
    "\n",
    "```python\n",
    "from langgraph.func import task\n",
    "\n",
    "@task  # This is automatically detected and instrumented by TruGraph\n",
    "def my_agent_function(state, config):\n",
    "    # Your agent logic here\n",
    "    return updated_state\n",
    "```\n",
    "\n",
    "The instrumentation happens automatically when you create a TruGraph instance - no manual setup required!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing OTel Compatibility\n",
    "\n",
    "Now let's test TruGraph with OpenTelemetry tracing enabled:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test OTel Mode Compatibility\n",
    "if LANGGRAPH_AVAILABLE:\n",
    "    import os\n",
    "    \n",
    "    print(\"üß™ Testing OTel Mode Compatibility:\")\n",
    "    \n",
    "    # Note: Enable OTel tracing (commented out to avoid conflicts in tutorial)\n",
    "    # Uncomment the next line to test OTel mode:\n",
    "    os.environ[\"TRULENS_OTEL_TRACING\"] = \"1\"\n",
    "    \n",
    "    # For demonstration, show how OTel would be enabled\n",
    "    print(\"  To enable OTel mode:\")\n",
    "    print('  os.environ[\"TRULENS_OTEL_TRACING\"] = \"1\"')\n",
    "    print()\n",
    "    \n",
    "    # Create a fresh session for OTel testing\n",
    "    from trulens.core.session import TruSession\n",
    "    otel_session = TruSession()\n",
    "    \n",
    "    try:\n",
    "        otel_tru_app = otel_session.App(graph, app_name=\"OTelCompatibilityTest\")\n",
    "        print(f\"‚úÖ TruGraph created successfully: {type(otel_tru_app)}\")\n",
    "        print(f\"   OTel Mode would automatically:\")\n",
    "        print(f\"   - Detect main method: invoke or run\")\n",
    "        print(f\"   - Use OpenTelemetry spans instead of traditional instrumentation\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating TruGraph: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Skipping OTel test - LangGraph not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LANGGRAPH_AVAILABLE:\n",
    "    # Test the detection\n",
    "    session = TruSession()\n",
    "    session.reset_database()\n",
    "    \n",
    "    print(\"üîç Testing LangGraph Detection:\")\n",
    "    print(f\"  Module check: {graph.__module__.startswith('langgraph')}\")\n",
    "    print(f\"  Type check: {session._is_langgraph_app(graph)}\")\n",
    "    print(f\"  Has graph attr: {hasattr(graph, 'graph')}\")\n",
    "    print(f\"  Has invoke method: {hasattr(graph, 'invoke')}\")\n",
    "    \n",
    "    # Test automatic detection\n",
    "    print(\"\\nüéØ Testing Automatic Detection:\")\n",
    "    tru_app = session.App(graph, app_name=\"AutoDetectionTest\")\n",
    "    \n",
    "    print(f\"  Created type: {type(tru_app)}\")\n",
    "    print(f\"  Is TruGraph: {isinstance(tru_app, TruGraph)}\")\n",
    "    print(f\"  App name: {tru_app.app_name}\")\n",
    "    \n",
    "    if isinstance(tru_app, TruGraph):\n",
    "        print(\"‚úÖ SUCCESS: Auto-detection worked!\")\n",
    "        \n",
    "        # Test basic functionality\n",
    "        test_input = {\"messages\": [HumanMessage(content=\"What is AI?\")]}\n",
    "        result = tru_app.app.invoke(test_input)\n",
    "        print(f\"  Test result: {result['messages'][-1].content[:50]}...\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå FAILED: Auto-detection didn't work\")\n",
    "        \n",
    "    print(\"\\nüéâ TruGraph Tutorial Complete!\")\n",
    "    print(\"For more examples, check the TruLens documentation at https://trulens.org/\")\n",
    "else:\n",
    "    print(\"‚ùå Skipping detection test - LangGraph not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_app = TruGraph(graph, app_name=\"simple graph api app\")\n",
    "with tru_app as recording:\n",
    "    result = graph.invoke({\"messages\": [HumanMessage(content=\"Hello!\")]})\n",
    "    print(result)\n",
    "session.force_flush()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Test Auto-Detection\n",
    "\n",
    "Let's test whether TruSession can automatically detect our LangGraph application:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic @task Detection\n",
    "\n",
    "One of the key features of TruGraph is its ability to automatically detect and instrument functions decorated with LangGraph's `@task` decorator. This means you can use standard LangGraph patterns without any additional instrumentation code.\n",
    "\n",
    "### How it works:\n",
    "\n",
    "1. **Automatic Detection**: TruGraph automatically scans for functions decorated with `@task`\n",
    "2. **Smart Attribute Extraction**: It intelligently extracts information from function arguments:\n",
    "   - Handles `BaseChatModel` and `BaseModel` objects\n",
    "   - Extracts data from dataclasses and Pydantic models\n",
    "   - Skips non-serializable objects like LLM pools\n",
    "   - Captures return values and exceptions\n",
    "3. **Seamless Integration**: No additional decorators or code changes required\n",
    "\n",
    "### Example Usage:\n",
    "\n",
    "```python\n",
    "from langgraph.func import task\n",
    "\n",
    "@task  # This is automatically detected and instrumented by TruGraph\n",
    "def my_agent_function(state, config):\n",
    "    # Your agent logic here\n",
    "    return updated_state\n",
    "```\n",
    "\n",
    "The instrumentation happens automatically when you create a TruGraph instance - no manual setup required!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### @task Example\n",
    "\n",
    "Create a real LangGraph application using the `@task` decorator to see automatic instrumentation in action:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LANGGRAPH_AVAILABLE:\n",
    "    \"\"\"\n",
    "    LangGraph Functional API Example with TruGraph\n",
    "\n",
    "    This example demonstrates the correct usage of LangGraph's functional API\n",
    "    with @task and @entrypoint decorators, showing how TruGraph automatically\n",
    "    instruments these functions for evaluation and monitoring.\n",
    "\n",
    "    Key concepts:\n",
    "    - @task: Decorates functions that perform discrete units of work\n",
    "    - @entrypoint: Decorates the main workflow function that orchestrates tasks\n",
    "    - Tasks return futures that must be resolved with .result() or await\n",
    "    - The entrypoint coordinates the overall workflow execution\n",
    "    \"\"\"\n",
    "\n",
    "    import os\n",
    "    from typing import Dict, Any, cast\n",
    "\n",
    "    # Import LangGraph functional API components\n",
    "    try:\n",
    "        from langchain_core.runnables import RunnableConfig\n",
    "        from langgraph.checkpoint.memory import MemorySaver\n",
    "        from langgraph.func import entrypoint\n",
    "        from langgraph.func import task\n",
    "        FUNCTIONAL_API_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è LangGraph Functional API not available\")\n",
    "        print(\"   Install with: pip install langgraph>=0.2.0\")\n",
    "        FUNCTIONAL_API_AVAILABLE = False\n",
    "        RunnableConfig = Dict[str, Any]  # Fallback type\n",
    "\n",
    "    # Import TruGraph for automatic instrumentation\n",
    "    try:\n",
    "        from trulens.apps.langgraph import TruGraph\n",
    "        TRUGRAPH_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è TruGraph not available\")\n",
    "        print(\"   Install with: pip install trulens-apps-langgraph\")\n",
    "        TRUGRAPH_AVAILABLE = False\n",
    "\n",
    "\n",
    "    # Define task functions using @task decorator\n",
    "    @task\n",
    "    def analyze_query(query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze the user's query to understand intent and complexity.\"\"\"\n",
    "        print(f\"  üîç Analyzing query: '{query}'\")\n",
    "        \n",
    "        # Simple analysis simulation - in practice this might call an LLM\n",
    "        words = query.split()\n",
    "        analysis = {\n",
    "            \"intent\": \"information_request\" if \"?\" in query else \"statement\",\n",
    "            \"complexity\": \"high\" if len(words) > 10 else \"medium\" if len(words) > 5 else \"simple\",\n",
    "            \"keywords\": words[:3],  # First 3 words as keywords\n",
    "            \"sentiment\": \"neutral\",\n",
    "            \"word_count\": len(words)\n",
    "        }\n",
    "        \n",
    "        print(f\"  ‚úÖ Analysis complete: {analysis}\")\n",
    "        return analysis\n",
    "\n",
    "    @task\n",
    "    def generate_response(query: str, analysis: Dict[str, Any]) -> str:\n",
    "        \"\"\"Generate a response based on the query and analysis.\"\"\"\n",
    "        print(f\"  ü§ñ Generating response for: '{query[:50]}...'\")\n",
    "        \n",
    "        keywords = analysis.get(\"keywords\", [])\n",
    "        complexity = analysis.get(\"complexity\", \"medium\")\n",
    "        \n",
    "        if keywords:\n",
    "            if complexity == \"high\":\n",
    "                response = f\"This is a complex question about {', '.join(keywords)}. Let me provide a comprehensive analysis: {query} involves multiple interconnected concepts that require careful consideration...\"\n",
    "            elif complexity == \"medium\":\n",
    "                response = f\"Regarding {', '.join(keywords)}, here's what you should know: This topic involves several key aspects that are important to understand...\"\n",
    "            else:\n",
    "                response = f\"About {', '.join(keywords)}: This is a straightforward topic that can be explained simply...\"\n",
    "        else:\n",
    "            response = \"I'd be happy to help you with information and analysis on any topic you're interested in.\"\n",
    "        \n",
    "        print(f\"  ‚úÖ Response generated: {response[:50]}...\")\n",
    "        return response\n",
    "\n",
    "\n",
    "    @task\n",
    "    def format_final_output(query: str, analysis: Dict[str, Any], response: str) -> Dict[str, Any]:\n",
    "        \"\"\"Format the final output with metadata.\"\"\"\n",
    "        print(\"  üìù Formatting final output...\")\n",
    "        \n",
    "        formatted_output = {\n",
    "            \"original_query\": query,\n",
    "            \"analysis_results\": analysis,\n",
    "            \"generated_response\": response,\n",
    "            \"metadata\": {\n",
    "                \"processing_steps\": [\"analysis\", \"generation\", \"formatting\"],\n",
    "                \"response_length\": len(response),\n",
    "                \"keywords_found\": len(analysis.get(\"keywords\", [])),\n",
    "                \"complexity_level\": analysis.get(\"complexity\", \"unknown\")\n",
    "            },\n",
    "            \"status\": \"completed\"\n",
    "        }\n",
    "        \n",
    "        print(\"  ‚úÖ Formatting complete\")\n",
    "        return formatted_output\n",
    "\n",
    "\n",
    "    @entrypoint(checkpointer=MemorySaver())\n",
    "    def intelligent_qa_workflow(input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Main workflow that orchestrates query analysis and response generation.\n",
    "        \n",
    "        This is the entrypoint that coordinates all the @task functions.\n",
    "        Each task returns a future that must be resolved with .result().\n",
    "        \"\"\"\n",
    "        query = input_data.get(\"query\", \"\")\n",
    "        \n",
    "        print(f\"üé¨ Starting intelligent Q&A workflow for: '{query[:50]}...'\")\n",
    "        \n",
    "\n",
    "        analysis = analyze_query(query)\n",
    "        \n",
    "        response = generate_response(query, analysis)\n",
    "\n",
    "\n",
    "        final_result = format_final_output(query, analysis, response)\n",
    "        \n",
    "        \n",
    "        print(f\"‚úÖ Workflow completed successfully\")\n",
    "        return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruGraph Tutorial: Instrumenting LangGraph Applications\n",
    "\n",
    "This notebook demonstrates how to use TruGraph to instrument LangGraph applications for evaluation and monitoring.\n",
    "\n",
    "## Overview\n",
    "\n",
    "TruGraph provides:\n",
    "- **Automatic detection** of LangGraph applications\n",
    "- **Combined instrumentation** of both LangChain and LangGraph components\n",
    "- **Multi-agent evaluation** capabilities\n",
    "- **Automatic @task instrumentation** with intelligent attribute extraction\n",
    "\n",
    "## Installation\n",
    "\n",
    "First, make sure you have the required packages installed:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "#!pip install trulens-apps-langgraph langgraph langchain-core langchain-openai langchain-community\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenTelemetry (OTel) Compatibility\n",
    "\n",
    "TruGraph supports both traditional TruLens instrumentation and OpenTelemetry (OTel) tracing mode:\n",
    "\n",
    "- **Traditional Mode** (default): Uses TruLens native instrumentation with `CombinedInstrument`\n",
    "- **OTel Mode**: Uses OpenTelemetry spans for interoperability with existing telemetry stacks\n",
    "\n",
    "To enable OTel mode, set the environment variable:\n",
    "```python\n",
    "import os\n",
    "os.environ[\"TRULENS_OTEL_TRACING\"] = \"1\"\n",
    "```\n",
    "\n",
    "‚ö†Ô∏è **Note**: When OTel tracing is enabled, TruGraph automatically detects the main method (`invoke` or `run`) and uses OTel-compatible instrumentation. The traditional instrumentation system is disabled in OTel mode.\n",
    "\n",
    "## Basic Setup\n",
    "\n",
    "Let's start by checking if LangGraph is available and importing the necessary components:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if LangGraph is available\n",
    "try:\n",
    "    from langgraph.graph import StateGraph, MessagesState, END\n",
    "    from langchain_core.messages import HumanMessage, AIMessage\n",
    "    from trulens.apps.langgraph import TruGraph\n",
    "    from trulens.core.session import TruSession\n",
    "    print(\"‚úÖ LangGraph and TruGraph are available!\")\n",
    "    LANGGRAPH_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå LangGraph not available: {e}\")\n",
    "    print(\"Please install with: pip install langgraph\")\n",
    "    LANGGRAPH_AVAILABLE = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Simple Multi-Agent Workflow\n",
    "\n",
    "Let's create a basic multi-agent workflow with a researcher and writer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core.otel.instrument import instrument\n",
    "from trulens.otel.semconv.trace import SpanAttributes\n",
    "\n",
    "if LANGGRAPH_AVAILABLE:\n",
    "    # Define agent functions\n",
    "    @instrument(\n",
    "            attributes=lambda ret, exception, *args, **kwargs: {\n",
    "                f\"{SpanAttributes.UNKNOWN.base}.best_baby\": \"Kojikun\"\n",
    "            }\n",
    "        )\n",
    "    def research_agent(state):\n",
    "        \"\"\"Agent that performs research on a topic.\"\"\"\n",
    "        messages = state.get(\"messages\", [])\n",
    "        if messages:\n",
    "            last_message = messages[-1]\n",
    "            if hasattr(last_message, \"content\"):\n",
    "                query = last_message.content\n",
    "            else:\n",
    "                query = str(last_message)\n",
    "            \n",
    "            # Simulate research (in a real app, this would call external APIs)\n",
    "            research_results = f\"Research findings for '{query}': This is a comprehensive analysis of the topic.\"\n",
    "            return {\"messages\": [AIMessage(content=research_results)]}\n",
    "        \n",
    "        return {\"messages\": [AIMessage(content=\"No research query provided\")]}\n",
    "    \n",
    "    def writer_agent(state):\n",
    "        \"\"\"Agent that writes articles based on research.\"\"\"\n",
    "        messages = state.get(\"messages\", [])\n",
    "        if messages:\n",
    "            last_message = messages[-1]\n",
    "            if hasattr(last_message, \"content\"):\n",
    "                research_content = last_message.content\n",
    "            else:\n",
    "                research_content = str(last_message)\n",
    "            \n",
    "            # Simulate article writing\n",
    "            article = f\"Article: Based on the research - {research_content[:100]}...\"\n",
    "            return {\"messages\": [AIMessage(content=article)]}\n",
    "        \n",
    "        return {\"messages\": [AIMessage(content=\"No research content provided\")]}\n",
    "    \n",
    "    # Create the workflow\n",
    "    workflow = StateGraph(MessagesState)\n",
    "    workflow.add_node(\"researcher\", research_agent)\n",
    "    workflow.add_node(\"writer\", writer_agent)\n",
    "    workflow.add_edge(\"researcher\", \"writer\")\n",
    "    workflow.add_edge(\"writer\", END)\n",
    "    workflow.set_entry_point(\"researcher\")\n",
    "    \n",
    "    # Compile the graph\n",
    "    graph = workflow.compile()\n",
    "    \n",
    "    print(\"‚úÖ Multi-agent workflow created successfully!\")\n",
    "    print(f\"Graph type: {type(graph)}\")\n",
    "    print(f\"Graph module: {graph.__module__}\")\n",
    "else:\n",
    "    print(\"‚ùå Skipping workflow creation - LangGraph not available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Test Auto-Detection\n",
    "\n",
    "Let's test whether TruSession can automatically detect our LangGraph application:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic @task Detection\n",
    "\n",
    "One of the key features of TruGraph is its ability to automatically detect and instrument functions decorated with LangGraph's `@task` decorator. This means you can use standard LangGraph patterns without any additional instrumentation code.\n",
    "\n",
    "### How it works:\n",
    "\n",
    "1. **Automatic Detection**: TruGraph automatically scans for functions decorated with `@task`\n",
    "2. **Smart Attribute Extraction**: It intelligently extracts information from function arguments:\n",
    "   - Handles `BaseChatModel` and `BaseModel` objects\n",
    "   - Extracts data from dataclasses and Pydantic models\n",
    "   - Skips non-serializable objects like LLM pools\n",
    "   - Captures return values and exceptions\n",
    "3. **Seamless Integration**: No additional decorators or code changes required\n",
    "\n",
    "### Example Usage:\n",
    "\n",
    "```python\n",
    "from langgraph.func import task\n",
    "\n",
    "@task  # This is automatically detected and instrumented by TruGraph\n",
    "def my_agent_function(state, config):\n",
    "    # Your agent logic here\n",
    "    return updated_state\n",
    "```\n",
    "\n",
    "The instrumentation happens automatically when you create a TruGraph instance - no manual setup required!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing OTel Compatibility\n",
    "\n",
    "Now let's test TruGraph with OpenTelemetry tracing enabled:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test OTel Mode Compatibility\n",
    "if LANGGRAPH_AVAILABLE:\n",
    "    import os\n",
    "    \n",
    "    print(\"üß™ Testing OTel Mode Compatibility:\")\n",
    "    \n",
    "    # Note: Enable OTel tracing (commented out to avoid conflicts in tutorial)\n",
    "    # Uncomment the next line to test OTel mode:\n",
    "    os.environ[\"TRULENS_OTEL_TRACING\"] = \"1\"\n",
    "    \n",
    "    # For demonstration, show how OTel would be enabled\n",
    "    print(\"  To enable OTel mode:\")\n",
    "    print('  os.environ[\"TRULENS_OTEL_TRACING\"] = \"1\"')\n",
    "    print()\n",
    "    \n",
    "    # Create a fresh session for OTel testing\n",
    "    from trulens.core.session import TruSession\n",
    "    otel_session = TruSession()\n",
    "    \n",
    "    try:\n",
    "        otel_tru_app = otel_session.App(graph, app_name=\"OTelCompatibilityTest\")\n",
    "        print(f\"‚úÖ TruGraph created successfully: {type(otel_tru_app)}\")\n",
    "        print(f\"   OTel Mode would automatically:\")\n",
    "        print(f\"   - Detect main method: invoke or run\")\n",
    "        print(f\"   - Use OpenTelemetry spans instead of traditional instrumentation\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating TruGraph: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Skipping OTel test - LangGraph not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LANGGRAPH_AVAILABLE:\n",
    "    # Test the detection\n",
    "    session = TruSession()\n",
    "    session.reset_database()\n",
    "    \n",
    "    print(\"üîç Testing LangGraph Detection:\")\n",
    "    print(f\"  Module check: {graph.__module__.startswith('langgraph')}\")\n",
    "    print(f\"  Type check: {session._is_langgraph_app(graph)}\")\n",
    "    print(f\"  Has graph attr: {hasattr(graph, 'graph')}\")\n",
    "    print(f\"  Has invoke method: {hasattr(graph, 'invoke')}\")\n",
    "    \n",
    "    # Test automatic detection\n",
    "    print(\"\\nüéØ Testing Automatic Detection:\")\n",
    "    tru_app = session.App(graph, app_name=\"AutoDetectionTest\")\n",
    "    \n",
    "    print(f\"  Created type: {type(tru_app)}\")\n",
    "    print(f\"  Is TruGraph: {isinstance(tru_app, TruGraph)}\")\n",
    "    print(f\"  App name: {tru_app.app_name}\")\n",
    "    \n",
    "    if isinstance(tru_app, TruGraph):\n",
    "        print(\"‚úÖ SUCCESS: Auto-detection worked!\")\n",
    "        \n",
    "        # Test basic functionality\n",
    "        test_input = {\"messages\": [HumanMessage(content=\"What is AI?\")]}\n",
    "        result = tru_app.app.invoke(test_input)\n",
    "        print(f\"  Test result: {result['messages'][-1].content[:50]}...\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå FAILED: Auto-detection didn't work\")\n",
    "        \n",
    "    print(\"\\nüéâ TruGraph Tutorial Complete!\")\n",
    "    print(\"For more examples, check the TruLens documentation at https://trulens.org/\")\n",
    "else:\n",
    "    print(\"‚ùå Skipping detection test - LangGraph not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_app as recording:\n",
    "    result = graph.invoke({\"messages\": [HumanMessage(content=\"Hello!\")]})\n",
    "TruSession().force_flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Advanced: Custom Classes with Multiple LangGraph Workflows\n",
    "\n",
    "TruGraph now supports complex custom classes that use multiple LangGraph workflows internally. This is useful for:\n",
    "- Multi-agent systems\n",
    "- Complex orchestration patterns  \n",
    "- Custom business logic with embedded LangGraph components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Multi-step agent with multiple LangGraph workflows\n",
    "class ComplexRAGAgent:\n",
    "    \"\"\"A sophisticated RAG agent that uses multiple LangGraph workflows.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        from langgraph.graph import StateGraph, MessagesState\n",
    "        from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "        \n",
    "        # Query planning workflow\n",
    "        def plan_query(state):\n",
    "            messages = state[\"messages\"]\n",
    "            query = messages[-1].content if messages else \"\"\n",
    "            \n",
    "            # Simple planning logic\n",
    "            plan = f\"Search plan for: {query}\"\n",
    "            return {\"messages\": messages + [AIMessage(content=plan)]}\n",
    "        \n",
    "        # Information retrieval workflow  \n",
    "        def retrieve_info(state):\n",
    "            messages = state[\"messages\"]\n",
    "            plan = messages[-1].content if messages else \"\"\n",
    "            \n",
    "            # Simulate retrieval\n",
    "            retrieved_info = f\"Retrieved info based on: {plan}\"\n",
    "            return {\"messages\": messages + [SystemMessage(content=retrieved_info)]}\n",
    "        \n",
    "        # Response synthesis workflow\n",
    "        def synthesize_response(state):\n",
    "            messages = state[\"messages\"]\n",
    "            # Get context from system messages\n",
    "            context = \"\\\\n\".join([msg.content for msg in messages if isinstance(msg, SystemMessage)])\n",
    "            # Get original query from human messages\n",
    "            original_query = next((msg.content for msg in messages if isinstance(msg, HumanMessage)), \"\")\n",
    "            \n",
    "            response = f\"Based on {context}, the answer to '{original_query}' is: [Generated response]\"\n",
    "            return {\"messages\": messages + [AIMessage(content=response)]}\n",
    "        \n",
    "        # Create separate workflows for each step\n",
    "        \n",
    "        # Planner workflow\n",
    "        planner_graph = StateGraph(MessagesState)\n",
    "        planner_graph.add_node(\"plan\", plan_query)\n",
    "        planner_graph.set_entry_point(\"plan\")\n",
    "        planner_graph.set_finish_point(\"plan\")\n",
    "        self.planner_workflow = planner_graph.compile()\n",
    "        \n",
    "        # Retriever workflow\n",
    "        retriever_graph = StateGraph(MessagesState)\n",
    "        retriever_graph.add_node(\"retrieve\", retrieve_info)\n",
    "        retriever_graph.set_entry_point(\"retrieve\")\n",
    "        retriever_graph.set_finish_point(\"retrieve\")\n",
    "        self.retriever_workflow = retriever_graph.compile()\n",
    "        \n",
    "        # Synthesizer workflow\n",
    "        synthesizer_graph = StateGraph(MessagesState)\n",
    "        synthesizer_graph.add_node(\"synthesize\", synthesize_response)\n",
    "        synthesizer_graph.set_entry_point(\"synthesize\")\n",
    "        synthesizer_graph.set_finish_point(\"synthesize\")\n",
    "        self.synthesizer_workflow = synthesizer_graph.compile()\n",
    "    \n",
    "    def run(self, query: str) -> str:\n",
    "        \"\"\"Main method that orchestrates multiple LangGraph workflows.\"\"\"\n",
    "        from langchain_core.messages import HumanMessage\n",
    "        \n",
    "        # Step 1: Plan the query\n",
    "        initial_state = {\"messages\": [HumanMessage(content=query)]}\n",
    "        \n",
    "        print(\"üß† Planning query...\")\n",
    "        planned_state = self.planner_workflow.invoke(initial_state)\n",
    "        \n",
    "        # Step 2: Retrieve information\n",
    "        print(\"üîç Retrieving information...\")\n",
    "        retrieved_state = self.retriever_workflow.invoke(planned_state)\n",
    "        \n",
    "        # Step 3: Synthesize response\n",
    "        print(\"‚ú® Synthesizing final response...\")\n",
    "        final_state = self.synthesizer_workflow.invoke(retrieved_state)\n",
    "        \n",
    "        # Extract final response\n",
    "        final_messages = final_state[\"messages\"]\n",
    "        final_response = final_messages[-1].content if final_messages else \"No response generated\"\n",
    "        \n",
    "        return final_response\n",
    "    \n",
    "    def quick_answer(self, query: str) -> str:\n",
    "        \"\"\"Alternative method for simple queries (bypasses planning).\"\"\"\n",
    "        from langchain_core.messages import HumanMessage, SystemMessage\n",
    "        \n",
    "        # Direct synthesis without planning\n",
    "        state = {\"messages\": [HumanMessage(content=query), SystemMessage(content=\"Direct answer mode\")]}\n",
    "        result = self.synthesizer_workflow.invoke(state)\n",
    "        return result[\"messages\"][-1].content\n",
    "\n",
    "# Create the complex agent\n",
    "complex_agent = ComplexRAGAgent()\n",
    "print(\"‚úÖ Created ComplexRAGAgent with 3 internal LangGraph workflows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TruGraph automatically detects and instruments the custom class\n",
    "if TRUGRAPH_AVAILABLE:\n",
    "    from trulens.apps.langgraph import TruGraph\n",
    "    \n",
    "    # TruGraph will:\n",
    "    # 1. Detect the 3 LangGraph components inside ComplexRAGAgent\n",
    "    # 2. Auto-detect 'run' as the main method to instrument \n",
    "    # 3. Instrument all LangGraph workflows for comprehensive tracing\n",
    "    \n",
    "    tru_complex_agent = TruGraph(\n",
    "        complex_agent,\n",
    "        app_name=\"ComplexRAGAgent\",\n",
    "        app_version=\"v1.0\"\n",
    "    )\n",
    "    \n",
    "    print(\"üéØ TruGraph wrapped ComplexRAGAgent successfully!\")\n",
    "    print(f\"üìä Main method detected: {tru_complex_agent._detect_main_method(complex_agent).__name__}\")\n",
    "    \n",
    "    # Test the complex agent with full tracing\n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"üîç Testing Complex Agent with TruGraph\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    with tru_complex_agent as recording:\n",
    "        response = complex_agent.run(\"How do neural networks learn?\")\n",
    "    \n",
    "    print(f\"\\\\nüìù Response: {response}\")\n",
    "    print(f\"üìä Record ID: {recording.get().record_id}\")\n",
    "    \n",
    "    # Alternative: Explicitly specify a different main method\n",
    "    tru_quick_agent = TruGraph(\n",
    "        complex_agent,\n",
    "        main_method=complex_agent.quick_answer,  # Explicitly specify method\n",
    "        app_name=\"ComplexRAGAgent\", \n",
    "        app_version=\"quick_mode\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\\\n‚ö° Testing quick_answer method with explicit main_method\")\n",
    "    with tru_quick_agent as recording:\n",
    "        quick_response = complex_agent.quick_answer(\"What is Python?\")\n",
    "    \n",
    "    print(f\"üöÄ Quick Response: {quick_response}\")\n",
    "    print(f\"üìä Record ID: {recording.get().record_id}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è TruGraph not available - skipping custom class example\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Key Benefits of Custom Class Support\n",
    "\n",
    "**1. Automatic Detection:**\n",
    "- TruGraph automatically finds LangGraph components within your custom classes\n",
    "- No need to manually specify what to instrument\n",
    "\n",
    "**2. Flexible Method Selection:**\n",
    "- Auto-detects common methods like `run()`, `invoke()`, `execute()`, `call()`, `__call__()`\n",
    "- Or explicitly specify: `TruGraph(app, main_method=app.custom_method)`\n",
    "\n",
    "**3. Comprehensive Tracing:**\n",
    "- Instruments both your custom orchestration logic AND internal LangGraph workflows\n",
    "- Captures the full execution flow across multiple LangGraph invocations\n",
    "\n",
    "**4. Multi-Workflow Support:**\n",
    "- Perfect for complex agents with planning ‚Üí retrieval ‚Üí synthesis patterns\n",
    "- Handles parallel workflow execution\n",
    "- Maintains trace relationships across workflow boundaries\n",
    "\n",
    "**5. Business Logic Integration:**\n",
    "- Your custom preprocessing/postprocessing steps are included in traces\n",
    "- Evaluate end-to-end performance, not just individual LangGraph components\n",
    "- Better insights into real-world application behavior\n",
    "\n",
    "**Usage Patterns:**\n",
    "```python\n",
    "# Simple case - auto-detect everything\n",
    "tru_app = TruGraph(my_custom_agent)\n",
    "\n",
    "# Explicit main method\n",
    "tru_app = TruGraph(my_custom_agent, main_method=my_custom_agent.process)\n",
    "\n",
    "# Multiple methods instrumented separately\n",
    "tru_fast = TruGraph(agent, main_method=agent.quick_mode, app_version=\"fast\")\n",
    "tru_full = TruGraph(agent, main_method=agent.full_mode, app_version=\"comprehensive\")\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trugraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
