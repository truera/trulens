{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruGraph Tutorial: Instrumenting LangGraph Applications with OTel\n",
    "\n",
    "This notebook demonstrates how to use TruGraph to instrument LangGraph applications for evaluation and monitoring.\n",
    "\n",
    "## Overview\n",
    "\n",
    "TruGraph provides:\n",
    "- **Automatic detection** of LangGraph applications\n",
    "- **Combined instrumentation** of both LangChain and LangGraph components\n",
    "- **Multi-agent evaluation** capabilities\n",
    "- **Automatic @task instrumentation** with intelligent attribute extraction\n",
    "\n",
    "## Installation\n",
    "\n",
    "First, make sure you have the required packages installed:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "#!pip install trulens-apps-langgraph langgraph langchain-core langchain-openai langchain-community\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenTelemetry (OTel) Compatibility\n",
    "\n",
    "TruGraph supports both traditional TruLens instrumentation and OpenTelemetry (OTel) tracing mode:\n",
    "\n",
    "- **Traditional Mode** (default): Uses TruLens native instrumentation with `CombinedInstrument`\n",
    "- **OTel Mode**: Uses OpenTelemetry spans for interoperability with existing telemetry stacks\n",
    "\n",
    "To enable OTel mode, set the environment variable:\n",
    "```python\n",
    "import os\n",
    "os.environ[\"TRULENS_OTEL_TRACING\"] = \"1\"\n",
    "```\n",
    "\n",
    "⚠️ **Note**: When OTel tracing is enabled, TruGraph automatically detects the main method (`invoke` or `run`) and uses OTel-compatible instrumentation. The traditional instrumentation system is disabled in OTel mode.\n",
    "\n",
    "## Basic Setup\n",
    "\n",
    "Let's start by checking if LangGraph is available and importing the necessary components:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "from trulens.core.session import TruSession\n",
    "\n",
    "os.environ[\"TRULENS_OTEL_TRACING\"] = \"1\"\n",
    "\n",
    "session = TruSession()\n",
    "session.reset_database()\n",
    "# Check if LangGraph is available\n",
    "try:\n",
    "    from langgraph.graph import StateGraph, MessagesState, END\n",
    "    from langchain_core.messages import HumanMessage, AIMessage\n",
    "    from trulens.apps.langgraph import TruGraph\n",
    "    print(\"✅ LangGraph and TruGraph are available!\")\n",
    "    LANGGRAPH_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    print(f\"❌ LangGraph not available: {e}\")\n",
    "    print(\"Please install with: pip install langgraph\")\n",
    "    LANGGRAPH_AVAILABLE = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Simple Multi-Agent Workflow\n",
    "\n",
    "Let's create a basic multi-agent workflow with a researcher and writer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LANGGRAPH_AVAILABLE:\n",
    "    # Define agent functions\n",
    " \n",
    "    def research_agent(state):\n",
    "        \"\"\"Agent that performs research on a topic.\"\"\"\n",
    "        messages = state.get(\"messages\", [])\n",
    "        if messages:\n",
    "            last_message = messages[-1]\n",
    "            if hasattr(last_message, \"content\"):\n",
    "                query = last_message.content\n",
    "            else:\n",
    "                query = str(last_message)\n",
    "            \n",
    "            # Simulate research (in a real app, this would call external APIs)\n",
    "            research_results = f\"Research findings for '{query}': This is a comprehensive analysis of the topic.\"\n",
    "            return {\"messages\": [AIMessage(content=research_results)]}\n",
    "        \n",
    "        return {\"messages\": [AIMessage(content=\"No research query provided\")]}\n",
    "    \n",
    "    def writer_agent(state):\n",
    "        \"\"\"Agent that writes articles based on research.\"\"\"\n",
    "        messages = state.get(\"messages\", [])\n",
    "        if messages:\n",
    "            last_message = messages[-1]\n",
    "            if hasattr(last_message, \"content\"):\n",
    "                research_content = last_message.content\n",
    "            else:\n",
    "                research_content = str(last_message)\n",
    "            \n",
    "            # Simulate article writing\n",
    "            article = f\"Article: Based on the research - {research_content[:100]}...\"\n",
    "            return {\"messages\": [AIMessage(content=article)]}\n",
    "        \n",
    "        return {\"messages\": [AIMessage(content=\"No research content provided\")]}\n",
    "    \n",
    "    # Create the workflow\n",
    "    workflow = StateGraph(MessagesState)\n",
    "    workflow.add_node(\"researcher\", research_agent)\n",
    "    workflow.add_node(\"writer\", writer_agent)\n",
    "    workflow.add_edge(\"researcher\", \"writer\")\n",
    "    workflow.add_edge(\"writer\", END)\n",
    "    workflow.set_entry_point(\"researcher\")\n",
    "    \n",
    "    # Compile the graph\n",
    "    graph = workflow.compile()\n",
    "    \n",
    "    print(\"✅ Multi-agent workflow created successfully!\")\n",
    "    print(f\"Graph type: {type(graph)}\")\n",
    "    print(f\"Graph module: {graph.__module__}\")\n",
    "else:\n",
    "    print(\"❌ Skipping workflow creation - LangGraph not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_app = TruGraph(graph, app_name=\"simple graph api app\")\n",
    "with tru_app as recording:\n",
    "    result = graph.invoke({\"messages\": [HumanMessage(content=\"Hello!\")]})\n",
    "    print(result)\n",
    "session.force_flush()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Test Auto-Detection\n",
    "\n",
    "Let's test whether TruSession can automatically detect our LangGraph application:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic @task Detection\n",
    "\n",
    "One of the key features of TruGraph is its ability to automatically detect and instrument functions decorated with LangGraph's `@task` decorator. This means you can use standard LangGraph patterns without any additional instrumentation code.\n",
    "\n",
    "### How it works:\n",
    "\n",
    "1. **Automatic Detection**: TruGraph automatically scans for functions decorated with `@task`\n",
    "2. **Smart Attribute Extraction**: It intelligently extracts information from function arguments:\n",
    "   - Handles `BaseChatModel` and `BaseModel` objects\n",
    "   - Extracts data from dataclasses and Pydantic models\n",
    "   - Skips non-serializable objects like LLM pools\n",
    "   - Captures return values and exceptions\n",
    "3. **Seamless Integration**: No additional decorators or code changes required\n",
    "\n",
    "### Example Usage:\n",
    "\n",
    "```python\n",
    "from langgraph.func import task\n",
    "\n",
    "@task  # This is automatically detected and instrumented by TruGraph\n",
    "def my_agent_function(state, config):\n",
    "    # Your agent logic here\n",
    "    return updated_state\n",
    "```\n",
    "\n",
    "The instrumentation happens automatically when you create a TruGraph instance - no manual setup required!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### @task Example\n",
    "\n",
    "Create a real LangGraph application using the `@task` decorator to see automatic instrumentation in action:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LANGGRAPH_AVAILABLE:\n",
    "    \"\"\"\n",
    "    LangGraph Functional API Example with TruGraph\n",
    "\n",
    "    This example demonstrates the correct usage of LangGraph's functional API\n",
    "    with @task and @entrypoint decorators, showing how TruGraph automatically\n",
    "    instruments these functions for evaluation and monitoring.\n",
    "\n",
    "    Key concepts:\n",
    "    - @task: Decorates functions that perform discrete units of work\n",
    "    - @entrypoint: Decorates the main workflow function that orchestrates tasks\n",
    "    - Tasks return futures that must be resolved with .result() or await\n",
    "    - The entrypoint coordinates the overall workflow execution\n",
    "    \"\"\"\n",
    "\n",
    "    import os\n",
    "    from typing import Dict, Any, cast\n",
    "\n",
    "    # Import LangGraph functional API components\n",
    "    try:\n",
    "        from langgraph.func import task, entrypoint\n",
    "        from langgraph.checkpoint.memory import MemorySaver\n",
    "        from langchain_core.runnables import RunnableConfig\n",
    "        FUNCTIONAL_API_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        print(\"⚠️ LangGraph Functional API not available\")\n",
    "        print(\"   Install with: pip install langgraph>=0.2.0\")\n",
    "        FUNCTIONAL_API_AVAILABLE = False\n",
    "        RunnableConfig = Dict[str, Any]  # Fallback type\n",
    "\n",
    "    # Import TruGraph for automatic instrumentation\n",
    "    try:\n",
    "        from trulens.apps.langgraph import TruGraph\n",
    "        from trulens.core import TruSession\n",
    "        TRUGRAPH_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        print(\"⚠️ TruGraph not available\")\n",
    "        print(\"   Install with: pip install trulens-apps-langgraph\")\n",
    "        TRUGRAPH_AVAILABLE = False\n",
    "\n",
    "\n",
    "    # Define task functions using @task decorator\n",
    "    @task\n",
    "    def analyze_query(query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze the user's query to understand intent and complexity.\"\"\"\n",
    "        print(f\"  🔍 Analyzing query: '{query}'\")\n",
    "        \n",
    "        # Simple analysis simulation - in practice this might call an LLM\n",
    "        words = query.split()\n",
    "        analysis = {\n",
    "            \"intent\": \"information_request\" if \"?\" in query else \"statement\",\n",
    "            \"complexity\": \"high\" if len(words) > 10 else \"medium\" if len(words) > 5 else \"simple\",\n",
    "            \"keywords\": words[:3],  # First 3 words as keywords\n",
    "            \"sentiment\": \"neutral\",\n",
    "            \"word_count\": len(words)\n",
    "        }\n",
    "        \n",
    "        print(f\"  ✅ Analysis complete: {analysis}\")\n",
    "        return analysis\n",
    "\n",
    "\n",
    "    @task\n",
    "    def generate_response(query: str, analysis: Dict[str, Any]) -> str:\n",
    "        \"\"\"Generate a response based on the query and analysis.\"\"\"\n",
    "        print(f\"  🤖 Generating response for: '{query[:50]}...'\")\n",
    "        \n",
    "        keywords = analysis.get(\"keywords\", [])\n",
    "        complexity = analysis.get(\"complexity\", \"medium\")\n",
    "        \n",
    "        if keywords:\n",
    "            if complexity == \"high\":\n",
    "                response = f\"This is a complex question about {', '.join(keywords)}. Let me provide a comprehensive analysis: {query} involves multiple interconnected concepts that require careful consideration...\"\n",
    "            elif complexity == \"medium\":\n",
    "                response = f\"Regarding {', '.join(keywords)}, here's what you should know: This topic involves several key aspects that are important to understand...\"\n",
    "            else:\n",
    "                response = f\"About {', '.join(keywords)}: This is a straightforward topic that can be explained simply...\"\n",
    "        else:\n",
    "            response = \"I'd be happy to help you with information and analysis on any topic you're interested in.\"\n",
    "        \n",
    "        print(f\"  ✅ Response generated: {response[:50]}...\")\n",
    "        return response\n",
    "\n",
    "\n",
    "    @task\n",
    "    def format_final_output(query: str, analysis: Dict[str, Any], response: str) -> Dict[str, Any]:\n",
    "        \"\"\"Format the final output with metadata.\"\"\"\n",
    "        print(\"  📝 Formatting final output...\")\n",
    "        \n",
    "        formatted_output = {\n",
    "            \"original_query\": query,\n",
    "            \"analysis_results\": analysis,\n",
    "            \"generated_response\": response,\n",
    "            \"metadata\": {\n",
    "                \"processing_steps\": [\"analysis\", \"generation\", \"formatting\"],\n",
    "                \"response_length\": len(response),\n",
    "                \"keywords_found\": len(analysis.get(\"keywords\", [])),\n",
    "                \"complexity_level\": analysis.get(\"complexity\", \"unknown\")\n",
    "            },\n",
    "            \"status\": \"completed\"\n",
    "        }\n",
    "        \n",
    "        print(\"  ✅ Formatting complete\")\n",
    "        return formatted_output\n",
    "\n",
    "\n",
    "    # Define the main workflow using @entrypoint\n",
    "    @entrypoint(checkpointer=MemorySaver())\n",
    "    def intelligent_qa_workflow(input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Main workflow that orchestrates query analysis and response generation.\n",
    "        \n",
    "        This is the entrypoint that coordinates all the @task functions.\n",
    "        Each task returns a future that must be resolved with .result().\n",
    "        \"\"\"\n",
    "        query = input_data.get(\"query\", \"\")\n",
    "        \n",
    "        print(f\"🎬 Starting intelligent Q&A workflow for: '{query[:50]}...'\")\n",
    "        \n",
    "        # Step 1: Analyze the query (returns a future)\n",
    "        analysis_future = analyze_query(query)\n",
    "        \n",
    "        # Step 2: Resolve the analysis future to get the actual result\n",
    "        analysis = analysis_future.result()\n",
    "        \n",
    "        # Step 3: Generate response based on query and analysis (returns a future)\n",
    "        response_future = generate_response(query, analysis)\n",
    "        \n",
    "        # Step 4: Resolve the response future\n",
    "        response = response_future.result()\n",
    "        \n",
    "        # Step 5: Format the final output (returns a future)\n",
    "        formatted_future = format_final_output(query, analysis, response)\n",
    "        \n",
    "        # Step 6: Resolve the final formatting future\n",
    "        final_result = formatted_future.result()\n",
    "        \n",
    "        print(f\"✅ Workflow completed successfully\")\n",
    "        return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = {\n",
    "        \"query\": \"What is machine learning and how does it work in practice?\"\n",
    "    }\n",
    "    \n",
    "config = cast(RunnableConfig, {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"test-thread-1\"\n",
    "    }\n",
    "})\n",
    "\n",
    "tru_task_app = TruGraph(intelligent_qa_workflow, app_name=\"Functional api app\")\n",
    "with tru_task_app as recording:\n",
    "    direct_result = intelligent_qa_workflow.invoke(test_input, config)\n",
    "\n",
    "session.force_flush()\n",
    "print(f\"\\n📊 Direct Execution Results:\")\n",
    "print(f\"  Query: {direct_result['original_query']}\")\n",
    "print(f\"  Analysis: {direct_result['analysis_results']}\")\n",
    "print(f\"  Response: {direct_result['generated_response'][:100]}...\")\n",
    "print(f\"  Metadata: {direct_result['metadata']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing OTel Compatibility\n",
    "\n",
    "Now let's test TruGraph with OpenTelemetry tracing enabled:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test OTel Mode Compatibility\n",
    "if LANGGRAPH_AVAILABLE:\n",
    "    import os\n",
    "    \n",
    "    print(\"🧪 Testing OTel Mode Compatibility:\")\n",
    "    \n",
    "    # Note: Enable OTel tracing (commented out to avoid conflicts in tutorial)\n",
    "    # Uncomment the next line to test OTel mode:\n",
    "    os.environ[\"TRULENS_OTEL_TRACING\"] = \"1\"\n",
    "    \n",
    "    # For demonstration, show how OTel would be enabled\n",
    "    print(\"  To enable OTel mode:\")\n",
    "    print('  os.environ[\"TRULENS_OTEL_TRACING\"] = \"1\"')\n",
    "    print()\n",
    "    \n",
    "    # Create a fresh session for OTel testing\n",
    "    from trulens.core.session import TruSession\n",
    "    otel_session = TruSession()\n",
    "    \n",
    "    try:\n",
    "        otel_tru_app = otel_session.App(graph, app_name=\"OTelCompatibilityTest\")\n",
    "        print(f\"✅ TruGraph created successfully: {type(otel_tru_app)}\")\n",
    "        print(f\"   OTel Mode would automatically:\")\n",
    "        print(f\"   - Detect main method: invoke or run\")\n",
    "        print(f\"   - Use OpenTelemetry spans instead of traditional instrumentation\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating TruGraph: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ Skipping OTel test - LangGraph not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LANGGRAPH_AVAILABLE:\n",
    "    # Test the detection\n",
    "    session = TruSession()\n",
    "    session.reset_database()\n",
    "    \n",
    "    print(\"🔍 Testing LangGraph Detection:\")\n",
    "    print(f\"  Module check: {graph.__module__.startswith('langgraph')}\")\n",
    "    print(f\"  Type check: {session._is_langgraph_app(graph)}\")\n",
    "    print(f\"  Has graph attr: {hasattr(graph, 'graph')}\")\n",
    "    print(f\"  Has invoke method: {hasattr(graph, 'invoke')}\")\n",
    "    \n",
    "    # Test automatic detection\n",
    "    print(\"\\n🎯 Testing Automatic Detection:\")\n",
    "    tru_app = session.App(graph, app_name=\"AutoDetectionTest\")\n",
    "    \n",
    "    print(f\"  Created type: {type(tru_app)}\")\n",
    "    print(f\"  Is TruGraph: {isinstance(tru_app, TruGraph)}\")\n",
    "    print(f\"  App name: {tru_app.app_name}\")\n",
    "    \n",
    "    if isinstance(tru_app, TruGraph):\n",
    "        print(\"✅ SUCCESS: Auto-detection worked!\")\n",
    "        \n",
    "        # Test basic functionality\n",
    "        test_input = {\"messages\": [HumanMessage(content=\"What is AI?\")]}\n",
    "        result = tru_app.app.invoke(test_input)\n",
    "        print(f\"  Test result: {result['messages'][-1].content[:50]}...\")\n",
    "        \n",
    "        with tru_app as recording:\n",
    "            result = tru_app.app.invoke(test_input)\n",
    "        session.force_flush()\n",
    "    else:\n",
    "        print(\"❌ FAILED: Auto-detection didn't work\")\n",
    "        \n",
    "    print(\"\\n🎉 TruGraph Tutorial Complete!\")\n",
    "    print(\"For more examples, check the TruLens documentation at https://trulens.org/\")\n",
    "else:\n",
    "    print(\"❌ Skipping detection test - LangGraph not available\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
