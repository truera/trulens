{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _LangGraph_ Quickstart\n",
    "\n",
    "In this quickstart you will create a multi-agent collaboration system with LangGraph and learn how to log it and get feedback on an LLM response.\n",
    "\n",
    "For evaluation, we will leverage the RAG triad of groundedness, context relevance and answer relevance.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/examples/expositional/frameworks/langchain/langgraph_quickstart.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### Add API keys\n",
    "For this quickstart you will need a [Tavily](https://app.tavily.com/home) and OpenAI key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install trulens langgraph trulens-providers-openai openai matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"tvly-dev-...\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\"\n",
    "os.environ[\"TRULENS_OTEL_TRACING\"] = \"1\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import from TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import TruSession\n",
    "\n",
    "session = TruSession()\n",
    "session.reset_database()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain.load.dump import dumps\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from trulens.core.otel.instrument import instrument\n",
    "from trulens.otel.semconv.trace import SpanAttributes\n",
    "\n",
    "\n",
    "tavily_tool = TavilySearchResults(max_results=5)\n",
    "\n",
    "# Warning: This executes code locally, which can be unsafe when not sandboxed\n",
    "\n",
    "repl = PythonREPL()\n",
    "\n",
    "\n",
    "@tool\n",
    "def python_repl_tool(\n",
    "    code: Annotated[str, \"The python code to execute to generate your chart.\"],\n",
    "):\n",
    "    \"\"\"Use this to execute python code. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    result_str = (\n",
    "        f\"Successfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\"\n",
    "    )\n",
    "    return (\n",
    "        result_str\n",
    "        + \"\\n\\nIf you have completed all tasks, respond with FINAL ANSWER.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Literal\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.types import Command\n",
    "from trulens.otel.semconv.trace import BASE_SCOPE\n",
    "from trulens.apps.langgraph.inline_evaluations import inline_evaluation\n",
    "import numpy as np\n",
    "from trulens.core import Feedback\n",
    "from trulens.core.feedback.selector import Selector\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model=\"o4-mini-2025-04-16\")\n",
    "\n",
    "@instrument()\n",
    "def orchestration_node(state: MessagesState) -> Command:\n",
    "    # Find the most recent user query\n",
    "    user_query = None\n",
    "    user_index = None\n",
    "    for i, msg in reversed(list(enumerate(state[\"messages\"]))):\n",
    "        if isinstance(msg, HumanMessage) and getattr(msg, \"name\", None) is None:\n",
    "            user_query = msg.content\n",
    "            user_index = i\n",
    "            break\n",
    "    if user_query is None:\n",
    "        user_query = state[\"messages\"][0].content\n",
    "        user_index = 0\n",
    "\n",
    "    # Find the most recent research answer after the user query\n",
    "    research_answer = None\n",
    "    for msg in reversed(state[\"messages\"][user_index+1:]):\n",
    "        if getattr(msg, \"name\", None) in (\"web_researcher\", \"cortex_agents_researcher\"):\n",
    "            research_answer = msg.content\n",
    "            break\n",
    "\n",
    "    # Compose prompt for the LLM\n",
    "    prompt = (\n",
    "        \"You are an orchestration agent for a multi-agent data assistant.\\n\"\n",
    "        \"Given the user query and the most recent research answer, decide if the research answer contains all the information needed to answer the query, or if more research is needed.\\n\"\n",
    "        \"- Respond 'done' if the research answer is sufficient to answer the query and no chart is requested or if a chart has been generated.\\n\"\n",
    "        \"- Respond 'search' if more information is needed.\\n\"\n",
    "        \"- Respond 'chart' if a chart is requested and the research answer is sufficient to generate a chart.\\n\"\n",
    "        \"\\n\"\n",
    "        \"User query:\\n\"\n",
    "        f\"{user_query}\\n\"\n",
    "        \"\\n\"\n",
    "        \"Most recent research answer:\\n\"\n",
    "        f\"{research_answer if research_answer else '[No research answer yet]'}\\n\"\n",
    "        \"\\n\"\n",
    "        \"Respond with only one word: 'cortex', 'web', or 'none'.\"\n",
    "    )\n",
    "    decision = llm.invoke(prompt).content.strip().lower()\n",
    "\n",
    "    # Route accordingly\n",
    "    if \"search\" in decision:\n",
    "        return Command(update={}, goto=\"researcher\")\n",
    "    elif \"done\" in decision:\n",
    "        return Command(update={}, goto=END)\n",
    "    elif \"chart\" in decision:\n",
    "        return Command(update={}, goto=\"chart_generator\")\n",
    "    else:\n",
    "        return Command(update={}, goto=\"chart_generator\")\n",
    "    \n",
    "def get_next_node(last_message: BaseMessage, goto: str):\n",
    "    if \"FINAL ANSWER\" in last_message.content:\n",
    "        # Any agent decided the work is done\n",
    "        return END\n",
    "    return goto\n",
    "\n",
    "\n",
    "def make_system_prompt(suffix: str) -> str:\n",
    "    return (\n",
    "        \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "        \" Use the provided tools to progress towards answering the question.\"\n",
    "        \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "        \" will help where you left off. Execute what you can to make progress.\"\n",
    "        \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "        \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "        f\"\\n{suffix}\"\n",
    "    )\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "\n",
    "def get_next_node(last_message: BaseMessage, goto: str):\n",
    "    if \"FINAL ANSWER\" in last_message.content:\n",
    "        # Any agent decided the work is done\n",
    "        return END\n",
    "    return goto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Research agent and node\n",
    "research_agent = create_react_agent(\n",
    "    llm,\n",
    "    tools=[tavily_tool],\n",
    "    prompt=make_system_prompt(\n",
    "        \"You can only do research. You are working with a chart generator colleague.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "@instrument(\n",
    "    span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
    "    attributes=lambda ret, exception, *args, **kwargs: {\n",
    "        SpanAttributes.RETRIEVAL.QUERY_TEXT: args[0][\"messages\"][-1].content,\n",
    "        SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS: [\n",
    "            json.loads(dumps(message)).get(\"kwargs\", {}).get(\"content\", \"\")\n",
    "            for message in ret.update[\"messages\"]\n",
    "            if isinstance(message, ToolMessage)\n",
    "        ]\n",
    "        if hasattr(ret, \"update\")\n",
    "        else \"No tool call\",\n",
    "    },\n",
    ")\n",
    "def research_node(\n",
    "    state: MessagesState,\n",
    ") -> Command[Literal[\"orchestrator\", END]]:\n",
    "    result = research_agent.invoke(state)\n",
    "    goto = get_next_node(result[\"messages\"][-1], \"orchestrator\")\n",
    "    # wrap in a human message, as not all providers allow\n",
    "    # AI message at the last position of the input messages list\n",
    "    result[\"messages\"][-1] = HumanMessage(\n",
    "        content=result[\"messages\"][-1].content, name=\"researcher\"\n",
    "    )\n",
    "    return Command(\n",
    "        update={\n",
    "            # share internal message history of research agent with other agents\n",
    "            \"messages\": result[\"messages\"],\n",
    "        },\n",
    "        goto=goto,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chart Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart generator agent and node\n",
    "# NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION, WHICH CAN BE UNSAFE WHEN NOT SANDBOXED\n",
    "chart_agent = create_react_agent(\n",
    "    llm,\n",
    "    [python_repl_tool],\n",
    "    prompt=make_system_prompt(\n",
    "        \"You can only generate charts. You are working with a researcher colleague.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "@instrument(\n",
    "    span_type=\"CHART_GENERATOR_NODE\",\n",
    "    attributes=lambda ret, exception, *args, **kwargs: {\n",
    "        f\"{BASE_SCOPE}.chart_node_input\": args[0][\"messages\"][-1].content,\n",
    "        f\"{BASE_SCOPE}.chart_node_response\": (\n",
    "            ret.update[\"messages\"][-1].content\n",
    "            if ret and hasattr(ret, \"update\") and ret.update\n",
    "            else \"No update response\"\n",
    "        ),\n",
    "    },\n",
    ")\n",
    "def chart_node(state: MessagesState) -> Command[Literal[\"researcher\", END]]:\n",
    "    result = chart_agent.invoke(state)\n",
    "    goto = get_next_node(result[\"messages\"][-1], \"researcher\")\n",
    "    # wrap in a human message, as not all providers allow\n",
    "    # AI message at the last position of the input messages list\n",
    "    result[\"messages\"][-1] = HumanMessage(\n",
    "        content=result[\"messages\"][-1].content, name=\"chart_generator\"\n",
    "    )\n",
    "    return Command(\n",
    "        update={\n",
    "            # share internal message history of chart agent with other agents\n",
    "            \"messages\": result[\"messages\"],\n",
    "        },\n",
    "        goto=goto,\n",
    "    )\n",
    "\n",
    "\n",
    "chart_summary_agent = create_react_agent(\n",
    "    llm,\n",
    "    tools=[],  # Add image processing tools if available/needed.\n",
    "    prompt=make_system_prompt(\n",
    "        \"You can only generate image captions. You are working with a researcher colleague and a chart generator colleague. \"\n",
    "        + \"Your task is to generate a concise summary for the provided chart image saved at a local PATH, where the PATH should be and only be provided by your chart generator colleague. The summary should be no more than 3 sentences.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "@instrument(\n",
    "    span_type=\"CHART_SUMMARY_NODE\",\n",
    "    attributes=lambda ret, exception, *args, **kwargs: {\n",
    "        f\"{BASE_SCOPE}.summary_node_input\": args[0][\"messages\"][-1].content,\n",
    "        f\"{BASE_SCOPE}.summary_node_output\": ret.update[\"messages\"][-1].content\n",
    "        if hasattr(ret, \"update\")\n",
    "        else \"NO SUMMARY GENERATED\",\n",
    "    },\n",
    ")\n",
    "def chart_summary_node(\n",
    "    state: MessagesState,\n",
    ") -> Command[Literal[\"orchestrator\", END]]:\n",
    "    result = chart_summary_agent.invoke(state)\n",
    "    # After captioning the image, we send control back (e.g., to the researcher)\n",
    "    goto = get_next_node(result[\"messages\"][-1], \"orchestrator\")\n",
    "    # Wrap the output message in a HumanMessage to maintain consistency in the conversation flow.\n",
    "    result[\"messages\"][-1] = HumanMessage(\n",
    "        content=result[\"messages\"][-1].content, name=\"chart_summarizer\"\n",
    "    )\n",
    "    return Command(\n",
    "        update={\"messages\": result[\"messages\"]},\n",
    "        goto=goto,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "from langgraph.graph import START\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"orchestrator\", orchestration_node)\n",
    "workflow.add_node(\"researcher\", research_node)\n",
    "workflow.add_node(\"chart_generator\", chart_node)\n",
    "workflow.add_node(\"chart_summarizer\", chart_summary_node)\n",
    "\n",
    "workflow.add_edge(START, \"orchestrator\")\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrument graph for logging with TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TruAgent:\n",
    "    def __init__(self):\n",
    "        self.graph = graph\n",
    "\n",
    "    @instrument()\n",
    "    def invoke_agent_graph(self, query: str) -> str:\n",
    "            # rebuild the graph for each query\n",
    "        # self.graph = workflow_compile()\n",
    "            # Initialize state with proper message format\n",
    "        state = {\"messages\": [HumanMessage(content=query)]}\n",
    "\n",
    "            # Stream events with recursion limit\n",
    "        events = self.graph.stream(\n",
    "                state,\n",
    "                {\"recursion_limit\": 100},\n",
    "            )\n",
    "\n",
    "            # Track all messages through the conversation\n",
    "        all_messages = []\n",
    "        for event in events:\n",
    "                # Get the payload from the event\n",
    "            _, payload = next(iter(event.items()))\n",
    "            if not payload:  # Skip empty payloads\n",
    "                continue\n",
    "\n",
    "            messages = payload.get(\"messages\")\n",
    "            if not messages:\n",
    "                continue\n",
    "            all_messages.extend(messages)\n",
    "\n",
    "            # Return the last message's content if available\n",
    "        return (\n",
    "            all_messages[-1].content\n",
    "            if all_messages and hasattr(all_messages[-1], \"content\")\n",
    "            else \"\"\n",
    "            )\n",
    "\n",
    "\n",
    "tru_agent = TruAgent()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Feedback Function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from trulens.core import Feedback\n",
    "from trulens.core.feedback.selector import Selector\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "provider = OpenAI(model_engine=\"gpt-4.1\")\n",
    "traj_eval_provider = OpenAI(model_engine=\"o3\")\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(\n",
    "        provider.groundedness_measure_with_cot_reasons, name=\"Groundedness\"\n",
    "    )\n",
    "    .on_context(collect_list=True)\n",
    "    .on_output()\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_answer_relevance = (\n",
    "    Feedback(provider.relevance_with_cot_reasons, name=\"Answer Relevance\")\n",
    "    .on_input()\n",
    "    .on_output()\n",
    ")\n",
    "\n",
    "# Context relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(\n",
    "        provider.context_relevance_with_cot_reasons, name=\"Context Relevance\"\n",
    "    )\n",
    "    .on_input()\n",
    "    .on_context(collect_list=False)\n",
    "    .aggregate(np.mean)  # choose a different aggregation method if you wish\n",
    ")\n",
    "\n",
    "# Trajectory evaluations: step relevance of trace given user query\n",
    "f_step_relevance = Feedback(\n",
    "    provider.trajectory_step_relevance_with_cot_reasons, name=\"Step Relevance\"\n",
    ").on({\n",
    "    \"trace\": Selector(trace_level=True),\n",
    "})\n",
    "\n",
    "# Trajectory evaluations: logical consistency of trace\n",
    "f_logical_consistency = Feedback(\n",
    "    provider.trajectory_logical_consistency_with_cot_reasons,\n",
    "    name=\"Logical Consistency\",\n",
    ").on({\n",
    "    \"trace\": Selector(trace_level=True),\n",
    "})\n",
    "\n",
    "# Trajectory evaluations: workflow efficiency of trace\n",
    "f_workflow_efficiency = Feedback(\n",
    "    provider.trajectory_workflow_efficiency_with_cot_reasons,\n",
    "    name=\"Workflow Efficiency\",\n",
    ").on({\n",
    "    \"trace\": Selector(trace_level=True),\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the Graph with TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.apps.app import TruApp\n",
    "\n",
    "tru_recorder = TruApp(\n",
    "   tru_agent,\n",
    "    app_name=\"Multi-Agent Chart Generation\",\n",
    "    app_version=\"Base\",\n",
    "    feedbacks=[\n",
    "     f_answer_relevance,\n",
    "         f_context_relevance,\n",
    "         f_groundedness,\n",
    "         f_step_relevance,\n",
    "         f_logical_consistency,\n",
    "         f_workflow_efficiency,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_recorder as recording:\n",
    "    # Run the multi-agent graph with a sample query\n",
    "    result = tru_agent.invoke_agent_graph(\n",
    "        \"Generate a chart showing the yearly US GDP and National Debt over the last 100 years. Color the graph based on the political party of the president at the time.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.get_leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "run_dashboard(session=session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add in-line evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@instrument()\n",
    "def orchestration_node(state: MessagesState) -> Command:\n",
    "    # Find the most recent user query\n",
    "    user_query = None\n",
    "    user_index = None\n",
    "    for i, msg in reversed(list(enumerate(state[\"messages\"]))):\n",
    "        if isinstance(msg, HumanMessage) and getattr(msg, \"name\", None) is None:\n",
    "            user_query = msg.content\n",
    "            user_index = i\n",
    "            break\n",
    "    if user_query is None:\n",
    "        user_query = state[\"messages\"][0].content\n",
    "        user_index = 0\n",
    "\n",
    "    # Find the most recent research answer after the user query\n",
    "    research_answer = None\n",
    "    for msg in reversed(state[\"messages\"][user_index+1:]):\n",
    "        if getattr(msg, \"name\", None) in (\"web_researcher\", \"cortex_agents_researcher\"):\n",
    "            research_answer = msg.content\n",
    "            break\n",
    "\n",
    "    # Compose prompt for the LLM\n",
    "    prompt = (\n",
    "        \"You are an orchestration agent for a multi-agent data assistant.\\n\"\n",
    "        \"Given the user query, the most recent research answer, and the evaluation score and explanation of the research if available.\\n\"\n",
    "        \"Decide if the research answer contains all the information needed to answer the query, or if more research is needed.\\n\"\n",
    "        \"- Respond 'done' if the research answer is sufficient to answer the query and no chart is requested or if a chart has been generated.\\n\"\n",
    "        \"- Respond 'search' if more information is needed.\\n\"\n",
    "        \"- Respond 'chart' if a chart is requested and the research answer is sufficient to generate a chart.\\n\"\n",
    "        \"\\n\"\n",
    "        \"User query:\\n\"\n",
    "        f\"{user_query}\\n\"\n",
    "        \"\\n\"\n",
    "        \"Most recent research answer:\\n\"\n",
    "        f\"{research_answer if research_answer else '[No research answer yet]'}\\n\"\n",
    "        \"\\n\"\n",
    "        \"Respond with only one word: 'cortex', 'web', or 'none'.\"\n",
    "    )\n",
    "    decision = llm.invoke(prompt).content.strip().lower()\n",
    "\n",
    "    # Route accordingly\n",
    "    if \"search\" in decision:\n",
    "        return Command(update={}, goto=\"researcher\")\n",
    "    elif \"done\" in decision:\n",
    "        return Command(update={}, goto=END)\n",
    "    elif \"chart\" in decision:\n",
    "        return Command(update={}, goto=\"chart_generator\")\n",
    "    else:\n",
    "        return Command(update={}, goto=\"chart_generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval for in-line evaluation\n",
    "f_context_relevance = (\n",
    "    Feedback(\n",
    "        provider.context_relevance_with_cot_reasons, name=\"Context Relevance\",\n",
    "        criteria = \"Context is fully relevant if it includes all of the information needed to answer the question, regardless of whether any visualizations or charts are included in the context.\"\n",
    "    )\n",
    "    .on({\n",
    "            \"question\": Selector(\n",
    "                span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
    "                span_attribute=SpanAttributes.RETRIEVAL.QUERY_TEXT,\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    .on({\n",
    "            \"context\": Selector(\n",
    "                span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
    "                span_attribute=SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS,\n",
    "                collect_list=False\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "@inline_evaluation(f_context_relevance)\n",
    "@instrument(\n",
    "    span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
    "    attributes=lambda ret, exception, *args, **kwargs: {\n",
    "        SpanAttributes.RETRIEVAL.QUERY_TEXT: args[0][\"messages\"][-1].content,\n",
    "        SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS: [\n",
    "            json.loads(dumps(message)).get(\"kwargs\", {}).get(\"content\", \"\")\n",
    "            for message in ret.update[\"messages\"]\n",
    "            if isinstance(message, ToolMessage)\n",
    "        ]\n",
    "        if hasattr(ret, \"update\")\n",
    "        else \"No tool call\",\n",
    "    },\n",
    ")\n",
    "def research_node(\n",
    "    state: MessagesState,\n",
    ") -> Command[Literal[\"orchestrator\", END]]:\n",
    "    result = research_agent.invoke(state)\n",
    "    goto = get_next_node(result[\"messages\"][-1], \"orchestrator\")\n",
    "    # wrap in a human message, as not all providers allow\n",
    "    # AI message at the last position of the input messages list\n",
    "    result[\"messages\"][-1] = HumanMessage(\n",
    "        content=result[\"messages\"][-1].content, name=\"researcher\"\n",
    "    )\n",
    "    return Command(\n",
    "        update={\n",
    "            # share internal message history of research agent with other agents\n",
    "            \"messages\": result[\"messages\"],\n",
    "        },\n",
    "        goto=goto,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recompile graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "from langgraph.graph import START\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"orchestrator\", orchestration_node)\n",
    "workflow.add_node(\"researcher\", research_node)\n",
    "workflow.add_node(\"chart_generator\", chart_node)\n",
    "workflow.add_node(\"chart_summarizer\", chart_summary_node)\n",
    "\n",
    "workflow.add_edge(START, \"orchestrator\")\n",
    "\n",
    "graph_inline_evals = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TruAgent:\n",
    "    def __init__(self):\n",
    "        self.graph = graph\n",
    "\n",
    "    @instrument()\n",
    "    def invoke_agent_graph(self, query: str) -> str:\n",
    "            # rebuild the graph for each query\n",
    "        # self.graph = workflow_compile()\n",
    "            # Initialize state with proper message format\n",
    "        state = {\"messages\": [HumanMessage(content=query)]}\n",
    "\n",
    "            # Stream events with recursion limit\n",
    "        events = self.graph.stream(\n",
    "                state,\n",
    "                {\"recursion_limit\": 100},\n",
    "            )\n",
    "\n",
    "            # Track all messages through the conversation\n",
    "        all_messages = []\n",
    "        for event in events:\n",
    "                # Get the payload from the event\n",
    "            _, payload = next(iter(event.items()))\n",
    "            if not payload:  # Skip empty payloads\n",
    "                continue\n",
    "\n",
    "            messages = payload.get(\"messages\")\n",
    "            if not messages:\n",
    "                continue\n",
    "            all_messages.extend(messages)\n",
    "\n",
    "            # Return the last message's content if available\n",
    "        return (\n",
    "            all_messages[-1].content\n",
    "            if all_messages and hasattr(all_messages[-1], \"content\")\n",
    "            else \"\"\n",
    "            )\n",
    "\n",
    "\n",
    "multi_agent_graph_inline_evals = TruAgent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register new version of the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.apps.app import TruApp\n",
    "\n",
    "tru_recorder_inline_evals = TruApp(\n",
    "    multi_agent_graph_inline_evals,\n",
    "    app_name=\"Multi-Agent Chart Generation\",\n",
    "    app_version=\"in-line evals\",\n",
    "    feedbacks=[\n",
    "     f_answer_relevance,\n",
    "         f_context_relevance,\n",
    "         f_groundedness,\n",
    "         f_step_relevance,\n",
    "         f_logical_consistency,\n",
    "         f_workflow_efficiency,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the new version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_recorder_inline_evals as recording:\n",
    "    # Run the multi-agent graph with a sample query\n",
    "    result = multi_agent_graph_inline_evals.invoke_agent_graph(\n",
    "        \"Generate a chart showing the yearly US GDP and National Debt over the last 100 years. Color the graph based on the political party of the president at the time.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate performance improvements\n",
    "\n",
    "Wait some time for all evaluations to be completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.get_leaderboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oss_rag_stack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
