{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruLens-Canopy Quickstart\n",
    "\n",
    " Canopy is an open-source framework and context engine built on top of the Pinecone vector database so you can build and host your own production-ready chat assistant at any scale. By integrating TruLens into your Canopy assistant, you can quickly iterate on and gain confidence in the quality of your chat assistant.\n",
    "\n",
    " [![Open In\n",
    "Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/expositional/frameworks/canopy/canopy_quickstart.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU canopy-sdk trulens-eval cohere ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "assert (\n",
    "    numpy.__version__ >= \"1.26\"\n",
    "), \"Numpy version did not updated, if you are working on Colab please restart the session.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = (\n",
    "    \"YOUR_PINECONE_API_KEY\"  # take free trial key from https://app.pinecone.io/\n",
    ")\n",
    "os.environ[\"OPENAI_API_KEY\"] = (\n",
    "    \"YOUR_OPENAI_API_KEY\"  # take free trial key from https://platform.openai.com/api-keys\n",
    ")\n",
    "os.environ[\"CO_API_KEY\"] = (\n",
    "    \"YOUR_COHERE_API_KEY\"  # take free trial key from https://dashboard.cohere.com/api-keys\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    os.environ[\"PINECONE_API_KEY\"] != \"YOUR_PINECONE_API_KEY\"\n",
    "), \"please provide PINECONE API key\"\n",
    "assert (\n",
    "    os.environ[\"OPENAI_API_KEY\"] != \"YOUR_OPENAI_API_KEY\"\n",
    "), \"please provide OpenAI API key\"\n",
    "assert (\n",
    "    os.environ[\"CO_API_KEY\"] != \"YOUR_COHERE_API_KEY\"\n",
    "), \"please provide Cohere API key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import PodSpec\n",
    "\n",
    "# Defines the cloud and region where the index should be deployed\n",
    "# Read more about it here - https://docs.pinecone.io/docs/create-an-index\n",
    "spec = PodSpec(environment=\"gcp-starter\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "Downloading Pinecone's documentation as data to ingest to our Canopy chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data = pd.read_parquet(\n",
    "    \"https://storage.googleapis.com/pinecone-datasets-dev/pinecone_docs_ada-002/raw/file1.parquet\"\n",
    ")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    data[\"text\"][50][:847]\n",
    "    .replace(\"\\n\\n\", \"\\n\")\n",
    "    .replace(\"[Suggest Edits](/edit/limits)\", \"\")\n",
    "    + \"\\n......\"\n",
    ")\n",
    "print(\"source: \", data[\"source\"][50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canopy.tokenizer import Tokenizer\n",
    "\n",
    "Tokenizer.initialize()\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.tokenize(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Load Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canopy.knowledge_base import KnowledgeBase\n",
    "from canopy.knowledge_base import list_canopy_indexes\n",
    "from canopy.models.data_models import Document\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "index_name = \"pinecone-docs\"\n",
    "\n",
    "kb = KnowledgeBase(index_name)\n",
    "\n",
    "if not any(name.endswith(index_name) for name in list_canopy_indexes()):\n",
    "    kb.create_canopy_index(spec=spec)\n",
    "\n",
    "kb.connect()\n",
    "\n",
    "documents = [Document(**row) for _, row in data.iterrows()]\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(documents), batch_size)):\n",
    "    kb.upsert(documents[i : i + batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create context and chat engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canopy.context_engine import ContextEngine\n",
    "\n",
    "context_engine = ContextEngine(kb)\n",
    "\n",
    "from canopy.chat_engine import ChatEngine\n",
    "\n",
    "chat_engine = ChatEngine(context_engine)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API for chat is exactly the same as for OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canopy.models.data_models import UserMessage\n",
    "\n",
    "chat_history = [\n",
    "    UserMessage(\n",
    "        content=\"What is the the maximum top-k for a query to Pinecone?\"\n",
    "    )\n",
    "]\n",
    "\n",
    "chat_engine.chat(chat_history).choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrument static methods used by engine with TruLens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "from canopy.context_engine import ContextEngine\n",
    "from trulens.core.app.custom import instrument\n",
    "\n",
    "instrument.method(ContextEngine, \"query\")\n",
    "\n",
    "from canopy.chat_engine import ChatEngine\n",
    "\n",
    "instrument.method(ChatEngine, \"chat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create feedback functions using instrumented methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import Tru\n",
    "\n",
    "tru = Tru(database_redact_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from trulens.core import Feedback\n",
    "from trulens.core import Select\n",
    "from trulens.ext.provider.openai import OpenAI as fOpenAI\n",
    "\n",
    "# Initialize provider class\n",
    "provider = fOpenAI()\n",
    "\n",
    "grounded = Groundedness(groundedness_provider=provider)\n",
    "\n",
    "intput = Select.RecordCalls.chat.args.messages[0].content\n",
    "context = (\n",
    "    Select.RecordCalls.context_engine.query.rets.content.root[:]\n",
    "    .snippets[:]\n",
    "    .text\n",
    ")\n",
    "output = Select.RecordCalls.chat.rets.choices[0].message.content\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(\n",
    "        provider.groundedness_measure_with_cot_reasons,\n",
    "        name=\"Groundedness\",\n",
    "        higher_is_better=True,\n",
    "    )\n",
    "    .on(context.collect())\n",
    "    .on(output)\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = (\n",
    "    Feedback(\n",
    "        provider.relevance_with_cot_reasons,\n",
    "        name=\"Answer Relevance\",\n",
    "        higher_is_better=True,\n",
    "    )\n",
    "    .on(intput)\n",
    "    .on(output)\n",
    ")\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(\n",
    "        provider.context_relevance_with_cot_reasons,\n",
    "        name=\"Context Relevance\",\n",
    "        higher_is_better=True,\n",
    "    )\n",
    "    .on(intput)\n",
    "    .on(context)\n",
    "    .aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create recorded app and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import TruCustomApp\n",
    "\n",
    "app_id = \"canopy default\"\n",
    "tru_recorder = TruCustomApp(\n",
    "    chat_engine,\n",
    "    app_id=app_id,\n",
    "    feedbacks=[f_groundedness, f_qa_relevance, f_context_relevance],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canopy.models.data_models import UserMessage\n",
    "\n",
    "queries = [\n",
    "    [\n",
    "        UserMessage(\n",
    "            content=\"What is the maximum dimension for a dense vector in Pinecone?\"\n",
    "        )\n",
    "    ],\n",
    "    [UserMessage(content=\"How can you get started with Pinecone and TruLens?\")],\n",
    "    [\n",
    "        UserMessage(\n",
    "            content=\"What is the the maximum top-k for a query to Pinecone?\"\n",
    "        )\n",
    "    ],\n",
    "]\n",
    "\n",
    "answers = []\n",
    "\n",
    "for query in queries:\n",
    "    with tru_recorder as recording:\n",
    "        response = chat_engine.chat(query)\n",
    "        answers.append(response.choices[0].message.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we got the wrong answer, the limits for sparse vectors instead of dense vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(queries[0][0].content + \"\\n\")\n",
    "print(answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_leaderboard(app_ids=[app_id])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Canopy with Cohere reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canopy.knowledge_base.reranker.cohere import CohereReranker\n",
    "\n",
    "kb = KnowledgeBase(\n",
    "    index_name=index_name, reranker=CohereReranker(top_n=3), default_top_k=30\n",
    ")\n",
    "kb.connect()\n",
    "\n",
    "reranker_chat_engine = ChatEngine(ContextEngine(kb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranking_app_id = \"canopy_reranking\"\n",
    "reranking_tru_recorder = TruCustomApp(\n",
    "    reranker_chat_engine,\n",
    "    app_id=reranking_app_id,\n",
    "    feedbacks=[f_groundedness, f_qa_relevance, f_context_relevance],\n",
    ")\n",
    "\n",
    "answers = []\n",
    "\n",
    "for query in queries:\n",
    "    with reranking_tru_recorder as recording:\n",
    "        answers.append(\n",
    "            reranker_chat_engine.chat(query).choices[0].message.content\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With reranking we get the right answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(queries[0][0].content + \"\\n\")\n",
    "print(answers[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the effect of reranking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_leaderboard(app_ids=[app_id, reranking_app_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore more in the TruLens dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "run_dashboard(tru)\n",
    "\n",
    "# stop_dashboard(tru) # stop if needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b49ef6d0b3ca0fd6117ebbca48c3d697c422d5d25bd8bdbbbbafb3db0f51ca63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
