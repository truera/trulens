{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruLens-Canopy Quickstart\n",
    "\n",
    " Canopy is an open-source framework and context engine built on top of the Pinecone vector database so you can build and host your own production-ready chat assistant at any scale. By integrating TruLens into your Canopy assistant, you can quickly iterate on and gain confidence in the quality of your chat assistant.\n",
    "\n",
    " [![Open In\n",
    "Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/expositional/frameworks/canopy/canopy_quickstart.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU canopy-sdk trulens-eval cohere ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "assert (\n",
    "    numpy.__version__ >= \"1.26\"\n",
    "), \"Numpy version did not updated, if you are working on Colab please restart the session.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = (\n",
    "    \"YOUR_PINECONE_API_KEY\"  # take free trial key from https://app.pinecone.io/\n",
    ")\n",
    "os.environ[\"OPENAI_API_KEY\"] = (\n",
    "    \"YOUR_OPENAI_API_KEY\"  # take free trial key from https://platform.openai.com/api-keys\n",
    ")\n",
    "os.environ[\"CO_API_KEY\"] = (\n",
    "    \"YOUR_COHERE_API_KEY\"  # take free trial key from https://dashboard.cohere.com/api-keys\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    os.environ[\"PINECONE_API_KEY\"] != \"YOUR_PINECONE_API_KEY\"\n",
    "), \"please provide PINECONE API key\"\n",
    "assert (\n",
    "    os.environ[\"OPENAI_API_KEY\"] != \"YOUR_OPENAI_API_KEY\"\n",
    "), \"please provide OpenAI API key\"\n",
    "assert (\n",
    "    os.environ[\"CO_API_KEY\"] != \"YOUR_COHERE_API_KEY\"\n",
    "), \"please provide Cohere API key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import PodSpec\n",
    "\n",
    "# Defines the cloud and region where the index should be deployed\n",
    "# Read more about it here - https://docs.pinecone.io/docs/create-an-index\n",
    "spec = PodSpec(environment=\"gcp-starter\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "Downloading Pinecone's documentation as data to ingest to our Canopy chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>728aeea1-1dcf-5d0a-91f2-ecccd4dd4272</td>\n",
       "      <td># Scale indexes\\n\\n[Suggest Edits](/edit/scali...</td>\n",
       "      <td>https://docs.pinecone.io/docs/scaling-indexes</td>\n",
       "      <td>{'created_at': '2023_10_25', 'title': 'scaling...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2f19f269-171f-5556-93f3-a2d7eabbe50f</td>\n",
       "      <td># Understanding organizations\\n\\n[Suggest Edit...</td>\n",
       "      <td>https://docs.pinecone.io/docs/organizations</td>\n",
       "      <td>{'created_at': '2023_10_25', 'title': 'organiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b2a71cb3-5148-5090-86d5-7f4156edd7cf</td>\n",
       "      <td># Manage datasets\\n\\n[Suggest Edits](/edit/dat...</td>\n",
       "      <td>https://docs.pinecone.io/docs/datasets</td>\n",
       "      <td>{'created_at': '2023_10_25', 'title': 'datasets'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1dafe68a-2e78-57f7-a97a-93e043462196</td>\n",
       "      <td># Architecture\\n\\n[Suggest Edits](/edit/archit...</td>\n",
       "      <td>https://docs.pinecone.io/docs/architecture</td>\n",
       "      <td>{'created_at': '2023_10_25', 'title': 'archite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8b07b24d-4ec2-58a1-ac91-c8e6267b9ffd</td>\n",
       "      <td># Moving to production\\n\\n[Suggest Edits](/edi...</td>\n",
       "      <td>https://docs.pinecone.io/docs/moving-to-produc...</td>\n",
       "      <td>{'created_at': '2023_10_25', 'title': 'moving-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id   \n",
       "0  728aeea1-1dcf-5d0a-91f2-ecccd4dd4272  \\\n",
       "1  2f19f269-171f-5556-93f3-a2d7eabbe50f   \n",
       "2  b2a71cb3-5148-5090-86d5-7f4156edd7cf   \n",
       "3  1dafe68a-2e78-57f7-a97a-93e043462196   \n",
       "4  8b07b24d-4ec2-58a1-ac91-c8e6267b9ffd   \n",
       "\n",
       "                                                text   \n",
       "0  # Scale indexes\\n\\n[Suggest Edits](/edit/scali...  \\\n",
       "1  # Understanding organizations\\n\\n[Suggest Edit...   \n",
       "2  # Manage datasets\\n\\n[Suggest Edits](/edit/dat...   \n",
       "3  # Architecture\\n\\n[Suggest Edits](/edit/archit...   \n",
       "4  # Moving to production\\n\\n[Suggest Edits](/edi...   \n",
       "\n",
       "                                              source   \n",
       "0      https://docs.pinecone.io/docs/scaling-indexes  \\\n",
       "1        https://docs.pinecone.io/docs/organizations   \n",
       "2             https://docs.pinecone.io/docs/datasets   \n",
       "3         https://docs.pinecone.io/docs/architecture   \n",
       "4  https://docs.pinecone.io/docs/moving-to-produc...   \n",
       "\n",
       "                                            metadata  \n",
       "0  {'created_at': '2023_10_25', 'title': 'scaling...  \n",
       "1  {'created_at': '2023_10_25', 'title': 'organiz...  \n",
       "2  {'created_at': '2023_10_25', 'title': 'datasets'}  \n",
       "3  {'created_at': '2023_10_25', 'title': 'archite...  \n",
       "4  {'created_at': '2023_10_25', 'title': 'moving-...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data = pd.read_parquet(\n",
    "    \"https://storage.googleapis.com/pinecone-datasets-dev/pinecone_docs_ada-002/raw/file1.parquet\"\n",
    ")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Limits\n",
      "This is a summary of current Pinecone limitations. For many of these, there is a workaround or we're working on increasing the limits.\n",
      "\n",
      "## Upserts\n",
      "\n",
      "Max vector dimensionality is 20,000.\n",
      "\n",
      "Max size for an upsert request is 2MB. Recommended upsert limit is 100 vectors per request.\n",
      "\n",
      "Vectors may not be visible to queries immediately after upserting. You can check if the vectors were indexed by looking at the total with `describe_index_stats()`, although this method may not work if the index has multiple replicas. Pinecone is eventually consistent.\n",
      "\n",
      "Pinecone supports sparse vector values of sizes up to 1000 non-zero values.\n",
      "\n",
      "## Queries\n",
      "\n",
      "Max value for `top_k`, the number of results to return, is 10,000. Max value for `top_k` for queries with `include_metadata=True` or `include_data=True` is 1,000.\n",
      "\n",
      "......\n",
      "source:  https://docs.pinecone.io/docs/limits\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    data[\"text\"][50][:847]\n",
    "    .replace(\"\\n\\n\", \"\\n\")\n",
    "    .replace(\"[Suggest Edits](/edit/limits)\", \"\")\n",
    "    + \"\\n......\"\n",
    ")\n",
    "print(\"source: \", data[\"source\"][50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ' world', '!']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from canopy.tokenizer import Tokenizer\n",
    "\n",
    "Tokenizer.initialize()\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.tokenize(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Load Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a6d6b172f14794a03956a0dcf55b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from canopy.knowledge_base import KnowledgeBase\n",
    "from canopy.knowledge_base import list_canopy_indexes\n",
    "from canopy.models.data_models import Document\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "index_name = \"pinecone-docs\"\n",
    "\n",
    "kb = KnowledgeBase(index_name)\n",
    "\n",
    "if not any(name.endswith(index_name) for name in list_canopy_indexes()):\n",
    "    kb.create_canopy_index(spec=spec)\n",
    "\n",
    "kb.connect()\n",
    "\n",
    "documents = [Document(**row) for _, row in data.iterrows()]\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(documents), batch_size)):\n",
    "    kb.upsert(documents[i : i + batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create context and chat engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canopy.context_engine import ContextEngine\n",
    "\n",
    "context_engine = ContextEngine(kb)\n",
    "\n",
    "from canopy.chat_engine import ChatEngine\n",
    "\n",
    "chat_engine = ChatEngine(context_engine)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API for chat is exactly the same as for OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The maximum value for `top_k` in a Pinecone query is 10,000. However, when the query includes metadata or data, the maximum value for `top_k` is limited to 1,000.\\n(Source: https://docs.pinecone.io/docs/limits)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from canopy.models.data_models import UserMessage\n",
    "\n",
    "chat_history = [\n",
    "    UserMessage(\n",
    "        content=\"What is the the maximum top-k for a query to Pinecone?\"\n",
    "    )\n",
    "]\n",
    "\n",
    "chat_engine.chat(chat_history).choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrument static methods used by engine with TruLens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "from canopy.context_engine import ContextEngine\n",
    "from trulens.core.app.custom import instrument\n",
    "\n",
    "instrument.method(ContextEngine, \"query\")\n",
    "\n",
    "from canopy.chat_engine import ChatEngine\n",
    "\n",
    "instrument.method(ChatEngine, \"chat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create feedback functions using instrumented methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n",
      "ðŸ”’ Secret keys will not be included in the database.\n"
     ]
    }
   ],
   "source": [
    "from trulens.core import Tru\n",
    "\n",
    "tru = Tru(database_redact_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In Groundedness, input source will be set to __record__.app.context_engine.query.rets.content.root[:].snippets[:].text.collect() .\n",
      "âœ… In Groundedness, input statement will be set to __record__.app.chat.rets.choices[0].message.content .\n",
      "âœ… In Answer Relevance, input prompt will be set to __record__.app.chat.args.messages[0].content .\n",
      "âœ… In Answer Relevance, input response will be set to __record__.app.chat.rets.choices[0].message.content .\n",
      "âœ… In Context Relevance, input question will be set to __record__.app.chat.args.messages[0].content .\n",
      "âœ… In Context Relevance, input statement will be set to __record__.app.context_engine.query.rets.content.root[:].snippets[:].text .\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from trulens.core import Feedback\n",
    "from trulens.core import Select\n",
    "from trulens.ext.provider.openai import OpenAI as fOpenAI\n",
    "\n",
    "# Initialize provider class\n",
    "provider = fOpenAI()\n",
    "\n",
    "grounded = Groundedness(groundedness_provider=provider)\n",
    "\n",
    "intput = Select.RecordCalls.chat.args.messages[0].content\n",
    "context = (\n",
    "    Select.RecordCalls.context_engine.query.rets.content.root[:]\n",
    "    .snippets[:]\n",
    "    .text\n",
    ")\n",
    "output = Select.RecordCalls.chat.rets.choices[0].message.content\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(\n",
    "        provider.groundedness_measure_with_cot_reasons,\n",
    "        name=\"Groundedness\",\n",
    "        higher_is_better=True,\n",
    "    )\n",
    "    .on(context.collect())\n",
    "    .on(output)\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = (\n",
    "    Feedback(\n",
    "        provider.relevance_with_cot_reasons,\n",
    "        name=\"Answer Relevance\",\n",
    "        higher_is_better=True,\n",
    "    )\n",
    "    .on(intput)\n",
    "    .on(output)\n",
    ")\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(\n",
    "        provider.context_relevance_with_cot_reasons,\n",
    "        name=\"Context Relevance\",\n",
    "        higher_is_better=True,\n",
    "    )\n",
    "    .on(intput)\n",
    "    .on(context)\n",
    "    .aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create recorded app and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import TruCustomApp\n",
    "\n",
    "app_id = \"canopy default\"\n",
    "tru_recorder = TruCustomApp(\n",
    "    chat_engine,\n",
    "    app_id=app_id,\n",
    "    feedbacks=[f_groundedness, f_qa_relevance, f_context_relevance],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canopy.models.data_models import UserMessage\n",
    "\n",
    "queries = [\n",
    "    [\n",
    "        UserMessage(\n",
    "            content=\"What is the maximum dimension for a dense vector in Pinecone?\"\n",
    "        )\n",
    "    ],\n",
    "    [UserMessage(content=\"How can you get started with Pinecone and TruLens?\")],\n",
    "    [\n",
    "        UserMessage(\n",
    "            content=\"What is the the maximum top-k for a query to Pinecone?\"\n",
    "        )\n",
    "    ],\n",
    "]\n",
    "\n",
    "answers = []\n",
    "\n",
    "for query in queries:\n",
    "    with tru_recorder as recording:\n",
    "        response = chat_engine.chat(query)\n",
    "        answers.append(response.choices[0].message.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we got the wrong answer, the limits for sparse vectors instead of dense vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the maximum dimension for a dense vector in Pinecone?\n",
      "\n",
      "The maximum dimension for a dense vector in Pinecone is 4 billion dimensions.\n"
     ]
    }
   ],
   "source": [
    "print(queries[0][0].content + \"\\n\")\n",
    "print(answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Groundedness</th>\n",
       "      <th>Context Relevance</th>\n",
       "      <th>Answer Relevance</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>canopy default</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.769524</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.002687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Groundedness  Context Relevance  Answer Relevance   latency   \n",
       "app_id                                                                        \n",
       "canopy default      0.333333           0.769524          0.966667  3.333333  \\\n",
       "\n",
       "                total_cost  \n",
       "app_id                      \n",
       "canopy default    0.002687  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.get_leaderboard(app_ids=[app_id])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Canopy with Cohere reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canopy.knowledge_base.reranker.cohere import CohereReranker\n",
    "\n",
    "kb = KnowledgeBase(\n",
    "    index_name=index_name, reranker=CohereReranker(top_n=3), default_top_k=30\n",
    ")\n",
    "kb.connect()\n",
    "\n",
    "reranker_chat_engine = ChatEngine(ContextEngine(kb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranking_app_id = \"canopy_reranking\"\n",
    "reranking_tru_recorder = TruCustomApp(\n",
    "    reranker_chat_engine,\n",
    "    app_id=reranking_app_id,\n",
    "    feedbacks=[f_groundedness, f_qa_relevance, f_context_relevance],\n",
    ")\n",
    "\n",
    "answers = []\n",
    "\n",
    "for query in queries:\n",
    "    with reranking_tru_recorder as recording:\n",
    "        answers.append(\n",
    "            reranker_chat_engine.chat(query).choices[0].message.content\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With reranking we get the right answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the maximum dimension for a dense vector in Pinecone?\n",
      "\n",
      "The maximum dimension for a dense vector in Pinecone is 20,000. (Source: https://docs.pinecone.io/docs/limits)\n"
     ]
    }
   ],
   "source": [
    "print(queries[0][0].content + \"\\n\")\n",
    "print(answers[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the effect of reranking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Groundedness</th>\n",
       "      <th>Context Relevance</th>\n",
       "      <th>Answer Relevance</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>canopy_reranking</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.002118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>canopy default</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.764286</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.002687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Groundedness  Context Relevance  Answer Relevance   latency   \n",
       "app_id                                                                          \n",
       "canopy_reranking      0.833333           0.775000          0.900000  3.333333  \\\n",
       "canopy default        0.333333           0.764286          0.966667  3.333333   \n",
       "\n",
       "                  total_cost  \n",
       "app_id                        \n",
       "canopy_reranking    0.002118  \n",
       "canopy default      0.002687  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.get_leaderboard(app_ids=[app_id, reranking_app_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore more in the TruLens dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n",
      "Config file already exists. Skipping writing process.\n",
      "Credentials file already exists. Skipping writing process.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0ed9f4b500407198db88d8920a0d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valuâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard started at http://192.168.1.157:8501 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "run_dashboard(tru)\n",
    "\n",
    "# stop_dashboard(tru) # stop if needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "b49ef6d0b3ca0fd6117ebbca48c3d697c422d5d25bd8bdbbbbafb3db0f51ca63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
