{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Sentence Window RAG\n",
    "\n",
    "In this notebook, we use the SentenceWindowNodeParser to parse documents into single sentences per node. Each node also contains a \"window\" with the sentences on either side of the node sentence.\n",
    "\n",
    "Then, after retrieval, before passing the retrieved sentences to the LLM, the single sentences are replaced with a window containing the surrounding sentences using the MetadataReplacementNodePostProcessor.\n",
    "\n",
    "Last we will show how to evaluate retrieval in this setting, and compare to base VectorStoreIndex.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/examples/expositional/frameworks/llama_index/llama_index_sentencewindow.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install trulens trulens-apps-llamaindex trulens-providers-openai llama_index==0.10.11 sentence-transformers transformers pypdf gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TRULENS_OTEL_TRACING\"] = \"0\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Engine Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import openai\n",
    "from trulens.core import Feedback\n",
    "from trulens.core import FeedbackMode\n",
    "from trulens.core import Select\n",
    "from trulens.core import TruSession\n",
    "from trulens.apps.llamaindex import TruLlama\n",
    "from trulens.providers.openai import OpenAI as fOpenAI\n",
    "\n",
    "session = TruSession()\n",
    "\n",
    "session.reset_database()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf --output IPCC_AR6_WGII_Chapter03.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"./IPCC_AR6_WGII_Chapter03.pdf\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence-window index\n",
    "!gdown \"https://drive.google.com/uc?id=16pH4NETEs43dwJUvYnJ9Z-bsR9_krkrP\"\n",
    "!tar -xzf sentence_index.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge into a single large document rather than one document per-page\n",
    "from llama_index.core import Document\n",
    "\n",
    "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# create the sentence window node parser w/ default settings\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=3,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\",\n",
    ")\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "text_splitter = SentenceSplitter()\n",
    "\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "Settings.node_parser = node_parser\n",
    "Settings.text_splitter = text_splitter\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "base_nodes = text_splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "sentence_index = VectorStoreIndex(nodes)\n",
    "\n",
    "base_index = VectorStoreIndex(base_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize OpenAI provider\n",
    "provider = fOpenAI()\n",
    "\n",
    "# Helpfulness\n",
    "f_helpfulness = Feedback(provider.helpfulness).on_output()\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = Feedback(provider.relevance_with_cot_reasons).on_input_output()\n",
    "\n",
    "# Question/statement relevance between question and each context chunk with context reasoning.\n",
    "# The context is located in a different place for the sub questions so we need to define that feedback separately\n",
    "f_context_relevance_subquestions = (\n",
    "    Feedback(provider.context_relevance_with_cot_reasons)\n",
    "    .on_input()\n",
    "    .on(Select.Record.calls[0].rets.source_nodes[:].node.text)\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "f_context_relevance = (\n",
    "    Feedback(provider.context_relevance_with_cot_reasons)\n",
    "    .on_input()\n",
    "    .on(TruLlama.select_context())\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "# Initialize groundedness\n",
    "# Groundedness with chain of thought reasoning\n",
    "# Similar to context relevance, we'll follow a strategy of defining it twice for the subquestions and overall question.\n",
    "f_groundedness_subquestions = (\n",
    "    Feedback(provider.groundedness_measure_with_cot_reasons)\n",
    "    .on(Select.Record.calls[0].rets.source_nodes[:].node.text.collect())\n",
    "    .on_output()\n",
    ")\n",
    "\n",
    "f_groundedness = (\n",
    "    Feedback(provider.groundedness_measure_with_cot_reasons)\n",
    "    .on(TruLlama.select_context())\n",
    "    .on_output()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor\n",
    "from trulens.apps.llamaindex import TruLlama\n",
    "\n",
    "sentence_query_engine = sentence_index.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    node_postprocessors=[\n",
    "        MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    ],\n",
    ")\n",
    "\n",
    "tru_sentence_query_engine_recorder = TruLlama(\n",
    "    sentence_query_engine,\n",
    "    app_name=\"climate query engine\",\n",
    "    app_version=\"sentence_window_index\",\n",
    "    feedbacks=[\n",
    "        f_qa_relevance,\n",
    "        f_context_relevance,\n",
    "        f_groundedness,\n",
    "        f_helpfulness,\n",
    "    ],\n",
    ")\n",
    "with tru_sentence_query_engine_recorder:\n",
    "    sentence_query_engine.query(\"What are the concerns surrounding the AMOC?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrast with normal VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = base_index.as_query_engine(similarity_top_k=2)\n",
    "\n",
    "tru_query_engine_recorder = TruLlama(\n",
    "    query_engine,\n",
    "    app_name=\"climate query engine\",\n",
    "    app_version=\"vector_store_index\",\n",
    "    feedbacks=[\n",
    "        f_qa_relevance,\n",
    "        f_context_relevance,\n",
    "        f_groundedness,\n",
    "        f_helpfulness,\n",
    "    ],\n",
    ")\n",
    "with tru_query_engine_recorder:\n",
    "    query_engine.query(\"What are the concerns surrounding the AMOC?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Also Compare with Sub-Question Query Engine + Sentence Window Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core.tools import ToolMetadata\n",
    "\n",
    "subquestion_query_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    [\n",
    "        QueryEngineTool(\n",
    "            query_engine=sentence_query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"climate_report\", description=\"Climate Report on Oceans.\"\n",
    "            ),\n",
    "        )\n",
    "    ],\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "tru_subquestion_query_engine_recorder = TruLlama(\n",
    "    subquestion_query_engine,\n",
    "    app_name=\"climate query engine\",\n",
    "    app_version=\"sub_question_query_engine\",\n",
    "    feedbacks=[\n",
    "        f_qa_relevance,\n",
    "        f_context_relevance,\n",
    "        f_context_relevance_subquestions,\n",
    "        f_groundedness,\n",
    "        f_groundedness_subquestions,\n",
    "        f_helpfulness,\n",
    "    ],\n",
    ")\n",
    "with tru_subquestion_query_engine_recorder:\n",
    "    subquestion_query_engine.query(\"What are the concerns surrounding the AMOC?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "run_dashboard(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Based on the provided text, discuss the impact of human activities on the natural carbon dynamics of estuaries, shelf seas, and other intertidal and shallow-water habitats. Provide examples from the text to support your answer.\",\n",
    "    \"Analyze the combined effects of exploitation and multi-decadal climate fluctuations on global fisheries yields. How do these factors make it difficult to assess the impacts of global climate change on fisheries yields? Use specific examples from the text to support your analysis.\",\n",
    "    \"Based on the study by Gutiérrez-Rodríguez, A.G., et al., 2018, what potential benefits do seaweeds have in the field of medicine, specifically in relation to cancer treatment?\",\n",
    "    \"According to the research conducted by Haasnoot, M., et al., 2020, how does the uncertainty in Antarctic mass-loss impact the coastal adaptation strategy of the Netherlands?\",\n",
    "    \"Based on the context, explain how the decline in warm water coral reefs is projected to impact the services they provide to society, particularly in terms of coastal protection.\",\n",
    "    \"Tell me something about the intricacies of tying a tie.\",\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "otel_nbs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
