{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# TruLens Scorers with MLflow\n",
    "\n",
    "TruLens feedback functions are available as first-class scorers in MLflow's GenAI evaluation framework starting with MLflow 3.10.0. This integration allows you to use TruLens' powerful evaluation capabilities directly within your MLflow workflows.\n",
    "\n",
    "In this notebook, we'll demonstrate how to:\n",
    "1. Use TruLens scorers for RAG evaluation (Groundedness, ContextRelevance, AnswerRelevance)\n",
    "2. Use output scorers (Coherence)\n",
    "3. Integrate with MLflow's batch evaluation via `mlflow.genai.evaluate`\n",
    "4. Use TruLens scorers with MLflow tracing\n",
    "5. **Evaluate agentic workflows** with Agent Trace Scorers (ToolSelection, ToolCalling, etc.)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/examples/expositional/frameworks/mlflow/mlflow_trulens_scorers.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install 'mlflow>=3.10.0' trulens trulens-providers-litellm openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Available TruLens Scorers in MLflow\n",
    "\n",
    "TruLens provides several categories of scorers:\n",
    "\n",
    "**RAG Evaluation Scorers:**\n",
    "- `Groundedness` - Evaluates whether the response is grounded in the provided context\n",
    "- `ContextRelevance` - Evaluates whether the retrieved context is relevant to the query\n",
    "- `AnswerRelevance` - Evaluates whether the response is relevant to the input query\n",
    "\n",
    "**Output Scorers:**\n",
    "- `Coherence` - Evaluates the coherence and logical flow of any LLM output\n",
    "\n",
    "**Agent Trace Scorers:**\n",
    "- `LogicalConsistency` - Evaluates logical consistency of agent decisions\n",
    "- `ExecutionEfficiency` - Evaluates efficiency of agent execution\n",
    "- `PlanAdherence` - Evaluates whether the agent followed its plan\n",
    "- `PlanQuality` - Evaluates the quality of agent planning\n",
    "- `ToolSelection` - Evaluates appropriateness of tool selection\n",
    "- `ToolCalling` - Evaluates correctness of tool calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Basic Usage: Direct Scorer Calls\n",
    "\n",
    "Let's start with a simple example of using TruLens scorers directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers.trulens import Groundedness\n",
    "\n",
    "# Create a Groundedness scorer\n",
    "groundedness_scorer = Groundedness(model=\"openai:/gpt-4o-mini\")\n",
    "\n",
    "# Evaluate a response against context\n",
    "feedback = groundedness_scorer(\n",
    "    outputs=\"Paris is the capital of France and is known for the Eiffel Tower.\",\n",
    "    expectations={\n",
    "        \"context\": \"France is a country in Western Europe. Its capital city is Paris. Paris is famous for the Eiffel Tower, which was built in 1889.\"\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"Groundedness: {feedback.value}\")  # \"yes\" or \"no\"\n",
    "print(f\"Score: {feedback.metadata['score']}\")  # 0.0 to 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a hallucinated response\n",
    "hallucinated_feedback = groundedness_scorer(\n",
    "    outputs=\"Paris is the capital of France and was founded by Julius Caesar in 52 BC.\",\n",
    "    expectations={\n",
    "        \"context\": \"France is a country in Western Europe. Its capital city is Paris. Paris is famous for the Eiffel Tower.\"\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"Groundedness: {hallucinated_feedback.value}\")\n",
    "print(f\"Score: {hallucinated_feedback.metadata['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Batch Evaluation with mlflow.genai.evaluate\n",
    "\n",
    "TruLens scorers integrate seamlessly with MLflow's batch evaluation framework. This is useful for evaluating multiple examples at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.genai.scorers.trulens import (\n",
    "    AnswerRelevance,\n",
    "    ContextRelevance,\n",
    "    Groundedness,\n",
    ")\n",
    "\n",
    "# Prepare evaluation dataset\n",
    "eval_dataset = [\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is MLflow?\"},\n",
    "        \"outputs\": \"MLflow is an open-source platform for managing the end-to-end machine learning lifecycle.\",\n",
    "        \"expectations\": {\n",
    "            \"context\": \"MLflow is an open-source platform created by Databricks for managing the end-to-end machine learning lifecycle. It includes experiment tracking, model registry, and deployment capabilities.\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is TruLens used for?\"},\n",
    "        \"outputs\": \"TruLens is used for evaluating and monitoring LLM applications using feedback functions.\",\n",
    "        \"expectations\": {\n",
    "            \"context\": \"TruLens is a library for evaluating LLM applications. It provides feedback functions like groundedness, relevance, and coherence to measure the quality of LLM outputs.\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What programming language is Python?\"},\n",
    "        \"outputs\": \"Python is a high-level, interpreted programming language known for its simplicity.\",\n",
    "        \"expectations\": {\n",
    "            \"context\": \"Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.\"\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run batch evaluation with multiple TruLens scorers\n",
    "results = mlflow.genai.evaluate(\n",
    "    data=eval_dataset,\n",
    "    scorers=[\n",
    "        Groundedness(model=\"openai:/gpt-4o-mini\"),\n",
    "        ContextRelevance(model=\"openai:/gpt-4o-mini\"),\n",
    "        AnswerRelevance(model=\"openai:/gpt-4o-mini\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# View results\n",
    "print(\"Evaluation Results:\")\n",
    "print(results.tables[\"eval_results\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View aggregate metrics\n",
    "print(\"\\nAggregate Metrics:\")\n",
    "for metric, value in results.metrics.items():\n",
    "    print(f\"  {metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Configuring Thresholds\n",
    "\n",
    "TruLens scorers return a score between 0 and 1. You can configure the threshold for pass/fail decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scorer with custom threshold\n",
    "strict_groundedness = Groundedness(model=\"openai:/gpt-4o-mini\", threshold=0.8)\n",
    "\n",
    "# Test with the same response\n",
    "feedback = strict_groundedness(\n",
    "    outputs=\"Paris is the capital of France.\",\n",
    "    expectations={\n",
    "        \"context\": \"France is a country in Europe. Its capital is Paris.\"\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"Pass/Fail: {feedback.value}\")  # \"yes\" if score >= 0.8, else \"no\"\n",
    "print(f\"Actual Score: {feedback.metadata['score']}\")\n",
    "print(f\"Threshold: {feedback.metadata['threshold']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Using Different LLM Providers\n",
    "\n",
    "TruLens scorers in MLflow support multiple LLM providers through LiteLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI\n",
    "openai_scorer = Groundedness(model=\"openai:/gpt-4o-mini\")\n",
    "\n",
    "# Anthropic (requires ANTHROPIC_API_KEY)\n",
    "# anthropic_scorer = Groundedness(model=\"anthropic:/claude-3-5-sonnet\")\n",
    "\n",
    "# Azure OpenAI (requires Azure configuration)\n",
    "# azure_scorer = Groundedness(model=\"azure:/my-deployment-name\")\n",
    "\n",
    "# AWS Bedrock\n",
    "# bedrock_scorer = Groundedness(model=\"bedrock:/anthropic.claude-3-sonnet\")\n",
    "\n",
    "# Google Vertex AI\n",
    "# vertex_scorer = Groundedness(model=\"vertex_ai:/gemini-pro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Dynamic Scorer Creation with get_scorer\n",
    "\n",
    "Use `get_scorer` to create scorers dynamically by name. This is useful when you need to configure scorers from configuration files or user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers.trulens import get_scorer\n",
    "\n",
    "# Define which scorers to use (could come from config file)\n",
    "scorer_names = [\"Groundedness\", \"ContextRelevance\", \"AnswerRelevance\"]\n",
    "\n",
    "# Create scorers dynamically\n",
    "scorers = [get_scorer(name, model=\"openai:/gpt-4o-mini\") for name in scorer_names]\n",
    "\n",
    "# Use in evaluation\n",
    "results = mlflow.genai.evaluate(\n",
    "    data=eval_dataset,\n",
    "    scorers=scorers,\n",
    ")\n",
    "\n",
    "print(results.tables[\"eval_results\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Output Scorer: Coherence\n",
    "\n",
    "The Coherence scorer evaluates the logical flow and coherence of LLM outputs, independent of any context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers.trulens import Coherence\n",
    "\n",
    "coherence_scorer = Coherence(model=\"openai:/gpt-4o-mini\")\n",
    "\n",
    "# Test coherent text\n",
    "coherent_feedback = coherence_scorer(\n",
    "    outputs=\"Machine learning is a subset of artificial intelligence that enables systems to learn from data. It uses algorithms to identify patterns and make decisions with minimal human intervention. Common applications include image recognition, natural language processing, and recommendation systems.\"\n",
    ")\n",
    "\n",
    "print(f\"Coherence (good text): {coherent_feedback.value}\")\n",
    "print(f\"Score: {coherent_feedback.metadata['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test incoherent text\n",
    "incoherent_feedback = coherence_scorer(\n",
    "    outputs=\"Machine learning uses pizza. The sun is blue therefore cats can fly. Python programming because weather patterns indicate database normalization.\"\n",
    ")\n",
    "\n",
    "print(f\"Coherence (poor text): {incoherent_feedback.value}\")\n",
    "print(f\"Score: {incoherent_feedback.metadata['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Integrating with MLflow Tracing\n",
    "\n",
    "TruLens scorers can be used with MLflow's tracing infrastructure to evaluate traced LLM applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from openai import OpenAI\n",
    "\n",
    "# Enable MLflow autologging for OpenAI\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "# Define a simple RAG function with tracing\n",
    "@mlflow.trace\n",
    "def simple_rag(question: str, context: str) -> str:\n",
    "    \"\"\"A simple RAG function that answers questions based on context.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Answer the question based only on the provided context.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Context: {context}\\n\\nQuestion: {question}\",\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the RAG function\n",
    "context = \"TruLens is an open-source library for evaluating and tracking LLM applications. It provides feedback functions for measuring groundedness, relevance, and other quality metrics.\"\n",
    "question = \"What does TruLens do?\"\n",
    "\n",
    "answer = simple_rag(question, context)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Building a Complete RAG Evaluation Pipeline\n",
    "\n",
    "Let's put it all together with a complete example that builds a simple RAG system and evaluates it using TruLens scorers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample knowledge base\n",
    "knowledge_base = {\n",
    "    \"mlflow\": \"MLflow is an open-source platform for the machine learning lifecycle, including experimentation, reproducibility, deployment, and a central model registry.\",\n",
    "    \"trulens\": \"TruLens is a library for evaluating LLM applications using feedback functions. It supports groundedness, relevance, and coherence evaluations.\",\n",
    "    \"langchain\": \"LangChain is a framework for developing applications powered by language models. It provides tools for prompt management, chains, and agents.\",\n",
    "    \"llamaindex\": \"LlamaIndex is a data framework for LLM applications. It provides tools for ingesting, structuring, and accessing private or domain-specific data.\",\n",
    "}\n",
    "\n",
    "\n",
    "def retrieve_context(question: str) -> str:\n",
    "    \"\"\"Simple keyword-based retrieval.\"\"\"\n",
    "    question_lower = question.lower()\n",
    "    for key, value in knowledge_base.items():\n",
    "        if key in question_lower:\n",
    "            return value\n",
    "    return \"No relevant context found.\"\n",
    "\n",
    "\n",
    "def generate_answer(question: str, context: str) -> str:\n",
    "    \"\"\"Generate answer using OpenAI.\"\"\"\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Answer the question based on the provided context. If the context doesn't contain relevant information, say so.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Context: {context}\\n\\nQuestion: {question}\",\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate answers for test questions\n",
    "test_questions = [\n",
    "    \"What is MLflow used for?\",\n",
    "    \"How does TruLens evaluate LLM applications?\",\n",
    "    \"What is LangChain?\",\n",
    "]\n",
    "\n",
    "# Build evaluation dataset\n",
    "rag_eval_dataset = []\n",
    "for question in test_questions:\n",
    "    context = retrieve_context(question)\n",
    "    answer = generate_answer(question, context)\n",
    "\n",
    "    rag_eval_dataset.append({\n",
    "        \"inputs\": {\"question\": question},\n",
    "        \"outputs\": answer,\n",
    "        \"expectations\": {\"context\": context},\n",
    "    })\n",
    "\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {answer}\")\n",
    "    print(f\"Context: {context[:100]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the RAG system\n",
    "rag_results = mlflow.genai.evaluate(\n",
    "    data=rag_eval_dataset,\n",
    "    scorers=[\n",
    "        Groundedness(model=\"openai:/gpt-4o-mini\", threshold=0.7),\n",
    "        ContextRelevance(model=\"openai:/gpt-4o-mini\", threshold=0.7),\n",
    "        AnswerRelevance(model=\"openai:/gpt-4o-mini\", threshold=0.7),\n",
    "        Coherence(model=\"openai:/gpt-4o-mini\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"\\nRAG Evaluation Results:\")\n",
    "print(rag_results.tables[\"eval_results\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary metrics\n",
    "print(\"\\nSummary Metrics:\")\n",
    "print(\"=\" * 40)\n",
    "for metric, value in sorted(rag_results.metrics.items()):\n",
    "    print(f\"{metric}: {value:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## Evaluating Agents with TruLens Scorers\n",
    "\n",
    "TruLens provides specialized scorers for evaluating agentic workflows. These scorers analyze agent traces to evaluate planning, tool usage, and execution quality.\n",
    "\n",
    "**Available Agent Scorers:**\n",
    "- `ToolSelection` - Evaluates whether the agent selected appropriate tools for each step\n",
    "- `ToolCalling` - Evaluates the correctness of tool call parameters and execution\n",
    "- `PlanQuality` - Evaluates the quality of the agent's planning\n",
    "- `PlanAdherence` - Evaluates whether the agent followed its plan\n",
    "- `LogicalConsistency` - Evaluates logical consistency across agent decisions\n",
    "- `ExecutionEfficiency` - Evaluates efficiency of the agent's execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import mlflow\n",
    "from mlflow.genai.scorers.trulens import ToolCalling, ToolSelection\n",
    "from openai import OpenAI\n",
    "\n",
    "# Define tools for the agent\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get the current weather for a location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g., San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                        \"description\": \"Temperature unit\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"search_web\",\n",
    "            \"description\": \"Search the web for information\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"query\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculate\",\n",
    "            \"description\": \"Perform mathematical calculations\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"expression\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The mathematical expression to evaluate\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"expression\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# Simulated tool execution\n",
    "def execute_tool(tool_name: str, arguments: dict) -> str:\n",
    "    if tool_name == \"get_weather\":\n",
    "        return json.dumps({\n",
    "            \"location\": arguments[\"location\"],\n",
    "            \"temperature\": 72,\n",
    "            \"unit\": arguments.get(\"unit\", \"fahrenheit\"),\n",
    "            \"conditions\": \"sunny\",\n",
    "        })\n",
    "    elif tool_name == \"search_web\":\n",
    "        return json.dumps({\n",
    "            \"query\": arguments[\"query\"],\n",
    "            \"results\": [\n",
    "                \"Result 1: Relevant information about \" + arguments[\"query\"],\n",
    "                \"Result 2: More details on \" + arguments[\"query\"],\n",
    "            ],\n",
    "        })\n",
    "    elif tool_name == \"calculate\":\n",
    "        try:\n",
    "            result = eval(arguments[\"expression\"])  # Note: use safe eval in production\n",
    "            return json.dumps({\"expression\": arguments[\"expression\"], \"result\": result})\n",
    "        except Exception:\n",
    "            return json.dumps({\"error\": \"Invalid expression\"})\n",
    "    return json.dumps({\"error\": \"Unknown tool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "@mlflow.trace\n",
    "def run_agent(user_query: str) -> str:\n",
    "    \"\"\"Run an agent that can use tools to answer questions.\"\"\"\n",
    "    client = OpenAI()\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant with access to tools. Use them when appropriate to answer the user's questions.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": user_query},\n",
    "    ]\n",
    "\n",
    "    # First call to get tool selection\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",\n",
    "    )\n",
    "\n",
    "    assistant_message = response.choices[0].message\n",
    "\n",
    "    # If the model wants to call tools\n",
    "    if assistant_message.tool_calls:\n",
    "        messages.append(assistant_message)\n",
    "\n",
    "        # Execute each tool call\n",
    "        for tool_call in assistant_message.tool_calls:\n",
    "            tool_name = tool_call.function.name\n",
    "            arguments = json.loads(tool_call.function.arguments)\n",
    "\n",
    "            # Execute the tool\n",
    "            tool_result = execute_tool(tool_name, arguments)\n",
    "\n",
    "            # Add tool result to messages\n",
    "            messages.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"content\": tool_result,\n",
    "            })\n",
    "\n",
    "        # Get final response after tool execution\n",
    "        final_response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "        )\n",
    "        return final_response.choices[0].message.content\n",
    "\n",
    "    return assistant_message.content\n",
    "\n",
    "\n",
    "# Run the agent with different queries\n",
    "agent_queries = [\n",
    "    \"What's the weather like in San Francisco?\",\n",
    "    \"Calculate 25 * 4 + 10\",\n",
    "    \"Search for information about TruLens evaluation\",\n",
    "]\n",
    "\n",
    "agent_results = []\n",
    "for query in agent_queries:\n",
    "    result = run_agent(query)\n",
    "    agent_results.append({\"query\": query, \"response\": result})\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Response: {result}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the traces from MLflow\n",
    "# search_traces returns a DataFrame with trace info including the trace objects\n",
    "traces_df = mlflow.search_traces(max_results=3)\n",
    "\n",
    "# The 'trace' column contains the actual Trace objects\n",
    "traces = traces_df[\"trace\"].tolist()\n",
    "\n",
    "# Evaluate agent traces with TruLens scorers\n",
    "tool_selection_scorer = ToolSelection(model=\"openai:/gpt-4o-mini\")\n",
    "tool_calling_scorer = ToolCalling(model=\"openai:/gpt-4o-mini\")\n",
    "\n",
    "print(\"Agent Evaluation Results:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, trace in enumerate(traces):\n",
    "    print(f\"\\nQuery {i+1}: {agent_queries[i]}\")\n",
    "\n",
    "    # Evaluate tool selection\n",
    "    selection_feedback = tool_selection_scorer(trace=trace)\n",
    "    print(f\"  Tool Selection: {selection_feedback.value}\")\n",
    "    if selection_feedback.rationale:\n",
    "        print(f\"    {selection_feedback.rationale[:100]}...\")\n",
    "\n",
    "    # Evaluate tool calling\n",
    "    calling_feedback = tool_calling_scorer(trace=trace)\n",
    "    print(f\"  Tool Calling: {calling_feedback.value}\")\n",
    "    if calling_feedback.rationale:\n",
    "        print(f\"    {calling_feedback.rationale[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use agent scorers in batch evaluation with mlflow.genai.evaluate\n",
    "# by providing traces directly in the evaluation dataset\n",
    "\n",
    "from mlflow.genai.scorers.trulens import (\n",
    "    ExecutionEfficiency,\n",
    "    LogicalConsistency,\n",
    "    PlanAdherence,\n",
    "    PlanQuality,\n",
    ")\n",
    "\n",
    "# Run comprehensive agent evaluation using the agent function directly\n",
    "agent_eval_results = mlflow.genai.evaluate(\n",
    "    data=[\n",
    "        {\"inputs\": {\"user_query\": q}} for q in agent_queries\n",
    "    ],\n",
    "    predict_fn=run_agent,\n",
    "    scorers=[\n",
    "        ToolSelection(model=\"openai:/gpt-4o-mini\"),\n",
    "        ToolCalling(model=\"openai:/gpt-4o-mini\"),\n",
    "        Coherence(model=\"openai:/gpt-4o-mini\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"\\nComprehensive Agent Evaluation:\")\n",
    "print(agent_eval_results.tables[\"eval_results\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "## Related Resources\n",
    "\n",
    "- [MLflow GenAI Evaluation Documentation](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html)\n",
    "- [TruLens Feedback Functions](https://www.trulens.org/component_guides/evaluation/feedback_functions/)\n",
    "- [TruLens MLflow Integration Guide](https://www.trulens.org/component_guides/evaluation/mlflow/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
