{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claude 3 Quickstart\n",
    "\n",
    "In this quickstart you will learn how to use Anthropic's Claude 3 to run feedback functions by using LiteLLM as the feedback provider.\n",
    "\n",
    "[Anthropic](https://www.anthropic.com/) Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems. Claude is Anthropics AI assistant, of which Claude 3 is the latest and greatest. Claude 3 comes in three varieties: Haiku, Sonnet and Opus which can all be used to run feedback functions.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://anthropic/claude3_quickstart.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install trulens trulens-providers-litellm chromadb openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # for running application only\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"  # for running feedback functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from litellm import completion\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Hey! how's it going?\"}]\n",
    "response = completion(model=\"claude-3-haiku-20240307\", messages=messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data\n",
    "\n",
    "In this case, we'll just initialize some simple text in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "university_info = \"\"\"\n",
    "The University of Washington, founded in 1861 in Seattle, is a public research university\n",
    "with over 45,000 students across three campuses in Seattle, Tacoma, and Bothell.\n",
    "As the flagship institution of the six public universities in Washington state,\n",
    "UW encompasses over 500 buildings and 20 million square feet of space,\n",
    "including one of the largest library systems in the world.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vector Store\n",
    "\n",
    "Create a chromadb vector store in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "oai_client = OpenAI()\n",
    "\n",
    "oai_client.embeddings.create(\n",
    "    model=\"text-embedding-ada-002\", input=university_info\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "\n",
    "embedding_function = OpenAIEmbeddingFunction(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    model_name=\"text-embedding-ada-002\",\n",
    ")\n",
    "\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "vector_store = chroma_client.get_or_create_collection(\n",
    "    name=\"Universities\", embedding_function=embedding_function\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the university_info to the embedding database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.add(\"uni_info\", documents=university_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build RAG from scratch\n",
    "\n",
    "Build a custom RAG from scratch, and add TruLens custom instrumentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import TruSession\n",
    "from trulens.apps.app import instrument\n",
    "\n",
    "session = TruSession()\n",
    "session.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG_from_scratch:\n",
    "    @instrument\n",
    "    def retrieve(self, query: str) -> list:\n",
    "        \"\"\"\n",
    "        Retrieve relevant text from vector store.\n",
    "        \"\"\"\n",
    "        results = vector_store.query(query_texts=query, n_results=2)\n",
    "        return results[\"documents\"][0]\n",
    "\n",
    "    @instrument\n",
    "    def generate_completion(self, query: str, context_str: list) -> str:\n",
    "        \"\"\"\n",
    "        Generate answer from context.\n",
    "        \"\"\"\n",
    "        completion = (\n",
    "            oai_client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                temperature=0,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"We have provided context information below. \\n\"\n",
    "                        f\"---------------------\\n\"\n",
    "                        f\"{context_str}\"\n",
    "                        f\"\\n---------------------\\n\"\n",
    "                        f\"Given this information, please answer the question: {query}\",\n",
    "                    }\n",
    "                ],\n",
    "            )\n",
    "            .choices[0]\n",
    "            .message.content\n",
    "        )\n",
    "        return completion\n",
    "\n",
    "    @instrument\n",
    "    def query(self, query: str) -> str:\n",
    "        context_str = self.retrieve(query)\n",
    "        completion = self.generate_completion(query, context_str)\n",
    "        return completion\n",
    "\n",
    "\n",
    "rag = RAG_from_scratch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up feedback functions.\n",
    "\n",
    "Here we'll use groundedness, answer relevance and context relevance to detect hallucination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from trulens.core import Metric, Selector\n",
    "from trulens.providers.litellm import LiteLLM\n",
    "\n",
    "# Initialize LiteLLM-based feedback function collection class:\n",
    "provider = LiteLLM(model_engine=\"claude-3-opus-20240229\")\n",
    "\n",
    "# Define a groundedness metric using OTEL-compatible selectors\n",
    "f_groundedness = Metric(\n",
    "    implementation=provider.groundedness_measure_with_cot_reasons,\n",
    "    name=\"Groundedness\",\n",
    "    selectors={\n",
    "        \"source\": Selector(function_name=\"retrieve\", function_attribute=\"return\", collect_list=True),\n",
    "        \"statement\": Selector.select_record_output(),\n",
    "    },\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_answer_relevance = Metric(\n",
    "    implementation=provider.relevance_with_cot_reasons,\n",
    "    name=\"Answer Relevance\",\n",
    "    selectors={\n",
    "        \"prompt\": Selector(function_name=\"retrieve\", function_attribute=\"query\"),\n",
    "        \"response\": Selector.select_record_output(),\n",
    "    },\n",
    ")\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = Metric(\n",
    "    implementation=provider.context_relevance_with_cot_reasons,\n",
    "    name=\"Context Relevance\",\n",
    "    selectors={\n",
    "        \"question\": Selector(function_name=\"retrieve\", function_attribute=\"query\"),\n",
    "        \"context\": Selector(function_name=\"retrieve\", function_attribute=\"return\", collect_list=True),\n",
    "    },\n",
    "    agg=np.mean,\n",
    ")\n",
    "\n",
    "# Coherence evaluation\n",
    "f_coherence = Metric(\n",
    "    implementation=provider.coherence_with_cot_reasons,\n",
    "    name=\"Coherence\",\n",
    "    selectors={\n",
    "        \"text\": Selector.select_record_output(),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grounded.groundedness_measure_with_cot_reasons(\n",
    "    \"\"\"e University of Washington, founded in 1861 in Seattle, is a public '\n",
    "  'research university\\n'\n",
    "  'with over 45,000 students across three campuses in Seattle, Tacoma, and '\n",
    "  'Bothell.\\n'\n",
    "  'As the flagship institution of the six public universities in Washington 'githugithub\n",
    "  'state,\\n'\n",
    "  'UW encompasses over 500 buildings and 20 million square feet of space,\\n'\n",
    "  'including one of the largest library systems in the world.\\n']]\"\"\",\n",
    "    \"The University of Washington was founded in 1861. It is the flagship institution of the state of washington.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the app\n",
    "Wrap the custom RAG with `TruApp`, add list of feedbacks for eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.apps.app import TruApp\n",
    "\n",
    "tru_rag = TruApp(\n",
    "    rag,\n",
    "    app_name=\"RAG\",\n",
    "    app_version=\"v1\",\n",
    "    feedbacks=[\n",
    "        f_groundedness,\n",
    "        f_answer_relevance,\n",
    "        f_context_relevance,\n",
    "        f_coherence,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the app\n",
    "Use `tru_rag` as a context manager for the custom RAG-from-scratch app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_rag as recording:\n",
    "    rag.query(\"Give me a long history of U Dub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.get_leaderboard(app_ids=[tru_rag.app_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "run_dashboard(session)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens18_release",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
