{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07349e67-8830-4cee-a520-c6a5e75bcbf9",
   "metadata": {},
   "source": [
    "# Vectara HHEM Evaluator Quickstart\n",
    "\n",
    "In this quickstart, you'll learn how to use the HHEM evaluator feedback function from TruLens in your application. The Vectra HHEM evaluator, or Hughes Hallucination Evaluation Model, is a tool used to determine if a summary produced by a large language model (LLM) might contain hallucinated information.\n",
    "\n",
    "- **Purpose:** The Vectra HHEM evaluator analyzes both inputs and assigns a score indicating the probability of response containing hallucinations.\n",
    "- **Score :** The returned value is a floating point number between zero and one that represents a boolean outcome : either a high likelihood of hallucination if the score is less than 0.5 or a low likelihood of hallucination if the score is more than 0.5 \n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/examples/models/Vectara_HHEM_evaluator.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f894d9",
   "metadata": {},
   "source": [
    "### Install Dependencies\n",
    "\n",
    "Run the cells below to install the utilities we'll use in this notebook to demonstrate Vectara's HHEM model.\n",
    "- uncomment the cell below if you haven't yet installed the langchain or TruEra's TruLens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a03458-3d25-455d-a353-b5fa0f1f54c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --pre trulens trulens-providers-huggingface 'langchain==0.0.354' 'langchain-community==0.0.20' 'langchain-core==0.1.23'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6a8601",
   "metadata": {},
   "source": [
    "### Import Utilities\n",
    "\n",
    "we're using LangChain utilities to facilitate RAG retrieval and demonstrate Vectara's HHEM.\n",
    "- run the cells below to get started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14b80ea-bc86-4045-8f68-a53dee91449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54673c22-83ec-4063-92da-c9786d5395e9",
   "metadata": {},
   "source": [
    "### PreProcess Your Data\n",
    "Run the cells below to split the Document TEXT into text Chunks to feed in ChromaDb.\n",
    "These are our primary sources for evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09940fd-ffd7-4b53-ab99-746e19c310b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(\"./data/\", glob=\"./*.txt\", loader_cls=TextLoader)\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=50\n",
    ")\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d607657-b583-4e43-b6d7-9c3d2634b0b7",
   "metadata": {},
   "source": [
    "### e5 Embeddings\n",
    "e5 embeddings set the SOTA on BEIR and MTEB benchmarks by using only synthetic data and less than 1k training steps. this method achieves\n",
    "strong performance on highly competitive text embedding benchmarks without using any labeled data. Furthermore, when fine-tuned with a mixture of synthetic and labeled data, this model sets new state-of-the-art results on the BEIR and MTEB benchmarks.[Improving Text Embeddings with Large Language Models](https://arxiv.org/pdf/2401.00368.pdf). It also requires a unique prompting mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0104dec4-2473-4e28-847e-b129538bf996",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_api_key = getpass.getpass(\"Enter your HF Inference API Key:\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4d6a42-adc0-4f12-b546-42f4080bb3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "\n",
    "embedding_function = HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=inference_api_key,\n",
    "    model_name=\"intfloat/multilingual-e5-large-instruct\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1a05d0",
   "metadata": {},
   "source": [
    "### Initialize a Vector Store\n",
    "\n",
    "Here we're using Chroma , our standard solution for all vector store requirements.\n",
    "- run the cells below to initialize the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cfb264-20d0-4b9f-aafd-a4f92a29c6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(texts, embedding_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9553a97-8221-4b5d-a846-87e719680388",
   "metadata": {},
   "source": [
    "### Wrap a Simple RAG application with TruLens\n",
    "- **Retrieval:** to get relevant docs from vector DB\n",
    "- **Generate completions:** to get response from LLM.\n",
    "\n",
    "run the cells below to create a RAG Class and Functions to Record the Context and LLM Response for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec11c7f5-2768-4b4a-a406-b790d407b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from trulens.apps.custom import instrument\n",
    "\n",
    "\n",
    "class Rag:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @instrument\n",
    "    def retrieve(self, query: str) -> str:\n",
    "        docs = db.similarity_search(query)\n",
    "        # Concatenate the content of the documents\n",
    "        content = \"\".join(doc.page_content for doc in docs)\n",
    "        return content\n",
    "\n",
    "    @instrument\n",
    "    def generate_completion(self, content: str, query: str) -> str:\n",
    "        url = \"https://api-inference.huggingface.co/models/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\n",
    "        headers = {\n",
    "            \"Authorization\": \"Bearer your hf token\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "\n",
    "        data = {\n",
    "            \"inputs\": f\"answer the following question from the information given Question:{query}\\nInformation:{content}\\n\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, json=data)\n",
    "            response.raise_for_status()\n",
    "            response_data = response.json()\n",
    "\n",
    "            # Extract the generated text from the response\n",
    "            generated_text = response_data[0][\"generated_text\"]\n",
    "            # Remove the input text from the generated text\n",
    "            response_text = generated_text[len(data[\"inputs\"]) :]\n",
    "\n",
    "            return response_text\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(\"Error:\", e)\n",
    "            return None\n",
    "\n",
    "    @instrument\n",
    "    def query(self, query: str) -> str:\n",
    "        context_str = self.retrieve(query)\n",
    "        completion = self.generate_completion(context_str, query)\n",
    "        return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51682668",
   "metadata": {},
   "source": [
    "# Instantiate the applications above\n",
    "- run the cells below to start the applications above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fc773a-fa13-4e79-bd05-832972beb006",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag1 = Rag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4118c2c6-6945-43e3-ba4b-9b5d2e683627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import Feedback\n",
    "from trulens.core import Select\n",
    "from trulens.core import TruSession\n",
    "from trulens.providers.huggingface import Huggingface\n",
    "\n",
    "tru = TruSession()\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0c38b7-f9b3-4735-998f-e6de10f6d8d8",
   "metadata": {},
   "source": [
    "### Initialize HHEM Feedback Function\n",
    "HHEM takes two inputs:\n",
    "\n",
    "1. The summary/answer itself generated by LLM.\n",
    "2. The original source text that the LLM used to generate the summary/answer (retrieval context).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80d8760-84a9-4ca2-8076-9f47a785f7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_provider = Huggingface()\n",
    "f_hhem_score = (\n",
    "    Feedback(huggingface_provider.hallucination_evaluator, name=\"HHEM_Score\")\n",
    "    .on(Select.RecordCalls.generate_completion.rets)\n",
    "    .on(Select.RecordCalls.retrieve.rets)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c51143",
   "metadata": {},
   "source": [
    "### Record The HHEM Score\n",
    "- run the cell below to create a feedback function for Vectara's HHEM model's score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8631816-0f68-4fcd-bd35-8f82c09b8d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "feedbacks = [f_hhem_score]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860e441e-68a5-4f60-99f6-6b6808cb395c",
   "metadata": {},
   "source": [
    "### Wrap the custom RAG with TruCustomApp, add HHEM  feedback for evaluation\n",
    "- it's as simple as running the cell below to complete the application and feedback wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0079734d-abbe-47d4-a229-5b4ef843503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.apps.custom import TruCustomApp\n",
    "\n",
    "tru_rag = TruCustomApp(rag1, app_name=\"RAG\", app_version=\"v1\", feedbacks=feedbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945891f8-0189-4d72-8f45-de5a384c4afc",
   "metadata": {},
   "source": [
    "### Run the App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cadc2e-f152-40a9-b39e-442ea4111cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_rag as recording:\n",
    "    rag1.query(\"What is Vint Cerf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926ece5c-b5b9-4343-bb05-948a5b0efe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_leaderboard(app_ids=[tru_rag.app_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1a44fa-01b8-492f-997d-f9d37d9421ce",
   "metadata": {},
   "source": [
    "### Explore in a Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69f90c6-34fb-492c-88b2-aa6b4859fe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "run_dashboard(tru)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
