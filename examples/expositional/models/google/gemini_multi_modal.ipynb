{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-modal LLMs and Multimodal RAG with Gemini\n",
    "\n",
    "In the first example, run and evaluate a multimodal Gemini model with a multimodal evaluator.\n",
    "\n",
    "In the second example, learn how to run semantic evaluations on a multi-modal RAG, including the RAG triad.\n",
    "\n",
    "Note: `google-generativeai` is only available for certain countries and regions. Original example attribution: LlamaIndex\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/examples/expositional/models/gemini_multi_modal.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --pre trulens trulens-providers-litellm trulens-instrument-llamaindex llama-index 'google-generativeai>=0.3.0' matplotlib qdrant_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Use Gemini to understand Images from URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize `GeminiMultiModal` and Load Images from URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.multi_modal_llms.gemini import GeminiMultiModal\n",
    "from llama_index.multi_modal_llms.generic_utils import load_image_urls\n",
    "\n",
    "image_urls = [\n",
    "    \"https://storage.googleapis.com/generativeai-downloads/data/scene.jpg\",\n",
    "    # Add yours here!\n",
    "]\n",
    "\n",
    "image_documents = load_image_urls(image_urls)\n",
    "\n",
    "gemini_pro = GeminiMultiModal(model_name=\"models/gemini-pro-vision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup TruLens Instrumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import Feedback\n",
    "from trulens.core import Select\n",
    "from trulens.core import Tru\n",
    "from trulens.core import TruCustomApp\n",
    "from trulens.core.app.custom import instrument\n",
    "from trulens.core.feedback import Provider\n",
    "\n",
    "tru = Tru()\n",
    "tru.reset_database()\n",
    "\n",
    "\n",
    "# create a custom class to instrument\n",
    "class Gemini:\n",
    "    @instrument\n",
    "    def complete(self, prompt, image_documents):\n",
    "        completion = gemini_pro.complete(\n",
    "            prompt=prompt,\n",
    "            image_documents=image_documents,\n",
    "        )\n",
    "        return completion\n",
    "\n",
    "\n",
    "gemini = Gemini()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup custom provider with Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a custom gemini feedback provider\n",
    "class Gemini_Provider(Provider):\n",
    "    def city_rating(self, image_url) -> float:\n",
    "        image_documents = load_image_urls([image_url])\n",
    "        city_score = float(\n",
    "            gemini_pro.complete(\n",
    "                prompt=\"Is the image of a city? Respond with the float likelihood from 0.0 (not city) to 1.0 (city).\",\n",
    "                image_documents=image_documents,\n",
    "            ).text\n",
    "        )\n",
    "        return city_score\n",
    "\n",
    "\n",
    "gemini_provider = Gemini_Provider()\n",
    "\n",
    "f_custom_function = Feedback(\n",
    "    gemini_provider.city_rating, name=\"City Likelihood\"\n",
    ").on(Select.Record.calls[0].args.image_documents[0].image_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test custom feedback function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_provider.city_rating(\n",
    "    image_url=\"https://storage.googleapis.com/generativeai-downloads/data/scene.jpg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrument custom app with TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_gemini = TruCustomApp(\n",
    "    gemini, app_id=\"gemini\", feedbacks=[f_custom_function]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_gemini as recording:\n",
    "    gemini.complete(\n",
    "        prompt=\"Identify the city where this photo was taken.\",\n",
    "        image_documents=image_documents,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Multi-Modal RAG for Restaurant Recommendation\n",
    "\n",
    "Our stack consists of TruLens + Gemini + LlamaIndex + Pydantic structured output capabilities.\n",
    "\n",
    "Pydantic structured output is great, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "input_image_path = Path(\"google_restaurants\")\n",
    "if not input_image_path.exists():\n",
    "    Path.mkdir(input_image_path)\n",
    "\n",
    "!wget \"https://docs.google.com/uc?export=download&id=1Pg04p6ss0FlBgz00noHAOAJ1EYXiosKg\" -O ./google_restaurants/miami.png\n",
    "!wget \"https://docs.google.com/uc?export=download&id=1dYZy17bD6pSsEyACXx9fRMNx93ok-kTJ\" -O ./google_restaurants/orlando.png\n",
    "!wget \"https://docs.google.com/uc?export=download&id=1ShPnYVc1iL_TA1t7ErCFEAHT74-qvMrn\" -O ./google_restaurants/sf.png\n",
    "!wget \"https://docs.google.com/uc?export=download&id=1WjISWnatHjwL4z5VD_9o09ORWhRJuYqm\" -O ./google_restaurants/toronto.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Pydantic Class for Strucutred Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class GoogleRestaurant(BaseModel):\n",
    "    \"\"\"Data model for a Google Restaurant.\"\"\"\n",
    "\n",
    "    restaurant: str\n",
    "    food: str\n",
    "    location: str\n",
    "    category: str\n",
    "    hours: str\n",
    "    price: str\n",
    "    rating: float\n",
    "    review: str\n",
    "    description: str\n",
    "    nearby_tourist_places: str\n",
    "\n",
    "\n",
    "google_image_url = \"./google_restaurants/miami.png\"\n",
    "image = Image.open(google_image_url).convert(\"RGB\")\n",
    "\n",
    "plt.figure(figsize=(16, 5))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.multi_modal_llms import GeminiMultiModal\n",
    "from llama_index.output_parsers import PydanticOutputParser\n",
    "from llama_index.program import MultiModalLLMCompletionProgram\n",
    "\n",
    "prompt_template_str = \"\"\"\\\n",
    "    can you summarize what is in the image\\\n",
    "    and return the answer with json format \\\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def pydantic_gemini(\n",
    "    model_name, output_class, image_documents, prompt_template_str\n",
    "):\n",
    "    gemini_llm = GeminiMultiModal(\n",
    "        api_key=os.environ[\"GOOGLE_API_KEY\"], model_name=model_name\n",
    "    )\n",
    "\n",
    "    llm_program = MultiModalLLMCompletionProgram.from_defaults(\n",
    "        output_parser=PydanticOutputParser(output_class),\n",
    "        image_documents=image_documents,\n",
    "        prompt_template_str=prompt_template_str,\n",
    "        multi_modal_llm=gemini_llm,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    response = llm_program()\n",
    "    return response\n",
    "\n",
    "\n",
    "google_image_documents = SimpleDirectoryReader(\n",
    "    \"./google_restaurants\"\n",
    ").load_data()\n",
    "\n",
    "results = []\n",
    "for img_doc in google_image_documents:\n",
    "    pydantic_response = pydantic_gemini(\n",
    "        \"models/gemini-pro-vision\",\n",
    "        GoogleRestaurant,\n",
    "        [img_doc],\n",
    "        prompt_template_str,\n",
    "    )\n",
    "    # only output the results for miami for example along with image\n",
    "    if \"miami\" in img_doc.image_path:\n",
    "        for r in pydantic_response:\n",
    "            print(r)\n",
    "    results.append(pydantic_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Text Nodes for Building Vector Store. Store metadata and description for each restaurant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.schema import TextNode\n",
    "\n",
    "nodes = []\n",
    "for res in results:\n",
    "    text_node = TextNode()\n",
    "    metadata = {}\n",
    "    for r in res:\n",
    "        # set description as text of TextNode\n",
    "        if r[0] == \"description\":\n",
    "            text_node.text = r[1]\n",
    "        else:\n",
    "            metadata[r[0]] = r[1]\n",
    "    text_node.metadata = metadata\n",
    "    nodes.append(text_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Gemini Embedding for building Vector Store for Dense retrieval. Index Restaurants as nodes into Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import ServiceContext\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings import GeminiEmbedding\n",
    "from llama_index.llms import Gemini\n",
    "from llama_index.vector_stores import QdrantVectorStore\n",
    "import qdrant_client\n",
    "\n",
    "# Create a local Qdrant vector store\n",
    "client = qdrant_client.QdrantClient(path=\"qdrant_gemini_4\")\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"collection\")\n",
    "\n",
    "# Using the embedding model to Gemini\n",
    "embed_model = GeminiEmbedding(\n",
    "    model_name=\"models/embedding-001\", api_key=os.environ[\"GOOGLE_API_KEY\"]\n",
    ")\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=Gemini(), embed_model=embed_model\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex(\n",
    "    nodes=nodes,\n",
    "    service_context=service_context,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Gemini to synthesize the results and recommend the restaurants to user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=1,\n",
    ")\n",
    "\n",
    "response = query_engine.query(\n",
    "    \"recommend an inexpensive Orlando restaurant for me and its nearby tourist places\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrument and Evaluate `query_engine` with TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from llama_index.llms import Gemini\n",
    "import numpy as np\n",
    "from trulens.core import Feedback\n",
    "from trulens.core import Select\n",
    "from trulens.core.feedback import Provider\n",
    "from trulens.feedback.v2.feedback import Groundedness\n",
    "from trulens.providers.litellm import LiteLLM\n",
    "\n",
    "aiplatform.init(project=\"trulens-testing\", location=\"us-central1\")\n",
    "\n",
    "gemini_provider = LiteLLM(model_engine=\"gemini-pro\")\n",
    "\n",
    "\n",
    "grounded = Groundedness(groundedness_provider=gemini_provider)\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(\n",
    "        grounded.groundedness_measure_with_cot_reasons, name=\"Groundedness\"\n",
    "    )\n",
    "    .on(\n",
    "        Select.RecordCalls._response_synthesizer.get_response.args.text_chunks[\n",
    "            0\n",
    "        ].collect()\n",
    "    )\n",
    "    .on_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = (\n",
    "    Feedback(gemini_provider.relevance, name=\"Answer Relevance\")\n",
    "    .on_input()\n",
    "    .on_output()\n",
    ")\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(gemini_provider.context_relevance, name=\"Context Relevance\")\n",
    "    .on_input()\n",
    "    .on(\n",
    "        Select.RecordCalls._response_synthesizer.get_response.args.text_chunks[\n",
    "            0\n",
    "        ]\n",
    "    )\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "\n",
    "gemini_text = Gemini()\n",
    "\n",
    "\n",
    "# create a custom gemini feedback provider to rate affordability. Do it with len() and math and also with an LLM.\n",
    "class Gemini_Provider(Provider):\n",
    "    def affordable_math(self, text: str) -> float:\n",
    "        \"\"\"\n",
    "        Count the number of money signs using len(). Then subtract 1 and divide by 3.\n",
    "        \"\"\"\n",
    "        affordability = 1 - ((len(text) - 1) / 3)\n",
    "        return affordability\n",
    "\n",
    "    def affordable_llm(self, text: str) -> float:\n",
    "        \"\"\"\n",
    "        Count the number of money signs using an LLM. Then subtract 1 and take the reciprocal.\n",
    "        \"\"\"\n",
    "        prompt = f\"Count the number of characters in the text: {text}. Then subtract 1 and divide the result by 3. Last subtract from 1. Final answer:\"\n",
    "        gemini_response = gemini_text.complete(prompt).text\n",
    "        # gemini is a bit verbose, so do some regex to get the answer out.\n",
    "        float_pattern = r\"[-+]?\\d*\\.\\d+|\\d+\"\n",
    "        float_numbers = re.findall(float_pattern, gemini_response)\n",
    "        rightmost_float = float(float_numbers[-1])\n",
    "        affordability = rightmost_float\n",
    "        return affordability\n",
    "\n",
    "\n",
    "gemini_provider_custom = Gemini_Provider()\n",
    "f_affordable_math = Feedback(\n",
    "    gemini_provider_custom.affordable_math, name=\"Affordability - Math\"\n",
    ").on(\n",
    "    Select.RecordCalls.retriever._index.storage_context.vector_stores.default.query.rets.nodes[\n",
    "        0\n",
    "    ].metadata.price\n",
    ")\n",
    "f_affordable_llm = Feedback(\n",
    "    gemini_provider_custom.affordable_llm, name=\"Affordability - LLM\"\n",
    ").on(\n",
    "    Select.RecordCalls.retriever._index.storage_context.vector_stores.default.query.rets.nodes[\n",
    "        0\n",
    "    ].metadata.price\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the feedback function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grounded.groundedness_measure_with_cot_reasons(\n",
    "    [\n",
    "        \"\"\"('restaurant', 'La Mar by Gaston Acurio')\n",
    "('food', 'South American')\n",
    "('location', '500 Brickell Key Dr, Miami, FL 33131')\n",
    "('category', 'Restaurant')\n",
    "('hours', 'Open ⋅ Closes 11 PM')\n",
    "('price', 'Moderate')\n",
    "('rating', 4.4)\n",
    "('review', '4.4 (2,104)')\n",
    "('description', 'Chic waterfront find offering Peruvian & fusion fare, plus bars for cocktails, ceviche & anticucho.')\n",
    "('nearby_tourist_places', 'Brickell Key Park')\"\"\"\n",
    "    ],\n",
    "    \"La Mar by Gaston Acurio is a delicious peruvian restaurant by the water\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_provider.context_relevance(\n",
    "    \"I'm hungry for Peruvian, and would love to eat by the water. Can you recommend a dinner spot?\",\n",
    "    \"\"\"('restaurant', 'La Mar by Gaston Acurio')\n",
    "('food', 'South American')\n",
    "('location', '500 Brickell Key Dr, Miami, FL 33131')\n",
    "('category', 'Restaurant')\n",
    "('hours', 'Open ⋅ Closes 11 PM')\n",
    "('price', 'Moderate')\n",
    "('rating', 4.4)\n",
    "('review', '4.4 (2,104)')\n",
    "('description', 'Chic waterfront find offering Peruvian & fusion fare, plus bars for cocktails, ceviche & anticucho.')\n",
    "('nearby_tourist_places', 'Brickell Key Park')\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_provider.relevance(\n",
    "    \"I'm hungry for Peruvian, and would love to eat by the water. Can you recommend a dinner spot?\",\n",
    "    \"La Mar by Gaston Acurio is a delicious peruvian restaurant by the water\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_provider_custom.affordable_math(\"$$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_provider_custom.affordable_llm(\"$$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up instrumentation and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.instrument.llamaindex import TruLlama\n",
    "\n",
    "tru_query_engine_recorder = TruLlama(\n",
    "    query_engine,\n",
    "    app_id=\"LlamaIndex_App1\",\n",
    "    feedbacks=[\n",
    "        f_affordable_math,\n",
    "        f_affordable_llm,\n",
    "        f_context_relevance,\n",
    "        f_groundedness,\n",
    "        f_qa_relevance,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "from trulens.dashboard import stop_dashboard\n",
    "\n",
    "stop_dashboard(tru, force=True)\n",
    "run_dashboard(tru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_query_engine_recorder as recording:\n",
    "    query_engine.query(\n",
    "        \"recommend an american restaurant in Orlando for me and its nearby tourist places\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dashboard(tru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_leaderboard(app_ids=[\"LlamaIndex_App1\"])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "gemini_multi_modal.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
