{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ut97AtzeF6N_",
   "metadata": {},
   "source": [
    "# Multimodal Evaluations with Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Par9tN3ZPQxy",
   "metadata": {},
   "source": [
    "## Installing the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MXRUvBwubd3T",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install trulens trulens-providers-google google-genai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb23bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TRULENS_OTEL_TRACING\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64168892",
   "metadata": {},
   "source": [
    "## Download data to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UIke4iCTpkL4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://docs.google.com/uc?export=download&id=1ShPnYVc1iL_TA1t7ErCFEAHT74-qvMrn\" -O ./sf.png\n",
    "!wget \"https://docs.google.com/uc?export=download&id=16oTISaB5m2uasHlezg7iPYV2FBiQYc4n\" -O ./customer_support_agnet.wav\n",
    "!wget \"https://docs.google.com/uc?export=download&id=1186BiByf2NUXmOOO8k7hGK2qGy8o5fCb\" -O ./chameleon.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf57a68",
   "metadata": {},
   "source": [
    "## Setting Gemini Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636c281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"...\"\n",
    "google_client = genai.Client(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da2964e",
   "metadata": {},
   "source": [
    "## Setup custom provider with Google\n",
    "\n",
    "In this tutorial, we leverage the multi-modal capabilities of Gemini models from Google to evaluate across different modalities, while using their structured output generation to reliably produce scores in the desired result format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8ba65d",
   "metadata": {},
   "source": [
    "### For images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vfDPiqlGJEll",
   "metadata": {},
   "source": [
    "For image input, Gemini supports the following [formats](https://ai.google.dev/gemini-api/docs/image-understanding#supported-formats): JPEG, PNG, WebP, HEIC, and HEIF. Make sure to pass the image with the correct MIME type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fyTmn1yuKSeA",
   "metadata": {},
   "source": [
    "#### Google Feedback Provider for evaluating Image Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cacfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.providers.google import Google\n",
    "from pydantic import BaseModel, Field\n",
    "from google.genai import types\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class ImageFaithfulnessScore(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a binary faithfulness score for an image response\n",
    "    with respect to the given query and/or retrieved context.\n",
    "    \"\"\"\n",
    "\n",
    "    value: float = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Binary faithfulness score. \"\n",
    "            \"1.0 → The image is faithful (accurately reflects the query/context). \"\n",
    "            \"0.0 → The image is unfaithful (introduces unsupported or contradictory content).\"\n",
    "        ),\n",
    "        ge=0.0,\n",
    "        le=1.0,\n",
    "    )\n",
    "\n",
    "    reason: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"A concise explanation describing why this score was given. \"\n",
    "            \"Should reference objects, attributes, or details in the image \"\n",
    "            \"and whether they are supported by the query/context.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "class Multimodal_Google_Provider(Google):\n",
    "    def multi_modal_faithfulness(\n",
    "        self, query: str, retrieved_context: List\n",
    "    ):\n",
    "        retrieved_context = [\n",
    "            (\n",
    "                types.Part(text=rc)\n",
    "                if isinstance(rc, str)\n",
    "                else types.Part.from_bytes(data=rc, mime_type=\"image/png\")\n",
    "            )\n",
    "            for rc in retrieved_context\n",
    "        ]\n",
    "        score = google_client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=[\n",
    "                types.Part(\n",
    "                    text=\"\"\"\n",
    "                    You are an AI system designed to judge whether a given piece of information is supported by the provided context, which may include both textual and visual content.\n",
    "\n",
    "                    ### TASK:\n",
    "\n",
    "                    Analyze the provided **information statement** and the **context** (including text and any images if available).\n",
    "                    Determine whether the information is supported by the context.\n",
    "\n",
    "                    Consider these factors:\n",
    "                    - **Support from Text**: Does the textual context explicitly or implicitly support the information?\n",
    "                    - **Support from Visuals**: If images are provided, do they support the information?\n",
    "                    - **Partial Evidence**: If any part of the context (text or image) supports the information, output **1**.\n",
    "                    - **Contradiction or Absence**: If the context does not support or contradicts the information, output **0**.\n",
    "\n",
    "                    The classification must be one of the following:\n",
    "                    [1, 0]\n",
    "\n",
    "                    IMPORTANT:\n",
    "                    - \"1\" → At least one piece of context (text or image) supports the information.\n",
    "                    - \"0\" → None of the context supports the information, or it contradicts it.\n",
    "\n",
    "                    ************\n",
    "\n",
    "                    Here is the information statement:\n",
    "                    \"\"\"\n",
    "                ),\n",
    "                types.Part(text=query),\n",
    "                types.Part(\n",
    "                    text=\"\"\"\n",
    "                    Here is the context:\n",
    "                    \"\"\"\n",
    "                ),\n",
    "                *retrieved_context,\n",
    "                types.Part(\n",
    "                    text=\"\"\"\n",
    "                    ************\n",
    "\n",
    "                    RESPONSE FORMAT:\n",
    "                    Provide a single digit (`1` or `0`) representing the judgment.\n",
    "\n",
    "                    ************\n",
    "\n",
    "                    ### EXAMPLES:\n",
    "\n",
    "                    Information: Apple pie is generally double-crusted.\n",
    "                    Context: An apple pie is a fruit pie in which the principal filling ingredient is apples.\n",
    "                    Apple pie is often served with whipped cream, ice cream ('apple pie à la mode'), custard or cheddar cheese.\n",
    "                    It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).\n",
    "                    Answer: 1\n",
    "\n",
    "                    Information: Apple pies taste bad.\n",
    "                    Context: An apple pie is a fruit pie in which the principal filling ingredient is apples.\n",
    "                    Apple pie is often served with whipped cream, ice cream ('apple pie à la mode'), custard or cheddar cheese.\n",
    "                    It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).\n",
    "                    Answer: 0\n",
    "\n",
    "                    ************\n",
    "\n",
    "                    Analyze the information statement and the context, and respond in this format.\n",
    "                    \"\"\"\n",
    "                ),\n",
    "            ],\n",
    "            config={\n",
    "                \"response_mime_type\": \"application/json\",\n",
    "                \"response_schema\": ImageFaithfulnessScore,\n",
    "            },\n",
    "        )\n",
    "        return score.parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3lT4L120HWts",
   "metadata": {},
   "source": [
    "#### Test custom feedback function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faad4c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal_gemini_provider = Multimodal_Google_Provider()\n",
    "\n",
    "image_file_name = \"sf.png\"\n",
    "with open(image_file_name, \"rb\") as f:\n",
    "    image_bytes = f.read()\n",
    "\n",
    "faithfulness = multimodal_gemini_provider.multi_modal_faithfulness(\n",
    "    query=\"Does Sam’s Grill have outdoor seating?\",\n",
    "    retrieved_context=[\n",
    "        image_bytes,\n",
    "        \"Customers can choose dine-in, curbside pickup, or delivery.\",\n",
    "    ],\n",
    ")\n",
    "faithfulness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0babc535",
   "metadata": {},
   "source": [
    "### For Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dG1ImPHTJ5-l",
   "metadata": {},
   "source": [
    "For audio input, Gemini supports specific [formats](https://ai.google.dev/gemini-api/docs/audio#supported-formats) — WAV, MP3, AIFF, AAC, OGG, and FLAC. Ensure that you provide the correct MIME type when passing audio files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M5RyPh8MLN_g",
   "metadata": {},
   "source": [
    "#### Evaluating Customer Support Chatbot Resolutions with Google Feedback Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2b702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.providers.google import Google\n",
    "from pydantic import BaseModel, Field\n",
    "from google.genai import types\n",
    "\n",
    "class ResolutionStatus(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents whether the support issue was resolved based on the agent's final utterance.\n",
    "    \"\"\"\n",
    "    value: float = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"1.0 if the final utterance clearly indicates resolution of the issue; 0.0 otherwise.\"\n",
    "        ),\n",
    "        ge=0.0,\n",
    "        le=1.0,\n",
    "    )\n",
    "\n",
    "    reason: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"A short explanation referencing the agent's final words \"\n",
    "            \"and the detected emotion (tone, confidence, reassurance).\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "class Multimodal_Google_Provider(Google):\n",
    "    def audio_resolution_detection(self, audio_bytes: bytes):\n",
    "        result = google_client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=[\n",
    "                types.Part(\n",
    "                    text=\"\"\"\n",
    "                    You are an AI system that checks customer support call endings.\n",
    "\n",
    "                    ### TASK:\n",
    "\n",
    "                    Based on both the transcript meaning AND the detected emotion in the audio, determine if the issue was **resolved**.\n",
    "\n",
    "                    Guidelines for resolution:\n",
    "                    - If the final utterance provides a clear action, resolution, or timeline in a confident or neutral/reassuring tone → value = 1.0.\n",
    "                    - If the final utterance is vague, evasive, non-committal, or delivered with frustration/hesitation → value = 0.0.\n",
    "\n",
    "                    ************\n",
    "\n",
    "                    Here is the audio to analyze:\n",
    "                    \"\"\"\n",
    "                ),\n",
    "                types.Part.from_bytes(\n",
    "                    data=audio_bytes,\n",
    "                    mime_type=\"audio/wav\",\n",
    "                ),\n",
    "                types.Part(\n",
    "                    text=\"\"\"\n",
    "                    RESPONSE FORMAT:\n",
    "                    Return JSON in the following schema:\n",
    "                    {\n",
    "                      \"resolved\": 1.0/0.0,\n",
    "                      \"reason\": \"short explanation with reference to transcript + audio tone\"\n",
    "                    }\n",
    "                    \"\"\"\n",
    "                ),\n",
    "            ],\n",
    "            config={\n",
    "                \"response_mime_type\": \"application/json\",\n",
    "                \"response_schema\": ResolutionStatus,\n",
    "            },\n",
    "        )\n",
    "        return result.parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G8D-M52OHd78",
   "metadata": {},
   "source": [
    "#### Test custom feedback function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de530cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal_gemini_provider = Multimodal_Google_Provider()\n",
    "\n",
    "# Only for audio of size <20Mb\n",
    "with open(\"customer_support_agnet.wav\", \"rb\") as f:\n",
    "    audio_bytes = f.read()\n",
    "\n",
    "multimodal_gemini_provider.audio_resolution_detection(audio_bytes=audio_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74899bed",
   "metadata": {},
   "source": [
    "### For Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3bfcf0",
   "metadata": {},
   "source": [
    "For video input, Gemini supports the following [formats](https://ai.google.dev/gemini-api/docs/video-understanding#supported-formats): [MP4, MPEG, MOV, AVI, FLV, MPG, WebM, WMV, 3GPP]. Ensure that you provide the correct MIME type when passing video files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XZocPYKVLtbq",
   "metadata": {},
   "source": [
    "#### Google Feedback Provider to evaluate Video Relevance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bee576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.providers.google import Google\n",
    "from pydantic import BaseModel, Field\n",
    "from google.genai import types\n",
    "\n",
    "\n",
    "class VideoRelevance(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents the relevance classification of a recommended video\n",
    "    with respect to a given search query.\n",
    "    \"\"\"\n",
    "\n",
    "    value: float = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"The classification of the video's relevance to the search query. \"\n",
    "            \"'1.0' → directly addresses the main intent, \"\n",
    "            \"'0.5' → overlaps but is incomplete or drifts, \"\n",
    "            \"'0.0' → does not address the query in a meaningful way.\"\n",
    "        ),\n",
    "        ge=0.0,\n",
    "        le=1.0,\n",
    "    )\n",
    "\n",
    "    reason: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"A concise explanation describing why this classification was chosen. \"\n",
    "            \"Should reference topic alignment, specificity, format/medium match, \"\n",
    "            \"and clarity of relevance.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "class Multimodal_Google_Provider(Google):\n",
    "    def video_relevance_scorer(self, query, video_bytes):\n",
    "        result = google_client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=[\n",
    "                types.Part(\n",
    "                    text=\"\"\"\n",
    "                    You are an AI system designed to judge whether a recommended video is relevant to a given search query.\n",
    "\n",
    "                    ### TASK:\n",
    "\n",
    "                    Analyze the provided search query and the recommended video.\n",
    "                    Determine whether the video’s main content is relevant to the search intent expressed in the query.\n",
    "\n",
    "                    Consider these factors:\n",
    "                    - **Topic Alignment**: Does the video content match the subject of the search query?\n",
    "                    - **Specificity**: Does it address the specific focus, details, or constraints of the query?\n",
    "                    - **Format & Medium**: If the query implies a certain type of content (tutorial, documentary, news, etc.), does the video match?\n",
    "                    - **Clarity of Relevance**: Is the connection to the query obvious or is it only loosely related?\n",
    "\n",
    "                    The classification must be one of the following:\n",
    "                    [1.0, 0.5, 0.0]\n",
    "\n",
    "                    IMPORTANT:\n",
    "                    - \"1.0\" → Directly addresses the main intent of the query.\n",
    "                    - \"0.5\" → Shares some overlap but is missing key details or drifts into unrelated topics.\n",
    "                    - \"0.0\" → Does not address the query’s intent in a meaningful way.\n",
    "                    - Avoid overusing \"partially_relevant\" — decide firmly whenever possible.\n",
    "\n",
    "                    ************\n",
    "\n",
    "                    Here is the search query:\n",
    "                    \"\"\"\n",
    "                ),\n",
    "                types.Part(text=query),\n",
    "                types.Part(\n",
    "                    text=\"\"\"\n",
    "                    Here is the recommended video information:\n",
    "                    \"\"\"\n",
    "                ),\n",
    "                types.Part(\n",
    "                    inline_data=types.Blob(data=video_bytes, mime_type=\"video/mp4\")\n",
    "                ),\n",
    "                types.Part(\n",
    "                    text=\"\"\"\n",
    "                    ************\n",
    "\n",
    "                    RESPONSE FORMAT:\n",
    "                    Provide a single word from the list above representing the relevance classification.\n",
    "\n",
    "                    ************\n",
    "\n",
    "                    EXAMPLE RESPONSE: relevant\n",
    "\n",
    "                    ************\n",
    "\n",
    "                    Analyze the query and the recommended video and respond in this format.\n",
    "                    \"\"\"\n",
    "                ),\n",
    "            ],\n",
    "            config={\n",
    "                \"response_mime_type\": \"application/json\",\n",
    "                \"response_schema\": VideoRelevance,\n",
    "            },\n",
    "        )\n",
    "        return result.parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-kYZL91SHfry",
   "metadata": {},
   "source": [
    "#### Test custom feedback function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecb8002",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_provider = Multimodal_Google_Provider()\n",
    "\n",
    "# Only for videos of size <20Mb\n",
    "video_file_name = \"chameleon.mp4\"\n",
    "with open(video_file_name, 'rb') as f:\n",
    "    video_bytes = f.read()\n",
    "\n",
    "relevance = gemini_provider.video_relevance_scorer(query=\"Chameleon hunting it's prey\",video_bytes=video_bytes)\n",
    "relevance"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
