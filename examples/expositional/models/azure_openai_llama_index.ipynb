{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure OpenAI Llama Index Quickstart\n",
    "\n",
    "In this quickstart you will create a simple Llama Index App and learn how to log it and get feedback on an LLM response using both an embedding and chat completion model from Azure OpenAI.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/examples/expositional/models/azure_openai_llama_index.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Install dependencies\n",
    "Let's install some of the dependencies for this notebook if we don't have them already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --pre trulens trulens-instrument-llamaindex trulens-providers-openai llama_index==0.9.13 llama-index-llms-azure-openai llama-index-embeddings-azure-openai langchain==0.0.346 html2text==2020.1.16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add API keys\n",
    "For this quickstart, you will need a larger set of information from Azure OpenAI compared to typical OpenAI usage. These can be retrieved from https://oai.azure.com/ . Deployment name below is also found on the oai azure page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your https://oai.azure.com dashboard to retrieve params:\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"  # azure\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = (\n",
    "    \"https://<your endpoint here>.openai.azure.com/\"  # azure\n",
    ")\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-07-01-preview\"  # may need updating\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import from TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports main tools:\n",
    "from trulens.core import Feedback\n",
    "from trulens.core import Tru\n",
    "from trulens.instrument.llamaindex import TruLlama\n",
    "\n",
    "tru = Tru()\n",
    "tru.reset_database()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Simple LLM Application\n",
    "\n",
    "This example uses LlamaIndex which internally uses an OpenAI LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.legacy import ServiceContext\n",
    "from llama_index.legacy import set_global_service_context\n",
    "from llama_index.legacy.readers import SimpleWebPageReader\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "\n",
    "# get model from Azure\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-35-turbo\",\n",
    "    deployment_name=\"<your deployment>\",\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
    ")\n",
    "\n",
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=\"<your deployment>\",\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
    ")\n",
    "\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\"http://paulgraham.com/worked.html\"]\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    ")\n",
    "\n",
    "set_global_service_context(service_context)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send your first request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is most interesting about this essay?\"\n",
    "answer = query_engine.query(query)\n",
    "\n",
    "print(answer.get_formatted_sources())\n",
    "print(\"query was:\", query)\n",
    "print(\"answer was:\", answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Feedback Function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from trulens.feedback.v2.feedback import Groundedness\n",
    "from trulens.providers.openai import AzureOpenAI\n",
    "\n",
    "# Initialize AzureOpenAI-based feedback function collection class:\n",
    "azopenai = AzureOpenAI(deployment_name=\"truera-gpt-35-turbo\")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = Feedback(\n",
    "    azopenai.relevance, name=\"Answer Relevance\"\n",
    ").on_input_output()\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(\n",
    "        azopenai.context_relevance_with_cot_reasons, name=\"Context Relevance\"\n",
    "    )\n",
    "    .on_input()\n",
    "    .on(TruLlama.select_source_nodes().node.text)\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "# groundedness of output on the context\n",
    "groundedness = Groundedness(groundedness_provider=azopenai)\n",
    "f_groundedness = (\n",
    "    Feedback(\n",
    "        groundedness.groundedness_measure_with_cot_reasons, name=\"Groundedness\"\n",
    "    )\n",
    "    .on(TruLlama.select_source_nodes().node.text.collect())\n",
    "    .on_output()\n",
    "    .aggregate(groundedness.grounded_statements_aggregator)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom functions can also use the Azure provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "from trulens.feedback import prompts\n",
    "\n",
    "\n",
    "class Custom_AzureOpenAI(AzureOpenAI):\n",
    "    def style_check_professional(self, response: str) -> float:\n",
    "        \"\"\"\n",
    "        Custom feedback function to grade the professional style of the response, extending AzureOpenAI provider.\n",
    "\n",
    "        Args:\n",
    "            response (str): text to be graded for professional style.\n",
    "\n",
    "        Returns:\n",
    "            float: A value between 0 and 1. 0 being \"not professional\" and 1 being \"professional\".\n",
    "        \"\"\"\n",
    "        professional_prompt = str.format(\n",
    "            \"Please rate the professionalism of the following text on a scale from 0 to 10, where 0 is not at all professional and 10 is extremely professional: \\n\\n{}\",\n",
    "            response,\n",
    "        )\n",
    "        return self.generate_score(system_prompt=professional_prompt)\n",
    "\n",
    "    def context_relevance_with_cot_reasons_extreme(\n",
    "        self, question: str, statement: str\n",
    "    ) -> Tuple[float, Dict]:\n",
    "        \"\"\"\n",
    "        Tweaked version of question statement relevance, extending AzureOpenAI provider.\n",
    "        A function that completes a template to check the relevance of the statement to the question.\n",
    "        Scoring guidelines for scores 5-8 are removed to push the LLM to more extreme scores.\n",
    "        Also uses chain of thought methodology and emits the reasons.\n",
    "\n",
    "        Args:\n",
    "            question (str): A question being asked.\n",
    "            statement (str): A statement to the question.\n",
    "\n",
    "        Returns:\n",
    "            float: A value between 0 and 1. 0 being \"not relevant\" and 1 being \"relevant\".\n",
    "        \"\"\"\n",
    "\n",
    "        system_prompt = str.format(\n",
    "            prompts.context_relevance, question=question, statement=statement\n",
    "        )\n",
    "\n",
    "        # remove scoring guidelines around middle scores\n",
    "        system_prompt = system_prompt.replace(\n",
    "            \"- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n\",\n",
    "            \"\",\n",
    "        )\n",
    "\n",
    "        system_prompt = system_prompt.replace(\n",
    "            \"RELEVANCE:\", prompts.COT_REASONS_TEMPLATE\n",
    "        )\n",
    "\n",
    "        return self.generate_score_and_reasons(system_prompt)\n",
    "\n",
    "\n",
    "custom_azopenai = Custom_AzureOpenAI(deployment_name=\"truera-gpt-35-turbo\")\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance_extreme = (\n",
    "    Feedback(\n",
    "        custom_azopenai.context_relevance_with_cot_reasons_extreme,\n",
    "        name=\"Context Relevance - Extreme\",\n",
    "    )\n",
    "    .on_input()\n",
    "    .on(TruLlama.select_source_nodes().node.text)\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "f_style_check = Feedback(\n",
    "    custom_azopenai.style_check_professional, name=\"Professional Style\"\n",
    ").on_output()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrument chain for logging with TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_query_engine_recorder = TruLlama(\n",
    "    query_engine,\n",
    "    app_id=\"LlamaIndex_App1_AzureOpenAI\",\n",
    "    feedbacks=[\n",
    "        f_groundedness,\n",
    "        f_qa_relevance,\n",
    "        f_context_relevance,\n",
    "        f_context_relevance_extreme,\n",
    "        f_style_check,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is most interesting about this essay?\"\n",
    "with tru_query_engine_recorder as recording:\n",
    "    answer = query_engine.query(query)\n",
    "    print(answer.get_formatted_sources())\n",
    "    print(\"query was:\", query)\n",
    "    print(\"answer was:\", answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore in a Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "run_dashboard(tru)  # open a local streamlit app to explore\n",
    "\n",
    "# stop_dashboard(tru) # stop if needed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or view results directly in your notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records, feedback = tru.get_records_and_feedback(\n",
    "    app_ids=[\"LlamaIndex_App1_AzureOpenAI\"]\n",
    ")  # pass an empty list of app_ids to get all\n",
    "\n",
    "records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_leaderboard(app_ids=[\"LlamaIndex_App1_AzureOpenAI\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "7d153714b979d5e6d08dd8ec90712dd93bff2c9b6c1f0c118169738af3430cd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
