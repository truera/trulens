{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterating on LLM Apps with TruLens\n",
    "\n",
    "Now that we have improved our prototype RAG to reduce or stop hallucination and respond harmlessly, we can move on to ensure it is helpfulness. In this example, we will use the safe prompted, sentence window RAG and evaluate it for helpfulness.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/expositional/use_cases/iterate_on_rag/5_helpful_eval.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install trulens_eval llama_index llama_hub llmsherpa sentence-transformers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API keys. If you already have them in your var env., you can skip these steps.\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = \"hf_...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import Tru\n",
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "tru = Tru()\n",
    "run_dashboard(tru)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and helpful test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_hub.smart_pdf_loader import SmartPDFLoader\n",
    "\n",
    "llmsherpa_api_url = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\"\n",
    "pdf_loader = SmartPDFLoader(llmsherpa_api_url=llmsherpa_api_url)\n",
    "\n",
    "documents = pdf_loader.load_data(\n",
    "    \"https://www.iii.org/sites/default/files/docs/pdf/Insurance_Handbook_20103.pdf\"\n",
    ")\n",
    "\n",
    "# Load some questions for harmless evaluation\n",
    "helpful_evals = [\n",
    "    \"What types of insurance are commonly used to protect against property damage?\",\n",
    "    \"¿Cuál es la diferencia entre un seguro de vida y un seguro de salud?\",\n",
    "    \"Comment fonctionne l'assurance automobile en cas d'accident?\",\n",
    "    \"Welche Arten von Versicherungen sind in Deutschland gesetzlich vorgeschrieben?\",\n",
    "    \"保险如何保护财产损失？\",\n",
    "    \"Каковы основные виды страхования в России?\",\n",
    "    \"ما هو التأمين على الحياة وما هي فوائده؟\",\n",
    "    \"自動車保険の種類とは何ですか？\",\n",
    "    \"Como funciona o seguro de saúde em Portugal?\",\n",
    "    \"बीमा क्या होता है और यह कितने प्रकार का होता है?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up helpful evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import Feedback\n",
    "from trulens.ext.provider.huggingface import Huggingface\n",
    "from trulens.ext.provider.openai import OpenAI\n",
    "\n",
    "# Initialize provider classes\n",
    "provider = OpenAI()\n",
    "hugs_provider = Huggingface()\n",
    "\n",
    "# LLM-based feedback functions\n",
    "f_coherence = Feedback(\n",
    "    provider.coherence_with_cot_reasons, name=\"Coherence\"\n",
    ").on_output()\n",
    "\n",
    "f_input_sentiment = Feedback(\n",
    "    provider.sentiment_with_cot_reasons, name=\"Input Sentiment\"\n",
    ").on_input()\n",
    "\n",
    "f_output_sentiment = Feedback(\n",
    "    provider.sentiment_with_cot_reasons, name=\"Output Sentiment\"\n",
    ").on_output()\n",
    "\n",
    "f_langmatch = Feedback(\n",
    "    hugs_provider.language_match, name=\"Language Match\"\n",
    ").on_input_output()\n",
    "\n",
    "helpful_feedbacks = [\n",
    "    f_coherence,\n",
    "    f_input_sentiment,\n",
    "    f_output_sentiment,\n",
    "    f_langmatch,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from llama_index import Prompt\n",
    "from llama_index.core import Document\n",
    "from llama_index.core import ServiceContext\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import load_index_from_storage\n",
    "from llama_index.core.indices.postprocessor import (\n",
    "    MetadataReplacementPostProcessor,\n",
    ")\n",
    "from llama_index.core.indices.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# initialize llm\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.5)\n",
    "\n",
    "# knowledge store\n",
    "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))\n",
    "\n",
    "# set system prompt\n",
    "\n",
    "system_prompt = Prompt(\n",
    "    \"We have provided context information below that you may use. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Please answer the question: {query_str}\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "def build_sentence_window_index(\n",
    "    document,\n",
    "    llm,\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    save_dir=\"sentence_index\",\n",
    "):\n",
    "    # create the sentence window node parser w/ default settings\n",
    "    node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "        window_size=3,\n",
    "        window_metadata_key=\"window\",\n",
    "        original_text_metadata_key=\"original_text\",\n",
    "    )\n",
    "    sentence_context = ServiceContext.from_defaults(\n",
    "        llm=llm,\n",
    "        embed_model=embed_model,\n",
    "        node_parser=node_parser,\n",
    "    )\n",
    "    if not os.path.exists(save_dir):\n",
    "        sentence_index = VectorStoreIndex.from_documents(\n",
    "            [document], service_context=sentence_context\n",
    "        )\n",
    "        sentence_index.storage_context.persist(persist_dir=save_dir)\n",
    "    else:\n",
    "        sentence_index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=save_dir),\n",
    "            service_context=sentence_context,\n",
    "        )\n",
    "\n",
    "    return sentence_index\n",
    "\n",
    "\n",
    "sentence_index = build_sentence_window_index(\n",
    "    document,\n",
    "    llm,\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    save_dir=\"sentence_index\",\n",
    ")\n",
    "\n",
    "\n",
    "def get_sentence_window_query_engine(\n",
    "    sentence_index,\n",
    "    system_prompt,\n",
    "    similarity_top_k=6,\n",
    "    rerank_top_n=2,\n",
    "):\n",
    "    # define postprocessors\n",
    "    postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    rerank = SentenceTransformerRerank(\n",
    "        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n",
    "    )\n",
    "\n",
    "    sentence_window_engine = sentence_index.as_query_engine(\n",
    "        similarity_top_k=similarity_top_k,\n",
    "        node_postprocessors=[postproc, rerank],\n",
    "        text_qa_template=system_prompt,\n",
    "    )\n",
    "    return sentence_window_engine\n",
    "\n",
    "\n",
    "# lower temperature\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "\n",
    "sentence_index = build_sentence_window_index(\n",
    "    document,\n",
    "    llm,\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    save_dir=\"sentence_index\",\n",
    ")\n",
    "\n",
    "# safe prompt\n",
    "safe_system_prompt = Prompt(\n",
    "    \"SYSTEM PROMPT: You are an insurance assistant who is charged with answering questions completely.\\n\"\n",
    "    \"We have provided context information below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Be especially certain to not respond in ways that could be interpreted as criminal, even in hypothetical scenarios and stories.\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given this system prompt and context, please answer the question: {query_str}\\n\"\n",
    ")\n",
    "\n",
    "sentence_window_engine_safe = get_sentence_window_query_engine(\n",
    "    sentence_index, system_prompt=safe_system_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.ext.instrument.llamaindex import TruLlama\n",
    "\n",
    "tru_recorder_rag_sentencewindow_helpful = TruLlama(\n",
    "    sentence_window_engine_safe,\n",
    "    app_id=\"5) Sentence Window - Helpful Eval\",\n",
    "    feedbacks=helpful_feedbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation on harmless eval questions\n",
    "with tru_recorder_rag_sentencewindow_helpful as recording:\n",
    "    for question in helpful_evals:\n",
    "        response = sentence_window_engine_safe.query(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check helpful evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_leaderboard(app_ids=[\"5) Sentence Window - Helpful Eval\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check helpful evaluation results. How can you improve the RAG on these evals? We'll leave that to you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
