{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed3e8c10",
   "metadata": {},
   "source": [
    "# Evaluating Summarization with TruLens\n",
    "\n",
    "In this notebook, we will evaluate a summarization application based on [DialogSum dataset](https://github.com/cylnlp/dialogsum) using a broad set of available metrics from TruLens. These metrics break down into three categories.\n",
    "\n",
    "1. Ground truth agreement: For these set of metrics, we will measure how similar the generated summary is to some human-created ground truth. We will use for different measures: BERT score, BLEU, ROUGE and a measure where an LLM is prompted to produce a similarity score.\n",
    "2. Groundedness: Estimate if the generated summary can be traced back to parts of the original transcript both with LLM and NLI methods.\n",
    "3. Comprehensivenss: Estimate if the generated summary contains all of the key points from the source text.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/expositional/use_cases/summarization_eval.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb7429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = \"hf_...\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "602ed89a",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "Let's first install the packages that this notebook depends on. Uncomment these linse to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85d254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install trulens_eval bert_score evaluate absl-py rouge-score pandas tenacity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f443aac",
   "metadata": {},
   "source": [
    "### Download and load data\n",
    "Now we will download a portion of the DialogSum dataset from github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6769a0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8133c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O dialogsum.dev.jsonl https://raw.githubusercontent.com/cylnlp/dialogsum/main/DialogSum_Data/dialogsum.dev.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0829ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_dev = \"dialogsum.dev.jsonl\"\n",
    "dev_df = pd.read_json(path_or_buf=file_path_dev, lines=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7e7714b",
   "metadata": {},
   "source": [
    "Let's preview the data to make sure that the data was properly loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad85d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "716d57bc",
   "metadata": {},
   "source": [
    "## Create a simple summarization app and instrument it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62ffb3d7",
   "metadata": {},
   "source": [
    "We will create a simple summarization app based on the OpenAI ChatGPT model and instrument it for use with TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2472f205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import TruCustomApp\n",
    "from trulens.core.app.custom import instrument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc60cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "\n",
    "class DialogSummaryApp:\n",
    "    @instrument\n",
    "    def summarize(self, dialog):\n",
    "        client = openai.OpenAI()\n",
    "        summary = (\n",
    "            client.chat.completions.create(\n",
    "                model=\"gpt-4-turbo\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"\"\"Summarize the given dialog into 1-2 sentences based on the following criteria: \n",
    "                     1. Convey only the most salient information; \n",
    "                     2. Be brief; \n",
    "                     3. Preserve important named entities within the conversation; \n",
    "                     4. Be written from an observer perspective; \n",
    "                     5. Be written in formal language. \"\"\",\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": dialog},\n",
    "                ],\n",
    "            )\n",
    "            .choices[0]\n",
    "            .message.content\n",
    "        )\n",
    "        return summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a81c191",
   "metadata": {},
   "source": [
    "## Initialize Database and view dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba28354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import Tru\n",
    "\n",
    "tru = Tru()\n",
    "tru.reset_database()\n",
    "# If you have a database you can connect to, use a URL. For example:\n",
    "# tru = Tru(database_url=\"postgresql://hostname/database?user=username&password=password\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990c0dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.start_dashboard(force=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad02e597",
   "metadata": {},
   "source": [
    "## Write feedback functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56247d16",
   "metadata": {},
   "source": [
    "We will now create the feedback functions that will evaluate the app. Remember that the criteria we were evaluating against were:\n",
    "1. Ground truth agreement: For these set of metrics, we will measure how similar the generated summary is to some human-created ground truth. We will use for different measures: BERT score, BLEU, ROUGE and a measure where an LLM is prompted to produce a similarity score.\n",
    "2. Groundedness: For this measure, we will estimate if the generated summary can be traced back to parts of the original transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3ee39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import Feedback\n",
    "from trulens.feedback import GroundTruthAgreement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4db1975",
   "metadata": {},
   "source": [
    "We select the golden dataset based on dataset we downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2168ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_set = (\n",
    "    dev_df[[\"dialogue\", \"summary\"]]\n",
    "    .rename(columns={\"dialogue\": \"query\", \"summary\": \"response\"})\n",
    "    .to_dict(\"records\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bc13e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import Select\n",
    "from trulens.providers.huggingface import Huggingface\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "provider = OpenAI(model_engine=\"gpt-4o\")\n",
    "hug_provider = Huggingface()\n",
    "\n",
    "ground_truth_collection = GroundTruthAgreement(golden_set, provider=provider)\n",
    "f_groundtruth = Feedback(\n",
    "    ground_truth_collection.agreement_measure, name=\"Similarity (LLM)\"\n",
    ").on_input_output()\n",
    "f_bert_score = Feedback(ground_truth_collection.bert_score).on_input_output()\n",
    "f_bleu = Feedback(ground_truth_collection.bleu).on_input_output()\n",
    "f_rouge = Feedback(ground_truth_collection.rouge).on_input_output()\n",
    "# Groundedness between each context chunk and the response.\n",
    "\n",
    "\n",
    "f_groundedness_llm = (\n",
    "    Feedback(\n",
    "        provider.groundedness_measure_with_cot_reasons,\n",
    "        name=\"Groundedness - LLM Judge\",\n",
    "    )\n",
    "    .on(Select.RecordInput)\n",
    "    .on(Select.RecordOutput)\n",
    ")\n",
    "f_groundedness_nli = (\n",
    "    Feedback(\n",
    "        hug_provider.groundedness_measure_with_nli,\n",
    "        name=\"Groundedness - NLI Judge\",\n",
    "    )\n",
    "    .on(Select.RecordInput)\n",
    "    .on(Select.RecordOutput)\n",
    ")\n",
    "f_comprehensiveness = (\n",
    "    Feedback(\n",
    "        provider.comprehensiveness_with_cot_reasons, name=\"Comprehensiveness\"\n",
    "    )\n",
    "    .on(Select.RecordInput)\n",
    "    .on(Select.RecordOutput)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d2f78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "provider.comprehensiveness_with_cot_reasons(\n",
    "    \"the white house is white. obama is the president\",\n",
    "    \"the white house is white. obama is the president\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c13f57a9",
   "metadata": {},
   "source": [
    "## Create the app and wrap it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed0c5432",
   "metadata": {},
   "source": [
    "Now we are ready to wrap our summarization app with TruLens as a `TruCustomApp`. Now each time it will be called, TruLens will log inputs, outputs and any instrumented intermediate steps and evaluate them ith the feedback functions we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf42a5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = DialogSummaryApp()\n",
    "print(app.summarize(dev_df.dialogue[498]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a31835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_recorder = TruCustomApp(\n",
    "    app,\n",
    "    app_id=\"Summarize_v1\",\n",
    "    feedbacks=[\n",
    "        f_groundtruth,\n",
    "        f_groundedness_llm,\n",
    "        f_groundedness_nli,\n",
    "        f_comprehensiveness,\n",
    "        f_bert_score,\n",
    "        f_bleu,\n",
    "        f_rouge,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2a63099",
   "metadata": {},
   "source": [
    "We can test a single run of the App as so. This should show up on the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8d4eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_recorder:\n",
    "    app.summarize(dialog=dev_df.dialogue[498])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4dd0f0c5",
   "metadata": {},
   "source": [
    "We'll make a lot of queries in a short amount of time, so we need tenacity to make sure that most of our requests eventually go through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4274c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import retry\n",
    "from tenacity import stop_after_attempt\n",
    "from tenacity import wait_random_exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b0c8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def run_with_backoff(doc):\n",
    "    return tru_recorder.with_record(app.summarize, dialog=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175df188",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in golden_set:\n",
    "    llm_response = run_with_backoff(pair[\"query\"])\n",
    "    print(llm_response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ae8b4b3",
   "metadata": {},
   "source": [
    "And that's it! This might take a few minutes to run, at the end of it, you can explore the dashboard to see how well your app does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5d49dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "run_dashboard(tru)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
