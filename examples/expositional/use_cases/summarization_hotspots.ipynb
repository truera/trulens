{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed3e8c10",
   "metadata": {},
   "source": [
    "# Debugging evaluation results with TruLens Hotspots\n",
    "\n",
    "This notebook is a companion notebook to a [general notebook showcasing TruLens using the Summeval benchmark](summarization_eval.ipynb). It can be run as a follow-up, but we will just copy all the relevant code from that notebook.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/examples/expositional/use_cases/summarization_eval.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac28d2aa-b80c-4951-9f9a-6b6d7df02821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = \"hf_...\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "file_path_dev = \"dialogsum.dev.jsonl\"\n",
    "dev_df = pd.read_json(path_or_buf=file_path_dev, lines=True)\n",
    "\n",
    "from trulens.apps.app import TruApp\n",
    "from trulens.apps.app import instrument\n",
    "\n",
    "import openai\n",
    "\n",
    "class DialogSummaryApp:\n",
    "    def __init__(self):\n",
    "        self.client = openai.OpenAI()\n",
    "    \n",
    "    @instrument\n",
    "    def summarize(self, dialog):       \n",
    "        summary = (\n",
    "            self.client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"\"\"Summarize the given dialog into 1-2 sentences based on the following criteria: \n",
    "                     1. Convey only the most salient information; \n",
    "                     2. Be brief; \n",
    "                     3. Preserve important named entities within the conversation; \n",
    "                     4. Be written from an observer perspective; \n",
    "                     5. Be written in formal language. \"\"\",\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": dialog},\n",
    "                ],\n",
    "            )\n",
    "            .choices[0]\n",
    "            .message.content\n",
    "        )\n",
    "        return summary\n",
    "\n",
    "from trulens.core import TruSession\n",
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "session = TruSession()\n",
    "session.reset_database()\n",
    "# If you have a database you can connect to, use a URL. For example:\n",
    "# session = TruSession(database_url=\"postgresql://hostname/database?user=username&password=password\")\n",
    "\n",
    "run_dashboard(session, force=True)\n",
    "\n",
    "from trulens.core import Feedback\n",
    "from trulens.feedback import GroundTruthAgreement\n",
    "\n",
    "golden_set = (\n",
    "    dev_df[[\"dialogue\", \"summary\"]]\n",
    "    .rename(columns={\"dialogue\": \"query\", \"summary\": \"response\"})\n",
    "    .to_dict(\"records\")\n",
    ")\n",
    "\n",
    "from trulens.core import Select\n",
    "from trulens.providers.huggingface import Huggingface\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "provider = OpenAI(model_engine=\"gpt-4o\")\n",
    "hug_provider = Huggingface()\n",
    "\n",
    "ground_truth_collection = GroundTruthAgreement(golden_set, provider=provider)\n",
    "f_groundtruth = Feedback(\n",
    "    ground_truth_collection.agreement_measure, name=\"Similarity (LLM)\"\n",
    ").on_input_output()\n",
    "\n",
    "# let's focus on Comprehensiveness\n",
    "f_rouge = Feedback(ground_truth_collection.rouge).on_input_output()\n",
    "f_comprehensiveness = (\n",
    "    Feedback(\n",
    "        provider.comprehensiveness_with_cot_reasons, name=\"Comprehensiveness\"\n",
    "    )\n",
    "    .on(Select.RecordInput)\n",
    "    .on(Select.RecordOutput)\n",
    ")\n",
    "\n",
    "app = DialogSummaryApp()\n",
    "\n",
    "tru_recorder = TruApp(\n",
    "    app,\n",
    "    app_name=\"Summarize\",\n",
    "    app_version=\"v1\",\n",
    "    feedbacks=[\n",
    "        f_groundtruth,\n",
    "        f_comprehensiveness,\n",
    "        f_rouge,\n",
    "    ],\n",
    ")\n",
    "\n",
    "from tenacity import retry\n",
    "from tenacity import stop_after_attempt\n",
    "from tenacity import wait_random_exponential\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def run_with_backoff(doc):\n",
    "    return tru_recorder.with_record(app.summarize, dialog=doc)\n",
    "\n",
    "for i, pair in enumerate(golden_set):\n",
    "    llm_response = run_with_backoff(pair[\"query\"])\n",
    "    if i % 25 == 0:\n",
    "        print(f\"{i+1} {llm_response[0][:30]}...\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "602ed89a",
   "metadata": {},
   "source": [
    "## Time for hotspots!\n",
    "\n",
    "You need to wait a little bit to check whether all evaluations have been done. **Be patient, it might take a couple of minutes**. You can check that in the TruLens dashboard (see the link at the beginning of the output for the previous cell).\n",
    "\n",
    "When all or most evaluations are done (some might be missing, no problem), you can run Hotspots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87269130-972e-4283-9aa5-5e5ea1427bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install trulens-hotspots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28d8ef6-fb19-4121-92eb-f834fd7e2f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.hotspots.tru_hotspots import get_hotspots\n",
    "\n",
    "hotspots_df = get_hotspots(session, feedback=\"Comprehensiveness\")\n",
    "\n",
    "hotspots_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57322d05-520d-4ab8-a7bc-ba2f2dce753b",
   "metadata": {},
   "source": [
    "(If you see a warning about >200 samples without a score, please wait more and re-run the above cell.)\n",
    "\n",
    "The exact table will depend on your particular run, for this particular one (see screenshot below), it turned out that, perhaps unsurprisingly, long inputs are a challange. The comprehensive score is 17 pp. worse than for short inputs. Short and long outputs are problematic as well. Another interesting observation is that, for instance, inputs with \"can\" and \"There\" are unusually hard when you try to generate a comprehensive summary. If we somehow fixed the problem, we would get, respectively, +4 pp and +1 pp in the _overall_ score.\n",
    "\n",
    "![Results for a Hotspots run](hotspots_output.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
